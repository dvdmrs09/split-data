{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_attrs(obj):\n    if not hasattr(obj, '__dict__'):\n        return []  # slots only\n    proxy_type = types.MappingProxyType if six.PY3 else types.DictProxyType\n    if not isinstance(obj.__dict__, (dict, proxy_type)):\n        print(type(obj.__dict__), obj)\n        raise TypeError(\"%s.__dict__ is not a dictionary\" % obj.__name__)\n    return obj.__dict__.keys()", "response": "Helper for dir2 implementation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dir2(obj):\n    attrs = set()\n\n    if not hasattr(obj, '__bases__'):\n        # obj is an instance\n        if not hasattr(obj, '__class__'):\n            # slots\n            return sorted(get_attrs(obj))\n        klass = obj.__class__\n        attrs.update(get_attrs(klass))\n    else:\n        # obj is a class\n        klass = obj\n\n    for cls in klass.__bases__:\n        attrs.update(get_attrs(cls))\n        attrs.update(dir2(cls))\n    attrs.update(get_attrs(obj))\n    return list(attrs)", "response": "Default dir implementation.\n\n    Inspired by gist: katyukha/dirmixin.py\n    https://gist.github.com/katyukha/c6e5e2b829e247c9b009"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove(self, item):\n        for entry in self:\n            if item.lower() == entry.lower():\n                list.remove(self, entry)\n                return\n        raise ValueError(': list.remove(x): x not in list')", "response": "Remove the first occurence of an item the caseless way."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds an item to the list and checks it s a string.", "response": "def append(self, item):\n        \"\"\"Adds an item to the list and checks it's a string.\"\"\"\n        if not isinstance(item, str):\n            raise TypeError(\n                'Members of this object must be strings. '\n                'You supplied \\\"%s\\\"' % type(item))\n        list.append(self, item)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extend(self, item):\n        if not isinstance(item, list):\n            raise TypeError(\n                'You can only extend lists with lists. '\n                'You supplied \\\"%s\\\"' % type(item))\n        for entry in item:\n            if not isinstance(entry, str):\n                raise TypeError(\n                    'Members of this object must be strings. '\n                    'You supplied \\\"%s\\\"' % type(entry))\n            list.append(self, entry)", "response": "Extend the list with another list. Each member of the list must be a string. Each member of the list must be a list. Each member of the list must be a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncounts the number of resources that reference the item in a caseless manner.", "response": "def count(self, item):\n        \"\"\"Counts references to 'item' in a caseless manner.\n        If item is not a string it will always return 0.\"\"\"\n        if not isinstance(item, str):\n            return 0\n        count = 0\n        for entry in self:\n            if item.lower() == entry.lower():\n                count += 1\n        return count"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef index(self, item, minindex=0, maxindex=None):\n        if maxindex is None:\n            maxindex = len(self)\n        minindex = max(0, minindex) - 1\n        maxindex = min(len(self), maxindex)\n        if not isinstance(item, str):\n            raise TypeError(\n                'Members of this object must be strings. '\n                'You supplied \\\"%s\\\"' % type(item))\n        index = minindex\n        while index < maxindex:\n            index += 1\n            if item.lower() == self[index].lower():\n                return index\n        raise ValueError(': list.index(x): x not in list')", "response": "Provide an index of first occurence of item in the list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninsert a new entry in the list at the specified position", "response": "def insert(self, i, x):\n        \"\"\"s.insert(i, x) same as s[i:i] = [x]\n        Raises TypeError if x isn't a string.\"\"\"\n        if not isinstance(x, str):\n            raise TypeError(\n                'Members of this object must be strings. '\n                'You supplied \\\"%s\\\"' % type(x))\n        list.insert(self, i, x)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _append(self, evt):\n        evts = self._evts\n        if len(evts) == self._max_buf:\n            evts.pop(0)\n        evts.append(evt)", "response": "Internal usage only. Appends a new entry to the internal buffer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __to_dev_failed(exc_type=None, exc_value=None, traceback=None):\n    try:\n        Except.throw_python_exception(exc_type, exc_value, traceback)\n    except Exception as e:\n        return e", "response": "Generate a TANGO DevFailed object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef candidates(word): \n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])", "response": "Generate possible spelling corrections for word. CDKConstants"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the string with the normalised words in the string.", "response": "def __normalize(self,text):\n        \"\"\"\n        Substitute words in the string, according to the specified Normal,\n        e.g. \"I'm\" -> \"I am\"\n\n        :type str: str\n        :param str: The string to be mapped\n        :rtype: str\n        \"\"\"\n        return self._normalizer_regex.sub(lambda mo:\n                self._normalizer[mo.string[mo.start():mo.end()]],\n                    text.lower())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _substitute(self, str):\n        if not self.attr.get(\"substitute\",True):return str\n        return self._regex.sub(lambda mo:\n                self._reflections[mo.string[mo.start():mo.end()]],\n                    str.lower())", "response": "Substitute words in the string according to the specified reflections."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __substituteFromClientStatement(self,match,prevResponse,extraSymbol=\"\",sessionID = \"general\"):\n        prev = 0\n        startPadding = 1+len(extraSymbol)\n        finalResponse = \"\"\n        for m in re.finditer(r'%'+extraSymbol+'[0-9]+', prevResponse):\n            start = m.start(0)\n            end = m.end(0)     \n            num = int(prevResponse[start+startPadding:end])\n            finalResponse += prevResponse[prev:start]\n            try:finalResponse += self._quote(self._substitute(match.group(num)),sessionID)\n            except IndexError as e:pass\n            prev = end\n        namedGroup = match.groupdict()\n        if namedGroup:\n            prevResponse = finalResponse + prevResponse[prev:]\n            finalResponse = \"\"\n            prev = 0\n            for m in re.finditer(r'%'+extraSymbol+'([a-zA-Z_][a-zA-Z_0-9]*)([^a-zA-Z_0-9]|$)', prevResponse):\n                start = m.start(1)\n                end = m.end(1)\n                finalResponse += prevResponse[prev:start]\n                try:\n                    value = namedGroup[prevResponse[start+startPadding:end]]\n                    if value:finalResponse += self._quote(self._substitute(value),sessionID)\n                except KeyError as e:pass\n                prev = end\n        return finalResponse + prevResponse[prev:]", "response": "Substitute from Client statement into respose\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a response to the user input.", "response": "def respond(self, text, sessionID = \"general\"):\n        \"\"\"\n        Generate a response to the user input.\n\n        :type text: str\n        :param text: The string to be mapped\n        :rtype: str\n        \"\"\"\n        text =  self.__normalize(text)\n        previousText = self.__normalize(self.conversation[sessionID][-2])\n        text_correction = self.__correction(text)\n        current_topic = self.topic[sessionID]\n        current_topic_order = current_topic.split(\".\")\n        while current_topic_order:\n          try:return self.__response_on_topic(text, previousText, text_correction, current_topic, sessionID)\n          except ValueError as e:pass\n          current_topic_order.pop()\n          current_topic = \".\".join(current_topic_order)\n        try:return self.__response_on_topic(text, previousText, text_correction, current_topic, sessionID)\n        except ValueError as e:return \"Sorry I couldn't find anything relevant\""}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwraps :obj:`fs.opener.registry.Registry.open_fs`, with default to local current working directory and expanding ~ in path. :param str fs_url: A filesystem URL :param parse_result: A parsed filesystem URL. :type parse_result: :class:`ParseResult` :param bool writeable: True if the filesystem must be writeable. :param bool create: True if the filesystem should be created if it does not exist. :param str cwd: The current working directory (generally only relevant for OS filesystems). :param str default_protocol: The protocol to use if one is not supplied in the FS URL (defaults to ``\"osfs\"``). :returns: :class:`fs.base.FS` object", "response": "def open_fs(fs_url=None, *args, **kwargs):\n    \"\"\"\n    Wraps :obj:`fs.opener.registry.Registry.open_fs`, with default to local current working directory and expanding ~ in\n    path.\n\n    :param str fs_url: A filesystem URL\n    :param parse_result: A parsed filesystem URL.\n    :type parse_result: :class:`ParseResult`\n    :param bool writeable: True if the filesystem must be writeable.\n    :param bool create: True if the filesystem should be created if it does not exist.\n    :param str cwd: The current working directory (generally only relevant for OS filesystems).\n    :param str default_protocol: The protocol to use if one is not supplied in the FS URL (defaults to ``\"osfs\"``).\n    :returns: :class:`fs.base.FS` object\n    \"\"\"\n    from fs.opener import open_fs as _open_fs\n    from os.path import expanduser\n    from os import getcwd\n\n    if fs_url is None:\n        fs_url = getcwd()\n\n    return _open_fs(expanduser(str(fs_url)), *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread an environment file into a collection of tuples.", "response": "def load_env_from_file(filename):\n    \"\"\"\n    Read an env file into a collection of (name, value) tuples.\n    \"\"\"\n    if not os.path.exists(filename):\n        raise FileNotFoundError(\"Environment file {} does not exist.\".format(filename))\n\n    with open(filename) as f:\n        for lineno, line in enumerate(f):\n            line = line.strip()\n            if not line or line.startswith(\"#\"):\n                continue\n            if \"=\" not in line:\n                raise SyntaxError(\"Invalid environment file syntax in {} at line {}.\".format(filename, lineno + 1))\n\n            name, value = parse_var(line)\n\n            yield name, value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_argument_parser(parser=None):\n    if parser is None:\n        parser = argparse.ArgumentParser()\n\n    # Store globally to be able to warn the user about the fact he's probably wrong not to pass a parser to\n    # parse_args(), later.\n    global _parser\n    _parser = parser\n\n    _parser.add_argument(\"--default-env-file\", \"-E\", action=\"append\")\n    _parser.add_argument(\"--default-env\", action=\"append\")\n    _parser.add_argument(\"--env-file\", action=\"append\")\n    _parser.add_argument(\"--env\", \"-e\", action=\"append\")\n\n    return _parser", "response": "Returns an argument parser with arguments to override the system environment."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the arguments and return a context manager that can be used to apply environment related options.", "response": "def parse_args(mixed=None):\n    \"\"\"\n    Context manager to extract and apply environment related options from the provided argparser result.\n\n    A dictionnary with unknown options will be yielded, so the remaining options can be used by the caller.\n\n    :api: bonobo.patch_environ\n\n    :param mixed: ArgumentParser instance, Namespace, or dict.\n    :return:\n    \"\"\"\n\n    if mixed is None:\n        global _parser\n        if _parser is not None:\n            warnings.warn(\n                \"You are calling bonobo.parse_args() without a parser argument, but it looks like you created a parser before. You probably want to pass your parser to this call, or if creating a new parser here is really what you want to do, please create a new one explicitely to silence this warning.\"\n            )\n        # use the api from bonobo namespace, in case a command patched it.\n        import bonobo\n\n        mixed = bonobo.get_argument_parser()\n\n    if isinstance(mixed, argparse.ArgumentParser):\n        options = mixed.parse_args()\n    else:\n        options = mixed\n\n    if not isinstance(options, dict):\n        options = options.__dict__\n\n    # make a copy so we don't polute our parent variables.\n    options = dict(options)\n\n    # storage for values before patch.\n    _backup = {}\n\n    # Priority order: --env > --env-file > system > --default-env > --default-env-file\n    #\n    # * The code below is reading default-env before default-env-file as if the first sets something, default-env-file\n    #   won't override it.\n    # * Then, env-file is read from before env, as the behaviour will be the oposite (env will override a var even if\n    #   env-file sets something.)\n    try:\n        # Set default environment\n        for name, value in map(parse_var, options.pop(\"default_env\", []) or []):\n            if not name in os.environ:\n                if not name in _backup:\n                    _backup[name] = os.environ.get(name, None)\n                os.environ[name] = value\n\n        # Read and set default environment from file(s)\n        for filename in options.pop(\"default_env_file\", []) or []:\n            for name, value in load_env_from_file(filename):\n                if not name in os.environ:\n                    if not name in _backup:\n                        _backup[name] = os.environ.get(name, None)\n                    os.environ[name] = value\n\n        # Read and set environment from file(s)\n        for filename in options.pop(\"env_file\", []) or []:\n            for name, value in load_env_from_file(filename):\n                if not name in _backup:\n                    _backup[name] = os.environ.get(name, None)\n                os.environ[name] = value\n\n        # Set environment\n        for name, value in map(parse_var, options.pop(\"env\", []) or []):\n            if not name in _backup:\n                _backup[name] = os.environ.get(name, None)\n            os.environ[name] = value\n\n        yield options\n    finally:\n        for name, value in _backup.items():\n            if value is None:\n                del os.environ[name]\n            else:\n                os.environ[name] = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters a function as being part of an API then returns the original function.", "response": "def register(self, x, graph=False):\n        \"\"\"Register a function as being part of an API, then returns the original function.\"\"\"\n\n        if graph:\n            # This function must comply to the \"graph\" API interface, meaning it can bahave like bonobo.run.\n            from inspect import signature\n\n            parameters = list(signature(x).parameters)\n            required_parameters = {\"plugins\", \"services\", \"strategy\"}\n            assert (\n                len(parameters) > 0 and parameters[0] == \"graph\"\n            ), 'First parameter of a graph api function must be \"graph\".'\n            assert (\n                required_parameters.intersection(parameters) == required_parameters\n            ), \"Graph api functions must define the following parameters: \" + \", \".join(sorted(required_parameters))\n\n        self.__all__.append(get_name(x))\n        return x"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _open_url(url):\n    response = requests.get(url, stream=True)\n    if response.status_code != 200:\n        raise IOError(\"Unable to download {}, HTTP {}\".format(url, response.status_code))\n    return response", "response": "Open a HTTP connection to the URL and return a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef xstatus(self):\n        return max(node.xstatus for node in self.nodes) if len(self.nodes) else 0", "response": "Return the maximum exit status of the nodes in the context."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write(self, *messages):\n\n        for i in self.graph.outputs_of(BEGIN):\n            for message in messages:\n                self[i].write(message)", "response": "Push a list of messages in the inputs of this graph s inputs matching the output of special node BEGIN in\n        our graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a key = val option string into a python tuple", "response": "def _parse_option(option):\n    \"\"\"\n    Parse a 'key=val' option string into a python (key, val) pair\n\n    :param option: str\n    :return: tuple\n    \"\"\"\n    try:\n        key, val = option.split(\"=\", 1)\n    except ValueError:\n        return option, True\n\n    try:\n        val = json.loads(val)\n    except json.JSONDecodeError:\n        pass\n\n    return key, val"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _resolve_transformations(transformations):\n    registry = _ModulesRegistry()\n    transformations = transformations or []\n    for t in transformations:\n        try:\n            mod, attr = t.split(\":\", 1)\n            yield getattr(registry.require(mod), attr)\n        except ValueError:\n            yield getattr(bonobo, t)", "response": "Resolve a collection of strings into the matching python objects defaulting to bonobo namespace"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef OrderFields(fields):\n    fields = list(fields)\n\n    @use_context\n    @use_raw_input\n    def _OrderFields(context, row):\n        nonlocal fields\n        context.setdefault(\"remaining\", None)\n        if not context.output_type:\n            context.remaining = list(sorted(set(context.get_input_fields()) - set(fields)))\n            context.set_output_fields(fields + context.remaining)\n\n        yield tuple(row.get(field) for field in context.get_output_fields())\n\n    return _OrderFields", "response": "Returns a transformation function that orders the fields in a data stream."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef SetFields(fields):\n\n    @use_context\n    @use_no_input\n    def _SetFields(context):\n        nonlocal fields\n        if not context.output_type:\n            context.set_output_fields(fields)\n        return NOT_MODIFIED\n\n    return _SetFields", "response": "Returns a transformation that sets the field names on first iteration without touching the values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef UnpackItems(*items, fields=None, defaults=None):\n    defaults = defaults or {}\n\n    @use_context\n    @use_raw_input\n    def _UnpackItems(context, bag):\n        nonlocal fields, items, defaults\n\n        if fields is None:\n            fields = ()\n            for item in items:\n                fields += tuple(bag[item].keys())\n            context.set_output_fields(fields)\n\n        values = ()\n        for item in items:\n            values += tuple(bag[item].get(field, defaults.get(field)) for field in fields)\n\n        return values\n\n    return _UnpackItems", "response": ">>> UnpackItems - Unpacks items into a single object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a transformation factory that maps function on the values of a row.", "response": "def MapFields(function, key=True):\n    \"\"\"\n    Transformation factory that maps `function` on the values of a row.\n    It can be applied either to\n    1. all columns (`key=True`),\n    2. no column (`key=False`), or\n    3. a subset of columns by passing a callable, which takes column name and returns `bool`\n     (same as the parameter `function` in `filter`).\n\n    :param function: callable\n    :param key: bool or callable\n    :return: callable\n    \"\"\"\n    @use_raw_input\n    def _MapFields(bag):\n        try:\n            factory = type(bag)._make\n        except AttributeError:\n            factory = type(bag)\n\n        if callable(key):\n            try:\n                fields = bag._fields\n            except AttributeError as e:\n                raise UnrecoverableAttributeError(\n                    'This transformation works only on objects with named'\n                    ' fields (namedtuple, BagType, ...).') from e\n\n            return factory(\n                function(value) if key(key_) else value for key_, value in zip(fields, bag)\n            )\n        elif key:\n            return factory(function(value) for value in bag)\n        else:\n            return NOT_MODIFIED\n\n    return _MapFields"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split_token(output):\n\n    output = ensure_tuple(output)\n\n    flags, i, len_output, data_allowed = set(), 0, len(output), True\n    while i < len_output and isflag(output[i]):\n        if output[i].must_be_first and i:\n            raise ValueError(\"{} flag must be first.\".format(output[i]))\n        if i and output[i - 1].must_be_last:\n            raise ValueError(\"{} flag must be last.\".format(output[i - 1]))\n        if output[i] in flags:\n            raise ValueError(\"Duplicate flag {}.\".format(output[i]))\n        flags.add(output[i])\n        data_allowed &= output[i].allows_data\n        i += 1\n\n    output = output[i:]\n    if not data_allowed and len(output):\n        raise ValueError(\"Output data provided after a flag that does not allow data.\")\n\n    return flags, output", "response": "Split an output into token tuple real output tuple."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstart the context manager.", "response": "def start(self):\n        \"\"\"\n        Starts this context, a.k.a the phase where you setup everything which will be necessary during the whole\n        lifetime of a transformation.\n\n        The \"ContextCurrifier\" is in charge of setting up a decorating stack, that includes both services and context\n        processors, and will call the actual node callable with additional parameters.\n\n        \"\"\"\n        super().start()\n\n        try:\n            initial = self._get_initial_context()\n            self._stack = ContextCurrifier(self.wrapped, *initial.args, **initial.kwargs)\n            if isconfigurabletype(self.wrapped):\n                try:\n                    self.wrapped = self.wrapped(_final=True)\n                except Exception as exc:\n                    # Not normal to have a partially configured object here, so let's warn the user instead of having get into\n                    # the hard trouble of understanding that by himself.\n                    raise TypeError(\n                        \"Configurables should be instanciated before execution starts.\\nGot {!r}.\\n\".format(\n                            self.wrapped\n                        )\n                    ) from exc\n                else:\n                    raise TypeError(\n                        \"Configurables should be instanciated before execution starts.\\nGot {!r}.\\n\".format(\n                            self.wrapped\n                        )\n                    )\n            self._stack.setup(self)\n        except Exception:\n            # Set the logging level to the lowest possible, to avoid double log.\n            self.fatal(sys.exc_info(), level=0)\n\n            # We raise again, so the error is not ignored out of execution loops.\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _step(self):\n\n        # Pull and check data\n        input_bag = self._get()\n\n        # Sent through the stack\n        results = self._stack(input_bag)\n\n        # self._exec_time += timer.duration\n        # Put data onto output channels\n\n        if isinstance(results, GeneratorType):\n            while True:\n                try:\n                    # if kill flag was step, stop iterating.\n                    if self._killed:\n                        break\n                    result = next(results)\n                except StopIteration:\n                    # That's not an error, we're just done.\n                    break\n                else:\n                    # Push data (in case of an iterator)\n                    self._put(self._cast(input_bag, result))\n        elif results:\n            # Push data (returned value)\n            self._put(self._cast(input_bag, results))\n        else:\n            # case with no result, an execution went through anyway, use for stats.\n            # self._exec_count += 1\n            pass", "response": "This method is called by the _get method in the main loop."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stop(self):\n        if self._stack:\n            try:\n                self._stack.teardown()\n            except Exception:\n                self.fatal(sys.exc_info())\n\n        super().stop()", "response": "Cleanup the context, after the loop ended."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write(self, *messages):\n        for message in messages:\n            if not isinstance(message, Token):\n                message = ensure_tuple(message, cls=self._input_type, length=self._input_length)\n                if self._input_length is None:\n                    self._input_length = len(message)\n            self.input.put(message)", "response": "Push a list of messages to this context s input queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntransforms a pair of input and output into a real slim shoutput.", "response": "def _cast(self, _input, _output):\n        \"\"\"\n        Transforms a pair of input/output into the real slim shoutput.\n\n        :param _input: Bag\n        :param _output: mixed\n        :return: Bag\n        \"\"\"\n\n        if isenvelope(_output):\n            _output, _flags, _options = _output.unfold()\n        else:\n            _flags, _options = [], {}\n\n        if len(_flags):\n            # TODO: parse flags to check constraints are respected (like not modified alone, etc.)\n\n            if F_NOT_MODIFIED in _flags:\n                return _input\n\n            if F_INHERIT in _flags:\n                if self._output_type is None:\n                    self._output_type = concat_types(\n                        self._input_type, self._input_length, self._output_type, len(_output)\n                    )\n                _output = _input + ensure_tuple(_output)\n\n        if not self._output_type:\n            if issubclass(type(_output), tuple):\n                self._output_type = type(_output)\n\n        return ensure_tuple(_output, cls=self._output_type)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a message to all of the context s outputs.", "response": "def _put(self, value, _control=False):\n        \"\"\"\n        Sends a message to all of this context's outputs.\n\n        :param mixed value: message\n        :param _control: if true, won't count in statistics.\n        \"\"\"\n\n        if not _control:\n            self.increment(\"out\")\n\n        for output in self.outputs:\n            output.put(value)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def _get(self):\n        input_bag = await self.input.get()\n\n        # Store or check input type\n        if self._input_type is None:\n            self._input_type = type(input_bag)\n        elif type(input_bag) != self._input_type:\n            try:\n                if self._input_type == tuple:\n                    input_bag = self._input_type(input_bag)\n                else:\n                    input_bag = self._input_type(*input_bag)\n            except Exception as exc:\n                raise UnrecoverableTypeError(\n                    \"Input type changed to incompatible type between calls to {!r}.\\nGot {!r} which is not of type {!r}.\".format(\n                        self.wrapped, input_bag, self._input_type\n                    )\n                ) from exc\n\n        # Store or check input length, which is a soft fallback in case we're just using tuples\n        if self._input_length is None:\n            self._input_length = len(input_bag)\n        elif len(input_bag) != self._input_length:\n            raise UnrecoverableTypeError(\n                \"Input length changed between calls to {!r}.\\nExpected {} but got {}: {!r}.\".format(\n                    self.wrapped, self._input_length, len(input_bag), input_bag\n                )\n            )\n\n        self.increment(\"in\")  # XXX should that go before type check ?\n\n        return input_bag", "response": "Read from the input queue and store the result in self. _input_type and self. _input_length."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_strategy(name=None):\n    import logging\n    from bonobo.execution.strategies.base import Strategy\n\n    if isinstance(name, Strategy):\n        return name\n\n    if name is None:\n        name = DEFAULT_STRATEGY\n\n    logging.debug(\"Creating execution strategy {!r}...\".format(name))\n\n    try:\n        factory = STRATEGIES[name]\n    except KeyError as exc:\n        raise RuntimeError(\n            \"Invalid strategy {}. Available choices: {}.\".format(repr(name), \", \".join(sorted(STRATEGIES.keys())))\n        ) from exc\n\n    return factory()", "response": "Create a strategy or just returns it if it s already one."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the given argument is an instance of bonobo. config. ConfigurableMeta.", "response": "def isconfigurabletype(mixed, *, strict=False):\n    \"\"\"\n    Check if the given argument is an instance of :class:`bonobo.config.ConfigurableMeta`, meaning it has all the\n    plumbery necessary to build :class:`bonobo.config.Configurable`-like instances.\n\n    :param mixed:\n    :param strict: should we consider partially configured objects?\n    :return: bool\n    \"\"\"\n    from bonobo.config.configurables import ConfigurableMeta, PartiallyConfigured\n\n    if isinstance(mixed, ConfigurableMeta):\n        return True\n\n    if strict:\n        return False\n\n    if isinstance(mixed, PartiallyConfigured):\n        return True\n\n    if hasattr(mixed, \"_partial\") and mixed._partial:\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget valid user credentials from storage.", "response": "def get_credentials(*, scopes):\n    \"\"\"Gets valid user credentials from storage.\n\n    If nothing has been stored, or if the stored credentials are invalid,\n    the OAuth2 flow is completed to obtain the new credentials.\n\n    Returns:\n        Credentials, the obtained credential.\n    \"\"\"\n    credential_dir = os.path.join(HOME_DIR, \".cache\", __package__, \"credentials\")\n    if not os.path.exists(credential_dir):\n        os.makedirs(credential_dir)\n    credential_path = os.path.join(credential_dir, \"googleapis.json\")\n\n    store = Storage(credential_path)\n    credentials = store.get()\n\n    # see https://developers.google.com/api-client-library/python/auth/web-app\n    # kw: \"incremental scopes\"\n    if not credentials or credentials.invalid or not credentials.has_scopes(scopes):\n        flow = client.flow_from_clientsecrets(GOOGLE_SECRETS, scopes)\n        flow.user_agent = \"Bonobo ETL (https://www.bonobo-project.org/)\"\n        flags = argparser.parse_args([\"--noauth_local_webserver\"])\n        credentials = tools.run_flow(flow, store, flags)\n        print(\"Storing credentials to \" + credential_path)\n    return credentials"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _repr_html_():\n    from bonobo.commands.version import get_versions\n\n    return (\n        '<div style=\"padding: 8px;\">'\n        '  <div style=\"float: left; width: 20px; height: 20px;\">{}</div>'\n        '  <pre style=\"white-space: nowrap; padding-left: 8px\">{}</pre>'\n        \"</div>\"\n    ).format(__logo__, \"<br/>\".join(get_versions(all=True)))", "response": "This allows to easily display a version snippet in Jupyter."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_graph(graph=None, *, _limit=(), _print=()):\n    graph = graph or bonobo.Graph()\n\n    producer = (\n        graph.get_cursor()\n        >> ODSReader(dataset=\"liste-des-cafes-a-un-euro\", netloc=\"opendata.paris.fr\")\n        >> PartialGraph(*_limit)\n        >> bonobo.UnpackItems(0)\n        >> bonobo.Rename(name=\"nom_du_cafe\", address=\"adresse\", zipcode=\"arrondissement\")\n        >> bonobo.Format(city=\"Paris\", country=\"France\")\n        >> bonobo.OrderFields([\"name\", \"address\", \"zipcode\", \"city\", \"country\", \"geometry\", \"geoloc\"])\n        >> PartialGraph(*_print)\n    )\n\n    # Comma separated values.\n    graph.get_cursor(producer.output) >> bonobo.CsvWriter(\n        \"coffeeshops.csv\", fields=[\"name\", \"address\", \"zipcode\", \"city\"], delimiter=\",\"\n    )\n\n    # Standard JSON\n    graph.get_cursor(producer.output) >> bonobo.JsonWriter(path=\"coffeeshops.json\")\n\n    # Line-delimited JSON\n    graph.get_cursor(producer.output) >> bonobo.LdjsonWriter(path=\"coffeeshops.ldjson\")\n\n    return graph", "response": "Returns a bonobo. Graph object that is a list of cafes with on euro in Paris."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef entrypoint(args=None):\n\n    mondrian.setup(excepthook=True)\n    logger = logging.getLogger()\n    logger.setLevel(settings.LOGGING_LEVEL.get())\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--debug\", \"-D\", action=\"store_true\")\n\n    subparsers = parser.add_subparsers(dest=\"command\")\n    subparsers.required = True\n\n    commands = {}\n\n    def register_extension(ext):\n        nonlocal commands\n\n        try:\n            parser = subparsers.add_parser(ext.name)\n            if isinstance(ext.plugin, type) and issubclass(ext.plugin, BaseCommand):\n                # current way, class based.\n                cmd = ext.plugin()\n                cmd.add_arguments(parser)\n                cmd.__name__ = ext.name\n                commands[ext.name] = cmd.handle\n            else:\n                # old school, function based.\n                commands[ext.name] = ext.plugin(parser)\n        except Exception:\n            logger.exception(\"Error while loading command {}.\".format(ext.name))\n\n    from stevedore import ExtensionManager\n\n    mgr = ExtensionManager(namespace=\"bonobo.commands\")\n    mgr.map(register_extension)\n\n    parsed_args = parser.parse_args(args).__dict__\n\n    if parsed_args.pop(\"debug\", False):\n        settings.DEBUG.set(True)\n        settings.LOGGING_LEVEL.set(logging.DEBUG)\n        logger.setLevel(settings.LOGGING_LEVEL.get())\n\n    logger.debug(\"Command: \" + parsed_args[\"command\"] + \" Arguments: \" + repr(parsed_args))\n\n    # Get command handler, execute, rince.\n    command = commands[parsed_args.pop(\"command\")]\n    command(**parsed_args)\n\n    return 0", "response": "Main function for Bonobo entrypoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nallows all readers to use eventually use output_fields XOR output_type options.", "response": "def output(self, context, *args, **kwargs):\n        \"\"\"\n        Allow all readers to use eventually use output_fields XOR output_type options.\n\n        \"\"\"\n\n        output_fields = self.output_fields\n        output_type = self.output_type\n\n        if output_fields and output_type:\n            raise UnrecoverableError(\"Cannot specify both output_fields and output_type option.\")\n\n        if self.output_type:\n            context.set_output_type(self.output_type)\n\n        if self.output_fields:\n            context.set_output_fields(self.output_fields)\n\n        yield"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read(self, file, *, fs):\n        for line in file:\n            yield line.rstrip(self.eol)", "response": "Read a line of a file and yield the next line."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef outputs_of(self, idx, create=False):\n        if create and not idx in self.edges:\n            self.edges[idx] = set()\n        return self.edges[idx]", "response": "Get a set of outputs for a given node index."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_node(self, c):\n        idx = len(self.nodes)\n        self.edges[idx] = set()\n        self.nodes.append(c)\n        return idx", "response": "Add a node without connections in this graph and returns its index."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a chain in this graph.", "response": "def add_chain(self, *nodes, _input=BEGIN, _output=None, _name=None):\n        \"\"\" Add a chain in this graph.\n        \"\"\"\n        if len(nodes):\n            _input = self._resolve_index(_input)\n            _output = self._resolve_index(_output)\n            _first = None\n            _last = None\n\n            for i, node in enumerate(nodes):\n                _last = self.add_node(node)\n                if not i and _name:\n                    if _name in self.named:\n                        raise KeyError(\"Duplicate name {!r} in graph.\".format(_name))\n                    self.named[_name] = _last\n                if _first is None:\n                    _first = _last\n                self.outputs_of(_input, create=True).add(_last)\n                _input = _last\n\n            if _output is not None:\n                self.outputs_of(_input, create=True).add(_output)\n\n            if hasattr(self, \"_topologcally_sorted_indexes_cache\"):\n                del self._topologcally_sorted_indexes_cache\n\n            return GraphRange(self, _first, _last)\n        return GraphRange(self, None, None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\niterate in topological order based on networkx s topological_sort function.", "response": "def topologically_sorted_indexes(self):\n        \"\"\"Iterate in topological order, based on networkx's topological_sort() function.\n        \"\"\"\n        try:\n            return self._topologcally_sorted_indexes_cache\n        except AttributeError:\n            seen = set()\n            order = []\n            explored = set()\n\n            for i in self.edges:\n                if i in explored:\n                    continue\n                fringe = [i]\n                while fringe:\n                    w = fringe[-1]  # depth first search\n                    if w in explored:  # already looked down this branch\n                        fringe.pop()\n                        continue\n                    seen.add(w)  # mark as seen\n                    # Check successors for cycles and for new nodes\n                    new_nodes = []\n                    for n in self.outputs_of(w):\n                        if n not in explored:\n                            if n in seen:  # CYCLE !!\n                                raise RuntimeError(\"Graph contains a cycle.\")\n                            new_nodes.append(n)\n                    if new_nodes:  # Add new_nodes to fringe\n                        fringe.extend(new_nodes)\n                    else:  # No new nodes so w is fully explored\n                        explored.add(w)\n                        order.append(w)\n                        fringe.pop()  # done considering this node\n            self._topologcally_sorted_indexes_cache = tuple(filter(lambda i: type(i) is int, reversed(order)))\n            return self._topologcally_sorted_indexes_cache"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds the index based on various strategies for a node.", "response": "def _resolve_index(self, mixed):\n        \"\"\"\n        Find the index based on various strategies for a node, probably an input or output of chain. Supported\n        inputs are indexes, node values or names.\n\n        \"\"\"\n        if mixed is None:\n            return None\n\n        if type(mixed) is int or mixed in self.edges:\n            return mixed\n\n        if isinstance(mixed, str) and mixed in self.named:\n            return self.named[mixed]\n\n        if mixed in self.nodes:\n            return self.nodes.index(mixed)\n\n        raise ValueError(\"Cannot find node matching {!r}.\".format(mixed))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_path():\n    # Theme directory is defined as our parent directory\n    return os.path.abspath(os.path.dirname(os.path.dirname(__file__)))", "response": "Get the absolute path to the theme directory"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate or update a django model instance.", "response": "def create_or_update(model, *, defaults=None, save=True, **kwargs):\n    \"\"\"\n    Create or update a django model instance.\n\n    :param model:\n    :param defaults:\n    :param kwargs:\n    :return: object, created, updated\n\n    \"\"\"\n    obj, created = model._default_manager.get_or_create(defaults=defaults, **kwargs)\n\n    updated = False\n    if not created:\n        if defaults:\n            for k, v in defaults.items():\n                if getattr(obj, k) != v:\n                    setattr(obj, k, v)\n                    updated = True\n\n        if updated and save:\n            obj.save()\n\n    return obj, created, updated"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _override_runner(runner):\n    import bonobo\n\n    _get_argument_parser = bonobo.util.environ.get_argument_parser\n    _run = bonobo.run\n    try:\n        # Original get_argument_parser would create or update an argument parser with environment options, but here we\n        # already had them parsed so let's patch with something that creates an empty one instead.\n        def get_argument_parser(parser=None):\n            return parser or argparse.ArgumentParser()\n\n        bonobo.util.environ.get_argument_parser = get_argument_parser\n        bonobo.run = runner\n\n        yield runner\n    finally:\n        # Restore our saved values.\n        bonobo.util.environ.get_argument_parser = _get_argument_parser\n        bonobo.run = _run", "response": "Context manager that monkey patches bonobo. run with our current command logic."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nregister generic arguments for the given parser.", "response": "def register_generic_run_arguments(parser, required=True):\n    \"\"\"\n    Only there for backward compatibility with third party extensions.\n    TODO: This should be deprecated (using the @deprecated decorator) in 0.7, and removed in 0.8 or 0.9.\n    \"\"\"\n    dummy_command = BaseGraphCommand()\n    dummy_command.required = required\n    dummy_command.add_arguments(parser)\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninstalls requirements given a path to requirements. txt file.", "response": "def _install_requirements(requirements):\n    \"\"\"Install requirements given a path to requirements.txt file.\"\"\"\n    import importlib\n    import pip\n\n    pip.main([\"install\", \"-r\", requirements])\n    # Some shenanigans to be sure everything is importable after this, especially .egg-link files which\n    # are referenced in *.pth files and apparently loaded by site.py at some magic bootstrap moment of the\n    # python interpreter.\n    pip.utils.pkg_resources = importlib.reload(pip.utils.pkg_resources)\n    import site\n\n    importlib.reload(site)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nensuring that tuple_or_mixed is a tuple of one item.", "response": "def ensure_tuple(tuple_or_mixed, *, cls=None):\n    \"\"\"\n    If it's not a tuple, let's make a tuple of one item.\n    Otherwise, not changed.\n\n    :param tuple_or_mixed:\n    :return: tuple\n\n    \"\"\"\n    if cls is None:\n        cls = tuple\n\n    if isinstance(tuple_or_mixed, cls):\n        return tuple_or_mixed\n\n    if tuple_or_mixed is None:\n        return tuple.__new__(cls, ())\n\n    if isinstance(tuple_or_mixed, tuple):\n        return tuple.__new__(cls, tuple_or_mixed)\n\n    return tuple.__new__(cls, (tuple_or_mixed,))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a container with reasonable default service implementations for commonly use standard - named services.", "response": "def create_container(services=None, factory=Container):\n    \"\"\"\n    Create a container with reasonable default service implementations for commonly use, standard-named, services.\n\n    Services:\n    - `fs` defaults to a fs2 instance based on current working directory\n    - `http`defaults to requests\n\n    :param services:\n    :return:\n    \"\"\"\n    container = factory(services) if services else factory()\n\n    if not \"fs\" in container:\n        import bonobo\n\n        container.setdefault(\"fs\", bonobo.open_fs())\n\n    if not \"http\" in container:\n        import requests\n\n        container.setdefault(\"http\", requests)\n\n    return container"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a reader instance for a node.", "response": "def create_reader(name, *args, format=None, registry=default_registry, **kwargs):\n    \"\"\"\n    Create a reader instance, guessing its factory using filename (and eventually format).\n\n    :param name:\n    :param args:\n    :param format:\n    :param registry:\n    :param kwargs:\n\n    :return: mixed\n    \"\"\"\n    return registry.get_reader_factory_for(name, format=format)(name, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_writer(name, *args, format=None, registry=default_registry, **kwargs):\n    return registry.get_writer_factory_for(name, format=format)(name, *args, **kwargs)", "response": "Create a new node in the registry using the given name and arguments."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a callable to build a reader for the provided filename eventually forcing a format.", "response": "def get_reader_factory_for(self, name, *, format=None):\n        \"\"\"\n        Returns a callable to build a reader for the provided filename, eventually forcing a format.\n\n        :param name: filename\n        :param format: format\n        :return: type\n        \"\"\"\n        return self.get_factory_for(READER, name, format=format)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a callable to build a writer for the provided filename eventually forcing a format.", "response": "def get_writer_factory_for(self, name, *, format=None):\n        \"\"\"\n        Returns a callable to build a writer for the provided filename, eventually forcing a format.\n\n        :param name: filename\n        :param format: format\n        :return: type\n        \"\"\"\n        return self.get_factory_for(WRITER, name, format=format)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def main():\n    async with ClientSession() as websession:\n        try:\n            # Create a client:\n            client = Client('<EMAIL>', '<PASSWORD>', websession)\n            await client.async_init()\n\n            print('Showing active Tiles:')\n            print(await client.tiles.all())\n\n            print('Showing all Tiles:')\n            print(await client.tiles.all(show_inactive=True))\n        except TileError as err:\n            print(err)", "response": "Main function for the sequence of Tiles."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a Tile session.", "response": "async def async_init(self) -> None:\n        \"\"\"Create a Tile session.\"\"\"\n        if not self._client_established:\n            await self.request(\n                'put',\n                'clients/{0}'.format(self.client_uuid),\n                data={\n                    'app_id': DEFAULT_APP_ID,\n                    'app_version': DEFAULT_APP_VERSION,\n                    'locale': self._locale\n                })\n            self._client_established = True\n\n        resp = await self.request(\n            'post',\n            'clients/{0}/sessions'.format(self.client_uuid),\n            data={\n                'email': self._email,\n                'password': self._password\n            })\n\n        if not self.user_uuid:\n            self.user_uuid = resp['result']['user']['user_uuid']\n        self._session_expiry = resp['result']['session_expiration_timestamp']\n\n        self.tiles = Tile(self.request, self.user_uuid)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def request(\n            self,\n            method: str,\n            endpoint: str,\n            *,\n            headers: dict = None,\n            params: dict = None,\n            data: dict = None) -> dict:\n        \"\"\"Make a request against AirVisual.\"\"\"\n        if (self._session_expiry\n                and self._session_expiry <= current_epoch_time()):\n            raise SessionExpiredError('Session has expired; make a new one!')\n\n        url = '{0}/{1}'.format(API_URL_SCAFFOLD, endpoint)\n\n        if not headers:\n            headers = {}\n        headers.update({\n            'Tile_app_id': DEFAULT_APP_ID,\n            'Tile_app_version': DEFAULT_APP_VERSION,\n            'Tile_client_uuid': self.client_uuid,\n        })\n\n        async with self._websession.request(method, url, headers=headers,\n                                            params=params, data=data) as resp:\n            try:\n                resp.raise_for_status()\n                return await resp.json(content_type=None)\n            except client_exceptions.ClientError as err:\n                raise RequestError(\n                    'Error requesting data from {0}: {1}'.format(\n                        endpoint, err)) from None", "response": "Make a request against AirVisual."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def all(\n            self, whitelist: list = None, show_inactive: bool = False) -> list:\n        \"\"\"Get all Tiles for a user's account.\"\"\"\n        list_data = await self._request(\n            'get', 'users/{0}/user_tiles'.format(self._user_uuid))\n        tile_uuid_list = [\n            tile['tile_uuid'] for tile in list_data['result']\n            if not whitelist or tile['tileType'] in whitelist\n        ]\n\n        tile_data = await self._request(\n            'get',\n            'tiles',\n            params=[('tile_uuids', uuid) for uuid in tile_uuid_list])\n        return [\n            tile for tile in tile_data['result'].values()\n            if show_inactive or tile['visible'] is True\n        ]", "response": "Get all Tiles for a user s account."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a string to the NX - OS device.", "response": "def _send_str(self, cmd, args):\n        \"\"\"\n        Format:\n            {Command}{args length(little endian)}{str}\n        Length:\n            {4}{4}{str length}\n        \"\"\"\n        logger.debug(\"{} {}\".format(cmd, args))\n        args = args.encode('utf-8')\n\n        le_args_len = self._little_endian(len(args))\n        data = cmd.encode() + le_args_len + args\n        logger.debug(\"Send string: {}\".format(data))\n        self.connection.write(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the permutation of specified range", "response": "def _get_p(df, column, shifts):\r\n        \"\"\" get the permutation of specified range\r\n\r\n        example:\r\n        index    x   x_-2,-1_p\r\n        0        1         NaN\r\n        1       -1         NaN\r\n        2        3           2  (0.x > 0, and assigned to weight 2)\r\n        3        5           1  (2.x > 0, and assigned to weight 1)\r\n        4        1           3\r\n\r\n        :param df: data frame\r\n        :param column: the column to calculate p from\r\n        :param shifts: the range to consider\r\n        :return:\r\n        \"\"\"\r\n        column_name = '{}_{}_p'.format(column, shifts)\r\n        # initialize the column if not\r\n        df.get(column)\r\n        shifts = StockDataFrame.to_ints(shifts)[::-1]\r\n        indices = None\r\n        count = 0\r\n        for shift in shifts:\r\n            shifted = df.shift(-shift)\r\n            index = (shifted[column] > 0) * (2 ** count)\r\n            if indices is None:\r\n                indices = index\r\n            else:\r\n                indices += index\r\n            count += 1\r\n        StockDataFrame.set_nan(indices, shifts)\r\n        df[column_name] = indices"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the rate of change of column", "response": "def _get_r(cls, df, column, shifts):\r\n        \"\"\" Get rate of change of column\r\n\r\n        :param df: DataFrame object\r\n        :param column: column name of the rate to calculate\r\n        :param shifts: days to shift, accept one shift only\r\n        :return: None\r\n        \"\"\"\r\n        shift = cls.to_int(shifts)\r\n        rate_key = '{}_{}_r'.format(column, shift)\r\n        df[rate_key] = df[column].pct_change(periods=-shift) * 100"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the column shifted by days", "response": "def _get_s(cls, df, column, shifts):\r\n        \"\"\" Get the column shifted by days\r\n\r\n        :param df: DataFrame object\r\n        :param column: name of the column to shift\r\n        :param shifts: days to shift, accept one shift only\r\n        :return: None\r\n        \"\"\"\r\n        shift = cls.to_int(shifts)\r\n        shifted_key = \"{}_{}_s\".format(column, shift)\r\n        df[shifted_key] = df[column].shift(-shift)\r\n        StockDataFrame.set_nan(df[shifted_key], shift)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the count of column in range ( shifts", "response": "def _get_c(cls, df, column, shifts):\r\n        \"\"\" get the count of column in range (shifts)\r\n\r\n        example: kdjj_0_le_20_c\r\n        :param df: stock data\r\n        :param column: column name\r\n        :param shifts: range to count, only to previous\r\n        :return:\r\n        \"\"\"\r\n        column_name = '{}_{}_{}'.format(column, shifts, 'c')\r\n        shifts = abs(cls.to_int(shifts))\r\n        df[column_name] = df[column].rolling(\r\n            center=False, window=shifts).apply(np.count_nonzero)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_rsv(df, n_days):\r\n        n_days = int(n_days)\r\n        column_name = 'rsv_{}'.format(n_days)\r\n        low_min = df['low'].rolling(\r\n            min_periods=1, window=n_days, center=False).min()\r\n        high_max = df['high'].rolling(\r\n            min_periods=1, window=n_days, center=False).max()\r\n\r\n        cv = (df['close'] - low_min) / (high_max - low_min)\r\n        df[column_name] = cv.fillna(0).astype('float64') * 100", "response": "Calculates the RSV value within N days"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_rsi(cls, df, n_days):\r\n        n_days = int(n_days)\r\n        d = df['close_-1_d']\r\n\r\n        df['closepm'] = (d + d.abs()) / 2\r\n        df['closenm'] = (-d + d.abs()) / 2\r\n        closepm_smma_column = 'closepm_{}_smma'.format(n_days)\r\n        closenm_smma_column = 'closenm_{}_smma'.format(n_days)\r\n        p_ema = df[closepm_smma_column]\r\n        n_ema = df[closenm_smma_column]\r\n\r\n        rs_column_name = 'rs_{}'.format(n_days)\r\n        rsi_column_name = 'rsi_{}'.format(n_days)\r\n        df[rs_column_name] = rs = p_ema / n_ema\r\n        df[rsi_column_name] = 100 - 100 / (1.0 + rs)\r\n\r\n        del df['closepm']\r\n        del df['closenm']\r\n        del df[closepm_smma_column]\r\n        del df[closenm_smma_column]", "response": "Calculates the RSI of the next N days with respect to the relative strength index of the next close - to - many entries."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_smma(cls, df, column, windows):\r\n        window = cls.get_only_one_positive_int(windows)\r\n        column_name = '{}_{}_smma'.format(column, window)\r\n        smma = df[column].ewm(\r\n            ignore_na=False, alpha=1.0 / window,\r\n            min_periods=0, adjust=True).mean()\r\n        df[column_name] = smma\r\n        return smma", "response": "get smoothed moving average."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_wr(cls, df, n_days):\r\n        n_days = int(n_days)\r\n        ln = df['low'].rolling(min_periods=1, window=n_days,\r\n                               center=False).min()\r\n\r\n        hn = df['high'].rolling(min_periods=1, window=n_days,\r\n                                center=False).max()\r\n        column_name = 'wr_{}'.format(n_days)\r\n        df[column_name] = (hn - df['close']) / (hn - ln) * 100", "response": "Returns a new dataframe with the highest and low entries in the given date range."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the composition channel index of the most recent available entry in the given DataFrame.", "response": "def _get_cci(cls, df, n_days=None):\r\n        \"\"\" Commodity Channel Index\r\n\r\n        CCI = (Typical Price  -  20-period SMA of TP) / (.015 x Mean Deviation)\r\n        Typical Price (TP) = (High + Low + Close)/3\r\n        TP is also implemented as 'middle'.\r\n\r\n        :param df: data\r\n        :param n_days: N days window\r\n        :return: None\r\n        \"\"\"\r\n        if n_days is None:\r\n            n_days = 14\r\n            column_name = 'cci'\r\n        else:\r\n            n_days = int(n_days)\r\n            column_name = 'cci_{}'.format(n_days)\r\n\r\n        tp = df['middle']\r\n        tp_sma = df['middle_{}_sma'.format(n_days)]\r\n        md = df['middle'].rolling(\r\n            min_periods=1, center=False, window=n_days).apply(\r\n            lambda x: np.fabs(x - x.mean()).mean())\r\n\r\n        df[column_name] = (tp - tp_sma) / (.015 * md)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_tr(cls, df):\r\n        prev_close = df['close_-1_s']\r\n        high = df['high']\r\n        low = df['low']\r\n        c1 = high - low\r\n        c2 = np.abs(high - prev_close)\r\n        c3 = np.abs(low - prev_close)\r\n        df['tr'] = np.max((c1, c2, c3), axis=0)", "response": "True Range of the trading\r\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_atr(cls, df, window=None):\r\n        if window is None:\r\n            window = 14\r\n            column_name = 'atr'\r\n        else:\r\n            window = int(window)\r\n            column_name = 'atr_{}'.format(window)\r\n        tr_smma_column = 'tr_{}_smma'.format(window)\r\n\r\n        df[column_name] = df[tr_smma_column]\r\n        del df[tr_smma_column]", "response": "Get the atr column from the DataFrame df."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the default setting for DMI", "response": "def _get_dmi(cls, df):\r\n        \"\"\" get the default setting for DMI\r\n\r\n        including:\r\n        +DI: 14 days SMMA of +DM,\r\n        -DI: 14 days SMMA of -DM,\r\n        DX: based on +DI and -DI\r\n        ADX: 6 days SMMA of DX\r\n        :param df: data\r\n        :return:\r\n        \"\"\"\r\n        df['pdi'] = cls._get_pdi(df, 14)\r\n        df['mdi'] = cls._get_mdi(df, 14)\r\n        df['dx'] = cls._get_dx(df, 14)\r\n        df['adx'] = df['dx_6_ema']\r\n        df['adxr'] = df['adx_6_ema']"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_um_dm(cls, df):\r\n        hd = df['high_delta']\r\n        df['um'] = (hd + hd.abs()) / 2\r\n        ld = -df['low_delta']\r\n        df['dm'] = (ld + ld.abs()) / 2", "response": "Initialize up move and down move\r\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_pdm(cls, df, windows):\r\n        window = cls.get_only_one_positive_int(windows)\r\n        column_name = 'pdm_{}'.format(window)\r\n        um, dm = df['um'], df['dm']\r\n        df['pdm'] = np.where(um > dm, um, 0)\r\n        if window > 1:\r\n            pdm = df['pdm_{}_ema'.format(window)]\r\n        else:\r\n            pdm = df['pdm']\r\n        df[column_name] = pdm", "response": "Returns the DataFrame with the most recent + DM positive directional moving\r\n            values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_mdm(cls, df, windows):\r\n        window = cls.get_only_one_positive_int(windows)\r\n        column_name = 'mdm_{}'.format(window)\r\n        um, dm = df['um'], df['dm']\r\n        df['mdm'] = np.where(dm > um, dm, 0)\r\n        if window > 1:\r\n            mdm = df['mdm_{}_ema'.format(window)]\r\n        else:\r\n            mdm = df['mdm']\r\n        df[column_name] = mdm", "response": "Returns the most recent mdm of the specified number of windows."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_pdi(cls, df, windows):\r\n        window = cls.get_only_one_positive_int(windows)\r\n        pdm_column = 'pdm_{}'.format(window)\r\n        tr_column = 'atr_{}'.format(window)\r\n        pdi_column = 'pdi_{}'.format(window)\r\n        df[pdi_column] = df[pdm_column] / df[tr_column] * 100\r\n        return df[pdi_column]", "response": "Get the pdi value of the data frame"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_kdjk(cls, df, n_days):\r\n        rsv_column = 'rsv_{}'.format(n_days)\r\n        k_column = 'kdjk_{}'.format(n_days)\r\n        df[k_column] = list(cls._calc_kd(df.get(rsv_column)))", "response": "Get the K of KDJ\r\n        K"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_kdjd(cls, df, n_days):\r\n        k_column = 'kdjk_{}'.format(n_days)\r\n        d_column = 'kdjd_{}'.format(n_days)\r\n        df[d_column] = list(cls._calc_kd(df.get(k_column)))", "response": "Get the KDJ of the next N days of the current time."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_kdjj(df, n_days):\r\n        k_column = 'kdjk_{}'.format(n_days)\r\n        d_column = 'kdjd_{}'.format(n_days)\r\n        j_column = 'kdjj_{}'.format(n_days)\r\n        df[j_column] = 3 * df[k_column] - 2 * df[d_column]", "response": "Get the J of KDJ\r\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting simple moving average", "response": "def _get_sma(cls, df, column, windows):\r\n        \"\"\" get simple moving average\r\n\r\n        :param df: data\r\n        :param column: column to calculate\r\n        :param windows: collection of window of simple moving average\r\n        :return: None\r\n        \"\"\"\r\n        window = cls.get_only_one_positive_int(windows)\r\n        column_name = '{}_{}_sma'.format(column, window)\r\n        df[column_name] = df[column].rolling(min_periods=1, window=window,\r\n                                             center=False).mean()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_ema(cls, df, column, windows):\r\n        window = cls.get_only_one_positive_int(windows)\r\n        column_name = '{}_{}_ema'.format(column, window)\r\n        if len(df[column]) > 0:\r\n            df[column_name] = df[column].ewm(\r\n                ignore_na=False, span=window,\r\n                min_periods=0, adjust=True).mean()\r\n        else:\r\n            df[column_name] = []", "response": "get exponential moving average\r\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_boll(cls, df):\r\n        moving_avg = df['close_{}_sma'.format(cls.BOLL_PERIOD)]\r\n        moving_std = df['close_{}_mstd'.format(cls.BOLL_PERIOD)]\r\n        df['boll'] = moving_avg\r\n        moving_avg = list(map(np.float64, moving_avg))\r\n        moving_std = list(map(np.float64, moving_std))\r\n        # noinspection PyTypeChecker\r\n        df['boll_ub'] = np.add(moving_avg,\r\n                               np.multiply(cls.BOLL_STD_TIMES, moving_std))\r\n        # noinspection PyTypeChecker\r\n        df['boll_lb'] = np.subtract(moving_avg,\r\n                                    np.multiply(cls.BOLL_STD_TIMES,\r\n                                                moving_std))", "response": "Get the Bollinger bands."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmoves Average Convergence Divergence This function will initialize all following columns. MACD Line (macd): (12-day EMA - 26-day EMA) Signal Line (macds): 9-day EMA of MACD Line MACD Histogram (macdh): MACD Line - Signal Line :param df: data :return: None", "response": "def _get_macd(df):\r\n        \"\"\" Moving Average Convergence Divergence\r\n\r\n        This function will initialize all following columns.\r\n\r\n        MACD Line (macd): (12-day EMA - 26-day EMA)\r\n        Signal Line (macds): 9-day EMA of MACD Line\r\n        MACD Histogram (macdh): MACD Line - Signal Line\r\n        :param df: data\r\n        :return: None\r\n        \"\"\"\r\n        fast = df['close_12_ema']\r\n        slow = df['close_26_ema']\r\n        df['macd'] = fast - slow\r\n        df['macds'] = df['macd_9_ema']\r\n        df['macdh'] = (df['macd'] - df['macds'])\r\n        log.critical(\"NOTE: Behavior of MACDH calculation has changed as of \"\r\n                     \"July 2017 - it is now 1/2 of previous calculated values\")\r\n        del df['macd_9_ema']\r\n        del fast\r\n        del slow"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting moving standard deviation of the items in a column", "response": "def _get_mstd(cls, df, column, windows):\r\n        \"\"\" get moving standard deviation\r\n\r\n        :param df: data\r\n        :param column: column to calculate\r\n        :param windows: collection of window of moving standard deviation\r\n        :return: None\r\n        \"\"\"\r\n        window = cls.get_only_one_positive_int(windows)\r\n        column_name = '{}_{}_mstd'.format(column, window)\r\n        df[column_name] = df[column].rolling(min_periods=1, window=window,\r\n                                             center=False).std()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_mvar(cls, df, column, windows):\r\n        window = cls.get_only_one_positive_int(windows)\r\n        column_name = '{}_{}_mvar'.format(column, window)\r\n        df[column_name] = df[column].rolling(\r\n            min_periods=1, window=window, center=False).var()", "response": "get moving variance of the items in a column"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts the input value to this class.", "response": "def retype(value, index_column=None):\r\n        \"\"\" if the input is a `DataFrame`, convert it to this class.\r\n\r\n        :param index_column: the column that will be used as index,\r\n                             default to `date`\r\n        :param value: value to convert\r\n        :return: this extended class\r\n        \"\"\"\r\n        if index_column is None:\r\n            index_column = 'date'\r\n\r\n        if isinstance(value, pd.DataFrame):\r\n            # use all lower case for column name\r\n            value.columns = map(lambda c: c.lower(), value.columns)\r\n\r\n            if index_column in value.columns:\r\n                value.set_index(index_column, inplace=True)\r\n            value = StockDataFrame(value)\r\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_queue(self):\n        m = pycurl.CurlMulti()\n        m.handles = []\n\n        # Loop the queue and create Curl objects for processing\n        for item in self._queue:\n            c = self._create_curl(*item)\n            m.add_handle(c)\n            m.handles.append(c)\n\n        # Process the collected Curl handles\n        num_handles = len(m.handles)\n        while num_handles:\n            while 1:\n                # Perform the calls\n                ret, num_handles = m.perform()\n                if ret != pycurl.E_CALL_MULTI_PERFORM:\n                    break\n            m.select(1.0)\n\n        # Collect data\n        results = []\n        for c in m.handles:\n            c.status = c.getinfo(c.HTTP_CODE)\n            if 'Content-Encoding: gzip' in c.response_headers.getvalue():\n                c.body = cStringIO.StringIO(self._gunzip_body(c.body.getvalue()))\n            result = { \"data\" : self._digest_result(c.body.getvalue()), \"code\": c.status }\n            if not c.status or c.status >= 400:\n                # Don't throw the exception because some might have succeeded\n                result['exception'] = HapiThreadedError(c)\n\n            results.append(result)\n\n            \n        # cleanup\n        for c in m.handles:\n            if hasattr(c, \"data_out\"):\n                c.data_out.close()\n\n            c.body.close()\n            c.response_headers.close()\n            c.close()\n            m.remove_handle(c)\n\n        m.close()\n        del m.handles\n        self._queue = []\n\n        return results", "response": "Processes all API calls in the queue and returns a list of data structures."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef camelcase_search_options(self, options):\n        new_options = {}\n        for key in options:\n            value = options[key]\n            new_key = SEARCH_OPTIONS_DICT.get(key, key)\n            if new_key == 'sort':\n                value = SORT_OPTIONS_DICT.get(value, value)\n            elif new_key == 'timePivot':\n                value = TIME_PIVOT_OPTIONS_DICT.get(value, value)\n            elif new_key in BOOLEAN_SEARCH_OPTIONS:\n                value = str(value).lower()\n            new_options[new_key] = value\n        return new_options", "response": "change all underscored variants back to what the API is expecting"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves all the leads in the API", "response": "def get_leads(self, *guids, **options):\n        \"\"\"Supports all the search parameters in the API as well as python underscored variants\"\"\"\n        original_options = options\n        options = self.camelcase_search_options(options.copy())\n        params = {}\n        for i in xrange(len(guids)):\n            params['guids[%s]'%i] = guids[i]\n        for k in options.keys():\n            if k in SEARCH_OPTIONS:\n                params[k] = options[k]\n                del options[k]\n        leads = self._call('list/', params, **options)\n        self.log.info(\"retrieved %s leads through API ( %soptions=%s )\" % \n                (len(leads), guids and 'guids=%s, '%guids or '', original_options))\n        return leads"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves a lead from the server", "response": "def retrieve_lead(self, *guid, **options):\n        cur_guid = guid or '' \n        params = {}\n        for key in options:\n            params[key] = options[key]\n        \"\"\" Set guid to -1 as default for not finding a user \"\"\"\n        lead = {'guid' : '-1'}\n        \"\"\" wrap lead call so that it doesn't error out when not finding a lead \"\"\"\n        try:\n            lead = self._call('lead/%s' % cur_guid, params, **options)\n        except:\n            \"\"\" no lead here \"\"\"\n        return lead"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_broadcast(self, broadcast_guid, **kwargs):\n        '''\n        Get a specific broadcast by guid\n        '''\n        params = kwargs\n        broadcast = self._call('broadcasts/%s' % broadcast_guid,\n            params=params, content_type='application/json')\n        return Broadcast(broadcast)", "response": "Get a specific broadcast by guid"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_broadcasts(self, type=\"\", page=None,\n            remote_content_id=None, limit=None, **kwargs):\n        '''\n        Get all broadcasts, with optional paging and limits.\n        Type filter can be 'scheduled', 'published' or 'failed'\n        '''\n        if remote_content_id:\n            return self.get_broadcasts_by_remote(remote_content_id)\n\n        params = {'type': type}\n        if page:\n            params['page'] = page\n\n        params.update(kwargs)\n\n        result = self._call('broadcasts', params=params,\n            content_type='application/json')\n        broadcasts = [Broadcast(b) for b in result]\n\n        if limit:\n            return broadcasts[:limit]\n        return broadcasts", "response": "Get all broadcasts with optional paging and limits."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cancel_broadcast(self, broadcast_guid):\n        '''\n        Cancel a broadcast specified by guid\n        '''\n        subpath = 'broadcasts/%s/update' % broadcast_guid\n        broadcast = {'status': 'CANCELED'}\n        bcast_dict = self._call(subpath, method='POST', data=broadcast,\n            content_type='application/json')\n        return bcast_dict", "response": "Cancel a broadcast specified by guid"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_channels(self, current=True, publish_only=False, settings=False):\n        if publish_only:\n            if current:\n                endpoint = 'channels/setting/publish/current'\n            else:\n                endpoint = 'channels/setting/publish'\n        else:\n            if current:\n                endpoint = 'channels/current'\n            else:\n                endpoint = 'channels'\n\n        result = self._call(endpoint, content_type='application/json', params=dict(settings=settings))\n        return [Channel(c) for c in result]", "response": "Returns a list of all channels that have been published to the current user."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_prospects(self, offset=None, orgoffset=None, limit=None):\n        params = {}\n        if limit:\n            params['count'] = limit\n\n        if offset:\n            params['timeOffset'] = offset\n            params['orgOffset'] = orgoffset\n\n        return self._call('timeline', params)", "response": "Return the prospects for the current API key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsearching for prospects by city reion or country.", "response": "def search_prospects(self, search_type, query, offset=None, orgoffset=None):\n        \"\"\" Supports doing a search for prospects by city, reion, or country.\n\n        search_type should be one of 'city' 'region' 'country'.\n\n        This method is intended to be called with one of the outputs from the\n        get_options_for_query method above.\n\n        \"\"\"\n        \n        params = {'q': query}\n        if offset and orgoffset:\n            params['orgOffset'] = orgoffset\n            params['timeOffset'] = offset\n          \n        return self._call('search/%s' % search_type, params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the current user from the request and prepares and connects a signal receiver with the user already attached to it.", "response": "def process_request(self, request):\n        \"\"\"\n        Gets the current user from the request and prepares and connects a signal receiver with the user already\n        attached to it.\n        \"\"\"\n        # Initialize thread local storage\n        threadlocal.auditlog = {\n            'signal_duid': (self.__class__, time.time()),\n            'remote_addr': request.META.get('REMOTE_ADDR'),\n        }\n\n        # In case of proxy, set 'original' address\n        if request.META.get('HTTP_X_FORWARDED_FOR'):\n            threadlocal.auditlog['remote_addr'] = request.META.get('HTTP_X_FORWARDED_FOR').split(',')[0]\n\n        # Connect signal for automatic logging\n        if hasattr(request, 'user') and is_authenticated(request.user):\n            set_actor = curry(self.set_actor, user=request.user, signal_duid=threadlocal.auditlog['signal_duid'])\n            pre_save.connect(set_actor, sender=LogEntry, dispatch_uid=threadlocal.auditlog['signal_duid'], weak=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_response(self, request, response):\n        if hasattr(threadlocal, 'auditlog'):\n            pre_save.disconnect(sender=LogEntry, dispatch_uid=threadlocal.auditlog['signal_duid'])\n\n        return response", "response": "Disconnects the signal receiver to prevent it from staying active."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_exception(self, request, exception):\n        if hasattr(threadlocal, 'auditlog'):\n            pre_save.disconnect(sender=LogEntry, dispatch_uid=threadlocal.auditlog['signal_duid'])\n\n        return None", "response": "Disconnects the signal receiver to prevent it from staying active in case of an exception."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_actor(user, sender, instance, signal_duid, **kwargs):\n        if hasattr(threadlocal, 'auditlog'):\n            if signal_duid != threadlocal.auditlog['signal_duid']:\n                return\n            try:\n                app_label, model_name = settings.AUTH_USER_MODEL.split('.')\n                auth_user_model = apps.get_model(app_label, model_name)\n            except ValueError:\n                auth_user_model = apps.get_model('auth', 'user')\n            if sender == LogEntry and isinstance(user, auth_user_model) and instance.actor is None:\n                instance.actor = user\n\n            instance.remote_addr = threadlocal.auditlog['remote_addr']", "response": "Set the actor of the current LogEntry instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn whether the given field should be tracked by Auditlog.", "response": "def track_field(field):\n    \"\"\"\n    Returns whether the given field should be tracked by Auditlog.\n\n    Untracked fields are many-to-many relations and relations to the Auditlog LogEntry model.\n\n    :param field: The field to check.\n    :type field: Field\n    :return: Whether the given field should be tracked.\n    :rtype: bool\n    \"\"\"\n    from auditlog.models import LogEntry\n    # Do not track many to many relations\n    if field.many_to_many:\n        return False\n\n    # Do not track relations to LogEntry\n    if getattr(field, 'remote_field', None) is not None and field.remote_field.model == LogEntry:\n        return False\n\n    # 1.8 check\n    elif getattr(field, 'rel', None) is not None and field.rel.to == LogEntry:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_fields_in_model(instance):\n    assert isinstance(instance, Model)\n\n    # Check if the Django 1.8 _meta API is available\n    use_api = hasattr(instance._meta, 'get_fields') and callable(instance._meta.get_fields)\n\n    if use_api:\n        return [f for f in instance._meta.get_fields() if track_field(f)]\n    return instance._meta.fields", "response": "Returns the list of fields in the given model instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the differences between two model instances.", "response": "def model_instance_diff(old, new):\n    \"\"\"\n    Calculates the differences between two model instances. One of the instances may be ``None`` (i.e., a newly\n    created model or deleted model). This will cause all fields with a value to have changed (from ``None``).\n\n    :param old: The old state of the model instance.\n    :type old: Model\n    :param new: The new state of the model instance.\n    :type new: Model\n    :return: A dictionary with the names of the changed fields as keys and a two tuple of the old and new field values\n             as value.\n    :rtype: dict\n    \"\"\"\n    from auditlog.registry import auditlog\n\n    if not(old is None or isinstance(old, Model)):\n        raise TypeError(\"The supplied old instance is not a valid model instance.\")\n    if not(new is None or isinstance(new, Model)):\n        raise TypeError(\"The supplied new instance is not a valid model instance.\")\n\n    diff = {}\n\n    if old is not None and new is not None:\n        fields = set(old._meta.fields + new._meta.fields)\n        model_fields = auditlog.get_model_fields(new._meta.model)\n    elif old is not None:\n        fields = set(get_fields_in_model(old))\n        model_fields = auditlog.get_model_fields(old._meta.model)\n    elif new is not None:\n        fields = set(get_fields_in_model(new))\n        model_fields = auditlog.get_model_fields(new._meta.model)\n    else:\n        fields = set()\n        model_fields = None\n\n    # Check if fields must be filtered\n    if model_fields and (model_fields['include_fields'] or model_fields['exclude_fields']) and fields:\n        filtered_fields = []\n        if model_fields['include_fields']:\n            filtered_fields = [field for field in fields\n                               if field.name in model_fields['include_fields']]\n        else:\n            filtered_fields = fields\n        if model_fields['exclude_fields']:\n            filtered_fields = [field for field in filtered_fields\n                               if field.name not in model_fields['exclude_fields']]\n        fields = filtered_fields\n\n    for field in fields:\n        old_value = get_field_value(old, field)\n        new_value = get_field_value(new, field)\n\n        if old_value != new_value:\n            diff[field.name] = (smart_text(old_value), smart_text(new_value))\n\n    if len(diff) == 0:\n        diff = None\n\n    return diff"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_authenticated(user):\n    if not hasattr(user, 'is_authenticated'):\n       return False\n    if callable(user.is_authenticated):\n        # Will be callable if django.version < 2.0, but is only necessary in\n        # v1.9 and earlier due to change introduced in v1.10 making\n        # `is_authenticated` a property instead of a callable.\n        return user.is_authenticated()\n    else:\n        return user.is_authenticated", "response": "Returns whether or not a User is authenticated."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsignaling receiver that creates a log entry when a model instance is first saved to the database.", "response": "def log_create(sender, instance, created, **kwargs):\n    \"\"\"\n    Signal receiver that creates a log entry when a model instance is first saved to the database.\n\n    Direct use is discouraged, connect your model through :py:func:`auditlog.registry.register` instead.\n    \"\"\"\n    if created:\n        changes = model_instance_diff(None, instance)\n\n        log_entry = LogEntry.objects.log_create(\n            instance,\n            action=LogEntry.Action.CREATE,\n            changes=json.dumps(changes),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef log_update(sender, instance, **kwargs):\n    if instance.pk is not None:\n        try:\n            old = sender.objects.get(pk=instance.pk)\n        except sender.DoesNotExist:\n            pass\n        else:\n            new = instance\n\n            changes = model_instance_diff(old, new)\n\n            # Log an entry only if there are changes\n            if changes:\n                log_entry = LogEntry.objects.log_create(\n                    instance,\n                    action=LogEntry.Action.UPDATE,\n                    changes=json.dumps(changes),\n                )", "response": "Signal receiver that creates a log entry when a model instance is changed and saved to the database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsignal receiver that creates a log entry when a model instance is deleted from the database.", "response": "def log_delete(sender, instance, **kwargs):\n    \"\"\"\n    Signal receiver that creates a log entry when a model instance is deleted from the database.\n\n    Direct use is discouraged, connect your model through :py:func:`auditlog.registry.register` instead.\n    \"\"\"\n    if instance.pk is not None:\n        changes = model_instance_diff(instance, None)\n\n        log_entry = LogEntry.objects.log_create(\n            instance,\n            action=LogEntry.Action.DELETE,\n            changes=json.dumps(changes),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register(self, model=None, include_fields=[], exclude_fields=[], mapping_fields={}):\n        def registrar(cls):\n            \"\"\"Register models for a given class.\"\"\"\n            if not issubclass(cls, Model):\n                raise TypeError(\"Supplied model is not a valid model.\")\n\n            self._registry[cls] = {\n                'include_fields': include_fields,\n                'exclude_fields': exclude_fields,\n                'mapping_fields': mapping_fields,\n            }\n            self._connect_signals(cls)\n\n            # We need to return the class, as the decorator is basically\n            # syntactic sugar for:\n            # MyClass = auditlog.register(MyClass)\n            return cls\n\n        if model is None:\n            # If we're being used as a decorator, return a callable with the\n            # wrapper.\n            return lambda cls: registrar(cls)\n        else:\n            # Otherwise, just register the model.\n            registrar(model)", "response": "Register a new model with auditlog."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconnect the signals for the given model.", "response": "def _connect_signals(self, model):\n        \"\"\"\n        Connect signals for the model.\n        \"\"\"\n        for signal in self._signals:\n            receiver = self._signals[signal]\n            signal.connect(receiver, sender=model, dispatch_uid=self._dispatch_uid(signal, model))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _disconnect_signals(self, model):\n        for signal, receiver in self._signals.items():\n            signal.disconnect(sender=model, dispatch_uid=self._dispatch_uid(signal, model))", "response": "Disconnects all the signals for the given model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef log_create(self, instance, **kwargs):\n        changes = kwargs.get('changes', None)\n        pk = self._get_pk_value(instance)\n\n        if changes is not None:\n            kwargs.setdefault('content_type', ContentType.objects.get_for_model(instance))\n            kwargs.setdefault('object_pk', pk)\n            kwargs.setdefault('object_repr', smart_text(instance))\n\n            if isinstance(pk, integer_types):\n                kwargs.setdefault('object_id', pk)\n\n            get_additional_data = getattr(instance, 'get_additional_data', None)\n            if callable(get_additional_data):\n                kwargs.setdefault('additional_data', get_additional_data())\n\n            # Delete log entries with the same pk as a newly created model. This should only be necessary when an pk is\n            # used twice.\n            if kwargs.get('action', None) is LogEntry.Action.CREATE:\n                if kwargs.get('object_id', None) is not None and self.filter(content_type=kwargs.get('content_type'), object_id=kwargs.get('object_id')).exists():\n                    self.filter(content_type=kwargs.get('content_type'), object_id=kwargs.get('object_id')).delete()\n                else:\n                    self.filter(content_type=kwargs.get('content_type'), object_pk=kwargs.get('object_pk', '')).delete()\n            # save LogEntry to same database instance is using\n            db = instance._state.db\n            return self.create(**kwargs) if db is None or db == '' else self.using(db).create(**kwargs)\n        return None", "response": "This method creates a new LogEntry object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_for_object(self, instance):\n        # Return empty queryset if the given model instance is not a model instance.\n        if not isinstance(instance, models.Model):\n            return self.none()\n\n        content_type = ContentType.objects.get_for_model(instance.__class__)\n        pk = self._get_pk_value(instance)\n\n        if isinstance(pk, integer_types):\n            return self.filter(content_type=content_type, object_id=pk)\n        else:\n            return self.filter(content_type=content_type, object_pk=smart_text(pk))", "response": "Returns a QuerySet of log entries for the given model instance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the log entries for the objects in the specified queryset.", "response": "def get_for_objects(self, queryset):\n        \"\"\"\n        Get log entries for the objects in the specified queryset.\n\n        :param queryset: The queryset to get the log entries for.\n        :type queryset: QuerySet\n        :return: The LogEntry objects for the objects in the given queryset.\n        :rtype: QuerySet\n        \"\"\"\n        if not isinstance(queryset, QuerySet) or queryset.count() == 0:\n            return self.none()\n\n        content_type = ContentType.objects.get_for_model(queryset.model)\n        primary_keys = list(queryset.values_list(queryset.model._meta.pk.name, flat=True))\n\n        if isinstance(primary_keys[0], integer_types):\n            return self.filter(content_type=content_type).filter(Q(object_id__in=primary_keys)).distinct()\n        elif isinstance(queryset.model._meta.pk, models.UUIDField):\n            primary_keys = [smart_text(pk) for pk in primary_keys]\n            return self.filter(content_type=content_type).filter(Q(object_pk__in=primary_keys)).distinct()\n        else:\n            return self.filter(content_type=content_type).filter(Q(object_pk__in=primary_keys)).distinct()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting all log entries for a given model.", "response": "def get_for_model(self, model):\n        \"\"\"\n        Get log entries for all objects of a specified type.\n\n        :param model: The model to get log entries for.\n        :type model: class\n        :return: QuerySet of log entries for the given model.\n        :rtype: QuerySet\n        \"\"\"\n        # Return empty queryset if the given object is not valid.\n        if not issubclass(model, models.Model):\n            return self.none()\n\n        content_type = ContentType.objects.get_for_model(model)\n\n        return self.filter(content_type=content_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_pk_value(self, instance):\n        pk_field = instance._meta.pk.name\n        pk = getattr(instance, pk_field, None)\n\n        # Check to make sure that we got an pk not a model object.\n        if isinstance(pk, models.Model):\n            pk = self._get_pk_value(pk)\n        return pk", "response": "Get the primary key value for a model instance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning all objects related to ``objs`` via this ``GenericRelation``.", "response": "def bulk_related_objects(self, objs, using=DEFAULT_DB_ALIAS):\n        \"\"\"\n        Return all objects related to ``objs`` via this ``GenericRelation``.\n        \"\"\"\n        if self.delete_related:\n            return super(AuditlogHistoryField, self).bulk_related_objects(objs, using)\n\n        # When deleting, Collector.collect() finds related objects using this\n        # method.  However, because we don't want to delete these related\n        # objects, we simply return an empty list.\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_stockprices(chart_range='1y'):\n    '''\n    This is a proxy to the main fetch function to cache\n    the result based on the chart range parameter.\n    '''\n\n    all_symbols = list_symbols()\n\n    @daily_cache(filename='iex_chart_{}'.format(chart_range))\n    def get_stockprices_cached(all_symbols):\n        return _get_stockprices(all_symbols, chart_range)\n\n    return get_stockprices_cached(all_symbols)", "response": "Returns a generator that returns the stock prices for the current chart range."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets stock data from IEX.", "response": "def _get_stockprices(symbols, chart_range='1y'):\n    '''Get stock data (key stats and previous) from IEX.\n    Just deal with IEX's 100 stocks limit per request.\n    '''\n\n    def fetch(symbols):\n        charts = _ensure_dict(\n            iexfinance.stocks.Stock(symbols).get_chart(range=chart_range),\n            symbols\n        )\n        result = {}\n        for symbol, obj in charts.items():\n            df = pd.DataFrame(\n                obj,\n                columns=('date', 'open', 'high', 'low', 'close', 'volume'),\n            ).set_index('date')\n            df.index = pd.to_datetime(df.index, utc=True)\n            result[symbol] = df\n        return result\n\n    return parallelize(fetch, splitlen=99)(symbols)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing a lifetimes matrix from our AssetFinder and then drop columns that didn t exist at all during the query dates.", "response": "def _compute_root_mask(self, start_date, end_date, extra_rows):\n        \"\"\"\n        Compute a lifetimes matrix from our AssetFinder, then drop columns that\n        didn't exist at all during the query dates.\n\n        Parameters\n        ----------\n        start_date : pd.Timestamp\n            Base start date for the matrix.\n        end_date : pd.Timestamp\n            End date for the matrix.\n        extra_rows : int\n            Number of extra rows to compute before `start_date`.\n            Extra rows are needed by terms like moving averages that require a\n            trailing window of data.\n\n        Returns\n        -------\n        lifetimes : pd.DataFrame\n            Frame of dtype `bool` containing dates from `extra_rows` days\n            before `start_date`, continuing through to `end_date`.  The\n            returned frame contains as columns all assets in our AssetFinder\n            that existed for at least one day between `start_date` and\n            `end_date`.\n        \"\"\"\n        calendar = self._calendar\n        start_idx, end_idx = calendar.slice_locs(start_date, end_date)\n        if start_idx < extra_rows:\n            raise NoFurtherDataError.from_lookback_window(\n                initial_message=\"Insufficient data to compute Pipeline:\",\n                first_date=calendar[0],\n                lookback_start=start_date,\n                lookback_length=extra_rows,\n            )\n\n        # Build lifetimes matrix reaching back to `extra_rows` days before\n        # `start_date.`\n        symbols = self._list_symbols()\n        dates = calendar[start_idx - extra_rows:end_idx]\n        symbols = sorted(symbols)\n        lifetimes = pd.DataFrame(True, index=dates, columns=symbols)\n\n        assert lifetimes.index[extra_rows] == start_date\n        assert lifetimes.index[-1] == end_date\n        if not lifetimes.columns.unique:\n            columns = lifetimes.columns\n            duplicated = columns[columns.duplicated()].unique()\n            raise AssertionError(\"Duplicated sids: %d\" % duplicated)\n\n        # Filter out columns that didn't exist between the requested start and\n        # end dates.\n        existed = lifetimes.iloc[extra_rows:].any()\n        ret = lifetimes.loc[:, existed]\n        shape = ret.shape\n        assert shape[0] * shape[1] != 0, 'root mask cannot be empty'\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _inputs_for_term(term, workspace, graph):\n        offsets = graph.offset\n        out = []\n        if term.windowed:\n            # If term is windowed, then all input data should be instances of\n            # AdjustedArray.\n            for input_ in term.inputs:\n                adjusted_array = ensure_adjusted_array(\n                    workspace[input_], input_.missing_value,\n                )\n                out.append(\n                    adjusted_array.traverse(\n                        window_length=term.window_length,\n                        offset=offsets[term, input_],\n                    )\n                )\n        else:\n            # If term is not windowed, input_data may be an AdjustedArray or\n            # np.ndarray.  Coerce the former to the latter.\n            for input_ in term.inputs:\n                input_data = ensure_ndarray(workspace[input_])\n                offset = offsets[term, input_]\n                # OPTIMIZATION: Don't make a copy by doing input_data[0:] if\n                # offset is zero.\n                if offset:\n                    input_data = input_data[offset:]\n                out.append(input_data)\n        return out", "response": "Compute inputs for the given term."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute_chunk(self, graph, dates, symbols, initial_workspace):\n        self._validate_compute_chunk_params(dates, symbols, initial_workspace)\n        # get_loader = self.get_loader\n\n        # Copy the supplied initial workspace so we don't mutate it in place.\n        workspace = initial_workspace.copy()\n\n        # If loadable terms share the same loader and extra_rows, load them all\n        # together.\n        loader_group_key = juxt(\n            lambda x: x.dataset.get_loader(), getitem(\n                graph.extra_rows))\n        loader_groups = groupby(loader_group_key, graph.loadable_terms)\n\n        refcounts = graph.initial_refcounts(workspace)\n\n        for term in graph.execution_order(refcounts):\n            # `term` may have been supplied in `initial_workspace`, and in the\n            # future we may pre-compute loadable terms coming from the same\n            # dataset.  In either case, we will already have an entry for this\n            # term, which we shouldn't re-compute.\n            if term in workspace:\n                continue\n\n            # Asset labels are always the same, but date labels vary by how\n            # many extra rows are needed.\n            mask, mask_dates = graph.mask_and_dates_for_term(\n                term,\n                self._root_mask_term,\n                workspace,\n                dates,\n            )\n\n            if isinstance(term, LoadableTerm):\n                to_load = sorted(\n                    loader_groups[loader_group_key(term)],\n                    key=lambda t: t.dataset\n                )\n                loader = term.dataset.get_loader()\n                loaded = loader.load_adjusted_array(\n                    to_load, mask_dates, symbols, mask,\n                )\n                workspace.update(loaded)\n            else:\n                workspace[term] = term._compute(\n                    self._inputs_for_term(term, workspace, graph),\n                    mask_dates,\n                    symbols,\n                    mask,\n                )\n                if term.ndim == 2:\n                    assert workspace[term].shape == mask.shape\n                else:\n                    assert workspace[term].shape == (mask.shape[0], 1)\n\n                # Decref dependencies of ``term``, and clear any terms whose\n                # refcounts hit 0.\n                for garbage_term in graph.decref_dependencies(term, refcounts):\n                    del workspace[garbage_term]\n\n        out = {}\n        graph_extra_rows = graph.extra_rows\n        for name, term in iteritems(graph.outputs):\n            # Truncate off extra rows from outputs.\n            out[name] = workspace[term][graph_extra_rows[term]:]\n        return out", "response": "Compute the pipeline terms for the requested start and end dates and symbols."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _to_narrow(self, terms, data, mask, dates, symbols):\n        assert len(dates) == 1\n        if not mask.any():\n            # Manually handle the empty DataFrame case. This is a workaround\n            # to pandas failing to tz_localize an empty dataframe with a\n            # MultiIndex. It also saves us the work of applying a known-empty\n            # mask to each array.\n            #\n            # Slicing `dates` here to preserve pandas metadata.\n            # empty_dates = dates[:0]\n            empty_assets = array([], dtype=object)\n            return DataFrame(\n                data={\n                    name: array([], dtype=arr.dtype)\n                    for name, arr in iteritems(data)\n                },\n                index=pd.Index(empty_assets),\n                # index=MultiIndex.from_arrays([empty_dates, empty_assets]),\n            )\n\n        # resolved_assets = array(self._finder.retrieve_all(assets))\n        # dates_kept = repeat_last_axis(dates.values, len(symbols))[mask]\n        # assets_kept = repeat_first_axis(resolved_assets, len(dates))[mask]\n        assets_kept = repeat_first_axis(symbols, len(dates))[mask]\n\n        final_columns = {}\n        for name in data:\n            # Each term that computed an output has its postprocess method\n            # called on the filtered result.\n            #\n            # As of Mon May 2 15:38:47 2016, we only use this to convert\n            # LabelArrays into categoricals.\n            final_columns[name] = terms[name].postprocess(data[name][mask])\n\n        return DataFrame(\n            data=final_columns,\n            index=pd.Index(assets_kept),\n            # index=MultiIndex.from_arrays([dates_kept, assets_kept]),\n        )", "response": "Convert raw computed pipeline results into a DataFrame for public APIs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _validate_compute_chunk_params(\n            self, dates, symbols, initial_workspace):\n        \"\"\"\n        Verify that the values passed to compute_chunk are well-formed.\n        \"\"\"\n        root = self._root_mask_term\n        clsname = type(self).__name__\n\n        # Writing this out explicitly so this errors in testing if we change\n        # the name without updating this line.\n        compute_chunk_name = self.compute_chunk.__name__\n        if root not in initial_workspace:\n            raise AssertionError(\n                \"root_mask values not supplied to {cls}.{method}\".format(\n                    cls=clsname,\n                    method=compute_chunk_name,\n                )\n            )\n\n        shape = initial_workspace[root].shape\n        implied_shape = len(dates), len(symbols)\n        if shape != implied_shape:\n            raise AssertionError(\n                \"root_mask shape is {shape}, but received dates/symbols \"\n                \"imply that shape should be {implied}\".format(\n                    shape=shape,\n                    implied=implied_shape,\n                )\n            )", "response": "Verify that the values passed to compute_chunk are well - formed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef preemphasis(signal, shift=1, cof=0.98):\n\n    rolled_signal = np.roll(signal, shift)\n    return signal - cof * rolled_signal", "response": "Returns the pre - emphasising of the given signal."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nframe a signal into overlapping frames. Args: sig (array): The audio signal to frame of size (N,). sampling_frequency (int): The sampling frequency of the signal. frame_length (float): The length of the frame in second. frame_stride (float): The stride between frames. filter (array): The time-domain filter for applying to each frame. By default it is one so nothing will be changed. zero_padding (bool): If the samples is not a multiple of frame_length(number of frames sample), zero padding will be done for generating last frame. Returns: array: Stacked_frames-Array of frames of size (number_of_frames x frame_len).", "response": "def stack_frames(\n        sig,\n        sampling_frequency,\n        frame_length=0.020,\n        frame_stride=0.020,\n        filter=lambda x: np.ones(\n            (x,\n             )),\n        zero_padding=True):\n    \"\"\"Frame a signal into overlapping frames.\n\n    Args:\n        sig (array): The audio signal to frame of size (N,).\n        sampling_frequency (int): The sampling frequency of the signal.\n        frame_length (float): The length of the frame in second.\n        frame_stride (float): The stride between frames.\n        filter (array): The time-domain filter for applying to each frame.\n            By default it is one so nothing will be changed.\n        zero_padding (bool): If the samples is not a multiple of\n            frame_length(number of frames sample), zero padding will\n            be done for generating last frame.\n\n    Returns:\n            array: Stacked_frames-Array of frames of size (number_of_frames x frame_len).\n\n    \"\"\"\n\n    # Check dimension\n    s = \"Signal dimention should be of the format of (N,) but it is %s instead\"\n    assert sig.ndim == 1, s % str(sig.shape)\n\n    # Initial necessary values\n    length_signal = sig.shape[0]\n    frame_sample_length = int(\n        np.round(\n            sampling_frequency *\n            frame_length))  # Defined by the number of samples\n    frame_stride = float(np.round(sampling_frequency * frame_stride))\n\n    # Zero padding is done for allocating space for the last frame.\n    if zero_padding:\n        # Calculation of number of frames\n        numframes = (int(math.ceil((length_signal\n                                      - frame_sample_length) / frame_stride)))\n        print(numframes,length_signal,frame_sample_length,frame_stride)\n\n        # Zero padding\n        len_sig = int(numframes * frame_stride + frame_sample_length)\n        additive_zeros = np.zeros((len_sig - length_signal,))\n        signal = np.concatenate((sig, additive_zeros))\n\n    else:\n        # No zero padding! The last frame which does not have enough\n        # samples(remaining samples <= frame_sample_length), will be dropped!\n        numframes = int(math.floor((length_signal\n                          - frame_sample_length) / frame_stride))\n\n        # new length\n        len_sig = int((numframes - 1) * frame_stride + frame_sample_length)\n        signal = sig[0:len_sig]\n\n    # Getting the indices of all frames.\n    indices = np.tile(np.arange(0,\n                                frame_sample_length),\n                      (numframes,\n                       1)) + np.tile(np.arange(0,\n                                               numframes * frame_stride,\n                                               frame_stride),\n                                     (frame_sample_length,\n                                      1)).T\n    indices = np.array(indices, dtype=np.int32)\n\n    # Extracting the frames based on the allocated indices.\n    frames = signal[indices]\n\n    # Apply the windows function\n    window = np.tile(filter(frame_sample_length), (numframes, 1))\n    Extracted_Frames = frames * window\n    return Extracted_Frames"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fft_spectrum(frames, fft_points=512):\n    SPECTRUM_VECTOR = np.fft.rfft(frames, n=fft_points, axis=-1, norm=None)\n    return np.absolute(SPECTRUM_VECTOR)", "response": "This function computes the one - dimensional n - point discrete Fourier tree transform of a real - valued array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef log_power_spectrum(frames, fft_points=512, normalize=True):\n    power_spec = power_spectrum(frames, fft_points)\n    power_spec[power_spec <= 1e-20] = 1e-20\n    log_power_spec = 10 * np.log10(power_spec)\n    if normalize:\n        return log_power_spec - np.max(log_power_spec)\n    else:\n        return log_power_spec", "response": "Log power spectrum of each frame in frames."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef derivative_extraction(feat, DeltaWindows):\n\n    # Getting the shape of the vector.\n    rows, cols = feat.shape\n\n    # Difining the vector of differences.\n    DIF = np.zeros(feat.shape, dtype=feat.dtype)\n    Scale = 0\n\n    # Pad only along features in the vector.\n    FEAT = np.lib.pad(feat, ((0, 0), (DeltaWindows, DeltaWindows)), 'edge')\n    for i in range(DeltaWindows):\n        # Start index\n        offset = DeltaWindows\n\n        # The dynamic range\n        Range = i + 1\n\n        dif = Range * FEAT[:, offset + Range:offset + Range + cols]\n        - FEAT[:, offset - Range:offset - Range + cols]\n        Scale += 2 * np.power(Range, 2)\n        DIF += dif\n\n    return DIF / Scale", "response": "This function returns the derivative features along the main feature vector."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cmvnw(vec, win_size=301, variance_normalization=False):\n    # Get the shapes\n    eps = 2**-30\n    rows, cols = vec.shape\n\n    # Windows size must be odd.\n    assert isinstance(win_size, int), \"Size must be of type 'int'!\"\n    assert win_size % 2 == 1, \"Windows size must be odd!\"\n\n    # Padding and initial definitions\n    pad_size = int((win_size - 1) / 2)\n    vec_pad = np.lib.pad(vec, ((pad_size, pad_size), (0, 0)), 'symmetric')\n    mean_subtracted = np.zeros(np.shape(vec), dtype=np.float32)\n\n    for i in range(rows):\n        window = vec_pad[i:i + win_size, :]\n        window_mean = np.mean(window, axis=0)\n        mean_subtracted[i, :] = vec[i, :] - window_mean\n\n    # Variance normalization\n    if variance_normalization:\n\n        # Initial definitions.\n        variance_normalized = np.zeros(np.shape(vec), dtype=np.float32)\n        vec_pad_variance = np.lib.pad(\n            mean_subtracted, ((pad_size, pad_size), (0, 0)), 'symmetric')\n\n        # Looping over all observations.\n        for i in range(rows):\n            window = vec_pad_variance[i:i + win_size, :]\n            window_variance = np.std(window, axis=0)\n            variance_normalized[i, :] \\\n            = mean_subtracted[i, :] / (window_variance + eps)\n        output = variance_normalized\n    else:\n        output = mean_subtracted\n\n    return output", "response": "This function is aimed to perform local cepstral mean and variance normalization on a sliding window."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef zero_handling(x):\n    return np.where(x == 0, np.finfo(float).eps, x)", "response": "This function handles the issue with zero values substituted with epsilon values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filterbanks(\n        num_filter,\n        coefficients,\n        sampling_freq,\n        low_freq=None,\n        high_freq=None):\n    \"\"\"Compute the Mel-filterbanks. Each filter will be stored in one rows.\n    The columns correspond to fft bins.\n\n    Args:\n        num_filter (int): the number of filters in the filterbank, default 20.\n        coefficients (int): (fftpoints//2 + 1). Default is 257.\n        sampling_freq (float): the samplerate of the signal we are working\n            with. It affects mel spacing.\n        low_freq (float): lowest band edge of mel filters, default 0 Hz\n        high_freq (float): highest band edge of mel filters,\n            default samplerate/2\n\n    Returns:\n           array: A numpy array of size num_filter x (fftpoints//2 + 1)\n               which are filterbank\n    \"\"\"\n    high_freq = high_freq or sampling_freq / 2\n    low_freq = low_freq or 300\n    s = \"High frequency cannot be greater than half of the sampling frequency!\"\n    assert high_freq <= sampling_freq / 2, s\n    assert low_freq >= 0, \"low frequency cannot be less than zero!\"\n\n    # Computing the Mel filterbank\n    # converting the upper and lower frequencies to Mels.\n    # num_filter + 2 is because for num_filter filterbanks we need\n    # num_filter+2 point.\n    mels = np.linspace(\n        functions.frequency_to_mel(low_freq),\n        functions.frequency_to_mel(high_freq),\n        num_filter + 2)\n\n    # we should convert Mels back to Hertz because the start and end-points\n    # should be at the desired frequencies.\n    hertz = functions.mel_to_frequency(mels)\n\n    # The frequency resolution required to put filters at the\n    # exact points calculated above should be extracted.\n    #  So we should round those frequencies to the closest FFT bin.\n    freq_index = (\n        np.floor(\n            (coefficients +\n             1) *\n            hertz /\n            sampling_freq)).astype(int)\n\n    # Initial definition\n    filterbank = np.zeros([num_filter, coefficients])\n\n    # The triangular function for each filter\n    for i in range(0, num_filter):\n        left = int(freq_index[i])\n        middle = int(freq_index[i + 1])\n        right = int(freq_index[i + 2])\n        z = np.linspace(left, right, num=right - left + 1)\n        filterbank[i,\n                   left:right + 1] = functions.triangle(z,\n                                                        left=left,\n                                                        middle=middle,\n                                                        right=right)\n\n    return filterbank", "response": "Compute the Mel - filterbanks for a given set of log entries."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute MFCC features from an audio signal.", "response": "def mfcc(\n        signal,\n        sampling_frequency,\n        frame_length=0.020,\n        frame_stride=0.01,\n        num_cepstral=13,\n        num_filters=40,\n        fft_length=512,\n        low_frequency=0,\n        high_frequency=None,\n        dc_elimination=True):\n    \"\"\"Compute MFCC features from an audio signal.\n\n    Args:\n\n         signal (array): the audio signal from which to compute features.\n             Should be an N x 1 array\n         sampling_frequency (int): the sampling frequency of the signal\n             we are working with.\n         frame_length (float): the length of each frame in seconds.\n             Default is 0.020s\n         frame_stride (float): the step between successive frames in seconds.\n             Default is 0.02s (means no overlap)\n         num_filters (int): the number of filters in the filterbank,\n             default 40.\n         fft_length (int): number of FFT points. Default is 512.\n         low_frequency (float): lowest band edge of mel filters.\n             In Hz, default is 0.\n         high_frequency (float): highest band edge of mel filters.\n             In Hz, default is samplerate/2\n         num_cepstral (int): Number of cepstral coefficients.\n         dc_elimination (bool): hIf the first dc component should\n             be eliminated or not.\n\n    Returns:\n        array: A numpy array of size (num_frames x num_cepstral) containing mfcc features.\n    \"\"\"\n    feature, energy = mfe(signal, sampling_frequency=sampling_frequency,\n                          frame_length=frame_length, frame_stride=frame_stride,\n                          num_filters=num_filters, fft_length=fft_length,\n                          low_frequency=low_frequency,\n                          high_frequency=high_frequency)\n    if len(feature) == 0:\n        return np.empty((0, num_cepstral))\n    feature = np.log(feature)\n    feature = dct(feature, type=2, axis=-1, norm='ortho')[:, :num_cepstral]\n\n    # replace first cepstral coefficient with log of frame energy for DC\n    # elimination.\n    if dc_elimination:\n        feature[:, 0] = np.log(energy)\n    return feature"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mfe(signal, sampling_frequency, frame_length=0.020, frame_stride=0.01,\n        num_filters=40, fft_length=512, low_frequency=0, high_frequency=None):\n    \"\"\"Compute Mel-filterbank energy features from an audio signal.\n    \n    Args:\n         signal (array): the audio signal from which to compute features.\n             Should be an N x 1 array\n         sampling_frequency (int): the sampling frequency of the signal\n             we are working with.\n         frame_length (float): the length of each frame in seconds.\n             Default is 0.020s\n         frame_stride (float): the step between successive frames in seconds.\n             Default is 0.02s (means no overlap)\n         num_filters (int): the number of filters in the filterbank,\n             default 40.\n         fft_length (int): number of FFT points. Default is 512.\n         low_frequency (float): lowest band edge of mel filters.\n             In Hz, default is 0.\n         high_frequency (float): highest band edge of mel filters.\n             In Hz, default is samplerate/2\n\n    Returns:\n              array: features - the energy of fiterbank of size num_frames x num_filters. The energy of each frame: num_frames x 1\n    \"\"\"\n\n    # Convert to float\n    signal = signal.astype(float)\n\n    # Stack frames\n    frames = processing.stack_frames(\n        signal,\n        sampling_frequency=sampling_frequency,\n        frame_length=frame_length,\n        frame_stride=frame_stride,\n        filter=lambda x: np.ones(\n            (x,\n             )),\n        zero_padding=False)\n\n    # getting the high frequency\n    high_frequency = high_frequency or sampling_frequency / 2\n\n    # calculation of the power sprectum\n    power_spectrum = processing.power_spectrum(frames, fft_length)\n    coefficients = power_spectrum.shape[1]\n    # this stores the total energy in each frame\n    frame_energies = np.sum(power_spectrum, 1)\n\n    # Handling zero enegies.\n    frame_energies = functions.zero_handling(frame_energies)\n\n    # Extracting the filterbank\n    filter_banks = filterbanks(\n        num_filters,\n        coefficients,\n        sampling_frequency,\n        low_frequency,\n        high_frequency)\n\n    # Filterbank energies\n    features = np.dot(power_spectrum, filter_banks.T)\n    features = functions.zero_handling(features)\n\n    return features, frame_energies", "response": "Compute Mel - filterbank energy features from an audio signal."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute log Mel - filterbank energy features from an audio signal.", "response": "def lmfe(signal, sampling_frequency, frame_length=0.020, frame_stride=0.01,\n         num_filters=40, fft_length=512, low_frequency=0, high_frequency=None):\n    \"\"\"Compute log Mel-filterbank energy features from an audio signal.\n\n\n    Args:\n         signal (array): the audio signal from which to compute features.\n             Should be an N x 1 array\n         sampling_frequency (int): the sampling frequency of the signal\n             we are working with.\n         frame_length (float): the length of each frame in seconds.\n             Default is 0.020s\n         frame_stride (float): the step between successive frames in seconds.\n             Default is 0.02s (means no overlap)\n         num_filters (int): the number of filters in the filterbank,\n             default 40.\n         fft_length (int): number of FFT points. Default is 512.\n         low_frequency (float): lowest band edge of mel filters.\n             In Hz, default is 0.\n         high_frequency (float): highest band edge of mel filters.\n             In Hz, default is samplerate/2\n\n    Returns:\n              array: Features - The log energy of fiterbank of size num_frames x num_filters frame_log_energies. The log energy of each frame num_frames x 1\n    \"\"\"\n\n    feature, frame_energies = mfe(signal,\n                                  sampling_frequency=sampling_frequency,\n                                  frame_length=frame_length,\n                                  frame_stride=frame_stride,\n                                  num_filters=num_filters,\n                                  fft_length=fft_length,\n                                  low_frequency=low_frequency,\n                                  high_frequency=high_frequency)\n    feature = np.log(feature)\n\n    return feature"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves the specified nodes from the cluster and returns the remaining nodes", "response": "def remove_umis(adj_list, cluster, nodes):\n    '''removes the specified nodes from the cluster and returns\n    the remaining nodes '''\n\n    # list incomprehension: for x in nodes: for node in adj_list[x]: yield node\n    nodes_to_remove = set([node\n                           for x in nodes\n                           for node in adj_list[x]] + nodes)\n\n    return cluster - nodes_to_remove"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_substr_slices(umi_length, idx_size):\n    '''\n    Create slices to split a UMI into approximately equal size substrings\n    Returns a list of tuples that can be passed to slice function\n    '''\n    cs, r = divmod(umi_length, idx_size)\n    sub_sizes = [cs + 1] * r + [cs] * (idx_size - r)\n    offset = 0\n    slices = []\n    for s in sub_sizes:\n        slices.append((offset, offset + s))\n        offset += s\n    return slices", "response": "Create slices to split a UMI into approximately equal size substrings\n    Returns a list of tuples that can be passed to slice function\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a dictionary of nearest neighbours using substrings can be used to reduce the number of pairwise comparisons.", "response": "def build_substr_idx(umis, umi_length, min_edit):\n    '''\n    Build a dictionary of nearest neighbours using substrings, can be used\n    to reduce the number of pairwise comparisons.\n    '''\n    substr_idx = collections.defaultdict(\n        lambda: collections.defaultdict(set))\n    slices = get_substr_slices(umi_length, min_edit + 1)\n    for idx in slices:\n        for u in umis:\n            u_sub = u[slice(*idx)]\n            substr_idx[idx][u_sub].add(u)\n    return substr_idx"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iter_nearest_neighbours(umis, substr_idx):\n    '''\n    Added by Matt 06/05/17\n    use substring dict to get (approximately) all the nearest neighbours to\n    each in a set of umis.\n    '''\n    for i, u in enumerate(umis, 1):\n        neighbours = set()\n        for idx, substr_map in substr_idx.items():\n            u_sub = u[slice(*idx)]\n            neighbours = neighbours.union(substr_map[u_sub])\n        neighbours.difference_update(umis[:i])\n        for nbr in neighbours:\n            yield u, nbr", "response": "Iterate over all the nearest neighbours to\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_best_min_account(self, cluster, adj_list, counts):\n        ''' return the min UMI(s) need to account for cluster'''\n        if len(cluster) == 1:\n            return list(cluster)\n\n        sorted_nodes = sorted(cluster, key=lambda x: counts[x],\n                              reverse=True)\n\n        for i in range(len(sorted_nodes) - 1):\n            if len(remove_umis(adj_list, cluster, sorted_nodes[:i+1])) == 0:\n                return sorted_nodes[:i+1]", "response": "return the best min account for cluster"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_best_percentile(self, cluster, counts):\n        ''' return all UMIs with counts >1% of the\n        median counts in the cluster '''\n\n        if len(cluster) == 1:\n            return list(cluster)\n        else:\n            threshold = np.median(list(counts.values()))/100\n            return [read for read in cluster if counts[read] > threshold]", "response": "return the best percentile UMIs in the cluster"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_adj_list_adjacency(self, umis, counts, threshold):\n        ''' identify all umis within hamming distance threshold'''\n\n        adj_list = {umi: [] for umi in umis}\n        if len(umis) > 25:\n            umi_length = len(umis[0])\n            substr_idx = build_substr_idx(umis, umi_length, threshold)\n            iter_umi_pairs = iter_nearest_neighbours(umis, substr_idx)\n        else:\n            iter_umi_pairs = itertools.combinations(umis, 2)\n        for umi1, umi2 in iter_umi_pairs:\n            if edit_distance(umi1, umi2) <= threshold:\n                adj_list[umi1].append(umi2)\n                adj_list[umi2].append(umi1)\n\n        return adj_list", "response": "identify all umis within hamming distance threshold"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _group_unique(self, clusters, adj_list, counts):\n        ''' return groups for unique method'''\n        if len(clusters) == 1:\n            groups = [clusters]\n        else:\n            groups = [[x] for x in clusters]\n\n        return groups", "response": "return groups for unique method"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn groups for directional method", "response": "def _group_directional(self, clusters, adj_list, counts):\n        ''' return groups for directional method'''\n\n        observed = set()\n        groups = []\n        for cluster in clusters:\n            if len(cluster) == 1:\n                groups.append(list(cluster))\n                observed.update(cluster)\n            else:\n                cluster = sorted(cluster, key=lambda x: counts[x],\n                                 reverse=True)\n                # need to remove any node which has already been observed\n                temp_cluster = []\n                for node in cluster:\n                    if node not in observed:\n                        temp_cluster.append(node)\n                        observed.add(node)\n                groups.append(temp_cluster)\n\n        return groups"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _group_adjacency(self, clusters, adj_list, counts):\n        ''' return groups for adjacency method'''\n\n        groups = []\n\n        for cluster in clusters:\n            if len(cluster) == 1:\n                groups.append(list(cluster))\n\n            else:\n                observed = set()\n\n                lead_umis = self._get_best_min_account(cluster,\n                                                       adj_list, counts)\n                observed.update(lead_umis)\n\n                for lead_umi in lead_umis:\n                    connected_nodes = set(adj_list[lead_umi])\n                    groups.append([lead_umi] +\n                                  list(connected_nodes - observed))\n                    observed.update(connected_nodes)\n\n        return groups", "response": "return groups for adjacency method"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _group_cluster(self, clusters, adj_list, counts):\n        ''' return groups for cluster or directional methods'''\n\n        groups = []\n        for cluster in clusters:\n            groups.append(sorted(cluster, key=lambda x: counts[x],\n                                 reverse=True))\n\n        return groups", "response": "return groups for cluster or directional methods"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the groups for the percentile method.", "response": "def _group_percentile(self, clusters, adj_list, counts):\n        ''' Return \"groups\" for the the percentile method. Note\n        that grouping isn't really compatible with the percentile\n        method. This just returns the retained UMIs in a structure similar\n        to other methods '''\n\n        retained_umis = self._get_best_percentile(clusters, counts)\n        groups = [[x] for x in retained_umis]\n\n        return groups"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_adj_list_directional(self, umis, counts):\n        ''' identify all umis within the hamming distance threshold\n        and where the counts of the first umi is > (2 * second umi counts)-1'''\n\n        adj_list = {umi: [] for umi in umis}\n\n        if self.fuzzy_match:\n            for umi1 in umis:\n                # we need a second regex for some insertions,\n                # e.g UMI1 = \"ATCG\", UMI2 = \"ATTC\"\n                comp_regex_err = regex.compile(\"(%s){e<=1}\" % str(umi1))\n                comp_regex_del = regex.compile(\"(%s){i<=1}\" % str(umi1)[::-1])\n                for umi2 in umis:\n                    if umi1 == umi2:\n                        continue\n                    if counts[umi1] >= (counts[umi2]*self.dir_threshold):\n                        if (max(len(umi1), len(umi2)) -\n                            min(len(umi1), len(umi2))) > 1:\n                            continue\n                        if (comp_regex_err.match(str(umi2)) or\n                            comp_regex_del.match(str(umi2))):\n                            adj_list[umi1].append(umi2)\n        else:\n            for umi1, umi2 in itertools.combinations(umis, 2):\n                if edit_distance(umi1, umi2) <= 1:\n                    if counts[umi1] >= (counts[umi2]*2)-1:\n                        adj_list[umi1].append(umi2)\n                    if counts[umi2] >= (counts[umi1]*2)-1:\n                        adj_list[umi2].append(umi1)\n\n        return adj_list", "response": "identify all umis within the hamming distance threshold\n        and where the counts of the first umi is > 2 * second umi counts - 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the connected components within an adjacency dictionary", "response": "def _get_connected_components_adjacency(self, graph, counts):\n        ''' find the connected UMIs within an adjacency dictionary'''\n\n        found = set()\n        components = list()\n\n        for node in sorted(graph, key=lambda x: counts[x], reverse=True):\n            if node not in found:\n                # component = self.search(node, graph)\n                component = breadth_first_search(node, graph)\n                found.update(component)\n                components.append(component)\n        return components"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef callbackShortHelp(option, opt, value, parser):\n    '''output short help (only command line options).'''\n    # clear usage and description\n    parser.set_description(None)\n    # parser.set_usage(None)\n    # output help\n    parser.print_help()\n    # exit\n    parser.exit()", "response": "callback for command line options."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef openFile(filename, mode=\"r\", create_dir=False):\n    '''open file in *filename* with mode *mode*.\n\n    If *create* is set, the directory containing filename\n    will be created if it does not exist.\n\n    gzip - compressed files are recognized by the\n    suffix ``.gz`` and opened transparently.\n\n    Note that there are differences in the file\n    like objects returned, for example in the\n    ability to seek.\n\n    returns a file or file-like object.\n    '''\n\n    _, ext = os.path.splitext(filename)\n\n    if create_dir:\n        dirname = os.path.dirname(filename)\n        if dirname and not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n    if ext.lower() in (\".gz\", \".z\"):\n        if sys.version_info.major >= 3:\n            if mode == \"r\":\n                return gzip.open(filename, 'rt', encoding=\"ascii\")\n            elif mode == \"w\":\n                return gzip.open(filename, 'wt',\n                                 compresslevel=global_options.compresslevel,\n                                 encoding=\"ascii\")\n            else:\n                raise NotImplementedError(\n                    \"mode '{}' not implemented\".format(mode))\n        else:\n            return gzip.open(filename, mode,\n                             compresslevel=global_options.compresslevel)\n    else:\n        return open(filename, mode)", "response": "open file in filename with mode * mode*."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getHeader():\n    system, host, release, version, machine = os.uname()\n\n    return \"# UMI-tools version: %s\\n# output generated by %s\\n# job started at %s on %s -- %s\\n# pid: %i, system: %s %s %s %s\" %\\\n           (__version__,\n            \" \".join(sys.argv),\n            time.asctime(time.localtime(time.time())),\n            host,\n            global_id,\n            os.getpid(),\n            system, release, version, machine)", "response": "return a header string with command line options and timestamp\n              "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a string containing script parameters.", "response": "def getParams(options=None):\n    \"\"\"return a string containing script parameters.\n\n    Parameters are all variables that start with ``param_``.\n    \"\"\"\n    result = []\n    if options:\n        members = options.__dict__\n        for k, v in sorted(members.items()):\n            result.append(\"# %-40s: %s\" % (k, str(v)))\n    else:\n        vars = inspect.currentframe().f_back.f_locals\n        for var in filter(lambda x: re.match(\"param_\", x), vars.keys()):\n            result.append(\"# %-40s: %s\" %\n                          (var, str(vars[var])))\n\n    if result:\n        return \"\\n\".join(result)\n    else:\n        return \"# no parameters.\""}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a header string with command line options and timestamp.", "response": "def getFooter():\n    \"\"\"return a header string with command line options and\n    timestamp.\n    \"\"\"\n    return \"# job finished in %i seconds at %s -- %s -- %s\" %\\\n           (time.time() - global_starting_time,\n            time.asctime(time.localtime(time.time())),\n            \" \".join(map(lambda x: \"%5.2f\" % x, os.times()[:4])),\n            global_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset up an experiment. The :py:func:`Start` method will set up a file logger and add some default and some optional options to the command line parser. It will then parse the command line and set up input/output redirection and start a timer for benchmarking purposes. The default options added by this method are: ``-v/--verbose`` the :term:`loglevel` ``timeit`` turn on benchmarking information and save to file ``timeit-name`` name to use for timing information, ``timeit-header`` output header for timing information. ``seed`` the random seed. If given, the python random number generator will be initialized with this seed. Optional options added are: Arguments --------- param parser : :py:class:`U.OptionParser` instance with command line options. argv : list command line options to parse. Defaults to :py:data:`sys.argv` quiet : bool set :term:`loglevel` to 0 - no logging return_parser : bool return the parser object, no parsing. Useful for inspecting the command line options of a script without running it. add_pipe_options : bool add common options for redirecting input/output add_extract_options : bool add options for extracting barcodes add_sam_options : bool add options for SAM/BAM input add_umi_grouping_options : bool add options for barcode grouping add_group_dedup_options : bool add options for UMI grouping and deduping Returns ------- tuple (:py:class:`U.OptionParser` object, list of positional arguments)", "response": "def Start(parser=None,\n          argv=sys.argv,\n          quiet=False,\n          add_pipe_options=True,\n          add_extract_options=False,\n          add_group_dedup_options=True,\n          add_sam_options=True,\n          add_umi_grouping_options=True,\n          return_parser=False):\n    \"\"\"set up an experiment.\n\n    The :py:func:`Start` method will set up a file logger and add some\n    default and some optional options to the command line parser.  It\n    will then parse the command line and set up input/output\n    redirection and start a timer for benchmarking purposes.\n\n    The default options added by this method are:\n\n    ``-v/--verbose``\n        the :term:`loglevel`\n\n    ``timeit``\n        turn on benchmarking information and save to file\n\n    ``timeit-name``\n         name to use for timing information,\n\n    ``timeit-header``\n         output header for timing information.\n\n    ``seed``\n         the random seed. If given, the python random\n         number generator will be initialized with this\n         seed.\n\n    Optional options added are:\n\n    Arguments\n    ---------\n\n    param parser : :py:class:`U.OptionParser`\n       instance with command line options.\n\n    argv : list\n        command line options to parse. Defaults to\n        :py:data:`sys.argv`\n\n    quiet : bool\n        set :term:`loglevel` to 0 - no logging\n\n    return_parser : bool\n        return the parser object, no parsing. Useful for inspecting\n        the command line options of a script without running it.\n\n    add_pipe_options : bool\n        add common options for redirecting input/output\n\n    add_extract_options : bool\n        add options for extracting barcodes\n\n    add_sam_options : bool\n        add options for SAM/BAM input\n\n    add_umi_grouping_options : bool\n        add options for barcode grouping\n\n    add_group_dedup_options : bool\n        add options for UMI grouping and deduping\n\n    Returns\n    -------\n    tuple\n       (:py:class:`U.OptionParser` object, list of positional\n       arguments)\n\n    \"\"\"\n\n    if not parser:\n        parser = OptionParser(\n            version=\"%prog version: $Id$\")\n\n    global global_options, global_args, global_starting_time\n\n    # save default values given by user\n    user_defaults = copy.copy(parser.defaults)\n\n    global_starting_time = time.time()\n\n    if add_extract_options:\n\n        group = OptionGroup(parser, \"fastq barcode extraction options\")\n\n        group.add_option(\"--extract-method\",\n                         dest=\"extract_method\", type=\"choice\",\n                         choices=[\"string\", \"regex\"],\n                         help=(\"How to extract the umi +/- cell barcodes, \"\n                               \"Choose from 'string' or 'regex'\"))\n        group.add_option(\"-p\", \"--bc-pattern\", dest=\"pattern\", type=\"string\",\n                         help=\"Barcode pattern\")\n        group.add_option(\"--bc-pattern2\", dest=\"pattern2\", type=\"string\",\n                         help=\"Barcode pattern for paired reads\")\n        group.add_option(\"--3prime\", dest=\"prime3\", action=\"store_true\",\n                         help=\"barcode is on 3' end of read.\")\n        group.add_option(\"--read2-in\", dest=\"read2_in\", type=\"string\",\n                         help=\"file name for read pairs\")\n        parser.add_option_group(group)\n\n    if add_sam_options:\n        group = OptionGroup(parser, \"Barcode extraction options\")\n\n        group.add_option(\"--extract-umi-method\", dest=\"get_umi_method\", type=\"choice\",\n                         choices=(\"read_id\", \"tag\", \"umis\"), default=\"read_id\",\n                         help=\"how is the read UMI +/ cell barcode encoded? \"\n                         \"[default=%default]\")\n\n        group.add_option(\"--umi-separator\", dest=\"umi_sep\",\n                         type=\"string\", help=\"separator between read id and UMI\",\n                         default=\"_\")\n\n        group.add_option(\"--umi-tag\", dest=\"umi_tag\",\n                         type=\"string\", help=\"tag containing umi\",\n                         default='RX')\n\n        group.add_option(\"--umi-tag-split\", dest=\"umi_tag_split\",\n                         type=\"string\",\n                         help=\"split UMI in tag and take the first element\",\n                         default=None)\n\n        group.add_option(\"--umi-tag-delimiter\", dest=\"umi_tag_delim\",\n                         type=\"string\",\n                         help=\"concatenate UMI in tag separated by delimiter\",\n                         default=None)\n\n        group.add_option(\"--cell-tag\", dest=\"cell_tag\",\n                         type=\"string\", help=\"tag containing cell barcode\",\n                         default=None)\n\n        group.add_option(\"--cell-tag-split\", dest=\"cell_tag_split\",\n                         type=\"string\",\n                         help=(\"split cell barcode in tag and take the first element\"\n                               \"for e.g 10X GEM tags\"),\n                         default='-')\n\n        group.add_option(\"--cell-tag-delimiter\", dest=\"cell_tag_delim\",\n                         type=\"string\",\n                         help=\"concatenate cell barcode in tag separated by delimiter\",\n                         default=None)\n\n        parser.add_option_group(group)\n\n    if add_umi_grouping_options:\n        group = OptionGroup(parser, \"UMI grouping options\")\n\n        group.add_option(\"--method\", dest=\"method\", type=\"choice\",\n                         choices=(\"adjacency\", \"directional\",\n                                  \"percentile\", \"unique\", \"cluster\"),\n                         default=\"directional\",\n                         help=\"method to use for umi grouping [default=%default]\")\n\n        group.add_option(\"--edit-distance-threshold\", dest=\"threshold\",\n                         type=\"int\",\n                         default=1,\n                         help=\"Edit distance theshold at which to join two UMIs \"\n                         \"when grouping UMIs. [default=%default]\")\n\n        group.add_option(\"--spliced-is-unique\", dest=\"spliced\",\n                         action=\"store_true\",\n                         help=\"Treat a spliced read as different to an unspliced\"\n                         \" one [default=%default]\",\n                         default=False)\n\n        group.add_option(\"--soft-clip-threshold\", dest=\"soft_clip_threshold\",\n                         type=\"float\",\n                         help=\"number of bases clipped from 5' end before \"\n                         \"read is counted as spliced [default=%default]\",\n                         default=4)\n\n        group.add_option(\"--read-length\", dest=\"read_length\",\n                         action=\"store_true\", default=False,\n                         help=\"use read length in addition to position and UMI \"\n                         \"to identify possible duplicates [default=%default]\")\n\n        parser.add_option_group(group)\n\n    if add_sam_options:\n        group = OptionGroup(parser, \"single-cell RNA-Seq options\")\n\n        group.add_option(\"--per-gene\", dest=\"per_gene\", action=\"store_true\",\n                         default=False,\n                         help=\"Group/Dedup/Count per gene. Must combine with \"\n                         \"either --gene-tag or --per-contig\")\n\n        group.add_option(\"--gene-tag\", dest=\"gene_tag\",\n                         type=\"string\",\n                         help=\"Gene is defined by this bam tag [default=%default]\",\n                         default=None)\n\n        group.add_option(\"--assigned-status-tag\", dest=\"assigned_tag\",\n                         type=\"string\",\n                         help=\"Bam tag describing whether read is assigned to a gene \"\n                         \"By defualt, this is set as the same tag as --gene-tag\",\n                         default=None)\n\n        group.add_option(\"--skip-tags-regex\", dest=\"skip_regex\",\n                         type=\"string\",\n                         help=\"Used with --gene-tag. \"\n                         \"Ignore reads where the gene-tag matches this regex\",\n                         default=\"^(__|Unassigned)\")\n\n        group.add_option(\"--per-contig\", dest=\"per_contig\", action=\"store_true\",\n                         default=False,\n                         help=\"group/dedup/count UMIs per contig (field 3 in BAM; RNAME),\"\n                         \" e.g for transcriptome where contig = gene\")\n\n        group.add_option(\"--gene-transcript-map\", dest=\"gene_transcript_map\",\n                         type=\"string\",\n                         help=\"File mapping transcripts to genes (tab separated)\",\n                         default=None)\n\n        group.add_option(\"--per-cell\", dest=\"per_cell\", action=\"store_true\",\n                         default=False,\n                         help=\"group/dedup/count per cell\")\n\n        parser.add_option_group(group)\n\n    if add_group_dedup_options:\n\n        group = OptionGroup(parser, \"group/dedup options\")\n\n        group.add_option(\"--buffer-whole-contig\", dest=\"whole_contig\",\n                         action=\"store_true\", default=False,\n                         help=\"Read whole contig before outputting bundles: \"\n                         \"guarantees that no reads are missed, but increases \"\n                         \"memory usage\")\n\n        group.add_option(\"--whole-contig\", dest=\"whole_contig\",\n                         action=\"store_true\", default=False,\n                         help=optparse.SUPPRESS_HELP)\n\n        group.add_option(\"--multimapping-detection-method\",\n                         dest=\"detection_method\", type=\"choice\",\n                         choices=(\"NH\", \"X0\", \"XT\"),\n                         default=None,\n                         help=\"Some aligners identify multimapping using bam \"\n                         \"tags. Setting this option to NH, X0 or XT will \"\n                         \"use these tags when selecting the best read \"\n                         \"amongst reads with the same position and umi \"\n                         \"[default=%default]\")\n\n        parser.add_option_group(group)\n\n    # options added separately here to maintain better output order\n    if add_sam_options:\n        group = OptionGroup(parser, \"SAM/BAM options\")\n\n        group.add_option(\"--mapping-quality\", dest=\"mapping_quality\",\n                         type=\"int\",\n                         help=\"Minimum mapping quality for a read to be retained\"\n                         \" [default=%default]\",\n                         default=0)\n\n        group.add_option(\"--output-unmapped\", dest=\"output_unmapped\", action=\"store_true\",\n                         default=False, help=optparse.SUPPRESS_HELP)\n\n        group.add_option(\"--unmapped-reads\", dest=\"unmapped_reads\",\n                         type=\"choice\",\n                         choices=(\"discard\", \"use\", \"output\"),\n                         default=\"discard\",\n                         help=(\"How to handle unmapped reads. Options are \"\n                               \"'discard', 'use' or 'correct' [default=%default]\"))\n\n        group.add_option(\"--chimeric-pairs\", dest=\"chimeric_pairs\",\n                         type=\"choice\",\n                         choices=(\"discard\", \"use\", \"output\"),\n                         default=\"use\",\n                         help=(\"How to handle chimeric read pairs. Options are \"\n                               \"'discard', 'use' or 'correct' [default=%default]\"))\n\n        group.add_option(\"--unpaired-reads\", dest=\"unpaired_reads\",\n                         type=\"choice\",\n                         choices=(\"discard\", \"use\", \"output\"),\n                         default=\"use\",\n                         help=(\"How to handle unpaired reads. Options are \"\n                               \"'discard', 'use' or 'correct' [default=%default]\"))\n\n        group.add_option(\"--ignore-umi\", dest=\"ignore_umi\",\n                         action=\"store_true\", help=\"Ignore UMI and dedup\"\n                         \" only on position\", default=False)\n\n        group.add_option(\"--chrom\", dest=\"chrom\", type=\"string\",\n                         help=\"Restrict to one chromosome\",\n                         default=None)\n\n        group.add_option(\"--subset\", dest=\"subset\", type=\"float\",\n                         help=\"Use only a fraction of reads, specified by subset\",\n                         default=None)\n\n        group.add_option(\"-i\", \"--in-sam\", dest=\"in_sam\", action=\"store_true\",\n                         help=\"Input file is in sam format [default=%default]\",\n                         default=False)\n\n        group.add_option(\"--paired\", dest=\"paired\", action=\"store_true\",\n                         default=False,\n                         help=\"paired input BAM. [default=%default]\")\n\n        group.add_option(\"-o\", \"--out-sam\", dest=\"out_sam\", action=\"store_true\",\n                         help=\"Output alignments in sam format [default=%default]\",\n                         default=False)\n\n        group.add_option(\"--no-sort-output\", dest=\"no_sort_output\",\n                         action=\"store_true\", default=False,\n                         help=\"Don't Sort the output\")\n\n        parser.add_option_group(group)\n\n    if add_pipe_options:\n        group = OptionGroup(parser, \"input/output options\")\n        group.add_option(\"-I\", \"--stdin\", dest=\"stdin\", type=\"string\",\n                         help=\"file to read stdin from [default = stdin].\",\n                         metavar=\"FILE\")\n        group.add_option(\"-L\", \"--log\", dest=\"stdlog\", type=\"string\",\n                         help=\"file with logging information \"\n                         \"[default = stdout].\",\n                         metavar=\"FILE\")\n        group.add_option(\"-E\", \"--error\", dest=\"stderr\", type=\"string\",\n                         help=\"file with error information \"\n                         \"[default = stderr].\",\n                         metavar=\"FILE\")\n        group.add_option(\"-S\", \"--stdout\", dest=\"stdout\", type=\"string\",\n                         help=\"file where output is to go \"\n                         \"[default = stdout].\",\n                         metavar=\"FILE\")\n        group.add_option(\"--temp-dir\", dest=\"tmpdir\", type=\"string\",\n                         help=\"Directory for temporary files. If not set,\"\n                         \" the bash environmental variable TMPDIR is used\"\n                         \"[default = None].\",\n                         metavar=\"FILE\")\n        group.add_option(\"--log2stderr\", dest=\"log2stderr\",\n                         action=\"store_true\", help=\"send logging information\"\n                         \" to stderr [default = False].\")\n        group.add_option(\"--compresslevel\", dest=\"compresslevel\", type=\"int\",\n                         help=\"Level of Gzip compression to use. Default (6) matches\"\n                         \"GNU gzip rather than python gzip default (which is 9)\")\n\n        parser.set_defaults(stderr=sys.stderr)\n        parser.set_defaults(stdout=sys.stdout)\n        parser.set_defaults(stdlog=sys.stdout)\n        parser.set_defaults(stdin=sys.stdin)\n        parser.set_defaults(tmpdir=None)\n        parser.set_defaults(log2stderr=False)\n        parser.set_defaults(compresslevel=6)\n\n    parser.add_option_group(group)\n\n    group = OptionGroup(parser, \"profiling options\")\n\n    group.add_option(\"--timeit\", dest='timeit_file', type=\"string\",\n                     help=\"store timeing information in file [%default].\")\n\n    group.add_option(\"--timeit-name\", dest='timeit_name', type=\"string\",\n                     help=\"name in timing file for this class of jobs \"\n                     \"[%default].\")\n\n    group.add_option(\"--timeit-header\", dest='timeit_header',\n                     action=\"store_true\",\n                     help=\"add header for timing information [%default].\")\n\n    parser.add_option_group(group)\n\n    group = OptionGroup(parser, \"common options\")\n\n    group.add_option(\"-v\", \"--verbose\", dest=\"loglevel\", type=\"int\",\n                     help=\"loglevel [%default]. The higher, the more output.\")\n\n    group.add_option(\"-h\", \"--help\", dest=\"short_help\", action=\"callback\",\n                     callback=callbackShortHelp,\n                     help=\"output short help (command line options only).\")\n\n    group.add_option('--help-extended', action='help',\n                     help='Output full documentation')\n\n    group.add_option(\"--random-seed\", dest='random_seed', type=\"int\",\n                     help=\"random seed to initialize number generator \"\n                     \"with [%default].\")\n\n    parser.add_option_group(group)\n\n    # restore user defaults\n    parser.defaults.update(user_defaults)\n\n    if quiet:\n        parser.set_defaults(loglevel=0)\n    else:\n        parser.set_defaults(loglevel=1)\n\n    parser.set_defaults(\n        timeit_file=None,\n        timeit_name='all',\n        timeit_header=None,\n        random_seed=None,\n    )\n\n    if return_parser:\n        return parser\n\n    global_options, global_args = parser.parse_args(argv[1:])\n\n    if global_options.random_seed is not None:\n        random.seed(global_options.random_seed)\n\n    if add_pipe_options:\n        if global_options.stdout != sys.stdout:\n            global_options.stdout = openFile(global_options.stdout, \"w\")\n        if global_options.stderr != sys.stderr:\n            if global_options.stderr == \"stderr\":\n                global_options.stderr = global_options.stderr\n            else:\n                global_options.stderr = openFile(global_options.stderr, \"w\")\n        if global_options.stdlog != sys.stdout:\n            global_options.stdlog = openFile(global_options.stdlog, \"a\")\n        elif global_options.log2stderr:\n            global_options.stdlog = global_options.stderr\n        if global_options.stdin != sys.stdin:\n            global_options.stdin = openFile(global_options.stdin, \"r\")\n    else:\n        global_options.stderr = sys.stderr\n        global_options.stdout = sys.stdout\n        global_options.stdin = sys.stdin\n        if global_options.log2stderr:\n            global_options.stdlog = sys.stderr\n        else:\n            global_options.stdlog = sys.stdout\n\n    if global_options.loglevel >= 1:\n        global_options.stdlog.write(getHeader() + \"\\n\")\n        global_options.stdlog.write(getParams(global_options) + \"\\n\")\n        global_options.stdlog.flush()\n\n    # configure logging\n    # map from 0-10 to logging scale\n    # 0: quiet\n    # 1: little verbositiy\n    # >1: increased verbosity\n    if global_options.loglevel == 0:\n        lvl = logging.ERROR\n    elif global_options.loglevel == 1:\n        lvl = logging.INFO\n    else:\n        lvl = logging.DEBUG\n\n    if global_options.stdout == global_options.stdlog:\n        format = '# %(asctime)s %(levelname)s %(message)s'\n    else:\n        format = '%(asctime)s %(levelname)s %(message)s'\n\n    logging.basicConfig(\n        level=lvl,\n        format=format,\n        stream=global_options.stdlog)\n\n    # set up multi-line logging\n    # Note that .handlers is not part of the API, might change\n    # Solution is to configure handlers explicitely.\n    for handler in logging.getLogger().handlers:\n        handler.setFormatter(MultiLineFormatter(format))\n\n    return global_options, global_args"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating the options for barcode extraction.", "response": "def validateExtractOptions(options):\n    ''' Check the validity of the option combinations for barcode extraction'''\n\n    if not options.pattern and not options.pattern2:\n        if not options.read2_in:\n            U.error(\"Must supply --bc-pattern for single-end\")\n        else:\n            U.error(\"Must supply --bc-pattern and/or --bc-pattern2 \"\n                    \"if paired-end \")\n\n    if options.pattern2:\n        if not options.read2_in:\n            U.error(\"must specify a paired fastq ``--read2-in``\")\n\n        if not options.pattern2:\n            options.pattern2 = options.pattern\n\n    extract_cell = False\n    extract_umi = False\n\n    # If the pattern is a regex we can compile the regex(es) prior to\n    # ExtractFilterAndUpdate instantiation\n    if options.extract_method == \"regex\":\n        if options.pattern:\n            try:\n                options.pattern = regex.compile(options.pattern)\n            except regex.error:\n                U.error(\"--bc-pattern '%s' is not a \"\n                        \"valid regex\" % options.pattern)\n\n        if options.pattern2:\n            try:\n                options.pattern2 = regex.compile(options.pattern2)\n            except regex.Error:\n                U.error(\"--bc-pattern2 '%s' is not a \"\n                        \"valid regex\" % options.pattern2)\n\n    # check whether the regex contains a umi group(s) and cell groups(s)\n    if options.extract_method == \"regex\":\n        if options.pattern:\n            for group in options.pattern.groupindex:\n                if group.startswith(\"cell_\"):\n                    extract_cell = True\n                elif group.startswith(\"umi_\"):\n                    extract_umi = True\n        if options.pattern2:\n            for group in options.pattern2.groupindex:\n                if group.startswith(\"cell_\"):\n                    extract_cell = True\n                elif group.startswith(\"umi_\"):\n                    extract_umi = True\n\n    # check whether the pattern string contains umi/cell bases\n    elif options.extract_method == \"string\":\n        if options.pattern:\n            if \"C\" in options.pattern:\n                extract_cell = True\n            if \"N\" in options.pattern:\n                extract_umi = True\n        if options.pattern2:\n            if \"C\" in options.pattern2:\n                extract_cell = True\n            if \"N\" in options.pattern2:\n                extract_umi = True\n\n    if not extract_umi:\n        if options.extract_method == \"string\":\n            U.error(\"barcode pattern(s) do not include any umi bases \"\n                    \"(marked with 'Ns') %s, %s\" % (\n                        options.pattern, options.pattern2))\n        elif options.extract_method == \"regex\":\n            U.error(\"barcode regex(es) do not include any umi groups \"\n                    \"(starting with 'umi_') %s, %s\" (\n                        options.pattern, options.pattern2))\n\n    return(extract_cell, extract_umi)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating the options for sam input", "response": "def validateSamOptions(options, group=False):\n    ''' Check the validity of the option combinations for sam/bam input '''\n\n    if options.per_gene:\n        if options.gene_tag and options.per_contig:\n            raise ValueError(\"need to use either --per-contig \"\n                             \"OR --gene-tag, please do not provide both\")\n\n        if not options.per_contig and not options.gene_tag:\n            raise ValueError(\"for per-gene applications, must supply \"\n                             \"--per-contig or --gene-tag\")\n\n    if options.per_contig and not options.per_gene:\n        raise ValueError(\"need to use --per-gene with --per-contig\")\n\n    if options.gene_tag and not options.per_gene:\n        raise ValueError(\"need to use --per-gene with --gene_tag\")\n\n    if options.gene_transcript_map and not options.per_contig:\n        raise ValueError(\"need to use --per-contig and --per-gene\"\n                         \"with --gene-transcript-map\")\n\n    if options.get_umi_method == \"tag\":\n        if options.umi_tag is None:\n            raise ValueError(\"Need to supply the --umi-tag option\")\n        if options.per_cell and options.cell_tag is None:\n            raise ValueError(\"Need to supply the --cell-tag option\")\n\n    if options.assigned_tag is None:\n        options.assigned_tag = options.gene_tag\n\n    if options.skip_regex:\n        try:\n            re.compile(options.skip_regex)\n        except re.error:\n            raise ValueError(\"skip-regex '%s' is not a \"\n                             \"valid regex\" % options.skip_regex)\n\n    if not group:\n        if options.unmapped_reads == \"output\":\n            raise ValueError(\"Cannot use --unmapped-reads=output. If you want \"\n                             \"to retain unmapped without deduplicating them, \"\n                             \"use the group command\")\n        if options.chimeric_pairs == \"output\":\n            raise ValueError(\"Cannot use --chimeric-pairs=output. If you want \"\n                             \"to retain chimeric read pairs without \"\n                             \"deduplicating them, use the group command\")\n        if options.unpaired_reads == \"output\":\n            raise ValueError(\"Cannot use --unpaired-reads=output. If you want \"\n                             \"to retain unmapped without deduplicating them, \"\n                             \"use the group command\")\n\n    if options.paired:\n        if options.chimeric_pairs == \"use\":\n            warn(\"Chimeric read pairs are being used. \"\n                 \"Some read pair UMIs may be grouped/deduplicated using \"\n                 \"just the mapping coordinates from read1.\"\n                 \"This may also increase the run time and memory usage. \"\n                 \"Consider --chimeric-pairs==discard to discard these reads \"\n                 \"or --chimeric-pairs==output (group command only) to \"\n                 \"output them without grouping\")\n        if options.unpaired_reads == \"use\":\n            warn(\"Unpaired read pairs are being used. \"\n                 \"Some read pair UMIs may be grouped/deduplicated using \"\n                 \"just the mapping coordinates from read1.\"\n                 \"This may also increase the run time and memory usage. \"\n                 \"Consider --unpared-reads==discard to discard these reads \"\n                 \"or --unpared-reads==output (group command only) to \"\n                 \"output them without grouping\")\n        if options.unmapped_reads == \"use\":\n            warn(\"Unmapped read pairs are being used. \"\n                 \"Some read pair UMIs may be grouped/deduplicated using \"\n                 \"just the mapping coordinates from read1. \"\n                 \"This may also increase the run time and memory usage. \"\n                 \"Consider --unmapped_reads==discard to discard these reads \"\n                 \"or --unmapped_reads==output (group command only) to \"\n                 \"output them without grouping\")\n\n    command = \" \".join(sys.argv)\n    info(\"command: %s\" % command)\n    if \"--umi-tag\" in command or \"--cell-tag\" in command:\n        if options.get_umi_method != \"tag\":\n            raise ValueError(\"--umi-tag and/or --cell-tag options provided. \"\n                             \"Need to set --extract-umi-method=tag\")\n\n    if options.unmapped_reads == \"use\":\n        if not options.paired:\n            raise ValueError(\"--unmapped-reads=use is only compatible with \"\n                             \"paired end reads (--paired)\")\n\n    if \"--chimeric-pairs\" in command:\n        info(\"command: %s\" % command)\n        if not options.paired:\n            raise ValueError(\"--chimeric-pairs is only compatible \"\n                             \"with paired end reads (--paired)\")\n\n    if \"--unpaired-reads\" in command:\n        if not options.paired:\n            raise ValueError(\"--unpaired-reads is only compatible \"\n                             \"with paired end reads (--paired)\")\n\n    # legacy support for --output-unmapped behaviour\n    if options.output_unmapped:\n        warn(\"--output-unmapped will be removed in the near future. \"\n             \"Use --unmapped-reads=output instead\")\n        # We will update the value of options.unmapped_reads so we want to\n        # check the user has not also supplied this option\n        if \"--unmapped_reads\" in command:\n            raise ValueError(\"Do not use --output-unmapped in combination with\"\n                             \"--unmapped-reads. Just use --unmapped-reads\")\n        options.unmapped_reads = \"output\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Stop():\n\n    if global_options.loglevel >= 1 and global_benchmark:\n        t = time.time() - global_starting_time\n        global_options.stdlog.write(\n            \"######### Time spent in benchmarked functions #########\\n\")\n        global_options.stdlog.write(\"# function\\tseconds\\tpercent\\n\")\n        for key, value in global_benchmark.items():\n            global_options.stdlog.write(\n                \"# %s\\t%6i\\t%5.2f%%\\n\" % (key, value,\n                                          (100.0 * float(value) / t)))\n        global_options.stdlog.write(\n            \"#######################################################\\n\")\n\n    if global_options.loglevel >= 1:\n        global_options.stdlog.write(getFooter() + \"\\n\")\n\n    # close files\n    if global_options.stdout != sys.stdout:\n        global_options.stdout.close()\n    # do not close log, otherwise error occurs in atext.py\n    # if global_options.stdlog != sys.stdout:\n    #   global_options.stdlog.close()\n\n    if global_options.stderr != sys.stderr:\n        global_options.stderr.close()\n\n    if global_options.timeit_file:\n\n        outfile = open(global_options.timeit_file, \"a\")\n\n        if global_options.timeit_header:\n            outfile.write(\"\\t\".join(\n                (\"name\", \"wall\", \"user\", \"sys\", \"cuser\", \"csys\",\n                 \"host\", \"system\", \"release\", \"machine\",\n                 \"start\", \"end\", \"path\", \"cmd\")) + \"\\n\")\n\n        csystem, host, release, version, machine = map(str, os.uname())\n        uusr, usys, c_usr, c_sys = map(lambda x: \"%5.2f\" % x, os.times()[:4])\n        t_end = time.time()\n        c_wall = \"%5.2f\" % (t_end - global_starting_time)\n\n        if sys.argv[0] == \"run.py\":\n            cmd = global_args[0]\n            if len(global_args) > 1:\n                cmd += \" '\" + \"' '\".join(global_args[1:]) + \"'\"\n        else:\n            cmd = sys.argv[0]\n\n        result = \"\\t\".join((global_options.timeit_name,\n                            c_wall, uusr, usys, c_usr, c_sys,\n                            host, csystem, release, machine,\n                            time.asctime(time.localtime(global_starting_time)),\n                            time.asctime(time.localtime(t_end)),\n                            os.path.abspath(os.getcwd()),\n                            cmd)) + \"\\n\"\n\n        outfile.write(result)\n        outfile.close()", "response": "stop the experiment.\n\n    This method performs final book-keeping, closes the output streams\n    and writes the final log messages indicating script completion."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getTempFile(dir=None, shared=False, suffix=\"\"):\n    '''get a temporary file.\n\n    The file is created and the caller needs to close and delete\n    the temporary file once it is not used any more.\n\n    Arguments\n    ---------\n    dir : string\n        Directory of the temporary file and if not given is set to the\n        default temporary location in the global configuration dictionary.\n    shared : bool\n        If set, the tempory file will be in a shared temporary\n        location (given by the global configuration directory).\n    suffix : string\n        Filename suffix\n\n    Returns\n    -------\n    file : File\n        A file object of the temporary file.\n    '''\n\n    return tempfile.NamedTemporaryFile(dir=dir, delete=False, prefix=\"ctmp\",\n                                       suffix=suffix)", "response": "get a temporary file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getTempFilename(dir=None, shared=False, suffix=\"\"):\n    '''return a temporary filename.\n\n    The file is created and the caller needs to delete the temporary\n    file once it is not used any more.\n\n    Arguments\n    ---------\n    dir : string\n        Directory of the temporary file and if not given is set to the\n        default temporary location in the global configuration dictionary.\n    shared : bool\n        If set, the tempory file will be in a shared temporary\n        location.\n    suffix : string\n        Filename suffix\n\n    Returns\n    -------\n    filename : string\n        Absolute pathname of temporary file.\n\n    '''\n    tmpfile = getTempFile(dir=dir, shared=shared, suffix=suffix)\n    tmpfile.close()\n    return tmpfile.name", "response": "Returns a temporary filename."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_barcode_read_id(read, cell_barcode=False, sep=\"_\"):\n    ''' extract the umi +/- cell barcode from the read id using the\n    specified separator '''\n\n    try:\n        if cell_barcode:\n            umi = read.qname.split(sep)[-1].encode('utf-8')\n            cell = read.qname.split(sep)[-2].encode('utf-8')\n        else:\n            umi = read.qname.split(sep)[-1].encode('utf-8')\n            cell = None\n\n        return umi, cell\n\n    except:\n        raise ValueError(\n            \"Could not extract UMI +/- cell barcode from the read\"\n            \"ID, please check UMI is encoded in the read name\")", "response": "extract the UMI + / - cell barcode from the read id using the cell barcode specified in the cell_barcode parameter"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_barcode_tag(read,\n                    cell_barcode=False,\n                    umi_tag='RX',\n                    cell_tag=None,\n                    umi_tag_split=None,\n                    umi_tag_delim=None,\n                    cell_tag_split=\"-\",\n                    cell_tag_delim=None):\n    ''' extract the umi +/- cell barcode from the specified tag '''\n\n    try:\n        if cell_barcode:\n            umi = read.get_tag(umi_tag)\n            cell = read.get_tag(cell_tag)\n        else:\n            umi = read.get_tag(umi_tag)\n            cell = None\n\n        if umi_tag_split:\n            umi = umi.split(umi_tag_split)[0]\n\n        if umi_tag_delim:\n            umi = \"\".join(umi.split(umi_tag_delim))\n\n        # 10X pipelines append a 'GEM' tag to the UMI, e.g\n        # GATAGATACCTAGATA-1, hence default to split by \"-\n        if cell and cell_tag_split:\n            cell = cell.split(cell_tag_split)[0]\n\n        if cell and cell_tag_delim:\n            cell = \"\".join(cell.split(cell_tag_delim))\n\n        if cell:\n            cell = cell.encode('utf-8')\n\n        return umi.encode('utf-8'), cell\n\n    except IndexError:\n        raise ValueError(\"Could not extract UMI +/- cell barcode from the \"\n                         \"read tag\")", "response": "extract the UMI + / - cell barcode from the specified tag"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_cell_umi_read_string(read_id, sep=\"_\"):\n    ''' extract the umi and cell barcode from the read id (input as a\n    string) using the specified separator '''\n\n    try:\n        return (read_id.split(sep)[-1].encode('utf-8'),\n                read_id.split(sep)[-2].encode('utf-8'))\n    except IndexError:\n        raise ValueError(\n            \"Could not extract UMI or CB from the read ID, please\"\n            \"check UMI and CB are encoded in the read name:\"\n            \"%s\" % read_id)", "response": "extract the umi and cell barcode from the read id"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_barcode_umis(read, cell_barcode=False):\n    ''' extract the umi +/- cell barcode from the read name where the barcodes\n    were extracted using umis'''\n\n    umi, cell = None, None\n    try:\n        read_name_elements = read.qname.split(\":\")\n        for element in read_name_elements:\n            if element.startswith(\"UMI_\"):\n                umi = element[4:].encode('utf-8')\n            elif element.startswith(\"CELL_\") and cell_barcode:\n                cell = element[5:].encode('utf-8')\n\n        if umi is None:\n            raise ValueError()\n\n        return umi, cell\n\n    except:\n        raise ValueError(\"Could not extract UMI +/- cell barcode from the \"\n                         \"read tag\")", "response": "extract the umi + / - cell barcode from the read name where the barcodes were extracted using umis"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nyields the counts per umi for each gene in the file infile.", "response": "def get_gene_count_tab(infile,\n                       bc_getter=None):\n\n    ''' Yields the counts per umi for each gene\n\n    bc_getter: method to get umi (plus optionally, cell barcode) from\n    read, e.g get_umi_read_id or get_umi_tag\n\n\n    TODO: ADD FOLLOWING OPTION\n\n    skip_regex: skip genes matching this regex. Useful to ignore\n                unassigned reads (as per get_bundles class above)\n\n    '''\n\n    gene = None\n    counts = collections.Counter()\n\n    for line in infile:\n\n        values = line.strip().split(\"\\t\")\n\n        assert len(values) == 2, \"line: %s does not contain 2 columns\" % line\n\n        read_id, assigned_gene = values\n\n        if assigned_gene != gene:\n            if gene:\n                yield gene, counts\n\n            gene = assigned_gene\n            counts = collections.defaultdict(collections.Counter)\n\n        cell, umi = bc_getter(read_id)\n        counts[cell][umi] += 1\n\n    # yield final values\n    yield gene, counts"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef metafetcher(bamfile, metacontig2contig, metatag):\n    ''' return reads in order of metacontigs'''\n    for metacontig in metacontig2contig:\n        for contig in metacontig2contig[metacontig]:\n            for read in bamfile.fetch(contig):\n                read.set_tag(metatag, metacontig)\n                yield read", "response": "yield reads in order of metacontigs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntake a cigar string and finds the first splice position as an offset from the start.", "response": "def find_splice(cigar):\n    '''Takes a cigar string and finds the first splice position as\n    an offset from the start. To find the 5' end (read coords) of\n    the junction for a reverse read, pass in the reversed cigar tuple'''\n\n    offset = 0\n    # a soft clip at the end of the read is taken as splicing\n    # where as a soft clip at the start is not.\n    if cigar[0][0] == 4:\n        offset = cigar[0][1]\n        cigar = cigar[1:]\n\n    for op, bases in cigar:\n        if op in (3, 4):\n            # N or S: found the splice\n            return offset\n        elif op in (0, 2, 7, 8):\n            # M, D, = or X: reference consuming\n            offset += bases\n        elif op in (1, 5, 6):\n            # I, H, P: non-reference consuming\n            continue\n        else:\n            raise ValueError(\"Bad Cigar operation: %i\" % op)\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the read position", "response": "def get_read_position(read, soft_clip_threshold):\n    ''' get the read position (taking account of clipping) '''\n    is_spliced = False\n\n    if read.is_reverse:\n        pos = read.aend\n        if read.cigar[-1][0] == 4:\n            pos = pos + read.cigar[-1][1]\n        start = read.pos\n\n        if ('N' in read.cigarstring or\n            (read.cigar[0][0] == 4 and\n             read.cigar[0][1] > soft_clip_threshold)):\n\n            cigar = read.cigar[::-1]\n            is_spliced = find_splice(cigar)\n    else:\n        pos = read.pos\n        if read.cigar[0][0] == 4:\n            pos = pos - read.cigar[0][1]\n        start = pos\n\n        if ('N' in read.cigarstring or\n            (read.cigar[-1][0] == 4 and\n             read.cigar[-1][1] > soft_clip_threshold)):\n            is_spliced = find_splice(read.cigar)\n\n    return start, pos, is_spliced"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write(self, read, unique_id=None, umi=None, unmapped=False):\n        '''Check if chromosome has changed since last time. If it has, scan\n        for mates. Write the read to outfile and save the identity for paired\n        end retrieval'''\n\n        if unmapped or read.mate_is_unmapped:\n            self.outfile.write(read)\n            return\n\n        if not self.chrom == read.reference_name:\n            self.write_mates()\n            self.chrom = read.reference_name\n\n        key = read.query_name, read.next_reference_name, read.next_reference_start\n        self.read1s.add(key)\n\n        self.outfile.write(read)", "response": "Write the read to outfile and save the identity for paired\n        end retrieval"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nscan the current chromosome for matches to any of the reads stored in the read1s buffer and write the mates to the output file.", "response": "def write_mates(self):\n        '''Scan the current chromosome for matches to any of the reads stored\n        in the read1s buffer'''\n        if self.chrom is not None:\n            U.debug(\"Dumping %i mates for contig %s\" % (\n                len(self.read1s), self.chrom))\n\n        for read in self.infile.fetch(reference=self.chrom, multiple_iterators=True):\n            if any((read.is_unmapped, read.mate_is_unmapped, read.is_read1)):\n                continue\n\n            key = read.query_name, read.reference_name, read.reference_start\n            if key in self.read1s:\n                self.outfile.write(read)\n                self.read1s.remove(key)\n\n        U.debug(\"%i mates remaining\" % len(self.read1s))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close(self):\n        '''Write mates for remaining chromsome. Search for matches to any\n        unmatched reads'''\n\n        self.write_mates()\n        U.info(\"Searching for mates for %i unmatched alignments\" %\n               len(self.read1s))\n\n        found = 0\n        for read in self.infile.fetch(until_eof=True, multiple_iterators=True):\n\n            if read.is_unmapped:\n                continue\n\n            key = read.query_name, read.reference_name, read.reference_start\n            if key in self.read1s:\n                self.outfile.write(read)\n                self.read1s.remove(key)\n                found += 1\n                continue\n\n        U.info(\"%i mates never found\" % len(self.read1s))\n        self.outfile.close()", "response": "Write mates for remaining chromsome. Search for matches to any\n        unmatched reads."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nestimate the number of true cell barcodes using a gaussian kde - based method", "response": "def getKneeEstimateDensity(cell_barcode_counts,\n                           expect_cells=False,\n                           cell_number=False,\n                           plotfile_prefix=None):\n    ''' estimate the number of \"true\" cell barcodes using a gaussian\n    density-based method\n\n    input:\n         cell_barcode_counts = dict(key = barcode, value = count)\n         expect_cells (optional) = define the expected number of cells\n         cell_number (optional) = define number of cell barcodes to accept\n         plotfile_prefix = (optional) prefix for plots\n\n    returns:\n         List of true barcodes\n    '''\n\n    # very low abundance cell barcodes are filtered out (< 0.001 *\n    # the most abundant)\n    threshold = 0.001 * cell_barcode_counts.most_common(1)[0][1]\n\n    counts = sorted(cell_barcode_counts.values(), reverse=True)\n    counts_thresh = [x for x in counts if x > threshold]\n    log_counts = np.log10(counts_thresh)\n\n    # guassian density with hardcoded bw\n    density = gaussian_kde(log_counts, bw_method=0.1)\n\n    xx_values = 10000  # how many x values for density plot\n    xx = np.linspace(log_counts.min(), log_counts.max(), xx_values)\n\n    local_min = None\n\n    if cell_number:  # we have a prior hard expectation on the number of cells\n        threshold = counts[cell_number]\n\n    else:\n        local_mins = argrelextrema(density(xx), np.less)[0]\n        local_mins_counts = []\n\n        for poss_local_min in local_mins[::-1]:\n\n            passing_threshold = sum([y > np.power(10, xx[poss_local_min])\n                                     for x, y in cell_barcode_counts.items()])\n            local_mins_counts.append(passing_threshold)\n\n            if not local_min:   # if we have selected a local min yet\n                if expect_cells:  # we have a \"soft\" expectation\n                    if (passing_threshold > expect_cells * 0.1 and\n                        passing_threshold <= expect_cells):\n                        local_min = poss_local_min\n\n                else:  # we have no prior expectation\n                    # TS: In abscence of any expectation (either hard or soft),\n                    # this set of heuristic thresholds are used to decide\n                    # which local minimum to select.\n                    # This is very unlikely to be the best way to achieve this!\n                    if (poss_local_min >= 0.2 * xx_values and\n                        (log_counts.max() - xx[poss_local_min] > 0.5 or\n                         xx[poss_local_min] < log_counts.max()/2)):\n                        local_min = poss_local_min\n\n        if local_min is not None:\n            threshold = np.power(10, xx[local_min])\n\n    if cell_number or local_min is not None:\n        final_barcodes = set([\n            x for x, y in cell_barcode_counts.items() if y > threshold])\n    else:\n        final_barcodes = None\n\n    if plotfile_prefix:\n\n        # colour-blind friendly colours - https://gist.github.com/thriveth/8560036\n        CB_color_cycle = ['#377eb8', '#ff7f00', '#4daf4a',\n                          '#f781bf', '#a65628', '#984ea3',\n                          '#999999', '#e41a1c', '#dede00']\n        user_line = mlines.Line2D(\n            [], [], color=CB_color_cycle[0], ls=\"dashed\",\n            markersize=15, label='User-defined')\n        selected_line = mlines.Line2D(\n            [], [], color=CB_color_cycle[0], ls=\"dashed\", markersize=15, label='Selected')\n        rejected_line = mlines.Line2D(\n            [], [], color=CB_color_cycle[3], ls=\"dashed\", markersize=15, label='Rejected')\n\n        # make density plot\n        fig = plt.figure()\n        fig1 = fig.add_subplot(111)\n        fig1.plot(xx, density(xx), 'k')\n        fig1.set_xlabel(\"Count per cell (log10)\")\n        fig1.set_ylabel(\"Density\")\n\n        if cell_number:\n            fig1.axvline(np.log10(threshold), ls=\"dashed\", color=CB_color_cycle[0])\n            lgd = fig1.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,\n                              handles=[user_line],\n                              title=\"Cell threshold\")\n\n        elif local_min is None:  # no local_min was accepted\n            for pos in xx[local_mins]:\n                fig1.axvline(x=pos, ls=\"dashed\", color=CB_color_cycle[3])\n            lgd = fig1.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,\n                              handles=[selected_line, rejected_line],\n                              title=\"Possible thresholds\")\n        else:\n            for pos in xx[local_mins]:\n                if pos == xx[local_min]:  # selected local minima\n                    fig1.axvline(x=xx[local_min], ls=\"dashed\", color=CB_color_cycle[0])\n                else:\n                    fig1.axvline(x=pos, ls=\"dashed\", color=CB_color_cycle[3])\n\n            lgd = fig1.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,\n                              handles=[selected_line, rejected_line],\n                              title=\"Possible thresholds\")\n\n        fig.savefig(\"%s_cell_barcode_count_density.png\" % plotfile_prefix,\n                    bbox_extra_artists=(lgd,), bbox_inches='tight')\n\n        # make knee plot\n        fig = plt.figure()\n        fig2 = fig.add_subplot(111)\n        fig2.plot(range(0, len(counts)), np.cumsum(counts), c=\"black\")\n\n        xmax = len(counts)\n        if local_min is not None:\n            # reasonable maximum x-axis value\n            xmax = min(len(final_barcodes) * 5, xmax)\n\n        fig2.set_xlim((0 - (0.01 * xmax), xmax))\n        fig2.set_xlabel(\"Rank\")\n        fig2.set_ylabel(\"Cumulative count\")\n\n        if cell_number:\n            fig2.axvline(x=cell_number, ls=\"dashed\", color=CB_color_cycle[0])\n            lgd = fig2.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,\n                              handles=[user_line],\n                              title=\"Cell threshold\")\n\n        elif local_min is None:  # no local_min was accepted\n            for local_mins_count in local_mins_counts:\n                fig2.axvline(x=local_mins_count, ls=\"dashed\",\n                             color=CB_color_cycle[3])\n            lgd = fig2.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,\n                              handles=[selected_line, rejected_line],\n                              title=\"Possible thresholds\")\n\n        else:\n            for local_mins_count in local_mins_counts:\n                if local_mins_count == len(final_barcodes):  # selected local minima\n                    fig2.axvline(x=local_mins_count, ls=\"dashed\",\n                                 color=CB_color_cycle[0])\n                else:\n                    fig2.axvline(x=local_mins_count, ls=\"dashed\",\n                                 color=CB_color_cycle[3])\n\n            lgd = fig2.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,\n                              handles=[selected_line, rejected_line],\n                              title=\"Possible thresholds\")\n\n        fig.savefig(\"%s_cell_barcode_knee.png\" % plotfile_prefix,\n                    bbox_extra_artists=(lgd,), bbox_inches='tight')\n\n        if local_min is not None:\n            colours_selected = [CB_color_cycle[0] for x in range(0, len(final_barcodes))]\n            colours_rejected = [\"black\" for x in range(0, len(counts)-len(final_barcodes))]\n            colours = colours_selected + colours_rejected\n        else:\n            colours = [\"black\" for x in range(0, len(counts))]\n\n        fig = plt.figure()\n        fig3 = fig.add_subplot(111)\n        fig3.scatter(x=range(1, len(counts)+1), y=counts,\n                     c=colours, s=10, linewidths=0)\n        fig3.loglog()\n        fig3.set_xlim(0, len(counts)*1.25)\n        fig3.set_xlabel('Barcode index')\n        fig3.set_ylabel('Count')\n\n        if cell_number:\n            fig3.axvline(x=cell_number, ls=\"dashed\", color=CB_color_cycle[0])\n            lgd = fig3.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,\n                              handles=[user_line],\n                              title=\"Cell threshold\")\n        elif local_min is None:  # no local_min was accepted\n            for local_mins_count in local_mins_counts:\n                fig3.axvline(x=local_mins_count, ls=\"dashed\",\n                             color=CB_color_cycle[3])\n            lgd = fig3.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,\n                              handles=[selected_line, rejected_line],\n                              title=\"Possible thresholds\")\n        else:\n            for local_mins_count in local_mins_counts:\n                if local_mins_count == len(final_barcodes):  # selected local minima\n                    fig3.axvline(x=local_mins_count, ls=\"dashed\",\n                                 color=CB_color_cycle[0])\n                else:\n                    fig3.axvline(x=local_mins_count, ls=\"dashed\",\n                                 color=CB_color_cycle[3])\n\n            lgd = fig3.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,\n                              handles=[selected_line, rejected_line],\n                              title=\"Possible thresholds\")\n\n        fig.savefig(\"%s_cell_barcode_counts.png\" % plotfile_prefix,\n                    bbox_extra_artists=(lgd,), bbox_inches='tight')\n\n        if not cell_number:\n            with U.openFile(\"%s_cell_thresholds.tsv\" % plotfile_prefix, \"w\") as outf:\n                outf.write(\"count\\taction\\n\")\n                for local_mins_count in local_mins_counts:\n                    if local_min and local_mins_count == len(final_barcodes):\n                        threshold_type = \"Selected\"\n                    else:\n                        threshold_type = \"Rejected\"\n\n                    outf.write(\"%s\\t%s\\n\" % (local_mins_count, threshold_type))\n\n    return final_barcodes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nestimating the number of \"true\" cell barcodes via a knee method which finds the point with maximum distance input: cell_barcode_counts = dict(key = barcode, value = count) cell_number (optional) = define number of cell barcodes to accept plotfile_prefix = (optional) prefix for plots returns: List of true barcodes", "response": "def getKneeEstimateDistance(cell_barcode_counts,\n                            cell_number=False,\n                            plotfile_prefix=None):\n    ''' estimate the number of \"true\" cell barcodes via a knee method\n    which finds the point with maximum distance\n\n    input:\n         cell_barcode_counts = dict(key = barcode, value = count)\n         cell_number (optional) = define number of cell barcodes to accept\n         plotfile_prefix = (optional) prefix for plots\n\n    returns:\n         List of true barcodes\n    '''\n\n    def getKneeDistance(values):\n        '''\n        This function is based on\n        https://stackoverflow.com/questions/2018178/finding-the-best-trade-off-point-on-a-curve\n\n        and https://dataplatform.cloud.ibm.com/analytics/notebooks/54d79c2a-f155-40ec-93ec-ed05b58afa39/view?access_token=6d8ec910cf2a1b3901c721fcb94638563cd646fe14400fecbb76cea6aaae2fb1\n\n        The idea is to draw a line from the first to last point on the\n        cumulative counts curve and then find the point on the curve\n        which is the maximum distance away from this line\n        '''\n\n        # get coordinates of all the points\n        nPoints = len(values)\n        allCoord = np.vstack((range(nPoints), values)).T\n\n        # get the first point\n        firstPoint = allCoord[0]\n        # get vector between first and last point - this is the line\n        lineVec = allCoord[-1] - allCoord[0]\n        lineVecNorm = lineVec / np.sqrt(np.sum(lineVec**2))\n\n        # find the distance from each point to the line:\n        # vector between all points and first point\n        vecFromFirst = allCoord - firstPoint\n\n        # To calculate the distance to the line, we split vecFromFirst into two\n        # components, one that is parallel to the line and one that is perpendicular\n        # Then, we take the norm of the part that is perpendicular to the line and\n        # get the distance.\n        # We find the vector parallel to the line by projecting vecFromFirst onto\n        # the line. The perpendicular vector is vecFromFirst - vecFromFirstParallel\n        # We project vecFromFirst by taking the scalar product of the vector with\n        # the unit vector that points in the direction of the line (this gives us\n        # the length of the projection of vecFromFirst onto the line). If we\n        # multiply the scalar product by the unit vector, we have vecFromFirstParallel\n\n        scalarProduct = np.sum(\n            vecFromFirst * npm.repmat(lineVecNorm, nPoints, 1), axis=1)\n        vecFromFirstParallel = np.outer(scalarProduct, lineVecNorm)\n        vecToLine = vecFromFirst - vecFromFirstParallel\n\n        # distance to line is the norm of vecToLine\n        distToLine = np.sqrt(np.sum(vecToLine ** 2, axis=1))\n\n        # knee/elbow is the point with max distance value\n        idxOfBestPoint = np.argmax(distToLine)\n\n        return(distToLine, idxOfBestPoint)\n\n    counts = [x[1] for x in cell_barcode_counts.most_common()]\n    values = list(np.cumsum(counts))\n\n    # We need to perform the distance knee iteratively with reduced\n    # number of CBs since it's sensitive to the number of CBs input\n    # and overestimates if too many CBs are used\n    previous_idxOfBestPoint = 0\n    distToLine, idxOfBestPoint = getKneeDistance(values)\n    if idxOfBestPoint == 0:\n        raise ValueError(\"Something's gone wrong here!!\")\n\n    max_iterations = 100\n    iterations = 0\n    while idxOfBestPoint - previous_idxOfBestPoint != 0:\n        previous_idxOfBestPoint = idxOfBestPoint\n        iterations += 1\n        if iterations > max_iterations:\n            break\n        distToLine, idxOfBestPoint = getKneeDistance(values[:idxOfBestPoint*3])\n\n    knee_final_barcodes = [x[0] for x in cell_barcode_counts.most_common()[\n        :idxOfBestPoint+1]]\n\n    if cell_number:\n        threshold = counts[cell_number]\n        final_barcodes = set([\n            x for x, y in cell_barcode_counts.items() if y > threshold])\n    else:\n        final_barcodes = knee_final_barcodes\n\n    if plotfile_prefix:\n\n        # colour-blind friendly colours - https://gist.github.com/thriveth/8560036\n        CB_color_cycle = ['#377eb8', '#ff7f00', '#4daf4a',\n                          '#f781bf', '#a65628', '#984ea3',\n                          '#999999', '#e41a1c', '#dede00']\n\n        user_line = mlines.Line2D(\n            [], [], color=CB_color_cycle[2], ls=\"dashed\",\n            markersize=15, label='User-defined')\n        selected_line = mlines.Line2D(\n            [], [], color=CB_color_cycle[0], ls=\"dashed\", markersize=15, label='Knee')\n\n        # plot of the original curve and its corresponding distances\n        plt.figure(figsize=(12, 6))\n        plt.plot(distToLine, label='Distance', color='r')\n        plt.plot(values, label='Cumulative', color='b')\n        plt.plot([idxOfBestPoint], values[idxOfBestPoint], marker='o',\n                 markersize=8, color=\"red\", label='Knee')\n\n        if cell_number:\n            plt.axvline(x=cell_number, ls=\"dashed\",\n                        color=CB_color_cycle[2], label=\"User-defined\")\n\n        plt.legend()\n        plt.savefig(\"%s_cell_barcode_knee.png\" % plotfile_prefix)\n\n        colours_selected = [CB_color_cycle[0] for x in range(0, len(final_barcodes))]\n        colours_rejected = [\"black\" for x in range(0, len(counts)-len(final_barcodes))]\n        colours = colours_selected + colours_rejected\n\n        fig = plt.figure()\n        fig3 = fig.add_subplot(111)\n        fig3.scatter(x=range(1, len(counts)+1), y=counts,\n                     c=colours, s=10, linewidths=0)\n        fig3.loglog()\n        fig3.set_xlim(0, len(counts)*1.25)\n        fig3.set_xlabel('Barcode index')\n        fig3.set_ylabel('Count')\n        fig3.axvline(x=len(knee_final_barcodes), ls=\"dashed\", color=CB_color_cycle[0])\n\n        if cell_number:\n            fig3.axvline(x=cell_number, ls=\"dashed\", color=CB_color_cycle[2])\n\n            lgd = fig3.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,\n                              handles=[selected_line, user_line],\n                              title=\"User threshold\")\n        else:\n            lgd = fig3.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,\n                              handles=[selected_line],\n                              title=\"Knee threshold\")\n\n        fig.savefig(\"%s_cell_barcode_counts.png\" % plotfile_prefix,\n                    bbox_extra_artists=(lgd,), bbox_inches='tight')\n\n        if not cell_number:\n            with U.openFile(\"%s_cell_thresholds.tsv\" % plotfile_prefix, \"w\") as outf:\n                outf.write(\"count\\n\")\n                outf.write(\"%s\\n\" % idxOfBestPoint)\n\n    return(final_barcodes)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getErrorCorrectMapping(cell_barcodes, whitelist, threshold=1):\n    ''' Find the mappings between true and false cell barcodes based\n    on an edit distance threshold.\n\n    Any cell barcode within the threshold to more than one whitelist\n    barcode will be excluded'''\n\n    true_to_false = collections.defaultdict(set)\n\n    whitelist = set([str(x).encode(\"utf-8\") for x in whitelist])\n\n    for cell_barcode in cell_barcodes:\n        match = None\n        barcode_in_bytes = str(cell_barcode).encode(\"utf-8\")\n        for white_cell in whitelist:\n\n            if barcode_in_bytes in whitelist:  # don't check if whitelisted\n                continue\n\n            if edit_distance(barcode_in_bytes, white_cell) <= threshold:\n                if match is not None:  # already matched one barcode\n                    match = None  # set match back to None\n                    break  # break and don't add to maps\n                else:\n                    match = white_cell.decode(\"utf-8\")\n\n        if match is not None:\n            true_to_false[match].add(cell_barcode)\n\n    return true_to_false", "response": "Find the mappings between true and false cell barcodes based on an edit distance threshold."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef checkError(barcode, whitelist, errors=1):\n    '''\n    Check for errors (substitutions, insertions, deletions) between a barcode\n    and a set of whitelist barcodes.\n\n    Returns the whitelist barcodes which match the input barcode\n    allowing for errors. Returns as soon as two are identified.\n    '''\n\n    near_matches = []\n    comp_regex = regex.compile(\"(%s){e<=%i}\" % (barcode, errors))\n    b_length = len(barcode)\n    for whitelisted_barcode in whitelist:\n        w_length = len(whitelisted_barcode)\n\n        # Don't check against itself\n        if barcode == whitelisted_barcode:\n            continue\n\n        # If difference in barcode lengths > number of allowed errors, continue\n        if (max(b_length, w_length) > (min(b_length, w_length) + errors)):\n            continue\n        if comp_regex.match(whitelisted_barcode):\n            near_matches.append(whitelisted_barcode)\n\n            # Assuming downstream processes are the same for\n            # (>1 -> Inf) near_matches this is OK\n            if len(near_matches) > 1:\n                return near_matches\n\n    return near_matches", "response": "Check for errors between a barcode and a set of whitelist barcodes. Returns the barcodes which match the input barcode."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef archive_context(filename):\n    tmpdir = tempfile.mkdtemp()\n    log.warn('Extracting in %s', tmpdir)\n    old_wd = os.getcwd()\n    try:\n        os.chdir(tmpdir)\n        with ContextualZipFile(filename) as archive:\n            archive.extractall()\n\n        # going in the directory\n        subdir = os.path.join(tmpdir, os.listdir(tmpdir)[0])\n        os.chdir(subdir)\n        log.warn('Now working in %s', subdir)\n        yield\n\n    finally:\n        os.chdir(old_wd)\n        shutil.rmtree(tmpdir)", "response": "A context manager that extracts a single file into a temporary directory and sets the cwd to the cwd."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nscripting main. parses command line options in sys.argv, unless *argv* is given.", "response": "def main(argv=None):\n    \"\"\"script main.\n\n    parses command line options in sys.argv, unless *argv* is given.\n    \"\"\"\n\n    if argv is None:\n        argv = sys.argv\n\n    # setup command line parser\n    parser = U.OptionParser(version=\"%prog version: $Id$\",\n                            usage=usage,\n                            description=globals()[\"__doc__\"])\n\n    group = U.OptionGroup(parser, \"group-specific options\")\n\n    group.add_option(\"--group-out\", dest=\"tsv\", type=\"string\",\n                     help=\"Outfile name for file mapping read id to read group\",\n                     default=None)\n\n    group.add_option(\"--output-bam\", dest=\"output_bam\", action=\"store_true\",\n                     default=False,\n                     help=(\"output a bam file with read groups tagged using the UG tag\"\n                           \"[default=%default]\"))\n\n    parser.add_option(\"--umi-group-tag\", dest=\"umi_group_tag\",\n                      type=\"string\", help=\"tag for the outputted umi group\",\n                      default='BX')\n\n    parser.add_option_group(group)\n\n    # add common options (-h/--help, ...) and parse command line\n    (options, args) = U.Start(parser, argv=argv)\n\n    U.validateSamOptions(options, group=True)\n\n    if options.stdin != sys.stdin:\n        in_name = options.stdin.name\n        options.stdin.close()\n    else:\n        raise ValueError(\"Input on standard in not currently supported\")\n\n    if options.stdout != sys.stdout:\n        if options.no_sort_output:\n            out_name = options.stdout.name\n        else:\n            out_name = U.getTempFilename(dir=options.tmpdir)\n            sorted_out_name = options.stdout.name\n        options.stdout.close()\n        assert options.output_bam, (\n            \"To output a bam you must include --output-bam option\")\n    else:\n        if options.no_sort_output:\n            out_name = \"-\"\n        else:\n            out_name = U.getTempFilename(dir=options.tmpdir)\n            sorted_out_name = \"-\"\n\n    if not options.no_sort_output:  # need to determine the output format for sort\n        if options.out_sam:\n            sort_format = \"sam\"\n        else:\n            sort_format = \"bam\"\n\n    if options.in_sam:\n        in_mode = \"r\"\n    else:\n        in_mode = \"rb\"\n\n    if options.out_sam:\n        out_mode = \"wh\"\n    else:\n        out_mode = \"wb\"\n\n    infile = pysam.Samfile(in_name, in_mode)\n\n    if options.output_bam:\n        outfile = pysam.Samfile(out_name, out_mode, template=infile)\n    else:\n        outfile = None\n\n    if options.tsv:\n        mapping_outfile = U.openFile(options.tsv, \"w\")\n        mapping_outfile.write(\"%s\\n\" % \"\\t\".join(\n            [\"read_id\", \"contig\", \"position\", \"gene\", \"umi\", \"umi_count\",\n             \"final_umi\", \"final_umi_count\", \"unique_id\"]))\n\n    nInput, nOutput, unique_id, input_reads, output_reads = 0, 0, 0, 0, 0\n\n    gene_tag = options.gene_tag\n    metacontig2contig = None\n\n    if options.unmapped_reads in [\"use\", \"output\"]:\n        output_unmapped = True\n    else:\n        output_unmapped = False\n\n    if options.chrom:\n        inreads = infile.fetch(reference=options.chrom)\n    else:\n        if options.per_gene and options.gene_transcript_map:\n            metacontig2contig = sam_methods.getMetaContig2contig(\n                infile, options.gene_transcript_map)\n            metatag = \"MC\"\n            inreads = sam_methods.metafetcher(infile, metacontig2contig, metatag)\n            gene_tag = metatag\n\n        else:\n            inreads = infile.fetch(until_eof=output_unmapped)\n\n    bundle_iterator = sam_methods.get_bundles(\n        options,\n        all_reads=True,\n        return_read2=True,\n        return_unmapped=output_unmapped,\n        metacontig_contig=metacontig2contig)\n\n    # set up UMIClusterer functor with methods specific to\n    # specified options.method\n    processor = network.UMIClusterer(options.method)\n\n    for bundle, key, status in bundle_iterator(inreads):\n\n        # write out read2s and unmapped/chimeric (if these options are set)\n        if status == 'single_read':\n            # bundle is just a single read here\n            nInput += 1\n\n            if outfile:\n                outfile.write(bundle)\n\n            nOutput += 1\n            continue\n\n        umis = bundle.keys()\n        counts = {umi: bundle[umi][\"count\"] for umi in umis}\n\n        nInput += sum(counts.values())\n\n        while nOutput >= output_reads + 10000:\n            output_reads += 10000\n            U.info(\"Written out %i reads\" % output_reads)\n\n        while nInput >= input_reads + 1000000:\n            input_reads += 1000000\n            U.info(\"Parsed %i input reads\" % input_reads)\n\n        # group the umis\n        groups = processor(\n            counts,\n            threshold=options.threshold)\n\n        for umi_group in groups:\n            top_umi = umi_group[0]\n\n            group_count = sum(counts[umi] for umi in umi_group)\n\n            for umi in umi_group:\n                reads = bundle[umi]['read']\n                for read in reads:\n                    if outfile:\n                        # Add the 'UG' tag to the read\n                        read.set_tag('UG', unique_id)\n                        read.set_tag(options.umi_group_tag, top_umi)\n                        outfile.write(read)\n\n                    if options.tsv:\n                        if options.per_gene:\n                            gene = read.get_tag(gene_tag)\n                        else:\n                            gene = \"NA\"\n                        mapping_outfile.write(\"%s\\n\" % \"\\t\".join(map(str, (\n                            read.query_name, read.reference_name,\n                            sam_methods.get_read_position(\n                                read, options.soft_clip_threshold)[1],\n                            gene,\n                            umi.decode(),\n                            counts[umi],\n                            top_umi.decode(),\n                            group_count,\n                            unique_id))))\n\n                    nOutput += 1\n\n            unique_id += 1\n\n    if outfile:\n        outfile.close()\n        if not options.no_sort_output:\n            # sort the output\n            pysam.sort(\"-o\", sorted_out_name, \"-O\", sort_format, out_name)\n            os.unlink(out_name)  # delete the tempfile\n\n    if options.tsv:\n        mapping_outfile.close()\n\n    # write footer and output benchmark information.\n    U.info(\n        \"Reads: %s\" % \", \".join([\"%s: %s\" % (x[0], x[1]) for x in\n                                 bundle_iterator.read_events.most_common()]))\n    U.info(\"Number of reads out: %i, Number of groups: %i\" %\n           (nOutput, unique_id))\n\n    U.info(\"Total number of positions deduplicated: %i\" %\n           processor.positions)\n    if processor.positions > 0:\n        U.info(\"Mean number of unique UMIs per position: %.2f\" %\n               (float(processor.total_umis_per_position) /\n                processor.positions))\n        U.info(\"Max. number of unique UMIs per position: %i\" %\n               processor.max_umis_per_position)\n    else:\n        U.warn(\"The BAM did not contain any valid \"\n               \"reads/read pairs for deduplication\")\n\n    U.Stop()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nscript main. parses command line options in sys.argv, unless *argv* is given.", "response": "def main(argv=None):\n    \"\"\"script main.\n\n    parses command line options in sys.argv, unless *argv* is given.\n    \"\"\"\n\n    if argv is None:\n        argv = sys.argv\n\n    # setup command line parser\n    parser = U.OptionParser(version=\"%prog version: $Id$\",\n                            usage=usage,\n                            description=globals()[\"__doc__\"])\n\n    group = U.OptionGroup(parser, \"count_tab-specific options\")\n\n    group.add_option(\"--barcode-separator\", dest=\"bc_sep\",\n                     type=\"string\", help=\"separator between read id and UMI \"\n                     \" and (optionally) the cell barcode\", default=\"_\")\n\n    group.add_option(\"--per-cell\", dest=\"per_cell\",\n                     action=\"store_true\",\n                     help=\"Readname includes cell barcode as well as UMI in \"\n                     \"format: read[sep]UMI[sep]CB\")\n\n    parser.add_option_group(group)\n\n    # add common options (-h/--help, ...) and parse command line\n    (options, args) = U.Start(parser, argv=argv, add_group_dedup_options=False,\n                              add_sam_options=False)\n\n    nInput, nOutput = 0, 0\n\n    # set the method with which to extract umis from reads\n    if options.per_cell:\n        bc_getter = partial(\n            sam_methods.get_cell_umi_read_string, sep=options.bc_sep)\n    else:\n        bc_getter = partial(\n            sam_methods.get_umi_read_string, sep=options.bc_sep)\n\n    if options.per_cell:\n        options.stdout.write(\"%s\\t%s\\t%s\\n\" % (\"cell\", \"gene\", \"count\"))\n    else:\n        options.stdout.write(\"%s\\t%s\\n\" % (\"gene\", \"count\"))\n\n    # set up UMIClusterer functor with methods specific to\n    # specified options.method\n    processor = network.UMIClusterer(options.method)\n\n    for gene, counts in sam_methods.get_gene_count_tab(\n            options.stdin,\n            bc_getter=bc_getter):\n\n        for cell in counts.keys():\n            umis = counts[cell].keys()\n\n            nInput += sum(counts[cell].values())\n\n            # group the umis\n            groups = processor(\n                counts[cell],\n                threshold=options.threshold)\n\n            gene_count = len(groups)\n            if options.per_cell:\n                options.stdout.write(\"%s\\t%s\\t%i\\n\" % (cell, gene, gene_count))\n            else:\n                options.stdout.write(\"%s\\t%i\\n\" % (gene, gene_count))\n                nOutput += gene_count\n\n    U.info(\"Number of reads counted: %i\" % nOutput)\n\n    U.Stop()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\niterate over contents of fastq file.", "response": "def fastqIterate(infile):\n    '''iterate over contents of fastq file.'''\n\n    def convert2string(b):\n        if type(b) == str:\n            return b\n        else:\n            return b.decode(\"utf-8\")\n\n    while 1:\n        line1 = convert2string(infile.readline())\n        if not line1:\n            break\n        if not line1.startswith('@'):\n            U.error(\"parsing error: expected '@' in line %s\" % line1)\n        line2 = convert2string(infile.readline())\n        line3 = convert2string(infile.readline())\n        if not line3.startswith('+'):\n            U.error(\"parsing error: expected '+' in line %s\" % line3)\n        line4 = convert2string(infile.readline())\n        # incomplete entry\n        if not line4:\n            U.error(\"incomplete entry for %s\" % line1)\n\n        yield Record(line1[1:-1], line2[:-1], line4[:-1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef joinedFastqIterate(fastq_iterator1, fastq_iterator2, strict=True):\n    '''This will return an iterator that returns tuples of fastq records.\n    At each step it will confirm that the first field of the read name\n    (before the first whitespace character) is identical between the\n    two reads. The response if it is not depends on the value of\n    :param:`strict`. If strict is true an error is returned. If strict\n    is `False` the second file is advanced until a read that matches\n    is found.\n\n    This allows for protocols where read one contains cell barcodes, and these\n    reads have been filtered and corrected before processing without regard\n    to read2\n\n    '''\n\n    for read1 in fastq_iterator1:\n        read2 = next(fastq_iterator2)\n        pair_id = read1.identifier.split()[0]\n        if not strict:\n            while read2.identifier.split()[0] != pair_id:\n                read2 = next(fastq_iterator2)\n        if not read2.identifier.split()[0] == pair_id:\n            raise ValueError(\"\\nRead pairs do not match\\n%s != %s\" %\n                             (pair_id, read2.identifier.split()[0]))\n        yield (read1, read2)", "response": "This function returns an iterator that returns tuples of fastq records."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef guessFormat(self):\n        '''return quality score format -\n        might return several if ambiguous.'''\n\n        c = [ord(x) for x in self.quals]\n        mi, ma = min(c), max(c)\n        r = []\n        for entry_format, v in iteritems(RANGES):\n            m1, m2 = v\n            if mi >= m1 and ma < m2:\n                r.append(entry_format)\n        return r", "response": "return quality score format - might return several if ambiguous."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef refill_random(self):\n        ''' refill the list of random_umis '''\n        self.random_umis = np.random.choice(\n            list(self.umis.keys()), self.random_fill_size, p=self.prob)\n        self.random_ix = 0", "response": "refill the list of random_umis"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fill(self):\n        ''' parse the BAM to obtain the frequency for each UMI'''\n        self.frequency2umis = collections.defaultdict(list)\n\n        for read in self.inbam:\n\n            if read.is_unmapped:\n                continue\n\n            if read.is_read2:\n                continue\n\n            self.umis[self.barcode_getter(read)[0]] += 1\n\n        self.umis_counter = collections.Counter(self.umis)\n        total_umis = sum(self.umis_counter.values())\n        U.info(\"total_umis %i\" % total_umis)\n        U.info(\"#umis %i\" % len(self.umis_counter))\n\n        self.prob = self.umis_counter.values()\n        sum_prob = sum(self.prob)\n        self.prob = [float(x) / sum_prob for x in self.prob]\n        self.refill_random()", "response": "fill the frequency2umis and probability2umis"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getUmis(self, n):\n        ''' return n umis from the random_umis atr.'''\n        if n < (self.random_fill_size - self.random_ix):\n            barcodes = self.random_umis[self.random_ix: self.random_ix+n]\n        else:\n            # could use the end of the random_umis but\n            # let's just make a new random_umis\n            if n > self.random_fill_size:  # ensure random_umis is long enough\n                self.random_fill_size = n * 2\n            self.refill_random()\n            barcodes = self.random_umis[self.random_ix: self.random_ix+n]\n\n        self.random_ix += n\n        return barcodes", "response": "return n umis from the random_umis atr."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef addBarcodesToIdentifier(read, UMI, cell):\n    '''extract the identifier from a read and append the UMI and\n    cell barcode before the first space'''\n\n    read_id = read.identifier.split(\" \")\n\n    if cell == \"\":\n        read_id[0] = read_id[0] + \"_\" + UMI\n    else:\n        read_id[0] = read_id[0] + \"_\" + cell + \"_\" + UMI\n\n    identifier = \" \".join(read_id)\n\n    return identifier", "response": "extract the identifier from a read and append the UMI and\n    cell barcode before the first space"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extractSeqAndQuals(seq, quals, umi_bases, cell_bases, discard_bases,\n                       retain_umi=False):\n    '''Remove selected bases from seq and quals\n    '''\n\n    new_seq = \"\"\n    new_quals = \"\"\n    umi_quals = \"\"\n    cell_quals = \"\"\n\n    ix = 0\n    for base, qual in zip(seq, quals):\n        if ((ix not in discard_bases) and\n            (ix not in cell_bases)):\n\n            # if we are retaining the umi, this base is both seq and umi\n            if retain_umi:\n                new_quals += qual\n                new_seq += base\n                umi_quals += qual\n\n            else:  # base is either seq or umi\n                if ix not in umi_bases:\n                    new_quals += qual\n                    new_seq += base\n                else:\n                    umi_quals += qual\n\n        elif ix in cell_bases:\n            cell_quals += qual\n\n        ix += 1\n\n    return new_seq, new_quals, umi_quals, cell_quals", "response": "Extract the sequence and quals from the given sequence and quals."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_below_threshold(umi_quals, quality_encoding,  quality_filter_threshold):\n    '''test whether the umi_quals are below the threshold'''\n    umi_quals = [x - RANGES[quality_encoding][0] for x in map(ord, umi_quals)]\n    below_threshold = [x < quality_filter_threshold for x in umi_quals]\n    return below_threshold", "response": "test whether the umi_quals are below the threshold"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef umi_below_threshold(umi_quals, quality_encoding,  quality_filter_threshold):\n    ''' return true if any of the umi quals is below the threshold'''\n    below_threshold = get_below_threshold(\n        umi_quals, quality_encoding, quality_filter_threshold)\n    return any(below_threshold)", "response": "return true if any of the umi quals is below the threshold"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmasks all positions where quals < threshold with N", "response": "def mask_umi(umi, umi_quals, quality_encoding,  quality_filter_threshold):\n    ''' Mask all positions where quals < threshold with \"N\" '''\n    below_threshold = get_below_threshold(\n        umi_quals, quality_encoding, quality_filter_threshold)\n    new_umi = \"\"\n\n    for base, test in zip(umi, below_threshold):\n        if test:\n            new_umi += \"N\"\n        else:\n            new_umi += base\n\n    return new_umi"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ExtractBarcodes(read, match,\n                    extract_umi=False,\n                    extract_cell=False,\n                    discard=False,\n                    retain_umi=False):\n    '''Extract the cell and umi barcodes using a regex.match object\n\n    inputs:\n\n    - read 1 and read2 = Record objects\n    - match = regex.match object\n    - extract_umi and extract_cell = switches to determine whether these\n                                     barcodes should be extracted\n    - discard = is there a region(s) of the sequence which should be\n      discarded entirely?\n    - retain_umi = Should UMI sequence be retained on the read sequence\n\n    returns:\n\n        - cell_barcode = Cell barcode string\n        - cell_barcode_quals = Cell barcode quality scores\n        - umi = UMI barcode string.\n        - umi_quals = UMI barcode quality scores\n        - new_seq = Read1 sequence after extraction\n        - new_quals = Read1 qualities after extraction\n\n    Barcodes and qualities default to empty strings where extract_cell\n    or extract_umi are false.\n\n    '''\n    cell_barcode, umi, cell_barcode_quals, umi_quals, new_seq, new_quals = (\"\",)*6\n\n    if not extract_cell and not extract_umi:\n        U.error(\"must set either extract_cell and/or extract_umi to true\")\n\n    groupdict = match.groupdict()\n    cell_bases = set()\n    umi_bases = set()\n    discard_bases = set()\n    for k in sorted(list(groupdict)):\n        span = match.span(k)\n        if extract_cell and k.startswith(\"cell_\"):\n            cell_barcode += groupdict[k]\n            cell_bases.update(range(span[0], span[1]))\n        elif extract_umi and k.startswith(\"umi_\"):\n            umi += groupdict[k]\n            umi_bases.update(range(span[0], span[1]))\n        elif discard and k.startswith(\"discard_\"):\n            discard_bases.update(range(span[0], span[1]))\n\n    new_seq, new_quals, umi_quals, cell_quals = extractSeqAndQuals(\n        read.seq, read.quals, umi_bases, cell_bases, discard_bases, retain_umi)\n\n    return (cell_barcode, cell_barcode_quals,\n            umi, umi_quals,\n            new_seq, new_quals)", "response": "Extract the cell and umi barcodes from a given read."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef maskQuality(self, umi, umi_quals):\n        '''mask low quality bases and return masked umi'''\n        masked_umi = mask_umi(umi, umi_quals,\n                              self.quality_encoding,\n                              self.quality_filter_mask)\n        if masked_umi != umi:\n            self.read_counts['UMI masked'] += 1\n            return masked_umi\n        else:\n            return umi", "response": "mask low quality bases and return masked umi"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filterCellBarcode(self, cell):\n        '''Filter out cell barcodes not in the whitelist, with\n        optional cell barcode error correction'''\n\n        if self.cell_blacklist and cell in self.cell_blacklist:\n            self.read_counts['Cell barcode in blacklist'] += 1\n            return None\n\n        if cell not in self.cell_whitelist:\n            if self.false_to_true_map:\n                if cell in self.false_to_true_map:\n                    cell = self.false_to_true_map[cell]\n                    self.read_counts['False cell barcode. Error-corrected'] += 1\n                else:\n                    self.read_counts['Filtered cell barcode. Not correctable'] += 1\n                    return None\n            else:\n                self.read_counts['Filtered cell barcode'] += 1\n                return None\n\n        if self.cell_blacklist and cell in self.cell_blacklist:\n            self.read_counts['Cell barcode corrected to barcode blacklist'] += 1\n            return None\n\n        return cell", "response": "Filter out cell barcodes not in the whitelist with\n            optional cell barcode error correction"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetecting multimapping features in the bam file and identify the tags", "response": "def detect_bam_features(bamfile, n_entries=1000):\n    ''' read the first n entries in the bam file and identify the tags\n    available detecting multimapping '''\n\n    inbam = pysam.Samfile(bamfile)\n    inbam = inbam.fetch(until_eof=True)\n\n    tags = [\"NH\", \"X0\", \"XT\"]\n    available_tags = {x: 1 for x in tags}\n\n    for n, read in enumerate(inbam):\n        if n > n_entries:\n            break\n\n        if read.is_unmapped:\n            continue\n\n        else:\n            for tag in tags:\n                if not read.has_tag(tag):\n                    available_tags[tag] = 0\n\n    return available_tags"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dataframe with aggregated counts per UMI", "response": "def aggregateStatsDF(stats_df):\n    ''' return a dataframe with aggregated counts per UMI'''\n\n    grouped = stats_df.groupby(\"UMI\")\n\n    agg_dict = {'counts': [np.median, len, np.sum]}\n    agg_df = grouped.agg(agg_dict)\n\n    agg_df.columns = ['median_counts', 'times_observed', 'total_counts']\n    return agg_df"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nscript main. parses command line options in sys.argv, unless *argv* is given.", "response": "def main(argv=None):\n    \"\"\"script main.\n\n    parses command line options in sys.argv, unless *argv* is given.\n    \"\"\"\n\n    if argv is None:\n        argv = sys.argv\n\n    # setup command line parser\n    parser = U.OptionParser(version=\"%prog version: $Id$\",\n                            usage=usage,\n                            description=globals()[\"__doc__\"])\n    group = U.OptionGroup(parser, \"dedup-specific options\")\n\n    group.add_option(\"--output-stats\", dest=\"stats\", type=\"string\",\n                     default=False,\n                     help=\"Specify location to output stats\")\n\n    parser.add_option_group(group)\n\n    # add common options (-h/--help, ...) and parse command line\n    (options, args) = U.Start(parser, argv=argv)\n\n    U.validateSamOptions(options, group=False)\n\n    if options.random_seed:\n        np.random.seed(options.random_seed)\n\n    if options.stdin != sys.stdin:\n        in_name = options.stdin.name\n        options.stdin.close()\n    else:\n        raise ValueError(\"Input on standard in not currently supported\")\n\n    if options.stdout != sys.stdout:\n        if options.no_sort_output:\n            out_name = options.stdout.name\n        else:\n            out_name = U.getTempFilename(dir=options.tmpdir)\n            sorted_out_name = options.stdout.name\n        options.stdout.close()\n    else:\n        if options.no_sort_output:\n            out_name = \"-\"\n        else:\n            out_name = U.getTempFilename(dir=options.tmpdir)\n            sorted_out_name = \"-\"\n\n    if not options.no_sort_output:  # need to determine the output format for sort\n        if options.out_sam:\n            sort_format = \"sam\"\n        else:\n            sort_format = \"bam\"\n\n    if options.in_sam:\n        in_mode = \"r\"\n    else:\n        in_mode = \"rb\"\n\n    if options.out_sam:\n        out_mode = \"wh\"\n    else:\n        out_mode = \"wb\"\n\n    if options.stats and options.ignore_umi:\n        raise ValueError(\"'--output-stats' and '--ignore-umi' options\"\n                         \" cannot be used together\")\n\n    infile = pysam.Samfile(in_name, in_mode)\n    outfile = pysam.Samfile(out_name, out_mode, template=infile)\n\n    if options.paired:\n        outfile = sam_methods.TwoPassPairWriter(infile, outfile)\n\n    nInput, nOutput, input_reads, output_reads = 0, 0, 0, 0\n\n    if options.detection_method:\n        bam_features = detect_bam_features(infile.filename)\n\n        if not bam_features[options.detection_method]:\n            if sum(bam_features.values()) == 0:\n                raise ValueError(\n                    \"There are no bam tags available to detect multimapping. \"\n                    \"Do not set --multimapping-detection-method\")\n            else:\n                raise ValueError(\n                    \"The chosen method of detection for multimapping (%s) \"\n                    \"will not work with this bam. Multimapping can be detected\"\n                    \" for this bam using any of the following: %s\" % (\n                        options.detection_method, \",\".join(\n                            [x for x in bam_features if bam_features[x]])))\n\n    gene_tag = options.gene_tag\n    metacontig2contig = None\n\n    if options.chrom:\n        inreads = infile.fetch(reference=options.chrom)\n\n    else:\n        if options.per_contig and options.gene_transcript_map:\n            metacontig2contig = sam_methods.getMetaContig2contig(\n                infile, options.gene_transcript_map)\n            metatag = \"MC\"\n            inreads = sam_methods.metafetcher(infile, metacontig2contig, metatag)\n            gene_tag = metatag\n\n        else:\n            inreads = infile.fetch()\n\n    # set up ReadCluster functor with methods specific to\n    # specified options.method\n    processor = network.ReadDeduplicator(options.method)\n\n    bundle_iterator = sam_methods.get_bundles(\n        options,\n        metacontig_contig=metacontig2contig)\n\n    if options.stats:\n        # set up arrays to hold stats data\n        stats_pre_df_dict = {\"UMI\": [], \"counts\": []}\n        stats_post_df_dict = {\"UMI\": [], \"counts\": []}\n        pre_cluster_stats = []\n        post_cluster_stats = []\n        pre_cluster_stats_null = []\n        post_cluster_stats_null = []\n        topology_counts = collections.Counter()\n        node_counts = collections.Counter()\n        read_gn = umi_methods.random_read_generator(\n            infile.filename, chrom=options.chrom,\n            barcode_getter=bundle_iterator.barcode_getter)\n\n    for bundle, key, status in bundle_iterator(inreads):\n\n        nInput += sum([bundle[umi][\"count\"] for umi in bundle])\n\n        while nOutput >= output_reads + 100000:\n            output_reads += 100000\n            U.info(\"Written out %i reads\" % output_reads)\n\n        while nInput >= input_reads + 1000000:\n            input_reads += 1000000\n            U.info(\"Parsed %i input reads\" % input_reads)\n\n        if options.stats:\n            # generate pre-dudep stats\n            average_distance = umi_methods.get_average_umi_distance(bundle.keys())\n            pre_cluster_stats.append(average_distance)\n            cluster_size = len(bundle)\n            random_umis = read_gn.getUmis(cluster_size)\n            average_distance_null = umi_methods.get_average_umi_distance(random_umis)\n            pre_cluster_stats_null.append(average_distance_null)\n\n        if options.ignore_umi:\n            for umi in bundle:\n                nOutput += 1\n                outfile.write(bundle[umi][\"read\"])\n\n        else:\n\n            # dedup using umis and write out deduped bam\n            reads, umis, umi_counts = processor(\n                bundle=bundle,\n                threshold=options.threshold)\n\n            for read in reads:\n                outfile.write(read)\n                nOutput += 1\n\n            if options.stats:\n\n                # collect pre-dudupe stats\n                stats_pre_df_dict['UMI'].extend(bundle)\n                stats_pre_df_dict['counts'].extend(\n                    [bundle[UMI]['count'] for UMI in bundle])\n\n                # collect post-dudupe stats\n                post_cluster_umis = [bundle_iterator.barcode_getter(x)[0] for x in reads]\n                stats_post_df_dict['UMI'].extend(umis)\n                stats_post_df_dict['counts'].extend(umi_counts)\n\n                average_distance = umi_methods.get_average_umi_distance(post_cluster_umis)\n                post_cluster_stats.append(average_distance)\n\n                cluster_size = len(post_cluster_umis)\n                random_umis = read_gn.getUmis(cluster_size)\n                average_distance_null = umi_methods.get_average_umi_distance(random_umis)\n                post_cluster_stats_null.append(average_distance_null)\n\n    outfile.close()\n\n    if not options.no_sort_output:\n        # sort the output\n        pysam.sort(\"-o\", sorted_out_name, \"-O\", sort_format, out_name)\n        os.unlink(out_name)  # delete the tempfile\n\n    if options.stats:\n\n        # generate the stats dataframe\n        stats_pre_df = pd.DataFrame(stats_pre_df_dict)\n        stats_post_df = pd.DataFrame(stats_post_df_dict)\n\n        # tally the counts per umi per position\n        pre_counts = collections.Counter(stats_pre_df[\"counts\"])\n        post_counts = collections.Counter(stats_post_df[\"counts\"])\n        counts_index = list(set(pre_counts.keys()).union(set(post_counts.keys())))\n        counts_index.sort()\n        with U.openFile(options.stats + \"_per_umi_per_position.tsv\", \"w\") as outf:\n            outf.write(\"counts\\tinstances_pre\\tinstances_post\\n\")\n            for count in counts_index:\n                values = (count, pre_counts[count], post_counts[count])\n                outf.write(\"\\t\".join(map(str, values)) + \"\\n\")\n\n        # aggregate stats pre/post per UMI\n        agg_pre_df = aggregateStatsDF(stats_pre_df)\n        agg_post_df = aggregateStatsDF(stats_post_df)\n\n        agg_df = pd.merge(agg_pre_df, agg_post_df, how='left',\n                          left_index=True, right_index=True,\n                          sort=True, suffixes=[\"_pre\", \"_post\"])\n\n        # TS - if count value not observed either pre/post-dedup,\n        # merge will leave an empty cell and the column will be cast as a float\n        # see http://pandas.pydata.org/pandas-docs/dev/missing_data.html\n        # --> Missing data casting rules and indexing\n        # so, back fill with zeros and convert back to int\n        agg_df = agg_df.fillna(0).astype(int)\n\n        agg_df.index = [x.decode() for x in agg_df.index]\n        agg_df.index.name = 'UMI'\n        agg_df.to_csv(options.stats + \"_per_umi.tsv\", sep=\"\\t\")\n\n        # bin distances into integer bins\n        max_ed = int(max(map(max, [pre_cluster_stats,\n                                   post_cluster_stats,\n                                   pre_cluster_stats_null,\n                                   post_cluster_stats_null])))\n\n        cluster_bins = range(-1, int(max_ed) + 2)\n\n        def bin_clusters(cluster_list, bins=cluster_bins):\n            ''' take list of floats and return bins'''\n            return np.digitize(cluster_list, bins, right=True)\n\n        def tallyCounts(binned_cluster, max_edit_distance):\n            ''' tally counts per bin '''\n            return np.bincount(binned_cluster,\n                               minlength=max_edit_distance + 3)\n\n        pre_cluster_binned = bin_clusters(pre_cluster_stats)\n        post_cluster_binned = bin_clusters(post_cluster_stats)\n        pre_cluster_null_binned = bin_clusters(pre_cluster_stats_null)\n        post_cluster_null_binned = bin_clusters(post_cluster_stats_null)\n\n        edit_distance_df = pd.DataFrame(\n            {\"unique\": tallyCounts(pre_cluster_binned, max_ed),\n             \"unique_null\": tallyCounts(pre_cluster_null_binned, max_ed),\n             options.method: tallyCounts(post_cluster_binned, max_ed),\n             \"%s_null\" % options.method: tallyCounts(post_cluster_null_binned, max_ed),\n             \"edit_distance\": cluster_bins},\n            columns=[\"unique\", \"unique_null\", options.method,\n                     \"%s_null\" % options.method, \"edit_distance\"])\n\n        # TS - set lowest bin (-1) to \"Single_UMI\"\n        edit_distance_df['edit_distance'][0] = \"Single_UMI\"\n\n        edit_distance_df.to_csv(options.stats + \"_edit_distance.tsv\",\n                                index=False, sep=\"\\t\")\n\n    # write footer and output benchmark information.\n    U.info(\n        \"Reads: %s\" % \", \".join([\"%s: %s\" % (x[0], x[1]) for x in\n                                 bundle_iterator.read_events.most_common()]))\n\n    U.info(\"Number of reads out: %i\" % nOutput)\n\n    if not options.ignore_umi:  # otherwise processor has not been used\n        U.info(\"Total number of positions deduplicated: %i\" %\n               processor.UMIClusterer.positions)\n        if processor.UMIClusterer.positions > 0:\n            U.info(\"Mean number of unique UMIs per position: %.2f\" %\n                   (float(processor.UMIClusterer.total_umis_per_position) /\n                    processor.UMIClusterer.positions))\n            U.info(\"Max. number of unique UMIs per position: %i\" %\n                   processor.UMIClusterer.max_umis_per_position)\n        else:\n            U.warn(\"The BAM did not contain any valid \"\n                   \"reads/read pairs for deduplication\")\n\n    U.Stop()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main(argv=None):\n\n    if argv is None:\n        argv = sys.argv\n\n    # setup command line parser\n    parser = U.OptionParser(version=\"%prog version: $Id$\",\n                            usage=usage,\n                            description=globals()[\"__doc__\"])\n\n    group = U.OptionGroup(parser, \"whitelist-specific options\")\n\n    group.add_option(\"--plot-prefix\",\n                     dest=\"plot_prefix\", type=\"string\",\n                     help=(\"Prefix for plots to visualise the automated \"\n                           \"detection of the number of 'true' cell barcodes\"))\n    group.add_option(\"--subset-reads\",\n                     dest=\"subset_reads\", type=\"int\",\n                     help=(\"Use the first N reads to automatically identify \"\n                           \"the true cell barcodes. If N is greater than the \"\n                           \"number of reads, all reads will be used. \"\n                           \"Default is 100,000,000\"))\n    group.add_option(\"--error-correct-threshold\",\n                     dest=\"error_correct_threshold\",\n                     type=\"int\",\n                     help=(\"Hamming distance for correction of barcodes to \"\n                           \"whitelist barcodes. This value will also be used \"\n                           \"for error detection above the knee if required \"\n                           \"(--ed-above-threshold)\"))\n    group.add_option(\"--method\",\n                     dest=\"method\",\n                     choices=[\"reads\", \"umis\"],\n                     help=(\"Use reads or unique umi counts per cell\"))\n    group.add_option(\"--knee-method\",\n                     dest=\"knee_method\",\n                     choices=[\"distance\", \"density\"],\n                     help=(\"Use distance or density methods for detection of knee\"))\n    group.add_option(\"--expect-cells\",\n                     dest=\"expect_cells\",\n                     type=\"int\",\n                     help=(\"Prior expectation on the upper limit on the \"\n                           \"number of cells sequenced\"))\n    group.add_option(\"--allow-threshold-error\",\n                     dest=\"allow_threshold_error\", action=\"store_true\",\n                     help=(\"Don't select a threshold. Will still \"\n                           \"output the plots if requested (--plot-prefix)\"))\n    group.add_option(\"--set-cell-number\",\n                     dest=\"cell_number\",\n                     type=\"int\",\n                     help=(\"Specify the number of cell barcodes to accept\"))\n\n    parser.add_option(\"--ed-above-threshold\",\n                      dest=\"ed_above_threshold\", type=\"choice\",\n                      choices=[\"discard\", \"correct\"],\n                      help=(\"Detect CBs above the threshold which may be \"\n                            \"sequence errors from another CB and either \"\n                            \"'discard' or 'correct'. Default=discard\"))\n    parser.add_option_group(group)\n\n    parser.set_defaults(method=\"reads\",\n                        knee_method=\"distance\",\n                        extract_method=\"string\",\n                        filter_cell_barcodes=False,\n                        whitelist_tsv=None,\n                        blacklist_tsv=None,\n                        error_correct_threshold=1,\n                        pattern=None,\n                        pattern2=None,\n                        read2_in=None,\n                        plot_prefix=None,\n                        subset_reads=100000000,\n                        expect_cells=False,\n                        allow_threshold_error=False,\n                        cell_number=False,\n                        ed_above_threshold=None)\n\n    # add common options (-h/--help, ...) and parse command line\n\n    (options, args) = U.Start(parser, argv=argv,\n                              add_extract_options=True,\n                              add_group_dedup_options=False,\n                              add_umi_grouping_options=False,\n                              add_sam_options=False)\n\n    if options.expect_cells:\n        if options.knee_method == \"distance\":\n            U.error(\"Cannot use --expect-cells with 'distance' knee \"\n                    \"method. Switch to --knee-method=density if you want to \"\n                    \"provide an expectation for the number of \"\n                    \"cells. Alternatively, if you know the number of cell \"\n                    \"barcodes, use --cell-number\")\n        if options.cell_number:\n            U.error(\"Cannot supply both --expect-cells and \"\n                    \"--cell-number options\")\n\n    extract_cell, extract_umi = U.validateExtractOptions(options)\n\n    if not extract_cell:\n        if options.extract_method == \"string\":\n            U.error(\"barcode pattern(s) do not include any cell bases \"\n                    \"(marked with 'Cs') %s, %s\" % (\n                        options.pattern, options.pattern2))\n        elif options.extract_method == \"regex\":\n            U.error(\"barcode regex(es) do not include any cell groups \"\n                    \"(starting with 'cell_') %s, %s\" (\n                        options.pattern, options.pattern2))\n\n    read1s = umi_methods.fastqIterate(options.stdin)\n\n    # set up read extractor\n    ReadExtractor = extract_methods.ExtractFilterAndUpdate(\n        method=options.extract_method,\n        pattern=options.pattern,\n        pattern2=options.pattern2,\n        prime3=options.prime3,\n        extract_cell=extract_cell)\n\n    cell_barcode_counts = collections.Counter()\n\n    n_reads = 0\n    n_cell_barcodes = 0\n\n    # if using the umis method, need to keep a set of umis observed\n    if options.method == \"umis\":\n        cell_barcode_umis = collections.defaultdict(set)\n\n    # variables for progress monitor\n    displayMax = 100000\n    U.info(\"Starting barcode extraction\")\n\n    if not options.read2_in:\n        for read1 in read1s:\n\n            # Update display in every 100kth iteration\n            if n_reads % displayMax == 0:\n                U.info(\"Parsed {} reads\".format(n_reads))\n\n            n_reads += 1\n            barcode_values = ReadExtractor.getBarcodes(read1)\n            if barcode_values is None:\n                continue\n            else:\n                cell, umi, _, _, _, _, _ = barcode_values\n                if options.method == \"umis\":\n                    cell_barcode_umis[cell].add(umi)\n                else:\n                    cell_barcode_counts[cell] += 1\n                n_cell_barcodes += 1\n\n            if options.subset_reads:\n                if n_cell_barcodes > options.subset_reads:\n                    break\n    else:\n        read2s = umi_methods.fastqIterate(U.openFile(options.read2_in))\n        for read1, read2 in izip(read1s, read2s):\n\n            # Update display in every 100kth iteration\n            if n_reads % displayMax == 0:\n                U.info(\"Parsed {} reads\".format(n_reads))\n\n            n_reads += 1\n\n            barcode_values = ReadExtractor.getBarcodes(read1, read2)\n            if barcode_values is None:\n                continue\n            else:\n                cell, umi, _, _, _, _, _ = barcode_values\n                if options.method == \"umis\":\n                    cell_barcode_umis[cell].add(umi)\n                else:\n                    cell_barcode_counts[cell] += 1\n                n_cell_barcodes += 1\n\n            if options.subset_reads:\n                if n_reads > options.subset_reads:\n                    break\n\n    U.info(\"Starting - whitelist determination\")\n\n    if options.method == \"umis\":\n        for cell in cell_barcode_umis:\n            cell_barcode_counts[cell] = len(cell_barcode_umis[cell])\n\n    if options.cell_number and options.cell_number > len(cell_barcode_counts):\n        raise ValueError(\n            \"--set-cell-barcode option specifies more cell barcodes than the \"\n            \"number of observed cell barcodes. This may be because \"\n            \"--subset-reads was set to a value too low to capture reads from \"\n            \"all cells. %s cell barcodes observed from %s parsed reads. \"\n            \"Expected>= %s cell barcodes\" % (\n                len(cell_barcode_counts),\n                options.subset_reads,\n                options.cell_number))\n\n    cell_whitelist, true_to_false_map = whitelist_methods.getCellWhitelist(\n        cell_barcode_counts,\n        options.knee_method,\n        options.expect_cells,\n        options.cell_number,\n        options.error_correct_threshold,\n        options.plot_prefix)\n\n    if cell_whitelist:\n        U.info(\"Top %s cell barcodes passed the selected threshold\" %\n               len(cell_whitelist))\n\n    if options.ed_above_threshold:\n        cell_whitelist, true_to_false_map = whitelist_methods.errorDetectAboveThreshold(\n            cell_barcode_counts,\n            cell_whitelist,\n            true_to_false_map,\n            errors=options.error_correct_threshold,\n            resolution_method=options.ed_above_threshold)\n\n    if cell_whitelist:\n        U.info(\"Writing out whitelist\")\n        total_correct_barcodes = 0\n        total_corrected_barcodes = 0\n        for barcode in sorted(list(cell_whitelist)):\n\n            total_correct_barcodes += cell_barcode_counts[barcode]\n\n            if true_to_false_map:\n                corrected_barcodes = \",\".join(\n                    sorted(true_to_false_map[barcode]))\n\n                correct_barcode_counts = [cell_barcode_counts[x] for x in\n                                          sorted(true_to_false_map[barcode])]\n                total_corrected_barcodes += sum(correct_barcode_counts)\n\n                corrected_barcode_counts = \",\".join(\n                    map(str, correct_barcode_counts))\n            else:\n                corrected_barcodes, corrected_barcode_counts = \"\", \"\"\n\n            options.stdout.write(\"%s\\t%s\\t%s\\t%s\\n\" % (\n                barcode, corrected_barcodes, cell_barcode_counts[barcode],\n                corrected_barcode_counts))\n    else:\n        msg = (\"No local minima was accepted. Recommend checking the plot \"\n               \"output and counts per local minima (requires `--plot-prefix`\"\n               \"option) and then re-running with manually selected threshold \"\n               \"(`--set-cell-number` option)\")\n\n        if options.allow_threshold_error:\n            U.info(msg)\n        else:\n            U.error(msg)\n\n    U.info(\"Parsed %i reads\" % n_reads)\n    U.info(\"%i reads matched the barcode pattern\" % n_cell_barcodes)\n    U.info(\"Found %i unique cell barcodes\" % len(cell_barcode_counts))\n\n    if cell_whitelist:\n        U.info(\"Found %i total reads matching the selected cell barcodes\" %\n               total_correct_barcodes)\n        U.info(\"Found %i total reads which can be error corrected to the \"\n               \"selected cell barcodes\" % total_corrected_barcodes)\n\n    U.Stop()", "response": "main function for the base script."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nscript main. parses command line options in sys.argv, unless *argv* is given.", "response": "def main(argv=None):\n    \"\"\"script main.\n\n    parses command line options in sys.argv, unless *argv* is given.\n    \"\"\"\n\n    if argv is None:\n        argv = sys.argv\n\n    # setup command line parser\n    parser = U.OptionParser(version=\"%prog version: $Id$\",\n                            usage=usage,\n                            description=globals()[\"__doc__\"])\n\n    group = U.OptionGroup(parser, \"count-specific options\")\n\n    parser.add_option(\"--wide-format-cell-counts\", dest=\"wide_format_cell_counts\",\n                      action=\"store_true\",\n                      default=False,\n                      help=(\"output the cell counts in a wide format \"\n                            \"(rows=genes, columns=cells)\"))\n\n    parser.add_option_group(group)\n\n    # add common options (-h/--help, ...) and parse command line\n    (options, args) = U.Start(parser, argv=argv, add_group_dedup_options=False)\n\n    options.per_gene = True  # hardcodes counting to per-gene only\n\n    U.validateSamOptions(options, group=False)\n\n    if options.random_seed:\n        np.random.seed(options.random_seed)\n\n    if options.stdin != sys.stdin:\n        in_name = options.stdin.name\n        options.stdin.close()\n    else:\n        raise ValueError(\"Input on standard in not currently supported\")\n\n    if options.in_sam:\n        in_mode = \"r\"\n    else:\n        in_mode = \"rb\"\n\n    infile = pysam.Samfile(in_name, in_mode)\n\n    # write out to tempfile and then sort to stdout\n    tmpfilename = U.getTempFilename(dir=options.tmpdir)\n    tmpfile = U.openFile(tmpfilename, mode=\"w\")\n\n    nInput, nOutput, input_reads = 0, 0, 0\n\n    gene_tag = options.gene_tag\n    metacontig2contig = None\n\n    if options.chrom:\n        inreads = infile.fetch(reference=options.chrom)\n    else:\n        if options.gene_transcript_map:\n            metacontig2contig = sam_methods.getMetaContig2contig(\n                infile, options.gene_transcript_map)\n            metatag = \"MC\"\n            inreads = sam_methods.metafetcher(infile, metacontig2contig, metatag)\n            gene_tag = metatag\n        else:\n            inreads = infile.fetch()\n\n    bundle_iterator = sam_methods.get_bundles(\n        options,\n        only_count_reads=True,\n        metacontig_contig=metacontig2contig)\n\n    # set up UMIClusterer functor with methods specific to\n    # specified options.method\n\n    processor = network.UMIClusterer(options.method)\n\n    for bundle, key, status in bundle_iterator(inreads):\n        if status == \"single_read\":\n            continue\n\n        gene, cell = key\n\n        umis = bundle.keys()\n        counts = {umi: bundle[umi][\"count\"] for umi in umis}\n\n        nInput += sum(counts.values())\n\n        while nInput >= input_reads + 1000000:\n            input_reads += 1000000\n            U.info(\"Parsed %i input reads\" % input_reads)\n\n        # group the umis\n        groups = processor(\n            counts,\n            threshold=options.threshold)\n\n        gene_count = len(groups)\n\n        if options.per_cell:\n            tmpfile.write(\"%s\\n\" % \"\\t\".join((gene, cell.decode(), str(gene_count))))\n        else:\n            tmpfile.write(\"%s\\n\" % \"\\t\".join((gene, str(gene_count))))\n\n        nOutput += gene_count\n\n    tmpfile.close()\n\n    if options.per_cell:\n\n        gene_counts_dict = {}\n\n        with U.openFile(tmpfilename, mode=\"r\") as inf:\n            genes = set()\n            cells = set()\n            for line in inf:\n                gene, cell, gene_count = line.strip().split(\"\\t\")\n                genes.add(gene)\n                cells.add(cell)\n\n                if gene not in gene_counts_dict:\n                    gene_counts_dict[gene] = {}\n\n                gene_counts_dict[gene][cell] = gene_count\n\n        if options.wide_format_cell_counts:  # write out in wide format\n\n            options.stdout.write(\n                \"%s\\t%s\\n\" % (\"gene\", \"\\t\".join(sorted(cells))))\n\n            for gene in sorted(genes):\n                counts = []\n                for cell in sorted(cells):\n                    if cell in gene_counts_dict[gene]:\n                        counts.append(gene_counts_dict[gene][cell])\n                    else:\n                        counts.append(0)\n                options.stdout.write(\n                    \"%s\\t%s\\n\" % (gene, \"\\t\".join(map(str, counts))))\n\n        else:  # write out in long format\n            options.stdout.write(\"%s\\t%s\\t%s\\n\" % (\"gene\", \"cell\", \"count\"))\n            for gene in sorted(genes):\n                for cell in sorted(list(gene_counts_dict[gene].keys())):\n                    options.stdout.write(\"%s\\t%s\\t%s\\n\" % (\n                        gene, cell, gene_counts_dict[gene][cell]))\n    else:\n        options.stdout.write(\"%s\\t%s\\n\" % (\"gene\", \"count\"))\n\n        with U.openFile(tmpfilename, mode=\"r\") as inf:\n            for line in inf:\n                options.stdout.write(line)\n\n    os.unlink(tmpfilename)\n\n    # output reads events and benchmark information.\n    for event in bundle_iterator.read_events.most_common():\n        U.info(\"%s: %s\" % (event[0], event[1]))\n\n    U.info(\"Number of (post deduplication) reads counted: %i\" % nOutput)\n\n    U.Stop()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main(argv=None):\n\n    if argv is None:\n        argv = sys.argv\n\n    # setup command line parser\n    parser = U.OptionParser(version=\"%prog version: $Id$\",\n                            usage=usage,\n                            description=globals()[\"__doc__\"])\n\n    group = U.OptionGroup(parser, \"extract-specific options\")\n\n    # (Experimental option) Retain the UMI in the sequence read\"\n    group.add_option(\"--retain-umi\", dest=\"retain_umi\", action=\"store_true\",\n                     help=optparse.SUPPRESS_HELP)\n    group.add_option(\"--read2-out\", dest=\"read2_out\", type=\"string\",\n                     help=\"file to output processed paired read to\")\n    group.add_option(\"--read2-stdout\", dest=\"read2_stdout\",\n                     action=\"store_true\",\n                     help=\"Paired reads, send read2 to stdout, discarding read1\")\n    group.add_option(\"--quality-filter-threshold\",\n                     dest=\"quality_filter_threshold\", type=\"int\",\n                     help=(\"Remove reads where any UMI base quality score \"\n                           \"falls below this threshold\"))\n    group.add_option(\"--quality-filter-mask\",\n                     dest=\"quality_filter_mask\", type=\"int\",\n                     help=(\"If a UMI base has a quality below this threshold, \"\n                           \"replace the base with 'N'\"))\n    group.add_option(\"--quality-encoding\",\n                     dest=\"quality_encoding\", type=\"choice\",\n                     choices=[\"phred33\", \"phred64\", \"solexa\"],\n                     help=(\"Quality score encoding. Choose from 'phred33'\"\n                           \"[33-77] 'phred64' [64-106] or 'solexa' [59-106]\"))\n    group.add_option(\"--filter-cell-barcode\",\n                     dest=\"filter_cell_barcode\",\n                     action=\"store_true\",\n                     help=\"Filter the cell barcodes\")\n    group.add_option(\"--error-correct-cell\",\n                     dest=\"error_correct_cell\",\n                     action=\"store_true\",\n                     help=(\"Correct errors in the cell barcode\"))\n    group.add_option(\"--whitelist\",\n                     dest=\"whitelist\", type=\"string\",\n                     help=(\"A whitelist of accepted cell barcodes\"))\n    group.add_option(\"--blacklist\",\n                     dest=\"blacklist\", type=\"string\",\n                     help=(\"A blacklist of rejected cell barcodes\"))\n    group.add_option(\"--subset-reads\", \"--reads-subset\",\n                     dest=\"reads_subset\", type=\"int\",\n                     help=(\"Only extract from the first N reads. If N is \"\n                           \"greater than the number of reads, all reads will \"\n                           \"be used\"))\n    group.add_option(\"--reconcile-pairs\",\n                     dest=\"reconcile\", action=\"store_true\",\n                     help=(\"Allow the presences of reads in read2 input that \"\n                           \"are not present in read1 input. This allows cell \"\n                           \"barcode filtering of read1s without \"\n                           \"considering read2s\"))\n\n    parser.add_option_group(group)\n\n    group = U.OptionGroup(parser, \"[EXPERIMENTAl] barcode extraction options\")\n\n    group.add_option(\"--either-read\", dest=\"either_read\", action=\"store_true\",\n                     help=\"UMI may be on either read (see \"\n                     \"--either-read-resolve) for options to resolve cases where\"\n                     \"UMI is on both reads\")\n    group.add_option(\"--either-read-resolve\",\n                     dest=\"either_read_resolve\", type=\"choice\",\n                     choices=[\"discard\", \"quality\"],\n                     help=(\"How to resolve instances where both reads \"\n                           \"contain a UMI but using --either-read.\"\n                           \"Choose from 'discard' or 'quality'\"\n                           \"(use highest quality). default=dicard\"))\n\n    parser.add_option_group(group)\n\n    parser.set_defaults(extract_method=\"string\",\n                        filter_cell_barcodes=False,\n                        whitelist=None,\n                        blacklist=None,\n                        error_correct_cell=False,\n                        pattern=None,\n                        pattern2=None,\n                        read2_in=None,\n                        read2_out=False,\n                        read2_stdout=False,\n                        quality_filter_threshold=None,\n                        quality_encoding=None,\n                        reconcile=False,\n                        either_read=False,\n                        either_read_resolve=\"discard\")\n\n    # add common options (-h/--help, ...) and parse command line\n\n    (options, args) = U.Start(parser, argv=argv,\n                              add_extract_options=True,\n                              add_group_dedup_options=False,\n                              add_umi_grouping_options=False,\n                              add_sam_options=False)\n\n    if options.retain_umi and not options.extract_method == \"regex\":\n        raise ValueError(\"option --retain-umi only works with --extract-method=regex\")\n\n    if options.quality_filter_threshold or options.quality_filter_mask:\n        if not options.quality_encoding:\n            U.error(\"must provide a quality encoding (--quality-\"\n                    \"encoding) to filter UMIs by quality (--quality\"\n                    \"-filter-threshold) or mask low quality bases \"\n                    \"with (--quality-filter-mask)\")\n\n    extract_cell, extract_umi = U.validateExtractOptions(options)\n\n    if options.either_read:\n        if extract_cell:\n            U.error(\"Option to extract from either read (--either-read) \"\n                    \"is not currently compatible with cell barcode extraction\")\n        if not options.extract_method == \"regex\":\n            U.error(\"Option to extract from either read (--either-read)\"\n                    \"requires --extract-method=regex\")\n        if not options.pattern or not options.pattern2:\n            U.error(\"Option to extract from either read (--either-read)\"\n                    \"requires --bc-pattern=[PATTERN1] and\"\n                    \"--bc-pattern2=[PATTERN2]\")\n\n    if options.filter_cell_barcodes:\n\n        if not options.whitelist:\n                U.error(\"must provide a whitelist (--whitelist) if using \"\n                        \"--filter-cell-barcode option\")\n\n        if not extract_cell:\n            if options.extract_method == \"string\":\n                U.error(\"barcode pattern(s) do not include any cell bases \"\n                        \"(marked with 'Cs') %s, %s\" % (\n                            options.pattern, options.pattern2))\n            elif options.extract_method == \"regex\":\n                U.error(\"barcode regex(es) do not include any cell groups \"\n                        \"(starting with 'cell_') %s, %s\" (\n                            options.pattern, options.pattern2))\n\n    read1s = umi_methods.fastqIterate(options.stdin)\n\n    # set up read extractor\n    ReadExtractor = extract_methods.ExtractFilterAndUpdate(\n        options.extract_method,\n        options.pattern,\n        options.pattern2,\n        options.prime3,\n        extract_cell,\n        options.quality_encoding,\n        options.quality_filter_threshold,\n        options.quality_filter_mask,\n        options.filter_cell_barcode,\n        options.retain_umi,\n        options.either_read,\n        options.either_read_resolve)\n\n    if options.filter_cell_barcode:\n        cell_whitelist, false_to_true_map = whitelist_methods.getUserDefinedBarcodes(\n            options.whitelist, options.error_correct_cell)\n\n        ReadExtractor.cell_whitelist = cell_whitelist\n        ReadExtractor.false_to_true_map = false_to_true_map\n\n    if options.blacklist:\n        blacklist = set()\n        with U.openFile(options.blacklist, \"r\") as inf:\n            for line in inf:\n                blacklist.add(line.strip().split(\"\\t\")[0])\n        ReadExtractor.cell_blacklist = blacklist\n\n    # variables for progress monitor\n    progCount = 0\n    displayMax = 100000\n    U.info(\"Starting barcode extraction\")\n\n    if options.read2_in is None:\n        for read in read1s:\n\n            # incrementing count for monitoring progress\n            progCount += 1\n\n            # Update display in every 100kth iteration\n            if progCount % displayMax == 0:\n                U.info(\"Parsed {} reads\".format(progCount))\n\n            new_read = ReadExtractor(read)\n\n            if options.reads_subset:\n                if (ReadExtractor.read_counts['Input Reads'] >\n                    options.reads_subset):\n                    break\n\n            if not new_read:\n                continue\n\n            options.stdout.write(str(new_read) + \"\\n\")\n\n    else:\n        read2s = umi_methods.fastqIterate(U.openFile(options.read2_in))\n\n        if options.read2_out:\n            read2_out = U.openFile(options.read2_out, \"w\")\n\n        if options.reconcile:\n            strict = False\n        else:\n            strict = True\n\n        for read1, read2 in umi_methods.joinedFastqIterate(\n                read1s, read2s, strict):\n\n            # incrementing count for monitoring progress\n            progCount += 1\n\n            # Update display in every 100kth iteration\n            if progCount % displayMax == 0:\n                U.info(\"Parsed {} reads\".format(progCount))\n                sys.stdout.flush()\n\n            reads = ReadExtractor(read1, read2)\n\n            if options.reads_subset:\n                if (ReadExtractor.read_counts['Input Reads'] >\n                    options.reads_subset):\n                    break\n\n            if not reads:\n                continue\n            else:\n                new_read1, new_read2 = reads\n\n            if options.read2_stdout:\n                options.stdout.write(str(new_read2) + \"\\n\")\n            else:\n                options.stdout.write(str(new_read1) + \"\\n\")\n\n                if options.read2_out:\n                    read2_out.write(str(new_read2) + \"\\n\")\n\n    if options.read2_out:\n        read2_out.close()\n\n    for k, v in ReadExtractor.getReadCounts().most_common():\n        U.info(\"%s: %s\" % (k, v))\n\n    U.Stop()", "response": "main function of the main script."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ordinary_diffusion(target,\n                       pore_area='pore.area',\n                       throat_area='throat.area',\n                       pore_diffusivity='pore.diffusivity',\n                       throat_diffusivity='throat.diffusivity',\n                       conduit_lengths='throat.conduit_lengths',\n                       conduit_shape_factors='throat.poisson_shape_factors'):\n    r\"\"\"\n    Calculate the diffusive conductance of conduits in network, where a\n    conduit is ( 1/2 pore - full throat - 1/2 pore ). See the notes section.\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object which this model is associated with. This controls the\n        length of the calculated array, and also provides access to other\n        necessary properties.\n\n    pore_diffusivity : string\n        Dictionary key of the pore diffusivity values\n\n    throat_diffusivity : string\n        Dictionary key of the throat diffusivity values\n\n    pore_area : string\n        Dictionary key of the pore area values\n\n    throat_area : string\n        Dictionary key of the throat area values\n\n    conduit_shape_factors : string\n        Dictionary key of the conduit DIFFUSION shape factor values\n\n    Returns\n    -------\n    g : ndarray\n        Array containing diffusive conductance values for conduits in the\n        geometry attached to the given physics object.\n\n    Notes\n    -----\n    (1) This function requires that all the necessary phase properties already\n    be calculated.\n\n    (2) This function calculates the specified property for the *entire*\n    network then extracts the values for the appropriate throats at the end.\n\n    (3) This function assumes cylindrical throats with constant cross-section\n    area. Corrections for different shapes and variable cross-section area can\n    be imposed by passing the proper flow_shape_factor argument.\n\n    \"\"\"\n    return generic_conductance(target=target, transport_type='diffusion',\n                               pore_area=pore_area,\n                               throat_area=throat_area,\n                               pore_diffusivity=pore_diffusivity,\n                               throat_diffusivity=throat_diffusivity,\n                               conduit_lengths=conduit_lengths,\n                               conduit_shape_factors=conduit_shape_factors)", "response": "r Returns the diffusive conductance of the given physics object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_dataframe(cls, network=None, phases=[], join=False, delim=' | '):\n        project, network, phases = cls._parse_args(network=network,\n                                                   phases=phases)\n\n        # Initialize pore and throat data dictionary using Dict class\n        pdata = Dict.to_dict(network=network, phases=phases, element='pore',\n                             interleave=True, flatten=True,\n                             categorize_by=['object'])\n        tdata = Dict.to_dict(network=network, phases=phases, element='throat',\n                             interleave=True, flatten=True,\n                             categorize_by=['object'])\n        pdata = FlatDict(pdata, delimiter=delim)\n        tdata = FlatDict(tdata, delimiter=delim)\n\n        # Scan data and convert non-1d arrays to multiple columns\n        for key in list(pdata.keys()):\n            if sp.shape(pdata[key]) != (network[0].Np,):\n                arr = pdata.pop(key)\n                tmp = sp.split(arr, arr.shape[1], axis=1)\n                cols = range(len(tmp))\n                pdata.update({key+'['+str(i)+']': tmp[i].squeeze()\n                              for i in cols})\n        for key in list(tdata.keys()):\n            if sp.shape(tdata[key]) != (network[0].Nt,):\n                arr = tdata.pop(key)\n                tmp = sp.split(arr, arr.shape[1], axis=1)\n                cols = range(len(tmp))\n                tdata.update({key+'['+str(i)+']': tmp[i].squeeze()\n                              for i in cols})\n\n        # Convert sanitized dictionaries to DataFrames\n        pdata = pd.DataFrame(sanitize_dict(pdata))\n        tdata = pd.DataFrame(sanitize_dict(tdata))\n\n        # Prepare DataFrames to be returned\n        if join:\n            data = tdata.join(other=pdata, how='left')\n        else:\n            nt = namedtuple('dataframes', ('pore', 'throat'))\n            data = nt(pore=pdata, throat=tdata)\n\n        return data", "response": "r Convert the Network and optionally Phase objects into Pandas DataFrames."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mason_morrow(target, throat_perimeter='throat.perimeter',\n                 throat_area='throat.area'):\n    r\"\"\"\n    Mason and Morrow relate the capillary pressure to the shaped factor in a\n    similar way to Mortensen but for triangles.\n\n    References\n    ----------\n    Mason, G. and Morrow, N.R.. Capillary behavior of a perfectly wetting\n    liquid in irregular triangular tubes. Journal of Colloid and Interface\n    Science, 141(1), pp.262-274 (1991).\n    \"\"\"\n    # Only apply to throats with an area\n    ts = target.throats()[target[throat_area] <= 0]\n    P = target[throat_perimeter]\n    A = target[throat_area]\n    value = A/(P**2)\n    value[ts] = 1/(4*_sp.pi)\n    return value", "response": "r Returns the capillary pressure of a Mason and Morrow throat."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef jenkins_rao(target, throat_perimeter='throat.perimeter',\n                throat_area='throat.area',\n                throat_diameter='throat.indiameter'):\n    r\"\"\"\n    Jenkins and Rao relate the capillary pressure in an eliptical throat to\n    the aspect ratio\n\n    References\n    ----------\n    Jenkins, R.G. and Rao, M.B., The effect of elliptical pores on\n    mercury porosimetry results. Powder technology, 38(2), pp.177-180. (1984)\n    \"\"\"\n    P = target[throat_perimeter]\n    A = target[throat_area]\n    r = target[throat_diameter]/2\n    # Normalized by value for perfect circle\n    value = (P/A)/(2/r)\n    return value", "response": "r Returns the value of Jenkins and Rao relate the capillary pressure in an eliptical throat to Jenkins and Rao."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _initialize_A(self):\n        # Matrix of coefficients size\n        self.m, self.n = (self.A).shape\n\n        # Create a petsc sparse matrix\n        self.petsc_A = PETSc.Mat()\n        self.petsc_A.create(PETSc.COMM_WORLD)\n        self.petsc_A.setSizes([self.m, self.n])\n        self.petsc_A.setType('aij')  # sparse\n        self.petsc_A.setUp()\n\n        # Pre-allocate memory for the coefficients matrix.\n        # Optional, but needed in the case where the\n        # matrix does not already exist.\n        # self.petsc_A.setPreallocationNNZ([ \\\n        # sp.sparse.csr_matrix.getnnz(A,axis=0)])\n\n        # Loop over owned block of rows on this processor\n        # and insert entry values (for parallel computing).\n        self.Istart, self.Iend = self.petsc_A.getOwnershipRange()\n\n        # Assign values to the coefficients matrix from the scipy\n        # sparse csr one: petscMat = \\\n        # PETSc.Mat().createAIJ(size=existingMat.shape, csr= \\\n        # (existingMat.indptr,existingMat.indices,existingMat.data))\n\n        size_tmp = self.A.shape\n        csr1 = (self.A.indptr[self.Istart:self.Iend+1] -\n                self.A.indptr[self.Istart])\n        ind1 = self.A.indptr[self.Istart]\n        ind2 = self.A.indptr[self.Iend]\n        csr2 = self.A.indices[ind1:ind2]\n        csr3 = self.A.data[ind1:ind2]\n        self.petsc_A = PETSc.Mat().createAIJ(size=size_tmp,\n                                             csr=(csr1, csr2, csr3))\n\n        # Communicate off-processor values\n        # and setup internal data structures\n        # for performing parallel operations\n\n        self.petsc_A.assemblyBegin()\n        self.petsc_A.assemblyEnd()", "response": "r This method creates the petsc sparse coefficients matrix from the OpenPNM scipy one and assigns values to the sparse matrix at the current level of the processor."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calc_effective_permeability(self, inlets=None, outlets=None,\n                                    domain_area=None, domain_length=None):\n        r\"\"\"\n        This calculates the effective permeability in this linear transport\n        algorithm.\n\n        Parameters\n        ----------\n        inlets : array_like\n            The pores where the inlet pressure boundary conditions were\n            applied.  If not given an attempt is made to infer them from the\n            algorithm.\n\n        outlets : array_like\n            The pores where the outlet pressure boundary conditions were\n            applied.  If not given an attempt is made to infer them from the\n            algorithm.\n\n        domain_area : scalar, optional\n            The area of the inlet (and outlet) boundary faces.  If not given\n            then an attempt is made to estimate it, but it is usually\n            underestimated.\n\n        domain_length : scalar, optional\n            The length of the domain between the inlet and outlet boundary\n            faces.  If not given then an attempt is made to estimate it, but it\n            is usually underestimated.\n\n        Notes\n        -----\n        The area and length of the domain are found using the bounding box\n        around the inlet and outlet pores which do not necessarily lie on the\n        edge of the domain, resulting in underestimation of sizes.\n        \"\"\"\n        phase = self.project.phases()[self.settings['phase']]\n        d_normal = self._calc_eff_prop(inlets=inlets, outlets=outlets,\n                                       domain_area=domain_area,\n                                       domain_length=domain_length)\n        K = d_normal * sp.mean(phase['pore.viscosity'])\n        return K", "response": "r This calculates the effective permeability of the given inlet and outlet pores."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setup(self, phase, entry_pressure='', pore_volume='', throat_volume=''):\n        self.settings['phase'] = phase.name\n        if pore_volume:\n            self.settings['pore_volume'] = pore_volume\n        if throat_volume:\n            self.settings['throat_volume'] = throat_volume\n        if entry_pressure:\n            self.settings['entry_pressure'] = entry_pressure\n\n        # Setup arrays and info\n        self['throat.entry_pressure'] = phase[self.settings['entry_pressure']]\n        # Indices into t_entry giving a sorted list\n        self['throat.sorted'] = sp.argsort(self['throat.entry_pressure'], axis=0)\n        self['throat.order'] = 0\n        self['throat.order'][self['throat.sorted']] = sp.arange(0, self.Nt)\n        self['throat.invasion_sequence'] = -1\n        self['pore.invasion_sequence'] = -1\n        self._tcount = 0", "response": "r This method is called by the constructor of the base class s constructor. It is called by the constructor of the base class s constructor."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_inlets(self, pores=[], overwrite=False):\n        if overwrite:\n            self['pore.invasion_sequence'] = -1\n        self['pore.invasion_sequence'][pores] = 0\n\n        # Perform initial analysis on input pores\n        Ts = self.project.network.find_neighbor_throats(pores=pores)\n        self.queue = []\n        [hq.heappush(self.queue, T) for T in self['throat.order'][Ts]]", "response": "r Set the inlet sequence of the given pores."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef results(self, Snwp):\n        net = self.project.network\n        P12 = net['throat.conns']\n        # Fetch void volume for pores and throats\n        Vp = net[self.settings['pore_volume']]\n        Vt = net[self.settings['throat_volume']]\n        # Fetch the order of filling\n        Np = self['pore.invasion_sequence']\n        Nt = self['throat.invasion_sequence']\n        # Create Nt-long mask of which pores were filled when throat was filled\n        Pinv = (Np[P12].T == Nt).T\n        # If a pore and throat filled together, find combined volume\n        Vinv = sp.vstack(((Pinv*Vp[P12]).T, Vt)).T\n        Vinv = sp.sum(Vinv, axis=1)\n        # Convert to cumulative volume filled as each throat is invaded\n        x = sp.argsort(Nt)  # Find order throats were invaded\n        Vinv_cum = np.cumsum(Vinv[x])\n        # Normalized cumulative volume filled into saturation\n        S = Vinv_cum/(Vp.sum() + Vt.sum())\n        # Find throat invasion step where Snwp was reached\n        try:\n            N = sp.where(S < Snwp)[0][-1]\n        except:\n            N = -np.inf\n        data = {'pore.occupancy': Np <= N, 'throat.occupancy': Nt <= N}\n        return data", "response": "r Returns the phase configuration at the specified non - wetting phase at the specified saturation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hagen_poiseuille(target,\n                     pore_area='pore.area',\n                     throat_area='throat.area',\n                     pore_viscosity='pore.viscosity',\n                     throat_viscosity='throat.viscosity',\n                     conduit_lengths='throat.conduit_lengths',\n                     conduit_shape_factors='throat.flow_shape_factors'):\n    r\"\"\"\n    Calculate the hydraulic conductance of conduits in network, where a\n    conduit is ( 1/2 pore - full throat - 1/2 pore ). See the notes section.\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object which this model is associated with. This controls the\n        length of the calculated array, and also provides access to other\n        necessary properties.\n\n    pore_viscosity : string\n        Dictionary key of the pore viscosity values\n\n    throat_viscosity : string\n        Dictionary key of the throat viscosity values\n\n    pore_area : string\n        Dictionary key of the pore area values\n\n    throat_area : string\n        Dictionary key of the throat area values\n\n    conduit_shape_factors : string\n        Dictionary key of the conduit FLOW shape factor values\n\n    Returns\n    -------\n    g : ndarray\n        Array containing hydraulic conductance values for conduits in the\n        geometry attached to the given physics object.\n\n    Notes\n    -----\n    (1) This function requires that all the necessary phase properties already\n    be calculated.\n\n    (2) This function calculates the specified property for the *entire*\n    network then extracts the values for the appropriate throats at the end.\n\n    (3) This function assumes cylindrical throats with constant cross-section\n    area. Corrections for different shapes and variable cross-section area can\n    be imposed by passing the proper flow_shape_factor argument.\n\n    \"\"\"\n    return generic_conductance(target=target, transport_type='flow',\n                               pore_area=pore_area,\n                               throat_area=throat_area,\n                               pore_diffusivity=pore_viscosity,\n                               throat_diffusivity=throat_viscosity,\n                               conduit_lengths=conduit_lengths,\n                               conduit_shape_factors=conduit_shape_factors)", "response": "r This function calculates the hydraulic conductance of conduits in the network."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generic_conductance(target, transport_type, pore_diffusivity,\n                        throat_diffusivity, pore_area, throat_area,\n                        conduit_lengths, conduit_shape_factors):\n    r\"\"\"\n    Calculate the generic conductance (could be mass, thermal, electrical,\n    or hydraylic) of conduits in the network, where a conduit is\n    ( 1/2 pore - full throat - 1/2 pore ).\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object which this model is associated with. This controls the\n        length of the calculated array, and also provides access to other\n        necessary properties.\n\n    pore_diffusivity : string\n        Dictionary key of the pore diffusivity values.\n\n    throat_diffusivity : string\n        Dictionary key of the throat diffusivity values.\n\n    pore_area : string\n        Dictionary key of the pore area values.\n\n    throat_area : string\n        Dictionary key of the throat area values.\n\n    shape_factor : string\n        Dictionary key of the conduit shape factor values.\n\n    Returns\n    -------\n    g : ndarray\n        Array containing conductance values for conduits in the geometry\n        attached to the given physics object.\n\n    Notes\n    -----\n    (1) This function requires that all the necessary phase properties already\n    be calculated.\n\n    (2) This function calculates the specified property for the *entire*\n    network then extracts the values for the appropriate throats at the end.\n\n    (3) This function assumes cylindrical throats with constant cross-section\n    area. Corrections for different shapes and variable cross-section area can\n    be imposed by passing the proper shape factor.\n\n    (4) shape_factor depends on the physics of the problem, i.e. diffusion-like\n    processes and fluid flow need different shape factors.\n\n    \"\"\"\n    network = target.project.network\n    throats = network.map_throats(throats=target.Ts, origin=target)\n    phase = target.project.find_phase(target)\n    cn = network['throat.conns'][throats]\n    # Getting equivalent areas\n    A1 = network[pore_area][cn[:, 0]]\n    At = network[throat_area][throats]\n    A2 = network[pore_area][cn[:, 1]]\n    # Getting conduit lengths\n    L1 = network[conduit_lengths + '.pore1'][throats]\n    Lt = network[conduit_lengths + '.throat'][throats]\n    L2 = network[conduit_lengths + '.pore2'][throats]\n    # Getting shape factors\n    try:\n        SF1 = phase[conduit_shape_factors+'.pore1'][throats]\n        SFt = phase[conduit_shape_factors+'.throat'][throats]\n        SF2 = phase[conduit_shape_factors+'.pore2'][throats]\n    except KeyError:\n        SF1 = SF2 = SFt = 1.0\n    # Interpolate pore phase property values to throats\n    try:\n        Dt = phase[throat_diffusivity][throats]\n    except KeyError:\n        Dt = phase.interpolate_data(propname=pore_diffusivity)[throats]\n    try:\n        D1 = phase[pore_diffusivity][cn[:, 0]]\n        D2 = phase[pore_diffusivity][cn[:, 1]]\n    except KeyError:\n        D1 = phase.interpolate_data(propname=throat_diffusivity)[cn[:, 0]]\n        D2 = phase.interpolate_data(propname=throat_diffusivity)[cn[:, 1]]\n    # Preallocating g\n    g1, g2, gt = _sp.zeros((3, len(Lt)))\n    # Setting g to inf when Li = 0 (ex. boundary pores)\n    # INFO: This is needed since area could also be zero, which confuses NumPy\n    m1, m2, mt = [Li != 0 for Li in [L1, L2, Lt]]\n    g1[~m1] = g2[~m2] = gt[~mt] = _sp.inf\n    # Find g for half of pore 1, throat, and half of pore 2\n    if transport_type == 'flow':\n        g1[m1] = A1[m1]**2 / (8*pi*D1*L1)[m1]\n        g2[m2] = A2[m2]**2 / (8*pi*D2*L2)[m2]\n        gt[mt] = At[mt]**2 / (8*pi*Dt*Lt)[mt]\n    elif transport_type == 'diffusion':\n        g1[m1] = (D1*A1)[m1] / L1[m1]\n        g2[m2] = (D2*A2)[m2] / L2[m2]\n        gt[mt] = (Dt*At)[mt] / Lt[mt]\n    else:\n        raise Exception('Unknown keyword for \"transport_type\", can only be' +\n                        ' \"flow\" or \"diffusion\"')\n    # Apply shape factors and calculate the final conductance\n    return (1/gt/SFt + 1/g1/SF1 + 1/g2/SF2)**(-1)", "response": "r This function calculates the generic conductance for a given target."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extend(self, obj):\n        if type(obj) is not list:\n            obj = [obj]\n        for item in obj:\n            if hasattr(item, '_mro'):\n                if 'GenericNetwork' in item._mro():\n                    if self.network:\n                        raise Exception('Project already has a network')\n                # Must use append since extend breaks the dicts up into\n                # separate objects, while append keeps it as a single object.\n                super().append(item)\n            else:\n                raise Exception('Only OpenPNM objects can be added')", "response": "r Extends the internal list of objects with the contents of obj."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copy(self, name=None):\n        if name is None:\n            name = ws._gen_name()\n        proj = deepcopy(self)\n        ws[name] = proj\n        return proj", "response": "r Creates a deep copy of the current project and returns a new project containing all the unique versions of all the objects."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_phase(self, obj):\n        # If received phase, just return self\n        if obj._isa('phase'):\n            return obj\n        # If phase happens to be in settings (i.e. algorithm), look it up\n        if 'phase' in obj.settings.keys():\n            phase = self.phases()[obj.settings['phase']]\n            return phase\n        # Otherwise find it using bottom-up approach (i.e. look in phase keys)\n        for phase in self.phases().values():\n            if ('pore.'+obj.name in phase) or ('throat.'+obj.name in phase):\n                return phase\n        # If all else fails, throw an exception\n        raise Exception('Cannot find a phase associated with '+obj.name)", "response": "r Returns the OpenPNM Phase associated with a given object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_full_domain(self, obj):\n        if 'Subdomain' not in obj._mro():\n            # Network, Phase, Alg\n            return obj\n        else:\n            if obj._isa() == 'geometry':\n                # Geom\n                return self.network\n            else:\n                # Phys\n                return self.find_phase(obj)", "response": "r Returns the full domain object associated with a given object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef purge_object(self, obj, deep=False):\n        if obj._isa() in ['physics', 'algorithm']:\n            self._purge(obj)\n        if obj._isa() == 'geometry':\n            if deep:\n                physics = self.find_physics(geometry=obj)\n                for phys in physics:\n                    self._purge(self.physics()[phys.name])\n            self._purge(obj)\n        if obj._isa() == 'phase':\n            if deep:\n                physics = self.find_physics(phase=obj)\n                for phys in physics:\n                    self._purge(self.physics()[phys.name])\n            self._purge(obj)\n        if obj._isa() == 'network':\n            raise Exception('Cannot purge a network, just make a new project')", "response": "r Removes an object from the project."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_object(self, filename):\n        p = Path(filename)\n        with open(p, 'rb') as f:\n            d = pickle.load(f)\n        obj = self._new_object(objtype=p.suffix.strip('.'),\n                               name=p.name.split('.')[0])\n        obj.update(d)", "response": "r Loads a single object from a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _fetch_data(self):\n        with h5py.File(self.name + '.hdf5') as f:\n            # Reload data into project\n            for item in f.keys():\n                obj_name, propname = item.split('|')\n                propname = propname.split('_')\n                propname = propname[0] + '.' + '_'.join(propname[1:])\n                self[obj_name][propname] = f[item]", "response": "r Fetches data from an HDF5 file and places onto correct objects in the project."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_geometry_health(self):\n        health = HealthDict()\n        health['overlapping_pores'] = []\n        health['undefined_pores'] = []\n        health['overlapping_throats'] = []\n        health['undefined_throats'] = []\n        geoms = self.geometries().keys()\n        if len(geoms):\n            net = self.network\n            Ptemp = np.zeros((net.Np,))\n            Ttemp = np.zeros((net.Nt,))\n            for item in geoms:\n                Pind = net['pore.'+item]\n                Tind = net['throat.'+item]\n                Ptemp[Pind] = Ptemp[Pind] + 1\n                Ttemp[Tind] = Ttemp[Tind] + 1\n            health['overlapping_pores'] = np.where(Ptemp > 1)[0].tolist()\n            health['undefined_pores'] = np.where(Ptemp == 0)[0].tolist()\n            health['overlapping_throats'] = np.where(Ttemp > 1)[0].tolist()\n            health['undefined_throats'] = np.where(Ttemp == 0)[0].tolist()\n        return health", "response": "r Perform a check to find pores with overlapping or undefined Geometries\n        Returns ------- HealthDict A"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_physics_health(self, phase):\n        health = HealthDict()\n        health['overlapping_pores'] = []\n        health['undefined_pores'] = []\n        health['overlapping_throats'] = []\n        health['undefined_throats'] = []\n        geoms = self.geometries().keys()\n        if len(geoms):\n            phys = self.find_physics(phase=phase)\n            if len(phys) == 0:\n                raise Exception(str(len(geoms))+' geometries were found, but' +\n                                ' no physics')\n            if None in phys:\n                raise Exception('Undefined physics found, check the grid')\n            Ptemp = np.zeros((phase.Np,))\n            Ttemp = np.zeros((phase.Nt,))\n            for item in phys:\n                    Pind = phase['pore.'+item.name]\n                    Tind = phase['throat.'+item.name]\n                    Ptemp[Pind] = Ptemp[Pind] + 1\n                    Ttemp[Tind] = Ttemp[Tind] + 1\n            health['overlapping_pores'] = np.where(Ptemp > 1)[0].tolist()\n            health['undefined_pores'] = np.where(Ptemp == 0)[0].tolist()\n            health['overlapping_throats'] = np.where(Ttemp > 1)[0].tolist()\n            health['undefined_throats'] = np.where(Ttemp == 0)[0].tolist()\n        return health", "response": "r Perform a check to find pores which have overlapping or missing Physics."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef results(self, Pc):\n        p_inv, t_inv = super().results(Pc).values()\n        phase = self.project.find_phase(self)\n        quantity = self.settings['quantity'].split('.')[-1]\n        lpf = np.array([1])\n        if self.settings['pore_partial_filling']:\n            # Set pressure on phase to current capillary pressure\n            phase['pore.'+quantity] = Pc\n            # Regenerate corresponding physics model\n            for phys in self.project.find_physics(phase=phase):\n                phys.regenerate_models(self.settings['pore_partial_filling'])\n            # Fetch partial filling fraction from phase object (0->1)\n            lpf = phase[self.settings['pore_partial_filling']]\n        # Calculate filled throat volumes\n        ltf = np.array([1])\n        if self.settings['throat_partial_filling']:\n            # Set pressure on phase to current capillary pressure\n            phase['throat.'+quantity] = Pc\n            # Regenerate corresponding physics model\n            for phys in self.project.find_physics(phase=phase):\n                phys.regenerate_models(self.settings['throat_partial_filling'])\n            # Fetch partial filling fraction from phase object (0->1)\n            ltf = phase[self.settings['throat_partial_filling']]\n        p_inv = p_inv*lpf\n        t_inv = t_inv*ltf\n        return {'pore.occupancy': p_inv, 'throat.occupancy': t_inv}", "response": "r Returns a dictionary of pores and throats that are related to the given pores and throats."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef random(target, element, seed=None, num_range=[0, 1]):\n    range_size = num_range[1] - num_range[0]\n    range_min = num_range[0]\n    if seed is not None:\n        np.random.seed(seed)\n    value = np.random.rand(target._count(element),)\n    value = value*range_size + range_min\n    return value", "response": "r Returns a random number of the specified size."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef linear(target, m, b, prop):\n    x = target[prop]\n    value = m*x + b\n    return value", "response": "r Calculates a property as a linear function of a given property."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef polynomial(target, a, prop, **kwargs):\n    x = target[prop]\n    value = 0.0\n    for i in range(0, len(a)):\n        value += a[i]*x**i\n    return value", "response": "r Calculates a property as a polynomial function of a given property."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef weibull(target, seeds, shape, scale, loc):\n    seeds = target[seeds]\n    value = spts.weibull_min.ppf(q=seeds, c=shape, scale=scale, loc=loc)\n    return value", "response": "r Returns a value from a Weibull distribution given a set of random numbers."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef spatially_correlated(target, weights=None, strel=None):\n    import scipy.ndimage as spim\n    network = target.project.network\n    # The following will only work on Cubic networks\n    x = network._shape[0]\n    y = network._shape[1]\n    z = network._shape[2]\n    im = _sp.rand(x, y, z)\n    if strel is None:  # Then generate a strel\n        if sum(weights) == 0:\n            # If weights of 0 are sent, then skip everything and return rands.\n            return im.flatten()\n        w = _sp.array(weights)\n        strel = _sp.zeros(w*2+1)\n        strel[:, w[1], w[2]] = 1\n        strel[w[0], :, w[2]] = 1\n        strel[w[0], w[1], :] = 1\n    im = spim.convolve(im, strel)\n    # Convolution is no longer randomly distributed, so fit a gaussian\n    # and find it's seeds\n    im = (im - _sp.mean(im))/_sp.std(im)\n    im = 1/2*_sp.special.erfc(-im/_sp.sqrt(2))\n    values = im.flatten()\n    values = values[network.pores(target.name)]\n    return values", "response": "r This function generates a random set of pore seeds that are spatailly correlated with their neighbors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cylinder(target, throat_diameter='throat.diameter'):\n    diams = target[throat_diameter]\n    value = _pi/4*(diams)**2\n    return value", "response": "r Calculates the cross - sectional area for a cylindrical throat."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _centroids(self, verts):\n        r'''\n        Function to calculate the centroid as the mean of a set of vertices.\n        Used for pore and throat.\n        '''\n        value = sp.zeros([len(verts), 3])\n        for i, i_verts in enumerate(verts):\n            value[i] = np.mean(i_verts, axis=0)\n        return value", "response": "r Function to calculate the centroid as the mean of a set of vertices. Used for pore and throat."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _throat_c2c(self):\n        net = self.network\n        Nt = net.num_throats()\n        p_cen = net['pore.centroid']\n        t_cen = net['throat.centroid']\n        conns = net['throat.conns']\n        p1 = conns[:, 0]\n        p2 = conns[:, 1]\n        v1 = t_cen-p_cen[p1]\n        v2 = t_cen-p_cen[p2]\n        check_nan = ~sp.any(sp.isnan(v1 + v2), axis=1)\n        value = sp.ones([Nt, 3], dtype=float)*sp.nan\n        for i in range(Nt):\n            if check_nan[i]:\n                value[i, 0] = sp.linalg.norm(v1[i]) - self.network.fiber_rad\n                value[i, 1] = self.network.fiber_rad*2\n                value[i, 2] = sp.linalg.norm(v2[i]) - self.network.fiber_rad\n        return value[net.throats(self.name)]", "response": "r Calculate the center to center distance from centroid of pore1 to centroid of throat to centroid of throat2 to centroid of throat1 to centroid of throat2 to centroid of throat2 to centroid of throat2."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef inhull(self, xyz, pore, tol=1e-7):\n        xyz = np.around(xyz, 10)\n        # Work out range to span over for pore hull\n        xmin = xyz[:, 0].min()\n        xr = (np.ceil(xyz[:, 0].max())-np.floor(xmin)).astype(int)+1\n        ymin = xyz[:, 1].min()\n        yr = (np.ceil(xyz[:, 1].max())-np.floor(ymin)).astype(int)+1\n        zmin = xyz[:, 2].min()\n        zr = (np.ceil(xyz[:, 2].max())-np.floor(zmin)).astype(int)+1\n\n        origin = np.array([xmin, ymin, zmin])\n        # start index\n        si = np.floor(origin).astype(int)\n        xyz -= origin\n        dom = np.zeros([xr, yr, zr], dtype=np.uint8)\n        indx, indy, indz = np.indices((xr, yr, zr))\n        # Calculate the tesselation of the points\n        hull = sptl.ConvexHull(xyz)\n        # Assume 3d for now\n        # Calc normals from the vector cross product of the vectors defined\n        # by joining points in the simplices\n        vab = xyz[hull.simplices[:, 0]]-xyz[hull.simplices[:, 1]]\n        vac = xyz[hull.simplices[:, 0]]-xyz[hull.simplices[:, 2]]\n        nrmls = np.cross(vab, vac)\n        # Scale normal vectors to unit length\n        nrmlen = np.sum(nrmls**2, axis=-1)**(1./2)\n        nrmls = nrmls*np.tile((1/nrmlen), (3, 1)).T\n        # Center of Mass\n        center = np.mean(xyz, axis=0)\n        # Any point from each simplex\n        a = xyz[hull.simplices[:, 0]]\n        # Make sure all normals point inwards\n        dp = np.sum((np.tile(center, (len(a), 1))-a)*nrmls, axis=-1)\n        k = dp < 0\n        nrmls[k] = -nrmls[k]\n        # Now we want to test whether dot(x,N) >= dot(a,N)\n        aN = np.sum(nrmls*a, axis=-1)\n        for plane_index in range(len(a)):\n            eqx = nrmls[plane_index][0]*(indx)\n            eqy = nrmls[plane_index][1]*(indy)\n            eqz = nrmls[plane_index][2]*(indz)\n            xN = eqx + eqy + eqz\n            dom[xN - aN[plane_index] >= 0-tol] += 1\n        dom[dom < len(a)] = 0\n        dom[dom == len(a)] = 1\n        ds = np.shape(dom)\n        temp_arr = np.zeros_like(self._hull_image, dtype=bool)\n        temp_arr[si[0]:si[0]+ds[0], si[1]:si[1]+ds[1], si[2]:si[2]+ds[2]] = dom\n        self._hull_image[temp_arr] = pore\n        del temp_arr", "response": "r Computes the tesselation of the points in a convex hull."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _process_pore_voxels(self):\n        r'''\n        Function to count the number of voxels in the pore and fiber space\n        Which are assigned to each hull volume\n        '''\n        num_Ps = self.num_pores()\n        pore_vox = sp.zeros(num_Ps, dtype=int)\n        fiber_vox = sp.zeros(num_Ps, dtype=int)\n        pore_space = self._hull_image.copy()\n        fiber_space = self._hull_image.copy()\n        pore_space[self._fiber_image == 0] = -1\n        fiber_space[self._fiber_image == 1] = -1\n        freq_pore_vox = itemfreq(pore_space)\n        freq_pore_vox = freq_pore_vox[freq_pore_vox[:, 0] > -1]\n        freq_fiber_vox = itemfreq(fiber_space)\n        freq_fiber_vox = freq_fiber_vox[freq_fiber_vox[:, 0] > -1]\n        pore_vox[freq_pore_vox[:, 0]] = freq_pore_vox[:, 1]\n        fiber_vox[freq_fiber_vox[:, 0]] = freq_fiber_vox[:, 1]\n        self['pore.volume'] = pore_vox*self.network.resolution**3\n        del pore_space\n        del fiber_space", "response": "r This function counts the number of voxels in the pore and fiber space and stores them in the self. pore. volume attribute."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _bresenham(self, faces, dx):\n        r'''\n        A Bresenham line function to generate points to fill in for the fibers\n        '''\n        line_points = []\n        for face in faces:\n            # Get in hull order\n            fx = face[:, 0]\n            fy = face[:, 1]\n            fz = face[:, 2]\n            # Find the axis with the smallest spread and remove it to make 2D\n            if (np.std(fx) < np.std(fy)) and (np.std(fx) < np.std(fz)):\n                f2d = np.vstack((fy, fz)).T\n            elif (np.std(fy) < np.std(fx)) and (np.std(fy) < np.std(fz)):\n                f2d = np.vstack((fx, fz)).T\n            else:\n                f2d = np.vstack((fx, fy)).T\n            hull = sptl.ConvexHull(f2d, qhull_options='QJ Pp')\n            face = np.around(face[hull.vertices].astype(float), 6)\n            for i in range(len(face)):\n                vec = face[i]-face[i-1]\n                vec_length = np.linalg.norm(vec)\n                increments = np.ceil(vec_length/dx)\n                check_p_old = np.array([-1, -1, -1])\n                for x in np.linspace(0, 1, increments):\n                    check_p_new = face[i-1]+(vec*x)\n                    if np.sum(check_p_new - check_p_old) != 0:\n                        line_points.append(check_p_new)\n                        check_p_old = check_p_new\n        return np.asarray(line_points)", "response": "r This function generates a list of line points for the bresenham."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_porosity_profile(self, fig=None):\n        if hasattr(self, '_fiber_image') is False:\n            logger.warning('This method only works when a fiber image exists')\n            return\n\n        l = sp.asarray(sp.shape(self._fiber_image))\n        px = sp.zeros(l[0])\n        py = sp.zeros(l[1])\n        pz = sp.zeros(l[2])\n\n        for x in sp.arange(l[0]):\n            px[x] = sp.sum(self._fiber_image[x, :, :])\n            px[x] /= sp.size(self._fiber_image[x, :, :])\n        for y in sp.arange(l[1]):\n            py[y] = sp.sum(self._fiber_image[:, y, :])\n            py[y] /= sp.size(self._fiber_image[:, y, :])\n        for z in sp.arange(l[2]):\n            pz[z] = sp.sum(self._fiber_image[:, :, z])\n            pz[z] /= sp.size(self._fiber_image[:, :, z])\n\n        if fig is None:\n            fig = plt.figure()\n        ax = fig.gca()\n        plots = []\n        plots.append(plt.plot(sp.arange(l[0])/l[0], px, 'r', label='x'))\n        plots.append(plt.plot(sp.arange(l[1])/l[1], py, 'g', label='y'))\n        plots.append(plt.plot(sp.arange(l[2])/l[2], pz, 'b', label='z'))\n        plt.xlabel('Normalized Distance')\n        plt.ylabel('Porosity')\n        handles, labels = ax.get_legend_handles_labels()\n        ax.legend(handles, labels, loc=1)\n        plt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.)\n        return fig", "response": "r Plot a porosity profile of the voxel volumes in all orthogonal directions by summing the voxel volumes in consectutive slices."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef vertex_dimension(self, face1=[], face2=[], parm='volume'):\n        prj = self.project\n        network = prj.network\n        pores = np.array([], dtype=int)\n        if 0 < len(face1):\n            pores = np.hstack((pores, face1))\n        if 0 < len(face2):\n            pores = np.hstack((pores, face2))\n        face1_coords = np.around(network['pore.coords'][face1], 12)\n        face2_coords = np.around(network['pore.coords'][face2], 12)\n        face1_planar = np.zeros(3)\n        face2_planar = np.zeros(3)\n        planar = np.zeros(3)\n        for i in range(3):\n            if len(np.unique(face1_coords[:, i])) == 1:\n                face1_planar[i] = 1\n            if len(np.unique(face2_coords[:, i])) == 1:\n                face2_planar[i] = 1\n        if 0 < len(face1) and 0 < len(face2):\n            planar = face1_planar*face2_planar\n        elif 0 < len(face1):\n            planar = face1_planar\n        elif 0 < len(face2):\n            planar = face2_planar\n        else:\n            return 0\n        verts = []\n        for pore in pores:\n            for vert in np.asarray(list(self['pore.vertices'][pore])):\n                verts.append(vert)\n        verts = np.asarray(verts)\n\n        vx_min = verts[:, 0].min()\n        vx_max = verts[:, 0].max()\n        vy_min = verts[:, 1].min()\n        vy_max = verts[:, 1].max()\n        vz_min = verts[:, 2].min()\n        vz_max = verts[:, 2].max()\n        output = 0\n        width = np.around(vx_max-vx_min, 10)\n        depth = np.around(vy_max-vy_min, 10)\n        height = np.around(vz_max-vz_min, 10)\n        if parm == 'volume':\n            output = width*depth*height\n        elif parm == 'area_xy' or (parm == 'area' and planar[2] == 1):\n            output = width*depth\n        elif parm == 'area_xz' or (parm == 'area' and planar[1] == 1):\n            output = width*height\n        elif parm == 'area_yz' or (parm == 'area' and planar[0] == 1):\n            output = depth*height\n        elif parm == 'length_x' or (parm == 'length' and planar[0] == 1):\n            output = width\n        elif parm == 'length_y'or (parm == 'length' and planar[1] == 1):\n            output = depth\n        elif parm == 'length_z'or (parm == 'length' and planar[2] == 1):\n            output = height\n        elif parm == 'minmax':\n            output = [vx_min, vx_max, vy_min, vy_max, vz_min, vz_max]\n        return output", "response": "r Returns the domain extent based on the vertices of the effective pore."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pore_to_pore(target):\n    network = target.project.network\n    throats = network.throats(target.name)\n    conns = network['throat.conns']\n    P1 = conns[:, 0]\n    P2 = conns[:, 1]\n    coords = network['pore.coords']\n    vec = coords[P2] - coords[P1]\n    unit_vec = tr.unit_vector(vec, axis=1)\n    return unit_vec[throats]", "response": "r Calculates throat vector as straight path between connected pores."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_value_BC(self, pores, values):\n        self._set_BC(pores=pores, bctype='value', bcvalues=values,\n                     mode='merge')", "response": "r Updates the value of the boundary condition for the specified pores and values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_BC(self, pores, bctype, bcvalues=None, mode='merge'):\n        # Hijack the parse_mode function to verify bctype argument\n        bctype = self._parse_mode(bctype, allowed=['value', 'rate'],\n                                  single=True)\n        mode = self._parse_mode(mode, allowed=['merge', 'overwrite', 'remove'],\n                                single=True)\n        pores = self._parse_indices(pores)\n\n        values = np.array(bcvalues)\n        if values.size > 1 and values.size != pores.size:\n            raise Exception('The number of boundary values must match the ' +\n                            'number of locations')\n\n        # Store boundary values\n        if ('pore.bc_'+bctype not in self.keys()) or (mode == 'overwrite'):\n            self['pore.bc_'+bctype] = np.nan\n        self['pore.bc_'+bctype][pores] = values", "response": "r This function is used to set the boundary conditions for a set of pores."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_BC(self, pores=None):\n        if pores is None:\n            pores = self.Ps\n        if 'pore.bc_value' in self.keys():\n            self['pore.bc_value'][pores] = np.nan\n        if 'pore.bc_rate' in self.keys():\n            self['pore.bc_rate'][pores] = np.nan", "response": "r Removes all boundary conditions from the specified pores."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_A(self, force=False):\n        if force:\n            self._pure_A = None\n        if self._pure_A is None:\n            network = self.project.network\n            phase = self.project.phases()[self.settings['phase']]\n            g = phase[self.settings['conductance']]\n            am = network.create_adjacency_matrix(weights=g, fmt='coo')\n            self._pure_A = spgr.laplacian(am)\n        self.A = self._pure_A.copy()", "response": "r Builds the A matrix based on conductances between pores."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_b(self, force=False):\n        if force:\n            self._pure_b = None\n        if self._pure_b is None:\n            b = np.zeros(shape=(self.Np, ), dtype=float)  # Create vector of 0s\n            self._pure_b = b\n        self.b = self._pure_b.copy()", "response": "r Builds the RHS matrix without applying any boundary conditions or\n            source terms."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _calc_eff_prop(self, inlets=None, outlets=None,\n                       domain_area=None, domain_length=None):\n        r\"\"\"\n        Calculate the effective transport through the network\n\n        Parameters\n        ----------\n        inlets : array_like\n            The pores where the inlet boundary conditions were applied.  If\n            not given an attempt is made to infer them from the algorithm.\n\n        outlets : array_like\n            The pores where the outlet boundary conditions were applied.  If\n            not given an attempt is made to infer them from the algorithm.\n\n        domain_area : scalar\n            The area of the inlet and/or outlet face (which shold match)\n\n        domain_length : scalar\n            The length of the domain between the inlet and outlet faces\n\n        Returns\n        -------\n        The effective transport property through the network\n\n        \"\"\"\n        if self.settings['quantity'] not in self.keys():\n            raise Exception('The algorithm has not been run yet. Cannot ' +\n                            'calculate effective property.')\n\n        Ps = np.isfinite(self['pore.bc_value'])\n        BCs = np.unique(self['pore.bc_value'][Ps])\n        Dx = np.abs(np.diff(BCs))\n        if inlets is None:\n            inlets = self._get_inlets()\n        flow = self.rate(pores=inlets)\n        # Fetch area and length of domain\n        if domain_area is None:\n            domain_area = self._get_domain_area(inlets=inlets,\n                                                outlets=outlets)\n        if domain_length is None:\n            domain_length = self._get_domain_length(inlets=inlets,\n                                                    outlets=outlets)\n        D = np.sum(flow)*domain_length/domain_area/Dx\n        return D", "response": "r Calculates the effective transport property through the network."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reynolds(target, u0, b, temperature='pore.temperature'):\n    value = u0*sp.exp(b*target[temperature])\n    return value", "response": "r Calculates the reynolds of an object in the specified temperature."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef chung(target, temperature='pore.temperature',\n          mol_weight='pore.molecular_weight',\n          critical_temperature='pore.critical_temperature',\n          critical_volume='pore.critical_volume'):\n    r\"\"\"\n    Uses Chung et al. [1] model to estimate viscosity for gases at low\n    pressure (much less than the critical pressure) at conditions of interest.\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object for which these values are being calculated.  This\n        controls the length of the calculated array, and also provides\n        access to other necessary thermofluid properties.\n\n    temperatre: string\n        The dictionary key containing the temperature values (K)\n\n    critical_temperature : string\n        The dictionary key containing the temperature values (K)\n\n    mol_weight: string\n        The dictionary key containing the molecular weight values (kg/mol)\n\n    critical_volume : string\n        The dictionary key containing the critical volume values (m3/kmol)\n\n    [1] Chung, T.H., Lee, L.L., and Starling, K.E., Applications of Kinetic Gas\n        Theories and Multiparameter Correlation for Prediction of Dilute Gas\n        Viscosity and Thermal Conductivity\u201d, Ind. Eng. Chem. Fundam.23:8, 1984.\n\n    \"\"\"\n    T = target[temperature]\n    MW = target[mol_weight]\n    Tc = target[critical_temperature]\n    Vc = target[critical_volume]\n    Tr = T / Tc\n    Tstar = 1.2593*Tr\n    A = 1.161415\n    B = 0.14874\n    C = 0.52487\n    D = 0.77320\n    E = 2.16178\n    F = 2.43787\n    omega = (A*(Tstar)**(-B)) + C*(sp.exp(-D*Tstar)) + E*(sp.exp(-F*Tstar))\n    sigma = 0.809*(Vc**(1/3))\n    value = 26.69E-9*sp.sqrt(MW*T)/(omega*sigma**2)\n    return value", "response": "r Estimates the viscosity of gases at a given temperature and critical temperature."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef keys(self, element=None, mode=None):\n        if mode is None:\n            return super().keys()\n        element = self._parse_element(element=element)\n        allowed = ['props', 'labels']\n        if 'all' in mode:\n            mode = allowed\n        mode = self._parse_mode(mode=mode, allowed=allowed)\n        keys = super().keys()\n        temp = []\n        if 'props' in mode:\n            temp.extend([i for i in keys if self.get(i).dtype != bool])\n        if 'labels' in mode:\n            temp.extend([i for i in keys if self.get(i).dtype == bool])\n        if element:\n            temp = [i for i in temp if i.split('.')[0] in element]\n        return temp", "response": "r Returns a list of keys for the given element and mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, keys, default=None):\n        # If a list of several keys is passed, then create a subdict\n        if isinstance(keys, list):\n            ret = {}\n            for k in keys:\n                ret[k] = super().get(k, default)\n        else:  # Otherwise return numpy array\n            ret = super().get(keys, default)\n        return ret", "response": "r This method returns a dictionary containing the items of the specified keys."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef props(self, element=None, mode='all', deep=False):\n        # Parse Inputs\n        element = self._parse_element(element=element)\n        allowed_modes = ['all', 'constants', 'models']\n        mode = self._parse_mode(mode=mode, allowed=allowed_modes, single=True)\n        if mode == 'all':\n            vals = set(self.keys(mode='props'))\n        if mode == 'constants':\n            if hasattr(self, 'models'):\n                temp = set(self.keys(mode='props'))\n                vals = temp.difference(self.models.keys())\n            else:\n                vals = set(self.keys(mode='props'))\n        if mode == 'models':\n            if hasattr(self, 'models'):\n                temp = set(self.keys(mode='props'))\n                vals = temp.intersection(self.models.keys())\n            else:\n                logger.warning('Object does not have a models attribute')\n                vals = set()\n        # Deal with hidden props\n        hide = set([i for i in self.keys() if i.split('.')[1].startswith('_')])\n        vals = vals.difference(hide)\n        # Remove values of the wrong element\n        temp = set([i for i in vals if i.split('.')[0] not in element])\n        vals = set(vals).difference(temp)\n        # Convert to nice list for printing\n        vals = PrintableList(list(vals))\n        # Repeat for associated objects if deep is True\n        if deep:\n            if self._isa('phase'):\n                for item in self.project.find_physics(phase=self):\n                    vals += item.props(element=element, mode=mode, deep=False)\n            if self._isa('network'):\n                for item in self.project.geometries().values():\n                    vals += item.props(element=element, mode=mode, deep=False)\n        return vals", "response": "r Returns a list containing the names of all pore or throat properties currently defined."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_labels(self, element, locations, mode):\n        # Parse inputs\n        locations = self._parse_indices(locations)\n        element = self._parse_element(element=element)\n        # Collect list of all pore OR throat labels\n        labels = self.keys(mode='labels', element=element)\n        labels.sort()\n        labels = sp.array(labels)  # Convert to ND-array for following checks\n        # Make an 2D array with locations in rows and labels in cols\n        arr = sp.vstack([self[item][locations] for item in labels]).T\n        num_hits = sp.sum(arr, axis=0)  # Number of locations with each label\n        if mode in ['or', 'union', 'any']:\n            temp = labels[num_hits > 0]\n        elif mode in ['and', 'intersection']:\n            temp = labels[num_hits == locations.size]\n        elif mode in ['xor', 'exclusive_or']:\n            temp = labels[num_hits == 1]\n        elif mode in ['nor', 'not', 'none']:\n            temp = labels[num_hits == 0]\n        elif mode in ['nand']:\n            temp = labels[num_hits == (locations.size - 1)]\n        elif mode in ['xnor', 'nxor']:\n            temp = labels[num_hits > 1]\n        else:\n            raise Exception('Unrecognized mode:'+str(mode))\n        return PrintableList(temp)", "response": "r This method returns a list of all the labels for the given element and locations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef labels(self, pores=[], throats=[], element=None, mode='union'):\n        # Short-circuit query when no pores or throats are given\n        if (sp.size(pores) == 0) and (sp.size(throats) == 0):\n            labels = PrintableList(self.keys(element=element, mode='labels'))\n        elif (sp.size(pores) > 0) and (sp.size(throats) > 0):\n            raise Exception('Cannot perform label query on pores and ' +\n                            'throats simultaneously')\n        elif sp.size(pores) > 0:\n            labels = self._get_labels(element='pore', locations=pores,\n                                      mode=mode)\n        elif sp.size(throats) > 0:\n            labels = self._get_labels(element='throat', locations=throats,\n                                      mode=mode)\n        return labels", "response": "r Returns a list of labels present on the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_indices(self, element, labels='all', mode='or'):\n        # Parse and validate all input values.\n        element = self._parse_element(element, single=True)\n        labels = self._parse_labels(labels=labels, element=element)\n        if element+'.all' not in self.keys():\n            raise Exception('Cannot proceed without {}.all'.format(element))\n\n        # Begin computing label array\n        if mode in ['or', 'any', 'union']:\n            union = sp.zeros_like(self[element+'.all'], dtype=bool)\n            for item in labels:  # Iterate over labels and collect all indices\n                union = union + self[element+'.'+item.split('.')[-1]]\n            ind = union\n        elif mode in ['and', 'all', 'intersection']:\n            intersect = sp.ones_like(self[element+'.all'], dtype=bool)\n            for item in labels:  # Iterate over labels and collect all indices\n                intersect = intersect*self[element+'.'+item.split('.')[-1]]\n            ind = intersect\n        elif mode in ['xor', 'exclusive_or']:\n            xor = sp.zeros_like(self[element+'.all'], dtype=int)\n            for item in labels:  # Iterate over labels and collect all indices\n                info = self[element+'.'+item.split('.')[-1]]\n                xor = xor + sp.int8(info)\n            ind = (xor == 1)\n        elif mode in ['nor', 'not', 'none']:\n            nor = sp.zeros_like(self[element+'.all'], dtype=int)\n            for item in labels:  # Iterate over labels and collect all indices\n                info = self[element+'.'+item.split('.')[-1]]\n                nor = nor + sp.int8(info)\n            ind = (nor == 0)\n        elif mode in ['nand']:\n            nand = sp.zeros_like(self[element+'.all'], dtype=int)\n            for item in labels:  # Iterate over labels and collect all indices\n                info = self[element+'.'+item.split('.')[-1]]\n                nand = nand + sp.int8(info)\n            ind = (nand < len(labels)) * (nand > 0)\n        elif mode in ['xnor', 'nxor']:\n            xnor = sp.zeros_like(self[element+'.all'], dtype=int)\n            for item in labels:  # Iterate over labels and collect all indices\n                info = self[element+'.'+item.split('.')[-1]]\n                xnor = xnor + sp.int8(info)\n            ind = (xnor > 1)\n        else:\n            raise Exception('Unsupported mode: '+mode)\n        # Extract indices from boolean mask\n        ind = sp.where(ind)[0]\n        ind = ind.astype(dtype=int)\n        return ind", "response": "r This method returns the indices of the entries in the array that are in the specified labels."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef map_pores(self, pores, origin, filtered=True):\n        ids = origin['pore._id'][pores]\n        return self._map(element='pore', ids=ids, filtered=filtered)", "response": "r Returns a list of indices of the pores on a target object that are in the calling object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef map_throats(self, throats, origin, filtered=True):\n        ids = origin['throat._id'][throats]\n        return self._map(element='throat', ids=ids, filtered=filtered)", "response": "r Returns a list of throat indices corresponding to the indices of the throats on the calling object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _tomask(self, indices, element):\n        element = self._parse_element(element, single=True)\n        indices = self._parse_indices(indices)\n        N = sp.shape(self[element + '.all'])[0]\n        ind = sp.array(indices, ndmin=1)\n        mask = sp.zeros((N, ), dtype=bool)\n        mask[ind] = True\n        return mask", "response": "r This is a generalized version of tomask that accepts a string of\n        pore or throat for programmatic access."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interleave_data(self, prop):\n        element = self._parse_element(prop.split('.')[0], single=True)\n        N = self.project.network._count(element)\n\n        # Fetch sources list depending on object type?\n        proj = self.project\n        if self._isa() in ['network', 'geometry']:\n            sources = list(proj.geometries().values())\n        elif self._isa() in ['phase', 'physics']:\n            sources = list(proj.find_physics(phase=self))\n        elif self._isa() in ['algorithm', 'base']:\n            sources = [self]\n        else:\n            raise Exception('Unrecognized object type, cannot find dependents')\n\n        # Attempt to fetch the requested array from each object\n        arrs = [item.get(prop, None) for item in sources]\n        locs = [self._get_indices(element, item.name) for item in sources]\n        sizes = [sp.size(a) for a in arrs]\n        if sp.all([item is None for item in arrs]):  # prop not found anywhere\n            raise KeyError(prop)\n\n        # Check the general type of each array\n        atype = []\n        for a in arrs:\n            if a is not None:\n                t = a.dtype.name\n                if t.startswith('int') or t.startswith('float'):\n                    atype.append('numeric')\n                elif t.startswith('bool'):\n                    atype.append('boolean')\n                else:\n                    atype.append('other')\n        if not all([item == atype[0] for item in atype]):\n            raise Exception('The array types are not compatible')\n        else:\n            dummy_val = {'numeric': sp.nan, 'boolean': False, 'other': None}\n\n        # Create an empty array of the right type and shape\n        for item in arrs:\n            if item is not None:\n                if len(item.shape) == 1:\n                    temp_arr = sp.zeros((N, ), dtype=item.dtype)\n                else:\n                    temp_arr = sp.zeros((N, item.shape[1]), dtype=item.dtype)\n                temp_arr.fill(dummy_val[atype[0]])\n\n        # Convert int arrays to float IF NaNs are expected\n        if temp_arr.dtype.name.startswith('int') and \\\n           (sp.any([i is None for i in arrs]) or sp.sum(sizes) != N):\n            temp_arr = temp_arr.astype(float)\n            temp_arr.fill(sp.nan)\n\n        # Fill new array with values in the corresponding locations\n        for vals, inds in zip(arrs, locs):\n            if vals is not None:\n                temp_arr[inds] = vals\n            else:\n                temp_arr[inds] = dummy_val[atype[0]]\n        return temp_arr", "response": "r Interleave data from a property in a set of associated objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef interpolate_data(self, propname):\n        mro = self._mro()\n        if 'GenericNetwork' in mro:\n            net = self\n            Ts = net.throats()\n            Ps = net.pores()\n            label = 'all'\n        elif ('GenericPhase' in mro) or ('GenericAlgorithm' in mro):\n            net = self.project.network\n            Ts = net.throats()\n            Ps = net.pores()\n            label = 'all'\n        elif ('GenericGeometry' in mro) or ('GenericPhysics' in mro):\n            net = self.project.network\n            Ts = net.throats(self.name)\n            Ps = net.pores(self.name)\n            label = self.name\n        if propname.startswith('throat'):\n            # Upcast data to full network size\n            temp = sp.ones((net.Nt,))*sp.nan\n            temp[Ts] = self[propname]\n            data = temp\n            temp = sp.ones((net.Np,))*sp.nan\n            for pore in Ps:\n                neighborTs = net.find_neighbor_throats(pore)\n                neighborTs = net.filter_by_label(throats=neighborTs,\n                                                 labels=label)\n                temp[pore] = sp.mean(data[neighborTs])\n            values = temp[Ps]\n        elif propname.startswith('pore'):\n            # Upcast data to full network size\n            data = sp.ones((net.Np, ))*sp.nan\n            data[Ps] = self[propname]\n            Ps12 = net['throat.conns'][Ts]\n            values = sp.mean(data[Ps12], axis=1)\n        return values", "response": "r Interpolates the data of a specific pore or throat property."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter_by_label(self, pores=[], throats=[], labels=None, mode='or'):\n        # Convert inputs to locations and element\n        if (sp.size(throats) > 0) and (sp.size(pores) > 0):\n            raise Exception('Can only filter either pores OR labels')\n        if sp.size(pores) > 0:\n            element = 'pore'\n            locations = self._parse_indices(pores)\n        elif sp.size(throats) > 0:\n            element = 'throat'\n            locations = self._parse_indices(throats)\n        else:\n            return(sp.array([], dtype=int))\n        labels = self._parse_labels(labels=labels, element=element)\n        labels = [element+'.'+item.split('.')[-1] for item in labels]\n        all_locs = self._get_indices(element=element, labels=labels, mode=mode)\n        mask = self._tomask(indices=all_locs, element=element)\n        ind = mask[locations]\n        return locations[ind]", "response": "r Returns a list of the locations of the given pores or throats that have the specified label."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef num_pores(self, labels='all', mode='or'):\n        # Count number of pores of specified type\n        Ps = self._get_indices(labels=labels, mode=mode, element='pore')\n        Np = sp.shape(Ps)[0]\n        return Np", "response": "r Returns the number of pores in the specified labels and optionally count them."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_data_health(self, props=[], element=None):\n        health = HealthDict()\n        if props == []:\n            props = self.props(element)\n        else:\n            if type(props) == str:\n                props = [props]\n        for item in props:\n            health[item] = []\n            if self[item].dtype == 'O':\n                health[item] = 'No checks on object'\n            elif sp.sum(sp.isnan(self[item])) > 0:\n                health[item] = 'Has NaNs'\n            elif sp.shape(self[item])[0] != self._count(item.split('.')[0]):\n                health[item] = 'Wrong Length'\n        return health", "response": "r Check the health of the data arrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_indices(self, indices):\n        if indices is None:\n            indices = sp.array([], ndmin=1, dtype=int)\n        locs = sp.array(indices, ndmin=1)\n        # If boolean array, convert to indices\n        if locs.dtype == bool:\n            if sp.size(locs) == self.Np:\n                locs = self.Ps[locs]\n            elif sp.size(locs) == self.Nt:\n                locs = self.Ts[locs]\n            else:\n                raise Exception('Mask of locations must be either ' +\n                                'Np nor Nt long')\n        locs = locs.astype(dtype=int)\n        return locs", "response": "r This private method parses the indices of the pore or throat locations and returns a Numpy array of indices."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_element(self, element, single=False):\n        if element is None:\n            element = ['pore', 'throat']\n        # Convert element to a list for subsequent processing\n        if type(element) is str:\n            element = [element]\n        # Convert 'pore.prop' and 'throat.prop' into just 'pore' and 'throat'\n        element = [item.split('.')[0] for item in element]\n        # Make sure all are lowercase\n        element = [item.lower() for item in element]\n        # Deal with an plurals\n        element = [item.rsplit('s', maxsplit=1)[0] for item in element]\n        for item in element:\n            if item not in ['pore', 'throat']:\n                raise Exception('Invalid element received: '+item)\n        # Remove duplicates if any\n        [element.remove(L) for L in element if element.count(L) > 1]\n        if single:\n            if len(element) > 1:\n                raise Exception('Both elements recieved when single element ' +\n                                'allowed')\n            else:\n                element = element[0]\n        return element", "response": "r This private method is used to parse the keyword element in many\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_mode(self, mode, allowed=None, single=False):\n        if type(mode) is str:\n            mode = [mode]\n        for item in mode:\n            if (allowed is not None) and (item not in allowed):\n                raise Exception('\\'mode\\' must be one of the following: ' +\n                                allowed.__str__())\n        # Remove duplicates, if any\n        [mode.remove(L) for L in mode if mode.count(L) > 1]\n        if single:\n            if len(mode) > 1:\n                raise Exception('Multiple modes received when only one mode ' +\n                                'allowed')\n            else:\n                mode = mode[0]\n        return mode", "response": "r This private method is used to parse the mode used in the calling\n            method."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_prop(self, propname, element):\n        element = self._parse_element(element, single=True)\n        return element + '.' + propname.split('.')[-1]", "response": "r Parse a property name from the element and return the element name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cube(target, pore_diameter='pore.diameter', throat_area='throat.area'):\n    network = target.project.network\n    D = target[pore_diameter]\n    Tn = network.find_neighbor_throats(pores=target.Ps, flatten=False)\n    Tsurf = _np.array([_np.sum(network[throat_area][Ts]) for Ts in Tn])\n    value = 6*D**2 - Tsurf\n    return value", "response": "r Calculates internal surface area of a pore assuming they are cubes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_inlets(self, pores=None, clusters=None):\n        if pores is not None:\n            logger.info(\"Setting inlet pores at shared pressure\")\n            clusters = []\n            clusters.append(pores)\n        elif clusters is not None:\n            logger.info(\"Setting inlet clusters at individual pressures\")\n        else:\n            logger.error(\"Either 'inlets' or 'clusters' must be passed to\" +\n                         \" setup method\")\n        self.queue = []\n        for i, cluster in enumerate(clusters):\n            self.queue.append([])\n            # Perform initial analysis on input pores\n            self['pore.invasion_sequence'][cluster] = 0\n            self['pore.cluster'][cluster] = i\n            self['pore.invasion_pressure'][cluster] = -np.inf\n            if np.size(cluster) > 1:\n                for elem_id in cluster:\n                    self._add_ts2q(elem_id, self.queue[i])\n            elif np.size(cluster) == 1:\n                self._add_ts2q(cluster, self.queue[i])\n            else:\n                logger.warning(\"Some inlet clusters have no pores\")\n        if self.settings['snap_off']:\n            self._apply_snap_off()", "response": "r Set the inlet queue for the given list of pores and clusters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_outlets(self, pores=[], overwrite=False):\n        if self.settings['trapping'] is False:\n            logger.warning('Setting outlets is meaningless unless trapping ' +\n                           'was set to True during setup')\n        Ps = self._parse_indices(pores)\n        if np.sum(self['pore.inlets'][Ps]) > 0:\n            raise Exception('Some outlets are already defined as inlets')\n        if overwrite:\n            self['pore.outlets'] = False\n        self['pore.outlets'][Ps] = True", "response": "r Set the locations through which defender exits the network."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds throats connected to newly invaded pore to the cluster queue", "response": "def _add_ts2q(self, pore, queue):\n        \"\"\"\n        Helper method to add throats to the cluster queue\n        \"\"\"\n        net = self.project.network\n        elem_type = 'throat'\n        # Find throats connected to newly invaded pore\n        Ts = net.find_neighbor_throats(pores=pore)\n        # Remove already invaded throats from Ts\n        Ts = Ts[self['throat.invasion_sequence'][Ts] <= 0]\n        tcp = self['throat.entry_pressure']\n        if len(Ts) > 0:\n            self._interface_Ts[Ts] = True\n            for T in Ts:\n                data = []\n                # Pc\n                if self._bidirectional:\n                    # Get index of pore being invaded next and apply correct\n                    # entry pressure\n                    pmap = net['throat.conns'][T] != pore\n                    pind = list(pmap).index(True)\n                    data.append(tcp[T][pind])\n                else:\n                    data.append(tcp[T])\n                # Element Index\n                data.append(T)\n                # Element Type (Pore of Throat)\n                data.append(elem_type)\n                hq.heappush(queue, data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_ps2q(self, throat, queue):\n        net = self.project.network\n        elem_type = 'pore'\n        # Find pores connected to newly invaded throat\n        Ps = net['throat.conns'][throat]\n        # Remove already invaded pores from Ps\n        Ps = Ps[self['pore.invasion_sequence'][Ps] <= 0]\n        if len(Ps) > 0:\n            self._interface_Ps[Ps] = True\n            for P in Ps:\n                data = []\n                # Pc\n                data.append(self[\"pore.entry_pressure\"][P])\n                # Element Index\n                data.append(P)\n                # Element Type (Pore of Throat)\n                data.append(elem_type)\n                hq.heappush(queue, data)", "response": "Add pores to the cluster queue"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self, max_pressure=None):\n        if 'throat.entry_pressure' not in self.keys():\n            logger.error(\"Setup method must be run first\")\n\n        if max_pressure is None:\n            self.max_pressure = sp.inf\n        else:\n            self.max_pressure = max_pressure\n        if len(self.queue) == 0:\n            logger.warn('queue is empty, this network is fully invaded')\n            return\n        # track whether each cluster has reached the maximum pressure\n        self.max_p_reached = [False]*len(self.queue)\n        # starting invasion sequence\n        self.count = 0\n        # highest pressure reached so far - used for porosimetry curve\n        self.high_Pc = np.ones(len(self.queue))*-np.inf\n        outlets = self['pore.outlets']\n        terminate_clusters = np.sum(outlets) > 0\n        if not hasattr(self, 'invasion_running'):\n            self.invasion_running = [True]*len(self.queue)\n        else:\n            # created by set_residual\n            pass\n        while np.any(self.invasion_running) and not np.all(self.max_p_reached):\n            # Loop over clusters\n            for c_num in np.argwhere(self.invasion_running).flatten():\n                self._invade_cluster(c_num)\n                queue = self.queue[c_num]\n                if len(queue) == 0 or self.max_p_reached[c_num]:\n                    # If the cluster contains no more entries invasion has\n                    # finished\n                    self.invasion_running[c_num] = False\n            if self.settings['invade_isolated_Ts']:\n                self._invade_isolated_Ts()\n            if terminate_clusters:\n                # terminated clusters\n                tcs = np.unique(self['pore.cluster'][outlets]).astype(int)\n                tcs = tcs[tcs >= 0]\n                if len(tcs) > 0:\n                    for tc in tcs:\n                        if self.invasion_running[tc] is True:\n                            self.invasion_running[tc] = False\n                            logger.info(\"Cluster \" + str(tc) + \" reached \" +\n                                        \" outlet at sequence \" + str(self.count))", "response": "r Runs the algorithm for the internal method."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef results(self, Pc):\n\n        phase = self.project.find_phase(self)\n        net = self.project.network\n        inv_p = self['pore.invasion_pressure'].copy()\n        inv_t = self['throat.invasion_pressure'].copy()\n        # Handle trapped pores and throats by moving their pressure up to be\n        # ignored\n        if np.sum(self['pore.invasion_sequence'] == -1) > 0:\n            inv_p[self['pore.invasion_sequence'] == -1] = Pc + 1\n        if np.sum(self['throat.invasion_sequence'] == -1) > 0:\n            inv_t[self['throat.invasion_sequence'] == -1] = Pc + 1\n        p_inv = inv_p <= Pc\n        t_inv = inv_t <= Pc\n\n        if self.settings['late_pore_filling']:\n            # Set pressure on phase to current capillary pressure\n            phase['pore.pressure'] = Pc\n            # Regenerate corresponding physics model\n            for phys in self.project.find_physics(phase=phase):\n                phys.regenerate_models(self.settings['late_pore_filling'])\n            # Fetch partial filling fraction from phase object (0->1)\n            frac = phase[self.settings['late_pore_filling']]\n            p_vol = net['pore.volume']*frac\n        else:\n            p_vol = net['pore.volume']\n        if self.settings['late_throat_filling']:\n            # Set pressure on phase to current capillary pressure\n            phase['throat.pressure'] = Pc\n            # Regenerate corresponding physics model\n            for phys in self.project.find_physics(phase=phase):\n                phys.regenerate_models(self.settings['late_throat_filling'])\n            # Fetch partial filling fraction from phase object (0->1)\n            frac = phase[self.settings['late_throat_filling']]\n            t_vol = net['throat.volume']*frac\n        else:\n            t_vol = net['throat.volume']\n        return {'pore.occupancy': p_inv*p_vol, 'throat.occupancy': t_inv*t_vol}", "response": "r Places the results of the IP simulation into the Phase object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply_flow(self, flowrate):\n        net = self.project.network\n        P12 = net['throat.conns']\n        aa = self['throat.invasion_sequence']\n        bb = sp.argsort(self['throat.invasion_sequence'])\n        P12_inv = self['pore.invasion_sequence'][P12]\n        # Find if the connected pores were invaded with or before each throat\n        P1_inv = P12_inv[:, 0] <= aa\n        P2_inv = P12_inv[:, 1] <= aa\n        cc = sp.column_stack((P1_inv, P2_inv))\n        # List of Pores invaded with each throat\n        dd = sp.sum(cc, axis=1, dtype=bool)\n        # Find volume of these pores\n        P12_vol = sp.sum(net['pore.volume'][P12]*cc, axis=1)*dd\n        # Add invaded throat volume to pore volume (if invaded)\n        T_vol = P12_vol + net['throat.volume']\n        # Cumulative sum on the sorted throats gives cumulated inject volume\n        ee = sp.cumsum(T_vol[bb] / flowrate)\n        t = sp.zeros((self.Nt,))\n        t[bb] = ee  # Convert back to original order\n        phase = self.project.find_phase(self)\n        phase['throat.invasion_time'] = t", "response": "r Converts the invaded sequence into an invaded time for a given flow rate."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot_intrusion_curve(self, fig=None, inv_points=None):\n        data = self.get_intrusion_data(inv_points)\n        if fig is None:\n            fig = plt.figure()\n        a = fig.add_subplot(111)\n        a.plot(data.Pcap, data.S_pore, 'r*-', label='pore')\n        a.plot(data.Pcap, data.S_throat, 'b*-', label='throat')\n        a.plot(data.Pcap, data.S_tot, 'g*-', label='total')\n        a.legend(bbox_to_anchor=(0, 1.02, 1, 0.102),\n                 loc=3, ncol=3, borderaxespad=0)\n        a.set_xlabel(\"Capillary Pressure [Pa]\")\n        a.set_ylabel(\"Saturation\")\n        a.set_ybound(lower=0.0, upper=1.0)\n        return fig", "response": "r Plot a simple drainage curve for the given set of inv_points."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _invade_isolated_Ts(self):\n        net = self.project.network\n        Ts = net['throat.conns'].copy()\n        invaded_Ps = self['pore.invasion_sequence'] > -1\n        uninvaded_Ts = self['throat.invasion_sequence'] == -1\n        isolated_Ts = np.logical_and(invaded_Ps[Ts[:, 0]],\n                                     invaded_Ps[Ts[:, 1]])\n        isolated_Ts = np.logical_and(isolated_Ts, uninvaded_Ts)\n        inv_Pc = self['pore.invasion_pressure']\n        inv_seq = self['pore.invasion_sequence']\n        if np.any(isolated_Ts):\n            max_array = Ts[:, 0]\n            second_higher = inv_seq[Ts][:, 1] > inv_seq[Ts][:, 0]\n            max_array[second_higher] = Ts[:, 1][second_higher]\n            mPc = inv_Pc[max_array]\n            mSeq = inv_seq[max_array]\n            mClu = self['pore.cluster'][max_array]\n            self['throat.invasion_pressure'][isolated_Ts] = mPc[isolated_Ts]\n            self['throat.invasion_sequence'][isolated_Ts] = mSeq[isolated_Ts]\n            self['throat.cluster'][isolated_Ts] = mClu[isolated_Ts]", "response": "r Updates the internal representation of the isolated Times."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _apply_cen_to_throats(self, p_cen, t_cen, t_norm, men_cen):\n        r'''\n        Take the pore center and throat center and work out which way\n        the throat normal is pointing relative to the vector between centers.\n        Offset the meniscus center along the throat vector in the correct\n        direction\n        '''\n        v = p_cen - t_cen\n        sign = np.sign(np.sum(v*t_norm, axis=1))\n        c3 = np.vstack((men_cen*sign,\n                        men_cen*sign,\n                        men_cen*sign)).T\n        coords = t_cen + c3*t_norm\n        return coords", "response": "r Apply the cen to throats"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_coop(self, pore, queue):\n        net = self.project.network\n        t_inv = 'throat.invasion_sequence'\n        p_inv = 'pore.invasion_sequence'\n        for throat in net.find_neighbor_throats(pores=pore):\n            # A pore has just been invaded, all it's throats now have\n            # An interface residing inside them\n            if self[t_inv][throat] == -1:\n                # If the throat is not the invading throat that gave access\n                # to this pore, get the pores that this throat connects with\n                a = set(net['throat.conns'][throat])\n                # Get a list of pre-calculated coop filling pressures for all\n                # Throats this throat can coop fill with\n                ts_Pc = self.tt_Pc.data[throat]\n                # Network indices of throats that can act as filling pairs\n                ts = self.tt_Pc.rows[throat]\n                # If there are any potential coop filling throats\n                if np.any(~np.isnan(ts_Pc)):\n                    ts_Pc = np.asarray(ts_Pc)\n                    ts = np.asarray(ts)\n                    ts = ts[~np.isnan(ts_Pc)]\n                    ts_Pc = ts_Pc[~np.isnan(ts_Pc)]\n                    # For each throat find the common pore and the uncommon\n                    # pores\n                    for i, t in enumerate(ts):\n                        # Find common pore (cP) and uncommon pores (uPs)\n                        b = set(net['throat.conns'][t])\n                        cP = list(a.intersection(b))\n                        uPs = list(a.symmetric_difference(b))\n                        # If the common pore is not invaded but the others are\n                        # The potential coop filling event can now happen\n                        # Add the coop pressure to the queue\n                        if ((np.all(self[p_inv][uPs] > -1)) and\n                           (self[p_inv][cP] == -1)):\n                            # Coop pore filling fills the common pore\n                            # The throats that gave access are not invaded now\n                            # However, isolated throats between invaded pores\n                            # Are taken care of elsewhere...\n                            hq.heappush(queue, [ts_Pc[i], list(cP), 'pore'])", "response": "r Check if a given pore is cooperatively filled with any of the throats that are not invaded to the common pore."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cylinder(target, throat_diameter='throat.diameter',\n             throat_length='throat.length'):\n    r\"\"\"\n    Calculate surface area for a cylindrical throat\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object which this model is associated with. This controls the\n        length of the calculated array, and also provides access to other\n        necessary properties.\n\n    throat_diameter : string\n        Dictionary key to the throat diameter array.  Default is\n        'throat.diameter'.\n\n    throat_length : string\n        Dictionary key to the throat length array.  Default is 'throat.length'.\n    \"\"\"\n    D = target[throat_diameter]\n    L = target[throat_length]\n    value = _sp.pi*D*L\n    return value", "response": "r Calculates the surface area of a cylindrical throat."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef spherical_pores(target, pore_diameter='pore.diameter',\n                    throat_diameter='throat.diameter',\n                    throat_centroid='throat.centroid'):\n    r\"\"\"\n    Calculate the coordinates of throat endpoints, assuming spherical pores.\n    This model accounts for the overlapping lens between pores and throats.\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object which this model is associated with. This controls the\n        length of the calculated array, and also provides access to other\n        necessary properties.\n\n    pore_diameter : string\n        Dictionary key of the pore diameter values.\n\n    throat_diameter : string\n        Dictionary key of the throat diameter values.\n\n    throat_centroid : string, optional\n        Dictionary key of the throat centroid values. See the notes.\n\n    Returns\n    -------\n    EP : dictionary\n        Coordinates of throat endpoints stored in Dict form. Can be accessed\n        via the dict keys 'head' and 'tail'.\n\n    Notes\n    -----\n    (1) This model should not be applied to true 2D networks. Use\n    `circular_pores` model instead.\n\n    (2) By default, this model assumes that throat centroid and pore\n    coordinates are colinear. If that's not the case, such as in extracted\n    networks, `throat_centroid` could be passed as an optional argument, and\n    the model takes care of the rest.\n\n    \"\"\"\n    network = target.project.network\n    throats = network.map_throats(throats=target.Ts, origin=target)\n    xyz = network['pore.coords']\n    cn = network['throat.conns'][throats]\n    L = _ctc(target=target, pore_diameter=pore_diameter)\n    Dt = network[throat_diameter][throats]\n    D1 = network[pore_diameter][cn[:, 0]]\n    D2 = network[pore_diameter][cn[:, 1]]\n    L1 = _sp.zeros_like(L)\n    L2 = _sp.zeros_like(L)\n    # Handle the case where Dt > Dp\n    mask = Dt > D1\n    L1[mask] = 0.5 * D1[mask]\n    L1[~mask] = _sp.sqrt(D1[~mask]**2 - Dt[~mask]**2) / 2\n    mask = Dt > D2\n    L2[mask] = 0.5 * D2[mask]\n    L2[~mask] = _sp.sqrt(D2[~mask]**2 - Dt[~mask]**2) / 2\n    # Handle non-colinear pores and throat centroids\n    try:\n        TC = network[throat_centroid][throats]\n        LP1T = _sp.linalg.norm(TC - xyz[cn[:, 0]], axis=1)\n        LP2T = _sp.linalg.norm(TC - xyz[cn[:, 1]], axis=1)\n        unit_vec_P1T = (TC - xyz[cn[:, 0]]) / LP1T[:, None]\n        unit_vec_P2T = (TC - xyz[cn[:, 1]]) / LP2T[:, None]\n    except KeyError:\n        unit_vec_P1T = (xyz[cn[:, 1]] - xyz[cn[:, 0]]) / L[:, None]\n        unit_vec_P2T = -1 * unit_vec_P1T\n    # Find throat endpoints\n    EP1 = xyz[cn[:, 0]] + L1[:, None] * unit_vec_P1T\n    EP2 = xyz[cn[:, 1]] + L2[:, None] * unit_vec_P2T\n    # Handle throats w/ overlapping pores\n    L1 = (4*L**2 + D1**2 - D2**2) / (8*L)\n    # L2 = (4*L**2 + D2**2 - D1**2) / (8*L)\n    h = (2*_sp.sqrt(D1**2/4 - L1**2)).real\n    overlap = L - 0.5 * (D1+D2) < 0\n    mask = overlap & (Dt < h)\n    EP1[mask] = EP2[mask] = (xyz[cn[:, 0]] + L1[:, None] * unit_vec_P1T)[mask]\n    return {'head': EP1, 'tail': EP2}", "response": "r This model calculates the coordinates of the throat endpoints assuming spherical pores."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_array(self, values):\n        if sp.shape(values)[0] > self.num_pores('internal'):\n            raise Exception('The array shape does not match the network')\n        Ps = sp.array(self['pore.index'][self.pores('internal')], dtype=int)\n        arr = sp.ones(self._shape)*sp.nan\n        ind = sp.unravel_index(Ps, self._shape)\n        arr[ind[0], ind[1], ind[2]] = values\n        return arr", "response": "r Converts the given array of values into a rectangular array with the same shape as the network and the same shape as the internal pore."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_array(self, array, propname):\n        array = sp.atleast_3d(array)\n        if sp.shape(array) != self._shape:\n            raise Exception('The array shape does not match the network')\n        temp = array.flatten()\n        Ps = sp.array(self['pore.index'][self.pores('internal')], dtype=int)\n        propname = 'pore.' + propname.split('.')[-1]\n        self[propname] = sp.nan\n        self[propname][self.pores('internal')] = temp[Ps]", "response": "r Apply data to the network based on a rectangular array filled with the values added to the internal property."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_adjacency_matrix(self, fmt='coo'):\n        # Retrieve existing matrix if available\n        if fmt in self._am.keys():\n            am = self._am[fmt]\n        elif self._am.keys():\n            am = self._am[list(self._am.keys())[0]]\n            tofmt = getattr(am, 'to'+fmt)\n            am = tofmt()\n            self._am[fmt] = am\n        else:\n            am = self.create_adjacency_matrix(weights=self.Ts, fmt=fmt)\n            self._am[fmt] = am\n        return am", "response": "r Returns an adjacency matrix in the specified sparse storage format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_incidence_matrix(self, fmt='coo'):\n        if fmt in self._im.keys():\n            im = self._im[fmt]\n        elif self._im.keys():\n            im = self._am[list(self._im.keys())[0]]\n            tofmt = getattr(im, 'to'+fmt)\n            im = tofmt()\n            self._im[fmt] = im\n        else:\n            im = self.create_incidence_matrix(weights=self.Ts, fmt=fmt)\n            self._im[fmt] = im\n        return im", "response": "r Returns an incidence matrix in the specified sparse storage format."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_adjacency_matrix(self, weights=None, fmt='coo', triu=False,\n                                drop_zeros=False):\n        r\"\"\"\n        Generates a weighted adjacency matrix in the desired sparse format\n\n        Parameters\n        ----------\n        weights : array_like, optional\n            An array containing the throat values to enter into the matrix\n            (in graph theory these are known as the 'weights').\n\n            If the array is Nt-long, it implies that the matrix is symmetric,\n            so the upper and lower triangular regions are mirror images.  If\n            it is 2*Nt-long then it is assumed that the first Nt elements are\n            for the upper triangle, and the last Nt element are for the lower\n            triangular.\n\n            If omitted, ones are used to create a standard adjacency matrix\n            representing connectivity only.\n\n        fmt : string, optional\n            The sparse storage format to return.  Options are:\n\n            **'coo'** : (default) This is the native format of OpenPNM data\n\n            **'lil'** : Enables row-wise slice of the matrix\n\n            **'csr'** : Favored by most linear algebra routines\n\n            **'dok'** : Enables subscript access of locations\n\n        triu : boolean (default is ``False``)\n            If ``True``, the returned sparse matrix only contains the upper-\n            triangular elements.  This argument is ignored if the ``weights``\n            array is 2*Nt-long.\n\n        drop_zeros : boolean (default is ``False``)\n            If ``True``, applies the ``eliminate_zeros`` method of the sparse\n            array to remove all zero locations.\n\n        Returns\n        -------\n        An adjacency matrix in the specified Scipy sparse format.\n\n        Notes\n        -----\n        The adjacency matrix is used by OpenPNM for finding the pores\n        connected to a give pore or set of pores.  Specifically, an adjacency\n        matrix has Np rows and Np columns.  Each row represents a pore,\n        containing non-zero values at the locations corresponding to the\n        indices of the pores connected to that pore.  The ``weights`` argument\n        indicates what value to place at each location, with the default\n        being 1's to simply indicate connections. Another useful option is\n        throat indices, such that the data values on each row indicate which\n        throats are connected to the pore.\n\n        Examples\n        --------\n        >>> import openpnm as op\n        >>> pn = op.network.Cubic(shape=[5, 5, 5])\n        >>> weights = sp.rand(pn.num_throats(), ) < 0.5\n        >>> am = pn.create_adjacency_matrix(weights=weights, fmt='csr')\n\n        \"\"\"\n        # Check if provided data is valid\n        if weights is None:\n            weights = sp.ones((self.Nt,), dtype=int)\n        elif sp.shape(weights)[0] not in [self.Nt, 2*self.Nt, (self.Nt, 2)]:\n            raise Exception('Received weights are of incorrect length')\n\n        # Append row & col to each other, and data to itself\n        conn = self['throat.conns']\n        row = conn[:, 0]\n        col = conn[:, 1]\n        if weights.shape == (2*self.Nt,):\n            row = sp.append(row, conn[:, 1])\n            col = sp.append(col, conn[:, 0])\n        elif weights.shape == (self.Nt, 2):\n            row = sp.append(row, conn[:, 1])\n            col = sp.append(col, conn[:, 0])\n            weights = weights.flatten(order='F')\n        elif not triu:\n            row = sp.append(row, conn[:, 1])\n            col = sp.append(col, conn[:, 0])\n            weights = sp.append(weights, weights)\n\n        # Generate sparse adjacency matrix in 'coo' format\n        temp = sprs.coo_matrix((weights, (row, col)), (self.Np, self.Np))\n\n        if drop_zeros:\n            temp.eliminate_zeros()\n\n        # Convert to requested format\n        if fmt == 'coo':\n            pass  # temp is already in coo format\n        elif fmt == 'csr':\n            temp = temp.tocsr()\n        elif fmt == 'lil':\n            temp = temp.tolil()\n        elif fmt == 'dok':\n            temp = temp.todok()\n\n        return temp", "response": "r Returns an adjacency matrix that is a weighted matrix of the given size."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_neighbor_throats(self, pores, mode='union', flatten=True):\n        pores = self._parse_indices(pores)\n        if sp.size(pores) == 0:\n            return sp.array([], ndmin=1, dtype=int)\n        if 'lil' not in self._im.keys():\n            self.get_incidence_matrix(fmt='lil')\n        neighbors = topotools.find_neighbor_bonds(sites=pores, logic=mode,\n                                                  im=self._im['lil'],\n                                                  flatten=flatten)\n        return neighbors", "response": "r Returns a list of unique throats neighboring the given pores."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_network_health(self):\n\n        health = HealthDict()\n        health['disconnected_clusters'] = []\n        health['isolated_pores'] = []\n        health['trim_pores'] = []\n        health['duplicate_throats'] = []\n        health['bidirectional_throats'] = []\n        health['headless_throats'] = []\n        health['looped_throats'] = []\n\n        # Check for headless throats\n        hits = sp.where(self['throat.conns'] > self.Np - 1)[0]\n        if sp.size(hits) > 0:\n            health['headless_throats'] = sp.unique(hits)\n            return health\n\n        # Check for throats that loop back onto the same pore\n        P12 = self['throat.conns']\n        hits = sp.where(P12[:, 0] == P12[:, 1])[0]\n        if sp.size(hits) > 0:\n            health['looped_throats'] = hits\n\n        # Check for individual isolated pores\n        Ps = self.num_neighbors(self.pores())\n        if sp.sum(Ps == 0) > 0:\n            health['isolated_pores'] = sp.where(Ps == 0)[0]\n\n        # Check for separated clusters of pores\n        temp = []\n        am = self.create_adjacency_matrix(fmt='coo', triu=True)\n        Cs = csg.connected_components(am, directed=False)[1]\n        if sp.unique(Cs).size > 1:\n            for i in sp.unique(Cs):\n                temp.append(sp.where(Cs == i)[0])\n            b = sp.array([len(item) for item in temp])\n            c = sp.argsort(b)[::-1]\n            for i in range(0, len(c)):\n                health['disconnected_clusters'].append(temp[c[i]])\n                if i > 0:\n                    health['trim_pores'].extend(temp[c[i]])\n\n        # Check for duplicate throats\n        am = self.create_adjacency_matrix(fmt='csr', triu=True).tocoo()\n        hits = sp.where(am.data > 1)[0]\n        if len(hits):\n            mergeTs = []\n            hits = sp.vstack((am.row[hits], am.col[hits])).T\n            ihits = hits[:, 0] + 1j*hits[:, 1]\n            conns = self['throat.conns']\n            iconns = conns[:, 0] + 1j*conns[:, 1]  # Convert to imaginary\n            for item in ihits:\n                mergeTs.append(sp.where(iconns == item)[0])\n            health['duplicate_throats'] = mergeTs\n\n        # Check for bidirectional throats\n        adjmat = self.create_adjacency_matrix(fmt='coo')\n        num_full = adjmat.sum()\n        temp = sprs.triu(adjmat, k=1)\n        num_upper = temp.sum()\n        if num_full > num_upper:\n            biTs = sp.where(self['throat.conns'][:, 0] >\n                            self['throat.conns'][:, 1])[0]\n            health['bidirectional_throats'] = biTs.tolist()\n\n        return health", "response": "r This method checks the topological health of the internal network and returns a HealthDict containing the health of the internal network."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tyn_calus(target, VA, VB, sigma_A, sigma_B, temperature='pore.temperature',\n              viscosity='pore.viscosity'):\n    r\"\"\"\n    Uses Tyn_Calus model to estimate diffusion coefficient in a dilute liquid\n    solution of A in B from first principles at conditions of interest\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object for which these values are being calculated.  This\n        controls the length of the calculated array, and also provides\n        access to other necessary thermofluid properties.\n\n    VA : float, array_like\n        Molar volume of component A at boiling temperature (m3/mol)\n\n    VB : float, array_like\n        Molar volume of component B at boiling temperature (m3/mol)\n\n    sigmaA:  float, array_like\n        Surface tension of component A at boiling temperature (N/m)\n\n    sigmaB:  float, array_like\n        Surface tension of component B at boiling temperature (N/m)\n\n    pressure : string\n        The dictionary key containing the pressure values in Pascals (Pa)\n\n    temperature : string\n        The dictionary key containing the temperature values in Kelvin (K)\n\n    \"\"\"\n    T = target[temperature]\n    mu = target[viscosity]\n    A = 8.93e-8*(VB*1e6)**0.267/(VA*1e6)**0.433*T\n    B = (sigma_B/sigma_A)**0.15/(mu*1e3)\n    value = A*B\n    return value", "response": "r Estimates the diffusion coefficient of a dilute liquid system in a dilute liquid system from a set of values in the target object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef water(target, temperature='pore.temperature', salinity='pore.salinity'):\n    T = target[temperature]\n    if salinity in target.keys():\n        S = target[salinity]\n    else:\n        S = 0\n    sigma_w = 0.2358*((1-(T/647.096))**1.256)*(1-0.625*(1-(T/647.096)))\n    a1 = 2.2637334337E-04\n    a2 = 9.4579521377E-03\n    a3 = 3.3104954843E-02\n    TC = T-273.15\n    sigma_sw = sigma_w*(1+(a1*TC+a2)*sp.log(1+a3*S))\n    value = sigma_sw\n    return value", "response": "r Calculates the surface tension of pure water or seawater at atmospheric temperature and saturation pressure."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef eotvos(target, k, temperature='pore.temperature',\n           critical_temperature='pore.critical_temperature',\n           molar_density='pore.molar_density'):\n    r\"\"\"\n    Missing description\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object for which these values are being calculated.  This\n        controls the length of the calculated array, and also provides\n        access to other necessary thermofluid properties.\n\n    k : float\n        Constant parameter specific to fluid\n\n    temperature : string\n        The dictionary key containing the temperature values (K)\n\n    critical_temperature : string\n        The dictionary key containing the critical temperature values (K)\n\n    molar_density : string\n        The dictionary key containing the molar density values (K)\n\n\n    TODO: Needs description, and improve definition of k\n\n    \"\"\"\n    Tc = target[critical_temperature]\n    T = target[temperature]\n    Vm = 1/target[molar_density]\n    value = k*(Tc-T)/(Vm**(2/3))\n    return value", "response": "r Calculates the value of the eotvos parameter in OpenPNM object for the specified temperature and critical temperature and molar density."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef guggenheim_katayama(target, K2, n, temperature='pore.temperature',\n                        critical_temperature='pore.critical_temperature',\n                        critical_pressure='pore.critical_pressure'):\n    r\"\"\"\n    Missing description\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object for which these values are being calculated.  This\n        controls the length of the calculated array, and also provides\n        access to other necessary thermofluid properties.\n\n    K2 : scalar\n        Fluid specific constant\n\n    n : scalar\n        Fluid specific constant\n\n    temperature : string\n        The dictionary key containing the temperature values (K)\n\n    critical_temperature : string\n        The dictionary key containing the critical temperature values (K)\n\n    critical_pressure : string\n        The dictionary key containing the critical pressure values (K)\n\n    \"\"\"\n    T = target[temperature]\n    Pc = target[critical_pressure]\n    Tc = target[critical_temperature]\n    sigma_o = K2*Tc**(1/3)*Pc**(2/3)\n    value = sigma_o*(1-T/Tc)**n\n    return value", "response": "r Calculates the value of the Katayama in the specified temperature and critical pressure."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef water(target, temperature='pore.temperature', salinity='pore.salinity'):\n    T = target[temperature]\n    if salinity in target.keys():\n        S = target[salinity]\n    else:\n        S = 0\n    T68 = 1.00024*T  # convert from T_90 to T_68\n    SP = S/1.00472  # convert from S to S_P\n    k_sw = 0.001*(10**(sp.log10(240+0.0002*SP) +\n                       0.434*(2.3-(343.5+0.037*SP)/T68) *\n                       ((1-T68/(647.3+0.03*SP)))**(1/3)))\n    value = k_sw\n    return value", "response": "r Calculates thermal conductivity of pure water or seawater at atmospheric temperature and the saturation pressure of the pure water or seawater at atmospheric temperature."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sato(target, mol_weight='pore.molecular_weight',\n         boiling_temperature='pore.boiling_point',\n         temperature='pore.temperature',\n         critical_temperature='pore.critical_temperature'):\n    r\"\"\"\n    Uses Sato et al. model to estimate thermal conductivity for pure liquids\n    from first principles at conditions of interest\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object for which these values are being calculated.  This\n        controls the length of the calculated array, and also provides\n        access to other necessary thermofluid properties.\n\n    boiling_temperature :  string\n        Dictionary key containing the toiling temperature of the component (K)\n\n    mol_weight : string\n        Dictionary key containing the molecular weight of the component\n        (kg/mol)\n\n    temperature : string\n        The dictionary key containing the temperature values (K)\n\n    critical_temperature : string\n        The dictionary key containing the critical temperature values (K)\n\n    \"\"\"\n    T = target[temperature]\n    Tc = target[critical_temperature]\n    MW = target[mol_weight]\n    Tbr = target[boiling_temperature]/Tc\n    Tr = T/Tc\n    value = (1.11/((MW*1e3)**0.5))*(3+20*(1-Tr)**(2/3))/(3+20*(1-Tbr)**(2/3))\n    return value", "response": "r Estimates thermal conductivity for pure liquids of a given set of properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef largest_sphere(target, fixed_diameter='pore.fixed_diameter', iters=5):\n    network = target.project.network\n    P12 = network['throat.conns']\n    C1 = network['pore.coords'][network['throat.conns'][:, 0]]\n    C2 = network['pore.coords'][network['throat.conns'][:, 1]]\n    L = _np.sqrt(_np.sum((C1 - C2)**2, axis=1))\n    try:\n        # Fetch any existing pore diameters on the network\n        D = network[fixed_diameter]\n        # Set any unassigned values (nans) to 0\n        D[_np.isnan(D)] = 0\n    except KeyError:\n        _logger.info('Pore sizes not present, calculating starting values ' +\n                     'as half-way to the nearest neighbor')\n        D = _np.inf*_np.ones([network.Np, ], dtype=float)\n        _np.minimum.at(D, P12[:, 0], L)\n        _np.minimum.at(D, P12[:, 1], L)\n    while iters >= 0:\n        iters -= 1\n        Lt = L - _np.sum(D[P12], axis=1)/2\n        Dadd = _np.ones_like(D)*_np.inf\n        _np.minimum.at(Dadd, P12[:, 0], Lt)\n        _np.minimum.at(Dadd, P12[:, 1], Lt)\n        D += Dadd\n    if _np.any(D < 0):\n        _logger.info('Negative pore diameters found!  Neighboring pores are ' +\n                     'larger than the pore spacing.')\n    return D[network.pores(target.name)]", "response": "r This function finds the maximum diameter pore that can be placed in each location without overlapping any neighbors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(cls, filename, project=None, delim=' | '):\n        net = {}\n\n        filename = cls._parse_filename(filename, ext='vtp')\n        tree = ET.parse(filename)\n        piece_node = tree.find('PolyData').find('Piece')\n\n        # Extract connectivity\n        conn_element = piece_node.find('Lines').find('DataArray')\n        conns = VTK._element_to_array(conn_element, 2)\n        # Extract coordinates\n        coord_element = piece_node.find('Points').find('DataArray')\n        coords = VTK._element_to_array(coord_element, 3)\n\n        # Extract pore data\n        for item in piece_node.find('PointData').iter('DataArray'):\n            key = item.get('Name')\n            array = VTK._element_to_array(item)\n            net[key] = array\n        # Extract throat data\n        for item in piece_node.find('CellData').iter('DataArray'):\n            key = item.get('Name')\n            array = VTK._element_to_array(item)\n            net[key] = array\n\n        if project is None:\n            project = ws.new_project()\n        project = Dict.from_dict(dct=net, project=project, delim=delim)\n\n        # Clean up data values, if necessary, like convert array's of\n        # 1's and 0's into boolean.\n        project = cls._convert_data(project)\n\n        # Add coords and conns to network\n        network = project.network\n        network.update({'throat.conns': conns})\n        network.update({'pore.coords': coords})\n\n        return project", "response": "r Read in a saved VTK file and return a GenericNetwork object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save(cls, network=None, phases=[], filename='', delim=' | '):\n        project, network, phases = cls._parse_args(network=network,\n                                                   phases=phases)\n        df = Pandas.to_dataframe(network=network, phases=phases,\n                                 join=True, delim=delim)\n\n        # Write to file\n        if filename == '':\n            filename = project.name\n        fname = cls._parse_filename(filename=filename, ext='csv')\n        df.to_csv(fname, index=False)", "response": "r Save all the pore and throat property data on the Network and Phases objects to a CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(cls, filename, project=None, delim=' | '):\n        if project is None:\n            project = ws.new_project()\n\n        fname = cls._parse_filename(filename, ext='csv')\n        a = pd.read_table(filepath_or_buffer=fname,\n                          sep=',',\n                          skipinitialspace=True,\n                          index_col=False,\n                          true_values=['T', 't', 'True', 'true', 'TRUE'],\n                          false_values=['F', 'f', 'False', 'false', 'FALSE'])\n\n        dct = {}\n        # First parse through all the items and re-merge columns\n        keys = sorted(list(a.keys()))\n        for item in keys:\n            m = re.search(r'\\[.\\]', item)  # The dot '.' is a wildcard\n            if m:  # m is None if pattern not found, otherwise merge cols\n                pname = re.split(r'\\[.\\]', item)[0]  # Get base propname\n                # Find all other keys with same base propname\n                merge_keys = [k for k in a.keys() if k.startswith(pname)]\n                # Rerieve and remove arrays with same base propname\n                merge_cols = [a.pop(k) for k in merge_keys]\n                # Merge arrays into multi-column array and store in DataFrame\n                dct[pname] = sp.vstack(merge_cols).T\n                # Remove key from list of keys\n                [keys.pop(keys.index(k)) for k in keys if k.startswith(pname)]\n            else:\n                dct[item] = sp.array(a.pop(item))\n\n        project = Dict.from_dict(dct, project=project, delim=delim)\n\n        return project", "response": "r Reads a CSV file and creates a GenericNetwork object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _trim_external_pores(self, shape):\n        r'''\n        '''\n        # Find all pores within the domain\n        Ps = topotools.isoutside(coords=self['pore.coords'], shape=shape)\n        self['pore.external'] = False\n        self['pore.external'][Ps] = True\n\n        # Find which internal pores are delaunay\n        Ps = (~self['pore.external'])*self['pore.delaunay']\n\n        # Find all pores connected to an internal delaunay pore\n        Ps = self.find_neighbor_pores(pores=Ps, include_input=True)\n\n        # Mark them all as keepers\n        self['pore.keep'] = False\n        self['pore.keep'][Ps] = True\n\n        # Trim all bad pores\n        topotools.trim(network=self, pores=~self['pore.keep'])\n\n        # Now label boundary pores\n        self['pore.boundary'] = False\n        self['pore.boundary'] = self['pore.delaunay']*self['pore.external']\n\n        # Label Voronoi pores on boundary\n        Ps = self.find_neighbor_pores(pores=self.pores('boundary'))\n        Ps = self['pore.voronoi']*self.tomask(pores=Ps)\n        self['pore.boundary'][Ps] = True\n\n        # Label Voronoi and interconnect throats on boundary\n        self['throat.boundary'] = False\n        Ps = self.pores('boundary')\n        Ts = self.find_neighbor_throats(pores=Ps, mode='xnor')\n        self['throat.boundary'][Ts] = True\n\n        # Trim throats between Delaunay boundary pores\n        Ps = self.pores(labels=['boundary', 'delaunay'], mode='xnor')\n        Ts = self.find_neighbor_throats(pores=Ps, mode='xnor')\n        topotools.trim(network=self, throats=Ts)\n\n        # Move Delaunay boundary pores to centroid of Voronoi facet\n        Ps = self.pores(labels=['boundary', 'delaunay'], mode='xnor')\n        for P in Ps:\n            Ns = self.find_neighbor_pores(pores=P)\n            Ns = Ps = self['pore.voronoi']*self.tomask(pores=Ns)\n            coords = sp.mean(self['pore.coords'][Ns], axis=0)\n            self['pore.coords'][P] = coords\n\n        self['pore.internal'] = ~self['pore.boundary']\n        Ps = self.pores('internal')\n        Ts = self.find_neighbor_throats(pores=Ps, mode='xnor')\n        self['throat.internal'] = False\n        self['throat.internal'][Ts] = True\n\n        # Label surface pores and throats between boundary and internal\n        Ts = self.throats(['boundary', 'internal'], mode='not')\n        self['throat.surface'] = False\n        self['throat.surface'][Ts] = True\n        surf_pores = self['throat.conns'][Ts].flatten()\n        surf_pores = sp.unique(surf_pores[~self['pore.boundary'][surf_pores]])\n        self['pore.surface'] = False\n        self['pore.surface'][surf_pores] = True\n        # Clean-up\n        del self['pore.external']\n        del self['pore.keep']", "response": "r Trims internal pores and other pores that are outside the domain."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_throat_facets(self, throats=None):\n        if throats is None:\n            throats = self.throats('delaunay')\n        temp = []\n        tvals = self['throat.interconnect'].astype(int)\n        am = self.create_adjacency_matrix(weights=tvals, fmt='lil',\n                                          drop_zeros=True)\n        for t in throats:\n            P12 = self['throat.conns'][t]\n            Ps = list(set(am.rows[P12][0]).intersection(am.rows[P12][1]))\n            temp.append(Ps)\n        return sp.array(temp, dtype=object)", "response": "r Returns the indices of the Voronoi nodes that define the facet or\n            ridge between the Delaunay nodes connected by the given throat."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_pore_hulls(self, pores=None):\n        if pores is None:\n            pores = self.pores('delaunay')\n        temp = []\n        tvals = self['throat.interconnect'].astype(int)\n        am = self.create_adjacency_matrix(weights=tvals, fmt='lil',\n                                          drop_zeros=True)\n        for p in pores:\n            Ps = am.rows[p]\n            temp.append(Ps)\n        return sp.array(temp, dtype=object)", "response": "r Returns the indices of the convex hulls around the given pores."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _label_faces(self):\n        r'''\n        Label the pores sitting on the faces of the domain in accordance with\n        the conventions used for cubic etc.\n        '''\n        coords = sp.around(self['pore.coords'], decimals=10)\n        min_labels = ['front', 'left', 'bottom']\n        max_labels = ['back', 'right', 'top']\n        min_coords = sp.amin(coords, axis=0)\n        max_coords = sp.amax(coords, axis=0)\n        for ax in range(3):\n            self['pore.' + min_labels[ax]] = coords[:, ax] == min_coords[ax]\n            self['pore.' + max_labels[ax]] = coords[:, ax] == max_coords[ax]", "response": "r Label the pores sitting on the faces of the domain in accordance with\n        the conventions used for cubic etc."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load(cls, filename, project=None):\n        filename = cls._parse_filename(filename=filename, ext='mat')\n        data = spio.loadmat(filename)\n        # Reinsert the '.' separator into the array names\n        for item in list(data.keys()):\n            if item in ['__header__', '__version__', '__globals__']:\n                data.pop(item)\n                continue\n            elif '_pore_' in item:\n                path, prop = item.split('_pore_')\n                new_key = path + '|pore.' + prop\n            elif '_throat_' in item:\n                path, prop = item.split('_throat_')\n                new_key = path + '|throat.' + prop\n            data[new_key] = data.pop(item)\n\n        if project is None:\n            project = ws.new_project()\n        project = Dict.from_dict(data, project=project, delim='|')\n\n        project = cls._convert_data(project)\n\n        return project", "response": "r Loads data onto the given network from an appropriately formatted MatLAB file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_connecting_bonds(sites, am):\n    if am.format != 'dok':\n        am = am.todok(copy=False)\n    sites = sp.array(sites, ndmin=2)\n    if sites.size == 0:\n        return []\n    z = tuple(zip(sites[:, 0], sites[:, 1]))\n    neighbors = [am.get(z[i], None) for i in range(len(z))]\n    return neighbors", "response": "r Given pairs of sites finds the bonds which connects each pair."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef istriu(am):\n    if am.shape[0] != am.shape[1]:\n        print('Matrix is not square, triangularity is irrelevant')\n        return False\n    if am.format != 'coo':\n        am = am.tocoo(copy=False)\n    return sp.all(am.row <= am.col)", "response": "r Returns True if the sparse adjacency matrix is upper triangular"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef issymmetric(am):\n    if am.shape[0] != am.shape[1]:\n        logger.warning('Matrix is not square, symmetrical is irrelevant')\n        return False\n    if am.format != 'coo':\n        am = am.tocoo(copy=False)\n    if istril(am) or istriu(am):\n        return False\n    # Compare am with its transpose, element wise\n    sym = ((am != am.T).size) == 0\n    return sym", "response": "r A method to check if a sparse adjacency matrix is symmetrical Returns True if the matrix is symmetrical False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef im_to_am(im):\n    if im.shape[0] == im.shape[1]:\n        print('Warning: Received matrix is square which is unlikely')\n    if im.shape[0] > im.shape[1]:\n        print('Warning: Received matrix has more sites than bonds')\n    if im.format != 'coo':\n        im = im.tocoo(copy=False)", "response": "r Converts an incidence matrix into an adjacency matrix"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tri_to_am(tri):\n    # Create an empty list-of-list matrix\n    lil = sprs.lil_matrix((tri.npoints, tri.npoints))\n    # Scan through Delaunay triangulation to retrieve pairs\n    indices, indptr = tri.vertex_neighbor_vertices\n    for k in range(tri.npoints):\n        lil.rows[k] = indptr[indices[k]:indices[k+1]]\n    # Convert to coo format\n    lil.data = lil.rows  # Just a dummy array to make things work properly\n    coo = lil.tocoo()\n    # Set weights to 1's\n    coo.data = sp.ones_like(coo.data)\n    # Remove diagonal, and convert to csr remove duplicates\n    am = sp.sparse.triu(A=coo, k=1, format='csr')\n    # The convert back to COO and return\n    am = am.tocoo()\n    return am", "response": "r Converts a Delaunay Triangulation object to a sparse adjacency matrix network representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef conns_to_am(conns, shape=None, force_triu=True, drop_diag=True,\n                drop_dupes=True, drop_negs=True):\n    r\"\"\"\n    Converts a list of connections into a Scipy sparse adjacency matrix\n\n    Parameters\n    ----------\n    conns : array_like, N x 2\n        The list of site-to-site connections\n\n    shape : list, optional\n        The shape of the array.  If none is given then it is inferred from the\n        maximum value in ``conns`` array.\n\n    force_triu : boolean\n        If True (default), then all connections are assumed undirected, and\n        moved to the upper triangular portion of the array\n\n    drop_diag : boolean\n        If True (default), then connections from a site and itself are removed.\n\n    drop_dupes : boolean\n        If True (default), then all pairs of sites sharing multiple connections\n        are reduced to a single connection.\n\n    drop_negs : boolean\n        If True (default), then all connections with one or both ends pointing\n        to a negative number are removed.\n\n    \"\"\"\n    if force_triu:  # Sort connections to [low, high]\n        conns = sp.sort(conns, axis=1)\n    if drop_negs:  # Remove connections to -1\n        keep = ~sp.any(conns < 0, axis=1)\n        conns = conns[keep]\n    if drop_diag:  # Remove connections of [self, self]\n        keep = sp.where(conns[:, 0] != conns[:, 1])[0]\n        conns = conns[keep]\n    # Now convert to actual sparse array in COO format\n    data = sp.ones_like(conns[:, 0], dtype=int)\n    if shape is None:\n        N = conns.max() + 1\n        shape = (N, N)\n    am = sprs.coo_matrix((data, (conns[:, 0], conns[:, 1])), shape=shape)\n    if drop_dupes:  # Convert to csr and back too coo\n        am = am.tocsr()\n        am = am.tocoo()\n    # Perform one last check on adjacency matrix\n    missing = sp.where(sp.bincount(conns.flatten()) == 0)[0]\n    if sp.size(missing) or sp.any(am.col.max() < (shape[0] - 1)):\n        warnings.warn('Some nodes are not connected to any bonds')\n    return am", "response": "r Converts a list of connections into a Scipy sparse adjacency matrix."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef isoutside(coords, shape):\n    # Label external pores for trimming below\n    if len(shape) == 1:  # Spherical\n        # Find external points\n        r = sp.sqrt(sp.sum(coords**2, axis=1))\n        Ps = r > shape[0]\n    elif len(shape) == 2:  # Cylindrical\n        # Find external pores outside radius\n        r = sp.sqrt(sp.sum(coords[:, [0, 1]]**2, axis=1))\n        Ps = r > shape[0]\n        # Find external pores above and below cylinder\n        if shape[1] > 0:\n            Ps = Ps + (coords[:, 2] > shape[1])\n            Ps = Ps + (coords[:, 2] < 0)\n        else:\n            pass\n    elif len(shape) == 3:  # Rectilinear\n        shape = sp.array(shape, dtype=float)\n        try:\n            lo_lim = shape[:, 0]\n            hi_lim = shape[:, 1]\n        except IndexError:\n            lo_lim = sp.array([0, 0, 0])\n            hi_lim = shape\n        Ps1 = sp.any(coords > hi_lim, axis=1)\n        Ps2 = sp.any(coords < lo_lim, axis=1)\n        Ps = Ps1 + Ps2\n    return Ps", "response": "r Returns a Np - long mask of True values indicating pores that lie outside the specified region."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ispercolating(am, inlets, outlets, mode='site'):\n    if am.format is not 'coo':\n        am = am.to_coo()\n    ij = sp.vstack((am.col, am.row)).T\n    if mode.startswith('site'):\n        occupied_sites = sp.zeros(shape=am.shape[0], dtype=bool)\n        occupied_sites[ij[am.data].flatten()] = True\n        clusters = site_percolation(ij, occupied_sites)\n    elif mode.startswith('bond'):\n        occupied_bonds = am.data\n        clusters = bond_percolation(ij, occupied_bonds)\n    ins = sp.unique(clusters.sites[inlets])\n    if ins[0] == -1:\n        ins = ins[1:]\n    outs = sp.unique(clusters.sites[outlets])\n    if outs[0] == -1:\n        outs = outs[1:]\n    hits = sp.in1d(ins, outs)\n    return sp.any(hits)", "response": "r Returns a boolean array indicating if a percolating cluster exists in the network spanning\n    the given inlets and outlets."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bond_percolation(ij, occupied_bonds):\n    from collections import namedtuple\n    Np = sp.amax(ij) + 1\n    adj_mat = sprs.csr_matrix((occupied_bonds, (ij[:, 0], ij[:, 1])),\n                              shape=(Np, Np))\n    adj_mat.eliminate_zeros()\n    clusters = csgraph.connected_components(csgraph=adj_mat, directed=False)[1]\n    valid_clusters = sp.bincount(clusters) > 1\n    mapping = -sp.ones(shape=(clusters.max()+1, ), dtype=int)\n    mapping[valid_clusters] = sp.arange(0, valid_clusters.sum())\n    s_labels = mapping[clusters]\n    b_labels = sp.amin(s_labels[ij], axis=1)\n    tup = namedtuple('cluster_labels', ('sites', 'bonds'))\n    return tup(s_labels, b_labels)", "response": "r Calculates the site and bond occupancy status for a bond percolation process given a list of occupied bonds."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves the given network from the specified pores or throats.", "response": "def trim(network, pores=[], throats=[]):\n    '''\n    Remove pores or throats from the network.\n\n    Parameters\n    ----------\n    network : OpenPNM Network Object\n        The Network from which pores or throats should be removed\n\n    pores (or throats) : array_like\n        The indices of the of the pores or throats to be removed from the\n        network.\n\n    Notes\n    -----\n    This is an in-place operation, meaning the received Network object will\n    be altered directly.\n\n\n    Examples\n    --------\n    >>> import openpnm as op\n    >>> pn = op.network.Cubic(shape=[5, 5, 5])\n    >>> pn.Np\n    125\n    >>> pn.Nt\n    300\n    >>> op.topotools.trim(network=pn, pores=[1])\n    >>> pn.Np\n    124\n    >>> pn.Nt\n    296\n\n    '''\n    pores = sp.array(pores, ndmin=1)\n    throats = sp.array(throats, ndmin=1)\n    Pkeep = sp.copy(network['pore.all'])\n    Tkeep = sp.copy(network['throat.all'])\n    if sp.size(pores) > 0:\n        Pkeep[pores] = False\n        if not sp.any(Pkeep):\n            raise Exception('Cannot delete ALL pores')\n        # Performing customized find_neighbor_throats which is much faster, but\n        # not general for other types of queries\n#        temp = sp.in1d(network['throat.conns'].flatten(), pores)\n#        temp = sp.reshape(temp, (network.Nt, 2))\n#        Ts = sp.any(temp, axis=1)\n#        Ts = network.Ts[Ts]\n#        tic()\n        Ts = network.find_neighbor_throats(pores=~Pkeep, mode='union')\n#        toc()\n        if len(Ts) > 0:\n            Tkeep[Ts] = False\n    if sp.size(throats) > 0:\n        Tkeep[throats] = False\n        # The following IF catches the special case of deleting ALL throats\n        # It removes all throat props, adds 'all', and skips rest of function\n        if not sp.any(Tkeep):\n            logger.info('Removing ALL throats from network')\n            for item in network.keys():\n                if item.split('.')[0] == 'throat':\n                    del network[item]\n            network['throat.all'] = sp.array([], ndmin=1)\n            return\n\n    # Temporarily store throat conns and pore map for processing later\n    Np_old = network.Np\n    Nt_old = network.Nt\n    Pkeep_inds = sp.where(Pkeep)[0]\n    Tkeep_inds = sp.where(Tkeep)[0]\n    Pmap = sp.ones((network.Np,), dtype=int)*-1\n    tpore1 = network['throat.conns'][:, 0]\n    tpore2 = network['throat.conns'][:, 1]\n\n    # Delete specified pores and throats from all objects\n    for obj in network.project[::-1]:\n        if (obj.Np == Np_old) and (obj.Nt == Nt_old):\n            Ps = Pkeep_inds\n            Ts = Tkeep_inds\n        else:\n            Ps = obj.map_pores(pores=Pkeep, origin=network)\n            Ts = obj.map_throats(throats=Tkeep, origin=network)\n        for key in list(obj.keys()):\n            temp = obj.pop(key)\n            if key.split('.')[0] == 'throat':\n                obj.update({key: temp[Ts]})\n            if key.split('.')[0] == 'pore':\n                obj.update({key: temp[Ps]})\n\n    # Remap throat connections\n    Pmap[Pkeep] = sp.arange(0, sp.sum(Pkeep))\n    Tnew1 = Pmap[tpore1[Tkeep]]\n    Tnew2 = Pmap[tpore2[Tkeep]]\n    network.update({'throat.conns': sp.vstack((Tnew1, Tnew2)).T})\n\n    # Clear adjacency and incidence matrices which will be out of date now\n    network._am.clear()\n    network._im.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reduce_coordination(network, z):\n    # Find minimum spanning tree using random weights\n    am = network.create_adjacency_matrix(weights=sp.rand(network.Nt),\n                                         triu=False)\n    mst = csgraph.minimum_spanning_tree(am, overwrite=True)\n    mst = mst.tocoo()\n\n    # Label throats on spanning tree to avoid deleting them\n    Ts = network.find_connecting_throat(mst.row, mst.col)\n    Ts = sp.hstack(Ts)\n    network['throat.mst'] = False\n    network['throat.mst'][Ts] = True\n\n    # Trim throats not on the spanning tree to acheive desired coordination\n    Ts = sp.random.permutation(network.throats('mst', mode='nor'))\n    Ts = Ts[:int(network.Nt - network.Np*(z/2))]\n    trim(network=network, throats=Ts)", "response": "r Reduce the coordination of the network to acheive desired coordination"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef label_faces(network, tol=0.0, label='surface'):\n    if label not in network.labels():\n        find_surface_pores(network, label=label)\n    Psurf = network['pore.'+label]\n    crds = network['pore.coords']\n    xmin, xmax = sp.amin(crds[:, 0]), sp.amax(crds[:, 0])\n    xspan = xmax - xmin\n    ymin, ymax = sp.amin(crds[:, 1]), sp.amax(crds[:, 1])\n    yspan = ymax - ymin\n    zmin, zmax = sp.amin(crds[:, 2]), sp.amax(crds[:, 2])\n    zspan = zmax - zmin\n    network['pore.back'] = (crds[:, 0] >= (1-tol)*xmax) * Psurf\n    network['pore.right'] = (crds[:, 1] >= (1-tol)*ymax) * Psurf\n    network['pore.top'] = (crds[:, 2] >= (1-tol)*zmax) * Psurf\n    network['pore.front'] = (crds[:, 0] <= (xmin + tol*xspan)) * Psurf\n    network['pore.left'] = (crds[:, 1] <= (ymin + tol*yspan)) * Psurf\n    network['pore.bottom'] = (crds[:, 2] <= (zmin + tol*zspan)) * Psurf", "response": "r Returns a new object that is a copy of the network with the given label."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef merge_networks(network, donor=[]):\n    if type(donor) == list:\n        donors = donor\n    else:\n        donors = [donor]\n\n    for donor in donors:\n        network['pore.coords'] = sp.vstack((network['pore.coords'],\n                                            donor['pore.coords']))\n        network['throat.conns'] = sp.vstack((network['throat.conns'],\n                                             donor['throat.conns'] +\n                                             network.Np))\n        p_all = sp.ones((sp.shape(network['pore.coords'])[0],), dtype=bool)\n        t_all = sp.ones((sp.shape(network['throat.conns'])[0],), dtype=bool)\n        network.update({'pore.all': p_all})\n        network.update({'throat.all': t_all})\n        for key in set(network.keys()).union(set(donor.keys())):\n            if key.split('.')[1] not in ['conns', 'coords', '_id', 'all']:\n                if key in network.keys():\n                    pop_flag = False\n                    if key not in donor.keys():\n                        logger.debug('Adding ' + key + ' to donor')\n                        # If key not on donor add it first\n                        if network[key].dtype == bool:\n                            donor[key] = False\n                        else:\n                            donor[key] = sp.nan\n                        pop_flag = True\n                    # Then merge it with existing array on network\n                    try:\n                        temp = sp.hstack((network[key], donor[key]))\n                    except ValueError:\n                        temp = sp.vstack((network[key], donor[key]))\n                    network[key] = temp\n                    if pop_flag:\n                        donor.pop(key, None)\n                else:\n                    # If key not on network add it first\n                    logger.debug('Adding ' + key + ' to network')\n                    if donor[key].dtype == bool:\n                        network[key] = False\n                    else:\n                        network[key] = sp.nan\n                    # Then append donor values to network\n                    s = sp.shape(donor[key])[0]\n                    network[key][-s:] = donor[key]\n\n    # Clear adjacency and incidence matrices which will be out of date now\n    network._am.clear()\n    network._im.clear()", "response": "r Combine multiple networks into one without doing any topological operations."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stitch(network, donor, P_network, P_donor, method='nearest',\n           len_max=sp.inf, len_min=0, label_suffix=''):\n    r'''\n    Stitches a second a network to the current network.\n\n    Parameters\n    ----------\n    networK : OpenPNM Network Object\n        The Network to which to donor Network will be attached\n\n    donor : OpenPNM Network Object\n        The Network to stitch on to the current Network\n\n    P_network : array_like\n        The pores on the current Network\n\n    P_donor : array_like\n        The pores on the donor Network\n\n    label_suffix : string or None\n        Some text to append to each label in the donor Network before\n        inserting them into the recipient.  The default is to append no\n        text, but a common option would be to append the donor Network's\n        name. To insert none of the donor labels, use None.\n\n    len_max : float\n        Set a length limit on length of new throats\n\n    method : string (default = 'delaunay')\n        The method to use when making pore to pore connections. Options are:\n\n        - 'delaunay' : Use a Delaunay tessellation\n        - 'nearest' : Connects each pore on the receptor network to its nearest\n                      pore on the donor network\n\n    Notes\n    -----\n    Before stitching it is necessary to translate the pore coordinates of\n    one of the Networks so that it is positioned correctly relative to the\n    other.\n\n    Examples\n    --------\n    >>> import openpnm as op\n    >>> pn = op.network.Cubic(shape=[5, 5, 5])\n    >>> pn2 = op.network.Cubic(shape=[5, 5, 5])\n    >>> [pn.Np, pn.Nt]\n    [125, 300]\n    >>> [pn2.Np, pn2.Nt]\n    [125, 300]\n    >>> pn2['pore.coords'][:, 2] += 5.0\n    >>> op.topotools.stitch(network=pn, donor=pn2, P_network=pn.pores('top'),\n    ...                     P_donor=pn2.pores('bottom'), method='nearest',\n    ...                     len_max=1.0)\n    >>> [pn.Np, pn.Nt]\n    [250, 625]\n\n    '''\n    # Ensure Networks have no associated objects yet\n    if (len(network.project) > 1) or (len(donor.project) > 1):\n        raise Exception('Cannot stitch a Network with active objects')\n    network['throat.stitched'] = False\n    # Get the initial number of pores and throats\n    N_init = {}\n    N_init['pore'] = network.Np\n    N_init['throat'] = network.Nt\n    if method == 'nearest':\n        P1 = P_network\n        P2 = P_donor + N_init['pore']  # Increment pores on donor\n        C1 = network['pore.coords'][P_network]\n        C2 = donor['pore.coords'][P_donor]\n        D = sp.spatial.distance.cdist(C1, C2)\n        [P1_ind, P2_ind] = sp.where(D <= len_max)\n        conns = sp.vstack((P1[P1_ind], P2[P2_ind])).T\n    else:\n        raise Exception('<{}> method not supported'.format(method))\n\n    # Enter donor's pores into the Network\n    extend(network=network, pore_coords=donor['pore.coords'])\n\n    # Enter donor's throats into the Network\n    extend(network=network, throat_conns=donor['throat.conns'] +\n           N_init['pore'])\n\n    # Trim throats that are longer then given len_max\n    C1 = network['pore.coords'][conns[:, 0]]\n    C2 = network['pore.coords'][conns[:, 1]]\n    L = sp.sum((C1 - C2)**2, axis=1)**0.5\n    conns = conns[L <= len_max]\n\n    # Add donor labels to recipient network\n    if label_suffix is not None:\n        if label_suffix != '':\n            label_suffix = '_'+label_suffix\n        for label in donor.labels():\n            element = label.split('.')[0]\n            locations = sp.where(network._get_indices(element) >=\n                                 N_init[element])[0]\n            if label + label_suffix not in network.keys():\n                network[label + label_suffix] = False\n            network[label+label_suffix][locations] = donor[label]\n\n    # Add the new stitch throats to the Network\n    extend(network=network, throat_conns=conns, labels='stitched')\n\n    # Remove donor from Workspace, if present\n    # This check allows for the reuse of a donor Network multiple times\n    for sim in list(ws.values()):\n        if donor in sim:\n            del ws[sim.name]", "response": "r This function stitches a second network to the current network."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef connect_pores(network, pores1, pores2, labels=[], add_conns=True):\n    r'''\n    Returns the possible connections between two group of pores, and optionally\n    makes the connections.\n\n    See ``Notes`` for advanced usage.\n\n    Parameters\n    ----------\n    network : OpenPNM Network Object\n\n    pores1 : array_like\n        The first group of pores on the network\n\n    pores2 : array_like\n        The second group of pores on the network\n\n    labels : list of strings\n        The labels to apply to the new throats.  This argument is only needed\n        if ``add_conns`` is True.\n\n    add_conns : bool\n        Indicates whether the connections should be added to the supplied\n        network (default is True).  Otherwise, the connections are returned\n        as an Nt x 2 array that can be passed directly to ``extend``.\n\n    Notes\n    -----\n    (1) The method also works if ``pores1`` and ``pores2`` are list of lists,\n    in which case it consecutively connects corresponding members of the two\n    lists in a 1-to-1 fashion. Example: pores1 = [[0, 1], [2, 3]] and\n    pores2 = [[5], [7, 9]] leads to creation of the following connections:\n        0 --> 5     2 --> 7     3 --> 7\n        1 --> 5     2 --> 9     3 --> 9\n\n    (2) If you want to use the batch functionality, make sure that each element\n    within ``pores1`` and ``pores2`` are of type list or ndarray.\n\n    (3) It creates the connections in a format which is acceptable by\n    the default OpenPNM connection ('throat.conns') and either adds them to\n    the network or returns them.\n\n    Examples\n    --------\n    >>> import openpnm as op\n    >>> pn = op.network.Cubic(shape=[5, 5, 5])\n    >>> pn.Nt\n    300\n    >>> op.topotools.connect_pores(network=pn, pores1=[22, 32],\n    ...                            pores2=[16, 80, 68])\n    >>> pn.Nt\n    306\n    >>> pn['throat.conns'][300:306]\n    array([[16, 22],\n           [22, 80],\n           [22, 68],\n           [16, 32],\n           [32, 80],\n           [32, 68]])\n\n    '''\n    # Assert that `pores1` and `pores2` are list of lists\n    try:\n        len(pores1[0])\n    except (TypeError, IndexError):\n        pores1 = [pores1]\n    try:\n        len(pores2[0])\n    except (TypeError, IndexError):\n        pores2 = [pores2]\n\n    if len(pores1) != len(pores2):\n        raise Exception('Running in batch mode! pores1 and pores2 must be' + \\\n                        ' of the same length.')\n\n    arr1, arr2 = [], []\n    for ps1, ps2 in zip(pores1, pores2):\n        size1 = sp.size(ps1)\n        size2 = sp.size(ps2)\n        arr1.append(sp.repeat(ps1, size2))\n        arr2.append(sp.tile(ps2, size1))\n    conns = sp.vstack([sp.concatenate(arr1), sp.concatenate(arr2)]).T\n    if add_conns:\n        extend(network=network, throat_conns=conns, labels=labels)\n    else:\n        return conns", "response": "r Creates a new set of possible connections between two pores."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_pore_to_pore_distance(network, pores1=None, pores2=None):\n    r'''\n    Find the distance between all pores on set one to each pore in set 2\n\n    Parameters\n    ----------\n    network : OpenPNM Network Object\n        The network object containing the pore coordinates\n\n    pores1 : array_like\n        The pore indices of the first set\n\n    pores2 : array_Like\n        The pore indices of the second set.  It's OK if these indices are\n        partially or completely duplicating ``pores``.\n\n    Returns\n    -------\n    A distance matrix with ``len(pores1)`` rows and ``len(pores2)`` columns.\n    The distance between pore *i* in ``pores1`` and *j* in ``pores2`` is\n    located at *(i, j)* and *(j, i)* in the distance matrix.\n\n    '''\n    from scipy.spatial.distance import cdist\n    p1 = sp.array(pores1, ndmin=1)\n    p2 = sp.array(pores2, ndmin=1)\n    coords = network['pore.coords']\n    return cdist(coords[p1], coords[p2])", "response": "r Finds the distance between all pores on set one to each pore in set 2."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subdivide(network, pores, shape, labels=[]):\n    r'''\n    It trim the pores and replace them by cubic networks with the sent shape.\n\n    Parameters\n    ----------\n    network : OpenPNM Network Object\n\n    pores : array_like\n        The first group of pores to be replaced\n\n    shape : array_like\n        The shape of cubic networks in the target locations\n\n    Notes\n    -----\n    - It works only for cubic networks.\n\n    Examples\n    --------\n    >>> import openpnm as op\n    >>> pn = op.network.Cubic(shape=[5, 6, 5], spacing=0.001)\n    >>> pn.Np\n    150\n    >>> nano_pores = [2, 13, 14, 15]\n    >>> op.topotools.subdivide(network=pn, pores=nano_pores, shape=[4, 7, 3],\n    ...                        labels='nano')\n    >>> pn.Np\n    482\n\n    '''\n    mro = network._mro()\n    if 'Cubic' not in mro:\n        raise Exception('Subdivide is only supported for Cubic Networks')\n    from openpnm.network import Cubic\n    pores = network._parse_indices(pores)\n\n    # Checks to find boundary pores in the selected pores\n    if 'pore.boundary' in network.labels():\n        if (sp.in1d(pores, network.pores('boundary'))).any():\n            raise Exception('boundary pores cannot be subdivided!')\n    if not hasattr(network, '_subdivide_flag'):\n        network._subdivide_flag = True\n    else:\n        raise Exception('The network has subdivided pores, so the method \\\n                         does not support another subdivision')\n    # Assigning right shape and division\n    if sp.size(shape) != 2 and sp.size(shape) != 3:\n        raise Exception('Subdivide not implemented for Networks other than 2D \\\n                         and 3D')\n    elif sp.size(shape) == 3 and 1 not in shape:\n        div = sp.array(shape, ndmin=1)\n        single_dim = None\n    else:\n        single_dim = sp.where(sp.array(network.shape) == 1)[0]\n        if sp.size(single_dim) == 0:\n            single_dim = None\n        if sp.size(shape) == 3:\n            div = sp.array(shape, ndmin=1)\n        else:\n            div = sp.zeros(3, dtype=sp.int32)\n            if single_dim is None:\n                dim = 2\n            else:\n                dim = single_dim\n            div[dim] = 1\n            div[-sp.array(div, ndmin=1, dtype=bool)] = sp.array(shape, ndmin=1)\n\n    # Creating small network and handling labels\n    networkspacing = network.spacing\n    new_netspacing = networkspacing/div\n    new_net = Cubic(shape=div, spacing=new_netspacing)\n    main_labels = ['left', 'right', 'front', 'back', 'top', 'bottom']\n    if single_dim is not None:\n        label_groups = sp.array([['front', 'back'],\n                                 ['left', 'right'],\n                                 ['top', 'bottom']])\n        non_single_labels = label_groups[sp.array([0, 1, 2]) != single_dim]\n    for l in main_labels:\n        new_net['pore.surface_' + l] = False\n        network['pore.surface_' + l] = False\n        if single_dim is None:\n            new_net['pore.surface_' + l][new_net.pores(labels=l)] = True\n        else:\n            for ind in [0, 1]:\n                loc = (non_single_labels[ind] == l)\n                temp_pores = new_net.pores(non_single_labels[ind][loc])\n                new_net['pore.surface_' + l][temp_pores] = True\n\n    old_coords = sp.copy(new_net['pore.coords'])\n    if labels == []:\n        labels = ['pore.subdivided_' + new_net.name]\n    for P in pores:\n        # Shifting the new network to the right location and attaching it to\n        # the main network\n        shift = network['pore.coords'][P] - networkspacing/2\n        new_net['pore.coords'] += shift\n        Pn = network.find_neighbor_pores(pores=P)\n        try:\n            Pn_new_net = network.pores(labels)\n        except KeyError:\n            Pn_new_net = []\n        Pn_old_net = Pn[~sp.in1d(Pn, Pn_new_net)]\n        Np1 = network.Np\n        extend(pore_coords=new_net['pore.coords'],\n               throat_conns=new_net['throat.conns'] + Np1,\n               labels=labels, network=network)\n\n        # Moving the temporary labels to the big network\n        for l in main_labels:\n            network['pore.surface_'+l][Np1:] = new_net['pore.surface_'+l]\n\n        # Stitching the old pores of the main network to the new extended pores\n        surf_pores = network.pores('surface_*')\n        surf_coord = network['pore.coords'][surf_pores]\n        for neighbor in Pn:\n            neighbor_coord = network['pore.coords'][neighbor]\n            dist = [round(sp.inner(neighbor_coord-x, neighbor_coord-x),\n                          20) for x in surf_coord]\n            nearest_neighbor = surf_pores[dist == sp.amin(dist)]\n            if neighbor in Pn_old_net:\n                coplanar_labels = network.labels(pores=nearest_neighbor)\n                new_neighbors = network.pores(coplanar_labels,\n                                              mode='xnor')\n                # This might happen to the edge of the small network\n                if sp.size(new_neighbors) == 0:\n                    labels = network.labels(pores=nearest_neighbor,\n                                            mode='xnor')\n                    common_label = [l for l in labels if 'surface_' in l]\n                    new_neighbors = network.pores(common_label)\n            elif neighbor in Pn_new_net:\n                new_neighbors = nearest_neighbor\n            connect_pores(network=network, pores1=neighbor,\n                          pores2=new_neighbors, labels=labels)\n\n        # Removing temporary labels\n        for l in main_labels:\n            network['pore.surface_' + l] = False\n        new_net['pore.coords'] = sp.copy(old_coords)\n\n    label_faces(network=network)\n    for l in main_labels:\n        del network['pore.surface_'+l]\n    trim(network=network, pores=pores)\n    ws = network.project.workspace\n    ws.close_project(new_net.project)", "response": "r This function is used to create a new object that is subdivided by a set of pores."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef trim_occluded_throats(network, mask='all'):\n    occluded_ts = network['throat.area'] == 0\n    if sp.sum(occluded_ts) > 0:\n        occluded_ts *= network[\"throat.\"+mask]\n        trim(network=network, throats=occluded_ts)", "response": "r Removes all throats that are isolated in the network and also remove those that are not in the network."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merge_pores(network, pores, labels=['merged']):\n    # Assert that `pores` is list of lists\n    try:\n        len(pores[0])\n    except (TypeError, IndexError):\n        pores = [pores]\n        \n    N = len(pores)\n    NBs, XYZs = [], []\n    \n    for Ps in pores:\n        NBs.append(network.find_neighbor_pores(pores=Ps,\n                                               mode='union',\n                                               flatten=True,\n                                               include_input=False))\n        XYZs.append(network['pore.coords'][Ps].mean(axis=0))\n    \n    extend(network, pore_coords=XYZs, labels=labels)\n    Pnew = network.Ps[-N::]\n    \n    # Possible throats between new pores: This only happens when running in\n    # batch mode, i.e. multiple groups of pores are to be merged. In case\n    # some of these groups share elements, possible throats between the \n    # intersecting elements is not captured and must be added manually.\n    pores_set = [set(items) for items in pores]\n    NBs_set = [set(items) for items in NBs]\n    ps1, ps2 = [], []    \n    from itertools import combinations\n    for i, j in combinations(range(N), 2):\n        if not NBs_set[i].isdisjoint(pores_set[j]):\n            ps1.append([network.Ps[-N+i]])\n            ps2.append([network.Ps[-N+j]])\n    \n    # Add (possible) connections between the new pores\n    connect_pores(network, pores1=ps1, pores2=ps2, labels=labels)\n    # Add connections between the new pores and the rest of the network\n    connect_pores(network, pores2=sp.split(Pnew, N), pores1=NBs, labels=labels)\n    # Trim merged pores from the network\n    trim(network=network, pores=sp.concatenate(pores))", "response": "r Merges a list of pores into a new single object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef template_sphere_shell(outer_radius, inner_radius=0):\n    img = _template_sphere_disc(dim=3, outer_radius=outer_radius,\n                                inner_radius=inner_radius)\n    return img", "response": "r This method generates an image array of a sphere - shell."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_connections(network, throats=None, fig=None, **kwargs):\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.mplot3d import Axes3D\n\n    if throats is None:\n        Ts = network.Ts\n    else:\n        Ts = network._parse_indices(indices=throats)\n\n    if len(sp.unique(network['pore.coords'][:, 2])) == 1:\n        ThreeD = False\n    else:\n        ThreeD = True\n\n    if fig is None:\n        fig = plt.figure()\n        if ThreeD:\n            ax = fig.add_subplot(111, projection='3d')\n        else:\n            ax = fig.gca()\n    else:\n        ax = fig.gca()\n\n    # Create dummy indexing to sp.inf\n    i = -1*sp.ones((sp.size(Ts)*3, ), dtype=int)\n    i[0::3] = network['throat.conns'][Ts, 0]\n    i[1::3] = network['throat.conns'][Ts, 1]\n\n    # Collect coordinates and scale axes to fit\n    Ps = sp.unique(network['throat.conns'][Ts])\n    X = network['pore.coords'][Ps, 0]\n    Y = network['pore.coords'][Ps, 1]\n    Z = network['pore.coords'][Ps, 2]\n    _scale_3d_axes(ax=ax, X=X, Y=Y, Z=Z)\n\n    # Add sp.inf to the last element of pore.coords (i.e. -1)\n    inf = sp.array((sp.inf,))\n    X = sp.hstack([network['pore.coords'][:, 0], inf])\n    Y = sp.hstack([network['pore.coords'][:, 1], inf])\n    Z = sp.hstack([network['pore.coords'][:, 2], inf])\n    if ThreeD:\n        ax.plot(xs=X[i], ys=Y[i], zs=Z[i], **kwargs)\n    else:\n        ax.plot(X[i], Y[i], **kwargs)\n\n    return fig", "response": "r Plot the topological connections of a network."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_networkx(network, plot_throats=True, labels=None, colors=None,\n                  scale=10):\n    r'''\n    Returns a pretty 2d plot for 2d OpenPNM networks.\n\n    Parameters\n    ----------\n    network : OpenPNM Network object\n\n    plot_throats : boolean\n        Plots throats as well as pores, if True.\n\n    labels : list\n        List of OpenPNM labels\n\n    colors : list\n        List of corresponding colors to the given `labels`.\n\n    scale : float\n        Scale factor for size of pores.\n    '''\n    import networkx as nx\n    x, y, z = network['pore.coords'].T\n    x, y = [j for j in [x, y, z] if not sp.allclose(j, j.mean())]\n\n    G = nx.Graph()\n    pos = {network.Ps[i]: [x[i], y[i]] for i in range(network.Np)}\n    if 'pore.diameter' in network.keys():\n        node_size = scale * network['pore.diameter']\n    else:\n        node_size = scale\n    node_color = sp.array(['r'] * len(network.Ps))\n\n    if labels:\n        if type(labels) is not list:\n            labels = [labels]\n        if type(colors) is not list:\n            colors = [colors]\n        if len(labels) != len(colors):\n            raise('len(colors) must be equal to len(labels)!')\n        for label, color in zip(labels, colors):\n            node_color[network.pores(label)] = color\n\n    nx.draw_networkx_nodes(G, pos=pos, nodelist=network.Ps.tolist(),\n                           node_color=node_color, edge_color='r',\n                           node_size=node_size)\n    if plot_throats:\n        nx.draw_networkx_edges(G, pos=pos, edge_color='k', alpha=0.8,\n                               edgelist=network['throat.conns'].tolist())\n    return G", "response": "r Plot a network with the given labels and colors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_clusters(network, mask=[], t_labels=False):\n    # Parse the input arguments\n    mask = sp.array(mask, ndmin=1)\n    if mask.dtype != bool:\n        raise Exception('Mask must be a boolean array of Np or Nt length')\n\n    # If pore mask was given perform site percolation\n    if sp.size(mask) == network.Np:\n        (p_clusters, t_clusters) = _site_percolation(network, mask)\n    # If pore mask was given perform bond percolation\n    elif sp.size(mask) == network.Nt:\n        (p_clusters, t_clusters) = _bond_percolation(network, mask)\n    else:\n        raise Exception('Mask received was neither Nt nor Np long')\n\n    return (p_clusters, t_clusters)", "response": "r This method finds connected clusters of pores in the network."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_boundary_pores(network, pores, offset, apply_label='boundary'):\n    # Parse the input pores\n    Ps = sp.array(pores, ndmin=1)\n    if Ps.dtype is bool:\n        Ps = network.toindices(Ps)\n    if sp.size(pores) == 0:  # Handle an empty array if given\n        return sp.array([], dtype=sp.int64)\n    # Clone the specifed pores\n    clone_pores(network=network, pores=Ps)\n    newPs = network.pores('pore.clone')\n    del network['pore.clone']\n    newTs = network.throats('clone')\n    del network['throat.clone']\n    # Offset the cloned pores\n    network['pore.coords'][newPs] += offset\n    # Apply labels to boundary pores (trim leading 'pores' if present)\n    label = apply_label.split('.')[-1]\n    plabel = 'pore.' + label\n    tlabel = 'throat.' + label\n    network[plabel] = False\n    network[plabel][newPs] = True\n    network[tlabel] = False\n    network[tlabel][newTs] = True", "response": "r This method adds a boundary pore to the network."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iscoplanar(coords):\n    r'''\n    Determines if given pores are coplanar with each other\n\n    Parameters\n    ----------\n    coords : array_like\n        List of pore coords to check for coplanarity.  At least 3 pores are\n        required.\n\n    Returns\n    -------\n    A boolean value of whether given points are coplanar (True) or not (False)\n    '''\n    coords = sp.array(coords, ndmin=1)\n    if sp.shape(coords)[0] < 3:\n        raise Exception('At least 3 input pores are required')\n\n    Px = coords[:, 0]\n    Py = coords[:, 1]\n    Pz = coords[:, 2]\n\n    # Do easy check first, for common coordinate\n    if sp.shape(sp.unique(Px))[0] == 1:\n        return True\n    if sp.shape(sp.unique(Py))[0] == 1:\n        return True\n    if sp.shape(sp.unique(Pz))[0] == 1:\n        return True\n\n    # Perform rigorous check using vector algebra\n    n1 = sp.array((Px[1] - Px[0], Py[1] - Py[0], Pz[1] - Pz[0])).T\n    n2 = sp.array((Px[2] - Px[1], Py[2] - Py[1], Pz[2] - Pz[1])).T\n    n = sp.cross(n1, n2)\n    r = sp.array((Px[1:-1] - Px[0], Py[1:-1] - Py[0], Pz[1:-1] - Pz[0]))\n\n    n_dot = sp.dot(n, r)\n\n    if sp.sum(n_dot) == 0:\n        return True\n    else:\n        return False", "response": "r Determines if given points are coplanar with each other."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_key_props(phase=None, diameter='throat.diameter',\n                   surface_tension='pore.surface_tension',\n                   contact_angle='pore.contact_angle'):\n    r\"\"\"\n    Many of the methods are generic to pores and throats. Some information may\n    be stored on either the pore or throat and needs to be interpolated.\n    This is a helper method to return the properties in the correct format.\n    To do:\n        Check for method to convert throat to pore data\n    \"\"\"\n    element = diameter.split('.')[0]\n    if element == 'pore':\n        if 'throat' in surface_tension:\n            sigma = phase.interpolate_data(propname=surface_tension)\n        else:\n            sigma = phase[surface_tension]\n        if 'throat' in contact_angle:\n            theta = phase.interpolate_data(propname=contact_angle)\n        else:\n            theta = phase[contact_angle]\n    if element == 'throat':\n        if 'pore' in surface_tension:\n            sigma = phase.interpolate_data(propname=surface_tension)\n        else:\n            sigma = phase[surface_tension]\n        if 'pore' in contact_angle:\n            theta = phase.interpolate_data(propname=contact_angle)\n        else:\n            theta = phase[contact_angle]\n\n    return element, sigma, theta", "response": "r Returns the properties of the throat and pore elements."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef washburn(target, surface_tension='pore.surface_tension',\n             contact_angle='pore.contact_angle',\n             diameter='throat.diameter'):\n    r\"\"\"\n    Computes the capillary entry pressure assuming the throat in a cylindrical\n    tube.\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object for which these values are being calculated.  This\n        controls the length of the calculated array, and also provides\n        access to other necessary thermofluid properties.\n\n    surface_tension : string\n        The dictionary key containing the surface tension values to be used. If\n        a pore property is given, it is interpolated to a throat list.\n\n    contact_angle : string\n        The dictionary key containing the contact angle values to be used. If\n        a pore property is given, it is interpolated to a throat list.\n\n    diameter : string\n        The dictionary key containing the throat diameter values to be used.\n\n    Notes\n    -----\n    The Washburn equation is:\n\n    .. math::\n        P_c = -\\frac{2\\sigma(cos(\\theta))}{r}\n\n    This is the most basic approach to calculating entry pressure and is\n    suitable for highly non-wetting invading phases in most materials.\n\n    \"\"\"\n    network = target.project.network\n    phase = target.project.find_phase(target)\n    element, sigma, theta = _get_key_props(phase=phase,\n                                           diameter=diameter,\n                                           surface_tension=surface_tension,\n                                           contact_angle=contact_angle)\n    r = network[diameter]/2\n    value = -2*sigma*_sp.cos(_sp.radians(theta))/r\n    if diameter.split('.')[0] == 'throat':\n        value = value[phase.throats(target.name)]\n    else:\n        value = value[phase.pores(target.name)]\n    value[_sp.absolute(value) == _sp.inf] = 0\n    return value", "response": "r This function calculates the capillary entry pressure assuming the throat in a cylindrical object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ransohoff_snap_off(target,\n                       shape_factor=2.0,\n                       wavelength=5e-6,\n                       require_pair=False,\n                       surface_tension='pore.surface_tension',\n                       contact_angle='pore.contact_angle',\n                       diameter='throat.diameter',\n                       vertices='throat.offset_vertices',\n                       **kwargs):\n    r\"\"\"\n    Computes the capillary snap-off pressure assuming the throat is cylindrical\n    with converging-diverging change in diamater - like the Purcell model.\n    The wavelength of the change in diamater is the fiber radius.\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object for which these values are being calculated.  This\n        controls the length of the calculated array, and also provides\n        access to other necessary thermofluid properties.\n\n    shape_factor :\n        constant dependent on the shape of throat cross-section 1.75 - 2.0, see\n        Ref\n\n    wavelength : float or array like\n        The transverse interfacial radius of curvature at the neck\n        (fiber radius in fibrous media)\n\n    require_pair : bool\n        Controls whether snap-off requires a pair of arc meniscii to occur.\n\n    surface_tension : dict key (string)\n        The dictionary key containing the surface tension values to be used.\n        If a pore property is given, it is interpolated to a throat list.\n\n    contact_angle : dict key (string)\n        The dictionary key containing the contact angle values to be used.\n        If a pore property is given, it is interpolated to a throat list.\n\n    throat_diameter : dict key (string)\n        The dictionary key containing the throat diameter values to be used.\n\n    Notes\n    -----\n    This equation should be used to calculate the snap off capillary pressure\n    in fribrous media\n\n    References\n    ----------\n    [1]: Ransohoff, T.C., Gauglitz, P.A. and Radke, C.J., 1987. Snap\u2010off of gas\n    bubbles in smoothly constricted noncircular capillaries. AIChE Journal,\n    33(5), pp.753-765.\n    \"\"\"\n    phase = target.project.find_phase(target)\n    geometry = target.project.find_geometry(target)\n    element, sigma, theta = _get_key_props(phase=phase,\n                                           diameter=diameter,\n                                           surface_tension=surface_tension,\n                                           contact_angle=contact_angle)\n    try:\n        all_verts = geometry[vertices]\n        # Work out whether throat geometry can support at least one pair of\n        # adjacent arc menisci that can grow and merge to form snap-off\n        # Only works if throat vertices are in convex hull order\n        angles_ok = np.zeros(geometry.Nt, dtype=bool)\n        for T in range(geometry.Nt):\n            verts = all_verts[T]\n            x = verts[:, 0]\n            y = verts[:, 1]\n            z = verts[:, 2]\n            # PLus\n            p = 1\n            # Minus\n            m = -1\n            verts_p = np.vstack((np.roll(x, p),\n                                 np.roll(y, p),\n                                 np.roll(z, p))).T\n            verts_m = np.vstack((np.roll(x, m),\n                                 np.roll(y, m),\n                                 np.roll(z, m))).T\n            v1 = verts_p - verts\n            v2 = verts_m - verts\n            corner_angles = np.rad2deg(tr.angle_between_vectors(v1,\n                                                                v2,\n                                                                axis=1))\n            # Logical test for existence of arc menisci\n            am = theta[T] <= 90 - corner_angles/2\n            if require_pair:\n                # Logical test for two adjacent arc menisci\n                pair_p = np.logical_and(am, np.roll(am, + p))\n                pair_m = np.logical_and(am, np.roll(am, + m))\n                am_pair = np.any(np.logical_or(pair_p, pair_m))\n                angles_ok[T] = am_pair\n            else:\n                # Logical test for any arc menisci\n                angles_ok[T] = np.any(am)\n    except:\n        logger.warning(\"Model is designed to work with property: \" +\n                       vertices)\n        angles_ok = np.ones(geometry.Nt, dtype=bool)\n\n    # Condition for arc menisci to form in corners\n    rad_Ts = geometry[diameter]/2\n    # Ransohoff and Radke eq. 4\n    C = 1/rad_Ts - 1/wavelength\n    value = sigma[phase.throats(target.name)]*C\n    # Only throats that can support arc menisci can snap-off\n    value[~angles_ok] = np.nan\n    logger.info(\"Snap off pressures calculated for \" +\n                str(np.around(100*np.sum(angles_ok)/np.size(angles_ok), 0)) +\n                \"% of throats\")\n    return value", "response": "r Returns the capillary snap - off pressure assuming the throat is cylindrical and the throat is cylindrical."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef purcell_bidirectional(target, r_toroid,\n                          num_points=1e2,\n                          surface_tension='pore.surface_tension',\n                          contact_angle='pore.contact_angle',\n                          throat_diameter='throat.diameter',\n                          pore_diameter='pore.diameter'):\n    r\"\"\"\n    Computes the throat capillary entry pressure assuming the throat is a\n    toroid. Makes use of the toroidal meniscus model with mode touch.\n    This model accounts for mensicus protrusion into adjacent pores and\n    touching solid features.\n    It is bidirectional becauase the connected pores generally have different\n    sizes and this determines how far the meniscus can protrude.\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object for which these values are being calculated.  This\n        controls the length of the calculated array, and also provides\n        access to other necessary thermofluid properties.\n\n    r_toroid : float or array_like\n        The radius of the toroid surrounding the pore\n\n    num_points : float (Default 100)\n        The number of divisions to make along the profile length to assess the\n        meniscus properties in order to find the touch length.\n\n    surface_tension : dict key (string)\n        The dictionary key containing the surface tension values to be used.\n        If a pore property is given, it is interpolated to a throat list.\n\n    contact_angle : dict key (string)\n        The dictionary key containing the contact angle values to be used.\n        If a pore property is given, it is interpolated to a throat list.\n\n    throat_diameter : dict key (string)\n        The dictionary key containing the throat diameter values to be used.\n\n    pore_diameter : dict key (string)\n        The dictionary key containing the pore diameter values to be used.\n    Notes\n    \"\"\"\n    network = target.project.network\n    conns = network['throat.conns']\n    values = {}\n    for p in range(2):\n        network['throat.temp_diameter'] = network[pore_diameter][conns[:, p]]\n        key = 'throat.touch_pore_'+str(p)\n        target.add_model(propname=key,\n                         model=pm.meniscus.toroidal,\n                         mode='touch',\n                         r_toroid=r_toroid,\n                         num_points=num_points,\n                         throat_diameter=throat_diameter,\n                         surface_tension=surface_tension,\n                         contact_angle=contact_angle,\n                         touch_length='throat.temp_diameter')\n        values[p] = target[key]\n        target.remove_model(key)\n    del network['throat.temp_diameter']\n    return np.vstack((values[0], values[1])).T", "response": "r Computes the throat capillary entry pressure assuming the toroidal meniscus protrusion is used."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sinusoidal_bidirectional(target,\n                             num_points=1e2,\n                             surface_tension='pore.surface_tension',\n                             contact_angle='pore.contact_angle',\n                             throat_diameter='throat.diameter',\n                             throat_amplitude='throat.amplitude',\n                             throat_length='throat.length',\n                             pore_diameter='pore.diameter'):\n    r\"\"\"\n    Computes the throat capillary entry pressure assuming the throat has a\n    sinusoisal profile.\n    Makes use of the toroidal meniscus model with mode touch.\n    This model accounts for mensicus protrusion into adjacent pores and\n    touching solid features.\n    It is bidirectional becauase the connected pores generally have different\n    sizes and this determines how far the meniscus can protrude.\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object for which these values are being calculated.  This\n        controls the length of the calculated array, and also provides\n        access to other necessary thermofluid properties\n\n    num_points : float (Default 100)\n        The number of divisions to make along the profile length to assess the\n        meniscus properties in order to find the touch length.\n\n    surface_tension : dict key (string)\n        The dictionary key containing the surface tension values to be used.\n        If a pore property is given, it is interpolated to a throat list.\n\n    contact_angle : dict key (string)\n        The dictionary key containing the contact angle values to be used.\n        If a pore property is given, it is interpolated to a throat list.\n\n    throat_diameter : dict key (string)\n        The dictionary key containing the throat diameter values to be used.\n\n    throat_amplitude : dict key (string)\n        The dictionary key containing the amplitude of variation in the throat\n        diameter about the mean.\n\n    throat_length : dict key (string)\n        The dictionary key containing the throat length values to be used.\n\n    pore_diameter : dict key (string)\n        The dictionary key containing the pore diameter values to be used.\n    Notes\n    \"\"\"\n    network = target.project.network\n    conns = network['throat.conns']\n    values = {}\n    for p in range(2):\n        network['throat.temp_diameter'] = network[pore_diameter][conns[:, p]]\n        key = 'throat.touch_pore_'+str(p)\n        target.add_model(propname=key,\n                         model=pm.meniscus.sinusoidal,\n                         mode='touch',\n                         num_points=num_points,\n                         surface_tension=surface_tension,\n                         contact_angle=contact_angle,\n                         throat_diameter=throat_diameter,\n                         throat_amplitude=throat_amplitude,\n                         throat_length=throat_length,\n                         touch_length='throat.temp_diameter')\n        values[p] = target[key]\n        target.remove_model(key)\n    del network['throat.temp_diameter']\n    return np.vstack((values[0], values[1])).T", "response": "r Returns the throat capillary entry pressure assuming the toroidal meniscus protrusion is bidirectional."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dependency_list(self):\n        r'''\n        Returns a list of dependencies in the order with which they should be\n        called to ensure data is calculated by one model before it's asked for\n        by another.\n\n        Notes\n        -----\n        This raises an exception if the graph has cycles which means the\n        dependencies are unresolvable (i.e. there is no order which the\n        models can be called that will work).  In this case it is possible\n        to visually inspect the graph using ``dependency_graph``.\n\n        See Also\n        --------\n        dependency_graph\n        dependency_map\n\n        '''\n        dtree = self.dependency_graph()\n        cycles = list(nx.simple_cycles(dtree))\n        if cycles:\n            raise Exception('Cyclic dependency found: ' + ' -> '.join(\n                            cycles[0] + [cycles[0][0]]))\n        d = nx.algorithms.dag.lexicographical_topological_sort(dtree, sorted)\n        return list(d)", "response": "r Returns a list of all the dependencies of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dependency_map(self):\n        dtree = self.dependency_graph()\n        fig = nx.draw_spectral(dtree,\n                               with_labels=True,\n                               arrowsize=50,\n                               node_size=2000,\n                               edge_color='lightgrey',\n                               width=3.0,\n                               font_size=32,\n                               font_weight='bold')\n        return fig", "response": "r Creates a graph of the dependency graph of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef regenerate_models(self, propnames=None, exclude=[], deep=False):\n        # If empty list of propnames was given, do nothing and return\n        if type(propnames) is list and len(propnames) == 0:\n            return\n        if type(propnames) is str:  # Convert string to list if necessary\n            propnames = [propnames]\n        if propnames is None:  # If no props given, then regenerate them all\n            propnames = self.models.dependency_list()\n            # If some props are to be excluded, remove them from list\n            if len(exclude) > 0:\n                propnames = [i for i in propnames if i not in exclude]\n        # Re-order given propnames according to dependency tree\n        self_models = self.models.dependency_list()\n        propnames = [i for i in self_models if i in propnames]\n\n        if deep:\n            other_models = None  # Will trigger regen of ALL models\n        else:\n            # Make list of given propnames that are not in self\n            other_models = list(set(propnames).difference(set(self_models)))\n        # The following has some redundant lines, but is easier to understand\n        if self._isa('phase'):\n            # Start be regenerating models on self\n            for item in propnames:\n                self._regen(item)\n            # Then regen models on associated objects, if any in other_models\n            for phys in self.project.find_physics(phase=self):\n                phys.regenerate_models(propnames=other_models, deep=False)\n        elif self._isa('network'):  # Repeat for other object types\n            for item in propnames:\n                self._regen(item)\n            for geom in self.project.geometries().values():\n                geom.regenerate_models(propnames=other_models, deep=False)\n        else:\n            for item in propnames:\n                self._regen(item)", "response": "r Re - runs the specified model or models."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_IC(self, values):\n        self[self.settings['quantity']] = values\n        converted_array = self[self.settings['quantity']].astype('float64')\n        self[self.settings['quantity']] = converted_array", "response": "r Set the initial conditions for the current simulation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self, t=None):\n        logger.info('\u2015'*80)\n        logger.info('Running TransientTransport')\n        # If solver used in steady mode, no need to add ICs\n        if (self.settings['t_scheme'] == 'steady'):\n            self[self.settings['quantity']] = 0.0\n        # If ICs are not defined, assume zero\n        try:\n            self[self.settings['quantity']]\n        except KeyError:\n            self.set_IC(0)\n        # Save A matrix of the steady sys of eqs (WITHOUT BCs applied)\n        self._A_steady = (self.A).copy()\n        # Initialize A and b with BCs applied\n        self._t_update_A()\n        self._t_update_b()\n        self._apply_BCs()\n        self._A_t = (self._A).copy()\n        self._b_t = (self._b).copy()\n        if t is None:\n            t = self.settings['t_initial']\n        # Create S1 & S1 for 1st Picard's iteration\n        self._update_physics()\n\n        self._run_transient(t=t)", "response": "r Runs the TransientTransport."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef exponential(target, X, A1='', A2='', A3='', A4='', A5='', A6=''):\n    A = _parse_args(target=target, key=A1, default=1.0)\n    B = _parse_args(target=target, key=A2, default=1.0)\n    C = _parse_args(target=target, key=A3, default=1.0)\n    D = _parse_args(target=target, key=A4, default=1.0)\n    E = _parse_args(target=target, key=A5, default=0.0)\n    F = _parse_args(target=target, key=A6, default=0.0)\n    X = target[X]\n\n    r = A * B ** (C * X ** D + E) + F\n    S1 = A * C * D * X ** (D - 1) * _sp.log(B) * B ** (C * X ** D + E)\n    S2 = A * B ** (C * X ** D + E) * (1 - C * D * _sp.log(B) * X ** D) + F\n    values = {'S1': S1, 'S2': S2, 'rate': r}\n    return values", "response": "r Calculates the exponential rate of the source term function at the given value of X."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _build_func(eq, **args):\n    r'''\n    Take a symbolic equation and return the lambdified version plus the\n    linearization of form S1 * x + S2\n    '''\n    eq_prime = eq.diff(args['x'])\n    s1 = eq_prime\n    s2 = eq - eq_prime*args['x']\n    EQ = _syp.lambdify(args.values(), expr=eq, modules='numpy')\n    S1 = _syp.lambdify(args.values(), expr=s1, modules='numpy')\n    S2 = _syp.lambdify(args.values(), expr=s2, modules='numpy')\n    return EQ, S1, S2", "response": "r Builds a function that takes a symbolic equation and returns the lambdified version plus the\n    linearization of form S1 * x + S2"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef linear_sym(target, X, A1='', A2=''):\n    A = _parse_args(target=target, key=A1, default=1.0)\n    B = _parse_args(target=target, key=A2, default=0.0)\n    X = target[X]\n    # Symbols used in symbolic function\n    a, b, x = _syp.symbols('a,b,x')\n    # Equation\n    y = a*x + b\n    # Callable functions\n    r, s1, s2 = _build_func(eq=y, a=a, b=b,  x=x)\n    # Values\n    r_val = r(A, B, X)\n    s1_val = s1(A, B, X)\n    s2_val = s2(A, B, X)\n    values = {'S1': s1_val, 'S2': s2_val, 'rate': r_val}\n    return values", "response": "r Linearized symbolic function for a single source term term."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exponential_sym(target, X, A1='', A2='', A3='', A4='', A5='', A6=''):\n    A = _parse_args(target=target, key=A1, default=1.0)\n    B = _parse_args(target=target, key=A2, default=1.0)\n    C = _parse_args(target=target, key=A3, default=1.0)\n    D = _parse_args(target=target, key=A4, default=1.0)\n    E = _parse_args(target=target, key=A5, default=0.0)\n    F = _parse_args(target=target, key=A6, default=0.0)\n    X = target[X]\n    # Symbols used in symbolic function\n    a, b, c, d, e, f, x = _syp.symbols('a,b,c,d,e,f,x')\n    # Equation\n    y = a*b**(c*x**d + e) + f\n    # Callable functions\n    r, s1, s2 = _build_func(eq=y, a=a, b=b, c=c, d=d, e=e, f=f, x=x)\n    # Values\n    r_val = r(A, B, C, D, E, F, X)\n    s1_val = s1(A, B, C, D, E, F, X)\n    s2_val = s2(A, B, C, D, E, F, X)\n    values = {'S1': s1_val, 'S2': s2_val, 'rate': r_val}\n    return values", "response": "r Calculates the exponential symmetrized exponential function for the given target object and current value of X."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef general_symbolic(target, eqn=None, arg_map=None):\n    r'''\n    A general function to interpret a sympy equation and evaluate the linear\n    components of the source term.\n\n    Parameters\n    ----------\n    target : OpenPNM object\n        The OpenPNM object where the result will be applied.\n\n    eqn : sympy symbolic expression for the source terms\n        e.g. y = a*x**b + c\n\n    arg_map : Dict mapping the symbols in the expression to OpenPNM data\n        on the target. Must contain 'x' which is the independent variable.\n        e.g. arg_map={'a':'pore.a', 'b':'pore.b', 'c':'pore.c', 'x':'pore.x'}\n\n    Example\n    ----------\n    >>> import openpnm as op\n    >>> from openpnm.models.physics import generic_source_term as gst\n    >>> import scipy as sp\n    >>> import sympy as _syp\n    >>> pn = op.network.Cubic(shape=[5, 5, 5], spacing=0.0001)\n    >>> water = op.phases.Water(network=pn)\n    >>> water['pore.a'] = 1\n    >>> water['pore.b'] = 2\n    >>> water['pore.c'] = 3\n    >>> water['pore.x'] = sp.random.random(water.Np)\n    >>> a, b, c, x = _syp.symbols('a,b,c,x')\n    >>> y = a*x**b + c\n    >>> arg_map = {'a':'pore.a', 'b':'pore.b', 'c':'pore.c', 'x':'pore.x'}\n    >>> water.add_model(propname='pore.general',\n    ...                 model=gst.general_symbolic,\n    ...                 eqn=y, arg_map=arg_map,\n    ...                 regen_mode='normal')\n    >>> assert 'pore.general.rate' in water.props()\n    >>> assert 'pore.general.S1' in water.props()\n    >>> assert 'pore.general.S1' in water.props()\n    '''\n    # First make sure all the symbols have been allocated dict items\n    for arg in _syp.postorder_traversal(eqn):\n        if _syp.srepr(arg)[:6] == 'Symbol':\n            key = _syp.srepr(arg)[7:].strip('(').strip(')').strip(\"'\")\n            if key not in arg_map.keys():\n                raise Exception('argument mapping incomplete, missing '+key)\n    if 'x' not in arg_map.keys():\n        raise Exception('argument mapping must contain \"x\" for the ' +\n                        'independent variable')\n    # Get the data\n    data = {}\n    args = {}\n    for key in arg_map.keys():\n        data[key] = target[arg_map[key]]\n        # Callable functions\n        args[key] = _syp.symbols(key)\n    r, s1, s2 = _build_func(eqn, **args)\n    r_val = r(*data.values())\n    s1_val = s1(*data.values())\n    s2_val = s2(*data.values())\n    values = {'S1': s1_val, 'S2': s2_val, 'rate': r_val}\n    return values", "response": "r General symbolic representation of the source term."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef methods_to_table(obj):\n    parent = obj.__class__.__mro__[1]\n    temp = inspect.getmembers(parent, predicate=inspect.isroutine)\n    parent_funcs = [i[0] for i in temp if not i[0].startswith('_')]\n\n    temp = inspect.getmembers(obj.__class__, predicate=inspect.isroutine)\n    obj_funcs = [i[0] for i in temp if not i[0].startswith('_')]\n    funcs = set(obj_funcs).difference(set(parent_funcs))\n\n    row = '+' + '-'*22 + '+' + '-'*49 + '+'\n    fmt = '{0:1s} {1:20s} {2:1s} {3:47s} {4:1s}'\n    lines = []\n    lines.append(row)\n    lines.append(fmt.format('|', 'Method', '|', 'Description', '|'))\n    lines.append(row.replace('-', '='))\n    for i, item in enumerate(funcs):\n        try:\n            s = getattr(obj, item).__doc__.strip()\n            end = s.find('\\n')\n            if end > 47:\n                s = s[:44] + '...'\n            lines.append(fmt.format('|', item, '|', s[:end], '|'))\n            lines.append(row)\n        except AttributeError:\n            pass\n    return '\\n'.join(lines)", "response": "r Converts a class method to a table of methods."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calc_effective_diffusivity(self, inlets=None, outlets=None,\n                                   domain_area=None, domain_length=None):\n        r\"\"\"\n        This calculates the effective diffusivity in this linear transport\n        algorithm.\n\n        Parameters\n        ----------\n        inlets : array_like\n            The pores where the inlet composition boundary conditions were\n            applied.  If not given an attempt is made to infer them from the\n            algorithm.\n\n        outlets : array_like\n            The pores where the outlet composition boundary conditions were\n            applied.  If not given an attempt is made to infer them from the\n            algorithm.\n\n        domain_area : scalar, optional\n            The area of the inlet (and outlet) boundary faces.  If not given\n            then an attempt is made to estimate it, but it is usually\n            underestimated.\n\n        domain_length : scalar, optional\n            The length of the domain between the inlet and outlet boundary\n            faces.  If not given then an attempt is made to estimate it, but it\n            is usually underestimated.\n\n        Notes\n        -----\n        The area and length of the domain are found using the bounding box\n        around the inlet and outlet pores which do not necessarily lie on the\n        edge of the domain, resulting in underestimation of sizes.\n        \"\"\"\n        return self._calc_eff_prop(inlets=inlets, outlets=outlets,\n                                   domain_area=domain_area,\n                                   domain_length=domain_length)", "response": "r This calculates the effective diffusivity of the inlet and outlet pores."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_locations(self, pores=[], throats=[]):\n        boss = self.project.find_full_domain(self)\n        pores = boss._parse_indices(pores)\n        throats = boss._parse_indices(throats)\n        if len(pores) > 0:\n            self._set_locations(element='pore', indices=pores, mode='add')\n        if len(throats) > 0:\n            self._set_locations(element='throat', indices=throats, mode='add')", "response": "r Adds associations between an object and its boss object at the given pore and or throat locations."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _set_locations(self, element, indices, mode, complete=False):\n        boss = self.project.find_full_domain(self)\n        element = self._parse_element(element=element, single=True)\n\n        # Make sure label array exists in boss\n        if (element+'.'+self.name) not in boss.keys():\n            boss[element+'.'+self.name] = False\n\n        # Check to ensure indices aren't already assigned\n        if mode == 'add':\n            if self._isa('geometry'):\n                objs = self.project.geometries().keys()\n            else:\n                objs = self.project.physics().keys()\n            for name in objs:\n                if element+'.'+name in boss.keys():\n                    if np.any(boss[element+'.'+name][indices]):\n                        raise Exception('Given indices are already assigned ' +\n                                        'to ' + name)\n\n        # Find mask of existing locations (network indexing)\n        mask = boss[element+'.'+self.name]\n        # Update mask with new locations (either add or remove)\n        if mode == 'add':\n            mask = mask + boss._tomask(indices=indices, element=element)\n        elif mode == 'drop':\n            mask = mask * (~boss._tomask(indices=indices, element=element))\n        # Change size of all arrays on self\n        for item in self.keys(element=element, mode='all'):\n            self.update({item: boss[item][mask]})\n        # Update label array in network\n        boss[element+'.'+self.name] = mask\n        # Remove label from boss if ALL locations are removed\n        if mode == 'drop':\n            if ~np.any(boss[element+'.'+self.name]):\n                if complete:\n                    del boss[element+'.'+self.name]\n                else:\n                    boss[element+'.'+self.name] = False", "response": "r This private method is called by _set_locations and _remove_locations as needed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ctc(target, pore_diameter='pore.diameter'):\n    _np.warnings.filterwarnings('ignore', category=RuntimeWarning)\n\n    network = target.project.network\n    throats = network.map_throats(throats=target.Ts, origin=target)\n    cn = network['throat.conns'][throats]\n    C1 = network['pore.coords'][cn[:, 0]]\n    C2 = network['pore.coords'][cn[:, 1]]\n    value = _sqrt(((C1 - C2)**2).sum(axis=1))\n\n    _np.warnings.filterwarnings('default', category=RuntimeWarning)\n\n    return value", "response": "r Returns the throat length assuming point - like pores and throat centroids are colinear."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef piecewise(target, throat_endpoints='throat.endpoints',\n              throat_centroid='throat.centroid'):\n    r\"\"\"\n    Calculate throat length from end points and optionally a centroid\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object which this model is associated with. This controls the\n        length of the calculated array, and also provides access to other\n        necessary properties.\n\n    throat_endpoints : string\n        Dictionary key of the throat endpoint values.\n\n    throat_centroid : string\n        Dictionary key of the throat centroid values, optional.\n\n    Returns\n    -------\n    Lt : ndarray\n        Array containing throat lengths for the given geometry.\n\n    Notes\n    -----\n    (1) By default, the model assumes that the centroids of pores and the\n    connecting throat in each conduit are colinear.\n\n    (2) If `throat_centroid` is passed, the model accounts for the extra\n    length. This could be useful for Voronoi or extracted networks.\n\n    \"\"\"\n    _np.warnings.filterwarnings('ignore', category=RuntimeWarning)\n    network = target.project.network\n    throats = network.map_throats(throats=target.Ts, origin=target)\n    # Get throat endpoints\n    EP1 = network[throat_endpoints + '.head'][throats]\n    EP2 = network[throat_endpoints + '.tail'][throats]\n    # Calculate throat length\n    Lt = _sqrt(((EP1 - EP2)**2).sum(axis=1))\n    # Handle the case where pores & throat centroids are not colinear\n    try:\n        Ct = network[throat_centroid][throats]\n        Lt = _sqrt(((Ct - EP1)**2).sum(axis=1)) + \\\n            _sqrt(((Ct - EP2)**2).sum(axis=1))\n    except KeyError:\n        pass\n\n    _np.warnings.filterwarnings('default', category=RuntimeWarning)\n\n    return Lt", "response": "r This function calculates the length of a piecewise throat in a given target object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef standard(target, mol_weight='pore.molecular_weight',\n             molar_density='pore.molar_density'):\n    r\"\"\"\n    Calculates the mass density from the molecular weight and molar density\n\n    Parameters\n    ----------\n    mol_weight : string\n        The dictionary key containing the molecular weight values\n\n    molar_density : string\n        The dictionary key containing the molar density values\n\n    \"\"\"\n    MW = target[mol_weight]\n    rho = target[molar_density]\n    value = rho*MW\n    return value", "response": "r Calculates the mass density from the molecular weight and molar density"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_dict(cls, network=None, phases=[], element=['pore', 'throat'],\n                interleave=True, flatten=True, categorize_by=[]):\n        r\"\"\"\n        Returns a single dictionary object containing data from the given\n        OpenPNM objects, with the keys organized differently depending on\n        optional arguments.\n\n        Parameters\n        ----------\n        network : OpenPNM Network Object (optional)\n            The network containing the desired data\n\n        phases : list of OpenPNM Phase Objects (optional, default is none)\n            A list of phase objects whose data are to be included\n\n        element : string or list of strings\n            An indication of whether 'pore' and/or 'throat' data are desired.\n            The default is both.\n\n        interleave : boolean (default is ``True``)\n            When ``True`` (default) the data from all Geometry objects (and\n            Physics objects if ``phases`` are given) is interleaved into\n            a single array and stored as a network property (or Phase\n            property for Physics data). When ``False``, the data for each\n            object are stored under their own dictionary key, the structuring\n            of which depends on the value of the ``flatten`` argument.\n\n        flatten : boolean (default is ``True``)\n            When ``True``, all objects are accessible from the top level\n            of the dictionary.  When ``False`` objects are nested under their\n            parent object.  If ``interleave`` is ``True`` this argument is\n            ignored.\n\n        categorize_by : string or list of strings\n            Indicates how the dictionaries should be organized.  The list can\n            contain any, all or none of the following strings:\n\n            **'object'** : If specified the dictionary keys will be stored\n            under a general level corresponding to their type (e.g.\n            'network/net_01/pore.all'). If  ``interleave`` is ``True`` then\n            only the only categories are *network* and *phase*, since\n            *geometry* and *physics* data get stored under their respective\n            *network* and *phase*.\n\n            **'data'** : If specified the data arrays are additionally\n            categorized by ``label`` and ``property`` to separate *boolean*\n            from *numeric* data.\n\n            **'element'** : If specified the data arrays are\n            additionally categorized by ``pore`` and ``throat``, meaning\n            that the propnames are no longer prepended by a 'pore.' or\n            'throat.'\n\n        Returns\n        -------\n        A dictionary with the data stored in a hierarchical data structure, the\n        actual format of which depends on the arguments to the function.\n\n        Notes\n        -----\n        There is a handy package called *flatdict* that can be used to\n        access this dictionary using a single key such that:\n\n        ``d[level_1][level_2] == d[level_1/level_2]``\n\n        Importantly, converting to a *flatdict* allows it be converted to an\n        *HDF5* file directly, since the hierarchy is dictated by the placement\n        of '/' characters.\n        \"\"\"\n        project, network, phases = cls._parse_args(network=network,\n                                                   phases=phases)\n        delim = ' | '\n        d = NestedDict(delimiter=delim)\n\n        def build_path(obj, key):\n            propname = delim + key\n            prefix = 'root'\n            datatype = ''\n            arr = obj[key]\n            if 'object' in categorize_by:\n                prefix = obj._isa()\n            if 'element' in categorize_by:\n                propname = delim + key.replace('.', delim)\n            if 'data' in categorize_by:\n                if arr.dtype == bool:\n                    datatype = delim + 'labels'\n                else:\n                    datatype = delim + 'properties'\n            path = prefix + delim + obj.name + datatype + propname\n            return path\n\n        for net in network:\n            for key in net.keys(element=element, mode='all'):\n                path = build_path(obj=net, key=key)\n                d[path] = net[key]\n\n            for geo in project.geometries().values():\n                for key in geo.keys(element=element, mode='all'):\n                    if interleave:\n                        path = build_path(obj=net, key=key)\n                        d[path] = net[key]\n                    else:\n                        path = build_path(obj=geo, key=key)\n                        if flatten:\n                            d[path] = geo[key]\n                        elif 'object' in categorize_by:\n                            path = path.split(delim)\n                            path.insert(0, 'network')\n                            path.insert(1, net.name)\n                            path = delim.join(path)\n                        else:\n                            path = path.split(delim)\n                            path.insert(1, net.name)\n                            path = delim.join(path)\n                        d[path] = geo[key]\n\n        for phase in phases:\n            for key in phase.keys(element=element, mode='all'):\n                path = build_path(obj=phase, key=key)\n                d[path] = phase[key]\n\n            for phys in project.find_physics(phase=phase):\n                if phys:\n                    for key in phys.keys(element=element, mode='all'):\n                        if interleave:\n                            path = build_path(obj=phase, key=key)\n                            d[path] = phase[key]\n                        else:\n                            path = build_path(obj=phys, key=key)\n                            if flatten:\n                                d[path] = phys[key]\n                            elif 'object' in categorize_by:\n                                path = path.split(delim)\n                                path.insert(0, 'phase')\n                                path.insert(1, phase.name)\n                                path = delim.join(path)\n                            else:\n                                path = path.split(delim)\n                                path.insert(1, phase.name)\n                                path = delim.join(path)\n                            d[path] = phys[key]\n\n        if 'root' in d.keys():\n            d = d['root']\n        if 'project' in categorize_by:\n            new_d = NestedDict()\n            new_d[project.name] = d\n            d = new_d\n        return d", "response": "r Returns a dictionary containing the data for the given OpenPNM object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save(cls, dct, filename):\n        fname = cls._parse_filename(filename=filename, ext='dct')\n        dct = sanitize_dict(dct)\n        with open(fname, 'wb') as f:\n            pickle.dump(dct, f)", "response": "r Saves data from the given dictionary into the specified file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load(cls, filename):\n        fname = cls._parse_filename(filename)\n        with open(fname, 'rb') as f:\n            dct = pickle.load(f)\n        return dct", "response": "r Loads data from the specified file into a Python dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reset(self):\n        self['pore.invasion_pressure'] = np.inf\n        self['throat.invasion_pressure'] = np.inf\n        self['pore.invasion_sequence'] = -1\n        self['throat.invasion_sequence'] = -1\n        self['pore.inlets'] = False\n        self['pore.outlets'] = False\n        self['pore.residual'] = False\n        self['throat.residual'] = False", "response": "r Resets the internal state of the internal state of the object to its original state."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_inlets(self, pores=[], overwrite=False):\n        Ps = self._parse_indices(pores)\n        if np.sum(self['pore.outlets'][Ps]) > 0:\n            raise Exception('Some inlets are already defined as outlets')\n        if overwrite:\n            self['pore.inlets'] = False\n        self['pore.inlets'][Ps] = True\n        self['pore.invasion_pressure'][Ps] = 0\n        self['pore.invasion_sequence'][Ps] = 0", "response": "r Set the locations from which the invader enters the network."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_residual(self, pores=[], throats=[], overwrite=False):\n        Ps = self._parse_indices(pores)\n        if overwrite:\n            self['pore.residual'] = False\n        self['pore.residual'][Ps] = True\n        Ts = self._parse_indices(throats)\n        if overwrite:\n            self['throat.residual'] = False\n        self['throat.residual'][Ts] = True", "response": "r Set the locations of any residual invader."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_intrusion_data(self, Pc=None):\n        net = self.project.network\n        if Pc is None:\n            # Infer list of applied capillary pressures\n            points = np.unique(self['throat.invasion_pressure'])\n            # Add a low pressure point to the list to improve graph\n            points = np.concatenate(([0], points))\n            if points[-1] == np.inf:  # Remove infinity from points if present\n                points = points[:-1]\n        else:\n            points = np.array(Pc)\n        # Get pore and throat volumes\n        Pvol = net[self.settings['pore_volume']]\n        Tvol = net[self.settings['throat_volume']]\n        Total_vol = np.sum(Pvol) + np.sum(Tvol)\n        # Find cumulative filled volume at each applied capillary pressure\n        Vnwp_t = []\n        Vnwp_p = []\n        Vnwp_all = []\n        for p in points:\n            # Calculate filled pore volumes\n            p_inv, t_inv = self.results(p).values()\n            Vp = np.sum(Pvol*p_inv)\n            Vt = np.sum(Tvol*t_inv)\n            Vnwp_p.append(Vp)\n            Vnwp_t.append(Vt)\n            Vnwp_all.append(Vp + Vt)\n        # Convert volumes to saturations by normalizing with total pore volume\n        Snwp_all = [V/Total_vol for V in Vnwp_all]\n        pc_curve = namedtuple('pc_curve', ('Pcap', 'Snwp'))\n        data = pc_curve(points, Snwp_all)\n        return data", "response": "r Returns the numerical values of the calculated intrusion curve and the corresponding array of values of the calculated capillary pressures and invading phase saturation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot_intrusion_curve(self, fig=None):\n        # Begin creating nicely formatted plot\n        x, y = self.get_intrusion_data()\n        if fig is None:\n            fig = plt.figure()\n        plt.semilogx(x, y, 'ko-')\n        plt.ylabel('Invading Phase Saturation')\n        plt.xlabel('Capillary Pressure')\n        plt.grid(True)\n        return fig", "response": "r Plot the percolation curve as the invader volume or number fraction vs\n        the applied capillary pressure."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef results(self, Pc):\n        Psatn = self['pore.invasion_pressure'] <= Pc\n        Tsatn = self['throat.invasion_pressure'] <= Pc\n        inv_phase = {}\n        inv_phase['pore.occupancy'] = sp.array(Psatn, dtype=float)\n        inv_phase['throat.occupancy'] = sp.array(Tsatn, dtype=float)\n        return inv_phase", "response": "r This method returns a dictionary containing the occupancy status of each pore and throat for the given capillary pressure."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setup(self, phase=None, quantity='', conductance='', r_tolerance=None,\n              max_iter=None, relaxation_source=None,\n              relaxation_quantity=None, **kwargs):\n        r\"\"\"\n        This method takes several arguments that are essential to running the\n        algorithm and adds them to the settings\n\n        Parameters\n        ----------\n        phase : OpenPNM Phase object\n            The phase on which the algorithm is to be run. If no value is\n            given, the existing value is kept.\n\n        quantity : string\n            The name of the physical quantity to be calcualted such as\n            ``'pore.xxx'``.\n\n        conductance : string\n            The name of the pore-scale transport conductance values. These\n            are typically calculated by a model attached to a *Physics* object\n            associated with the given *Phase*. Example; ``'throat.yyy'``.\n\n        r_tolerance : scalar\n            Tolerance to achieve. The solver returns a solution when 'residual'\n            falls below 'r_tolerance'. The default value is 0.001.\n\n        max_iter : scalar\n            The maximum number of iterations the solver can perform to find\n            a solution. The default value is 5000.\n\n        relaxation_source : scalar, between 0 and 1\n            A relaxation factor to control under-relaxation of the source term.\n            Factor approaching 0 : improved stability but slow simulation.\n            Factor approaching 1 : fast simulation but may be unstable.\n            Default value is 1 (no under-relaxation).\n\n        relaxation_quantity :  scalar, between 0 and 1\n            A relaxation factor to control under-relaxation for the quantity\n            solving for.\n            Factor approaching 0 : improved stability but slow simulation.\n            Factor approaching 1 : fast simulation but may be unstable.\n            Default value is 1 (no under-relaxation).\n\n        Notes\n        -----\n        Under-relaxation is a technique used for improving stability of a\n        computation, particularly in the presence of highly non-linear terms.\n        Under-relaxation used here limits the change in a variable from one\n        iteration to the next. An optimum choice of the relaxation factor is\n        one that is small enough to ensure stable simulation and large enough\n        to speed up the computation.\n        \"\"\"\n        if phase:\n            self.settings['phase'] = phase.name\n        if quantity:\n            self.settings['quantity'] = quantity\n        if conductance:\n            self.settings['conductance'] = conductance\n        if r_tolerance:\n            self.settings['r_tolerance'] = r_tolerance\n        if max_iter:\n            self.settings['max_iter'] = max_iter\n        if relaxation_source:\n            self.settings['relaxation_source'] = relaxation_source\n        if relaxation_quantity:\n            self.settings['relaxation_quantity'] = relaxation_quantity\n        super().setup(**kwargs)", "response": "r This method is essential to running the OpenPNM object store and returns a new value for the object store."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self, x=None):\n        logger.info('Running ReactiveTransport')\n        # Create S1 & S1 for 1st Picard's iteration\n        if x is None:\n            x = np.zeros(shape=[self.Np, ], dtype=float)\n        self[self.settings['quantity']] = x\n        self._update_physics()\n\n        x = self._run_reactive(x=x)\n        self[self.settings['quantity']] = x", "response": "r Runs the ReactiveTransport on the specified variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sphere(target, pore_diameter='pore.diameter'):\n    diams = target[pore_diameter]\n    value = _pi/6*diams**3\n    return value", "response": "r Calculates the pore volume from diameter assuming a spherical pore body."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(cls, path, voxel_size=1, project=None):\n\n        net = {}\n        path = Path(path)\n        path = path.resolve()\n        for file in os.listdir(path):\n            if file.endswith(\".np2th\"):\n                np2th_file = os.path.join(path, file)\n            elif file.endswith(\".th2np\"):\n                th2np_file = os.path.join(path, file)\n\n        with open(np2th_file, mode='rb') as f:\n            [Np, Nt] = sp.fromfile(file=f, count=2, dtype='u4')\n            net['pore.boundary_type'] = sp.ndarray([Np, ], int)\n            net['throat.conns'] = sp.ones([Nt, 2], int)*(-1)\n            net['pore.coordination'] = sp.ndarray([Np, ], int)\n            net['pore.ID_number'] = sp.ndarray([Np, ], int)\n            for i in range(0, Np):\n                ID = sp.fromfile(file=f, count=1, dtype='u4')\n                net['pore.ID_number'][i] = ID\n                net['pore.boundary_type'][i] = sp.fromfile(file=f, count=1,\n                                                           dtype='u1')\n                z = sp.fromfile(file=f, count=1, dtype='u4')[0]\n                net['pore.coordination'][i] = z\n                att_pores = sp.fromfile(file=f, count=z, dtype='u4')\n                att_throats = sp.fromfile(file=f, count=z, dtype='u4')\n                for j in range(0, len(att_throats)):\n                    t = att_throats[j] - 1\n                    p = att_pores[j] - 1\n                    net['throat.conns'][t] = [i, p]\n            net['throat.conns'] = sp.sort(net['throat.conns'], axis=1)\n            net['pore.volume'] = sp.fromfile(file=f, count=Np, dtype='u4')\n            nx = sp.fromfile(file=f, count=1, dtype='u4')\n            nxy = sp.fromfile(file=f, count=1, dtype='u4')\n            pos = sp.fromfile(file=f, count=Np, dtype='u4')\n            ny = nxy/nx\n            ni = sp.mod(pos, nx)\n            nj = sp.mod(sp.floor(pos/nx), ny)\n            nk = sp.floor(sp.floor(pos/nx)/ny)\n            net['pore.coords'] = sp.array([ni, nj, nk]).T\n\n        with open(th2np_file, mode='rb') as f:\n            Nt = sp.fromfile(file=f, count=1, dtype='u4')[0]\n            net['throat.area'] = sp.ones([Nt, ], dtype=int)*(-1)\n            for i in range(0, Nt):\n                ID = sp.fromfile(file=f, count=1, dtype='u4')\n                net['throat.area'][i] = sp.fromfile(file=f, count=1,\n                                                    dtype='f4')\n                numvox = sp.fromfile(file=f, count=1, dtype='u4')\n                att_pores = sp.fromfile(file=f, count=2, dtype='u4')\n            nx = sp.fromfile(file=f, count=1, dtype='u4')\n            nxy = sp.fromfile(file=f, count=1, dtype='u4')\n            pos = sp.fromfile(file=f, count=Nt, dtype='u4')\n            ny = nxy/nx\n            ni = sp.mod(pos, nx)\n            nj = sp.mod(sp.floor(pos/nx), ny)\n            nk = sp.floor(sp.floor(pos/nx)/ny)\n            net['throat.coords'] = sp.array([ni, nj, nk]).T\n            net['pore.internal'] = net['pore.boundary_type'] == 0\n\n        # Convert voxel area and volume to actual dimensions\n        net['throat.area'] = (voxel_size**2)*net['throat.area']\n        net['pore.volume'] = (voxel_size**3)*net['pore.volume']\n\n        if project is None:\n            project = Project(name=path)\n        network = GenericNetwork(project=project)\n        network = cls._update_network(network=network, net=net)\n\n        # Trim headless throats before returning\n        ind = sp.where(network['throat.conns'][:, 0] == -1)[0]\n        trim(network=network, throats=ind)\n\n        return project", "response": "r Loads data from a 3DMA - Rock extracted network."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef water(target, temperature='pore.temperature', salinity='pore.salinity'):\n    T = target[temperature]\n    if salinity in target.keys():\n        S = target[salinity]\n    else:\n        S = 0\n    a1 = -5.8002206E+03\n    a2 = 1.3914993E+00\n    a3 = -4.8640239E-02\n    a4 = 4.1764768E-05\n    a5 = -1.4452093E-08\n    a6 = 6.5459673E+00\n    Pv_w = np.exp((a1/T) + a2 + a3*T + a4*T**2 + a5*T**3 + a6*np.log(T))\n    Pv_sw = Pv_w/(1+0.57357*(S/(1000-S)))\n    value = Pv_sw\n    return value", "response": "r Calculates the vapor pressure of pure water or seawater at a given temperature and salinity."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cylinder(target, throat_length='throat.length',\n             throat_diameter='throat.diameter'):\n    r\"\"\"\n    Calculate throat volume assuing a cylindrical shape\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object which this model is associated with. This controls the\n        length of the calculated array, and also provides access to other\n        necessary properties.\n\n    throat_length and throat_diameter : strings\n        The dictionary keys containing the arrays with the throat diameter and\n        length values.\n\n    Notes\n    -----\n    At present this models does NOT account for the volume reprsented by the\n    intersection of the throat with a spherical pore body.\n    \"\"\"\n    leng = target[throat_length]\n    diam = target[throat_diameter]\n    value = _sp.pi/4*leng*diam**2\n    return value", "response": "r Calculates the throat volume assuing a cylindrical shape."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cuboid(target, throat_length='throat.length',\n           throat_diameter='throat.diameter'):\n    r\"\"\"\n    Calculate throat volume assuing a square cross-section\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object which this model is associated with. This controls the\n        length of the calculated array, and also provides access to other\n        necessary properties.\n\n    throat_length and throat_diameter : strings\n        The dictionary keys containing the arrays with the throat diameter and\n        length values.\n\n    Notes\n    -----\n    At present this models does NOT account for the volume reprsented by the\n    intersection of the throat with a spherical pore body.\n    \"\"\"\n    leng = target[throat_length]\n    diam = target[throat_diameter]\n    value = leng*diam**2\n    return value", "response": "r Calculates the throat volume assuing a square cross - section of the object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extrusion(target, throat_length='throat.length',\n              throat_area='throat.area'):\n    r\"\"\"\n    Calculate throat volume from the throat area and the throat length. This\n    method is useful for abnormal shaped throats.\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object which this model is associated with. This controls the\n        length of the calculated array, and also provides access to other\n        necessary properties.\n\n    throat_length and throat_area : strings\n        The dictionary keys containing the arrays with the throat area and\n        length values.\n\n    Notes\n    -----\n    At present this models does NOT account for the volume reprsented by the\n    intersection of the throat with a spherical pore body.\n        \"\"\"\n    leng = target[throat_length]\n    area = target[throat_area]\n    value = leng*area\n    return value", "response": "r This method calculates the throat volume from the area and the length of the throat and the area of the throat."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of items in a sequence", "response": "def get_objects_in_sequence(brain_or_object, ctype, cref):\n    \"\"\"Return a list of items\n    \"\"\"\n    obj = api.get_object(brain_or_object)\n    if ctype == \"backreference\":\n        return get_backreferences(obj, cref)\n    if ctype == \"contained\":\n        return get_contained_items(obj, cref)\n    raise ValueError(\"Reference value is mandatory for sequence type counter\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_backreferences(obj, relationship):\n    refs = get_backuidreferences(obj, relationship)\n\n    # TODO remove after all ReferenceField get ported to UIDReferenceField\n    # At this moment, there are still some content types that are using the\n    # ReferenceField, so we need to fallback to traditional getBackReferences\n    # for these cases.\n    if not refs:\n        refs = obj.getBackReferences(relationship)\n\n    return refs", "response": "Returns the backreferences of the object with the given relationship"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the type id for the passed in", "response": "def get_type_id(context, **kw):\n    \"\"\"Returns the type id for the context passed in\n    \"\"\"\n    portal_type = kw.get(\"portal_type\", None)\n    if portal_type:\n        return portal_type\n\n    # Override by provided marker interface\n    if IAnalysisRequestPartition.providedBy(context):\n        return \"AnalysisRequestPartition\"\n    elif IAnalysisRequestRetest.providedBy(context):\n        return \"AnalysisRequestRetest\"\n    elif IAnalysisRequestSecondary.providedBy(context):\n        return \"AnalysisRequestSecondary\"\n\n    return api.get_portal_type(context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the suffix of the ID e. g. - R01 or - P05.", "response": "def get_suffix(id, regex=\"-[A-Z]{1}[0-9]{1,2}$\"):\n    \"\"\"Get the suffix of the ID, e.g. '-R01' or '-P05'\n\n    The current regex determines a pattern of a single uppercase character with\n    at most 2 numbers following at the end of the ID as the suffix.\n    \"\"\"\n    parts = re.findall(regex, id)\n    if not parts:\n        return \"\"\n    return parts[0]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef strip_suffix(id):\n    suffix = get_suffix(id)\n    if not suffix:\n        return id\n    return re.split(suffix, id)[0]", "response": "Strip any suffix from the ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_partition_count(context, default=0):\n    if not is_ar(context):\n        return default\n\n    parent = context.getParentAnalysisRequest()\n\n    if not parent:\n        return default\n\n    return len(parent.getDescendants())", "response": "Returns the number of partitions of this AR"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the number of secondary ARs of this AR", "response": "def get_secondary_count(context, default=0):\n    \"\"\"Returns the number of secondary ARs of this AR\n    \"\"\"\n    if not is_ar(context):\n        return default\n\n    primary = context.getPrimaryAnalysisRequest()\n\n    if not primary:\n        return default\n\n    return len(primary.getSecondaryAnalysisRequests())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_config(context, **kw):\n    # get the ID formatting config\n    config_map = api.get_bika_setup().getIDFormatting()\n\n    # allow portal_type override\n    portal_type = get_type_id(context, **kw)\n\n    # check if we have a config for the given portal_type\n    for config in config_map:\n        if config['portal_type'].lower() == portal_type.lower():\n            return config\n\n    # return a default config\n    default_config = {\n        'form': '%s-{seq}' % portal_type.lower(),\n        'sequence_type': 'generated',\n        'prefix': '%s' % portal_type.lower(),\n    }\n    return default_config", "response": "Fetch the config dict from the Bika Setup for the given portal_type\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprepare a dictionary of key - value pairs usable for ID formatting", "response": "def get_variables(context, **kw):\n    \"\"\"Prepares a dictionary of key->value pairs usable for ID formatting\n    \"\"\"\n    # allow portal_type override\n    portal_type = get_type_id(context, **kw)\n\n    # The variables map hold the values that might get into the constructed id\n    variables = {\n        \"context\": context,\n        \"id\": api.get_id(context),\n        \"portal_type\": portal_type,\n        \"year\": get_current_year(),\n        \"parent\": api.get_parent(context),\n        \"seq\": 0,\n        \"alpha\": Alphanumber(0),\n    }\n\n    # Augment the variables map depending on the portal type\n    if portal_type in AR_TYPES:\n        now = DateTime()\n        sampling_date = context.getSamplingDate()\n        sampling_date = sampling_date and DT2dt(sampling_date) or DT2dt(now)\n        date_sampled = context.getDateSampled()\n        date_sampled = date_sampled and DT2dt(date_sampled) or DT2dt(now)\n        test_count = 1\n\n        variables.update({\n            \"clientId\": context.getClientID(),\n            \"dateSampled\": date_sampled,\n            \"samplingDate\": sampling_date,\n            \"sampleType\": context.getSampleType().getPrefix(),\n            \"test_count\": test_count\n        })\n\n        # Partition\n        if portal_type == \"AnalysisRequestPartition\":\n            parent_ar = context.getParentAnalysisRequest()\n            parent_ar_id = api.get_id(parent_ar)\n            parent_base_id = strip_suffix(parent_ar_id)\n            partition_count = get_partition_count(context)\n            variables.update({\n                \"parent_analysisrequest\": parent_ar,\n                \"parent_ar_id\": parent_ar_id,\n                \"parent_base_id\": parent_base_id,\n                \"partition_count\": partition_count,\n            })\n\n        # Retest\n        elif portal_type == \"AnalysisRequestRetest\":\n            # Note: we use \"parent\" instead of \"invalidated\" for simplicity\n            parent_ar = context.getInvalidated()\n            parent_ar_id = api.get_id(parent_ar)\n            parent_base_id = strip_suffix(parent_ar_id)\n            # keep the full ID if the retracted AR is a partition\n            if context.isPartition():\n                parent_base_id = parent_ar_id\n            retest_count = get_retest_count(context)\n            test_count = test_count + retest_count\n            variables.update({\n                \"parent_analysisrequest\": parent_ar,\n                \"parent_ar_id\": parent_ar_id,\n                \"parent_base_id\": parent_base_id,\n                \"retest_count\": retest_count,\n                \"test_count\": test_count,\n            })\n\n        # Secondary\n        elif portal_type == \"AnalysisRequestSecondary\":\n            primary_ar = context.getPrimaryAnalysisRequest()\n            primary_ar_id = api.get_id(primary_ar)\n            parent_base_id = strip_suffix(primary_ar_id)\n            secondary_count = get_secondary_count(context)\n            variables.update({\n                \"parent_analysisrequest\": primary_ar,\n                \"parent_ar_id\": primary_ar_id,\n                \"parent_base_id\": parent_base_id,\n                \"secondary_count\": secondary_count,\n            })\n\n    elif portal_type == \"ARReport\":\n        variables.update({\n            \"clientId\": context.aq_parent.getClientID(),\n        })\n\n    return variables"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef slice(string, separator=\"-\", start=None, end=None):\n    # split by wildcards/keywords first\n    # AR-{sampleType}-{parentId}{alpha:3a2d}\n    segments = filter(None, re.split('(\\{.+?\\})', string))\n    # ['AR-', '{sampleType}', '-', '{parentId}', '{alpha:3a2d}']\n    if separator:\n        # Keep track of singleton separators as empties\n        # We need to do this to prevent duplicates later, when splitting\n        segments = map(lambda seg: seg!=separator and seg or \"\", segments)\n        # ['AR-', '{sampleType}', '', '{parentId}', '{alpha:3a2d}']\n        # Split each segment at the given separator\n        segments = map(lambda seg: split(seg, separator), segments)\n        # [['AR', ''], ['{sampleType}'], [''], ['{parentId}'], ['{alpha:3a2d}']]\n        # Flatten the list\n        segments = list(itertools.chain.from_iterable(segments))\n        # ['AR', '', '{sampleType}', '', '{parentId}', '{alpha:3a2d}']\n        # And replace empties with separator\n        segments = map(lambda seg: seg!=\"\" and seg or separator, segments)\n        # ['AR', '-', '{sampleType}', '-', '{parentId}', '{alpha:3a2d}']\n\n    # Get the start and end positions from the segments without separator\n    cleaned_segments = filter(lambda seg: seg!=separator, segments)\n    start_pos = to_int(start, 0)\n    # Note \"end\" is not a position, but the number of elements to join!\n    end_pos = to_int(end, len(cleaned_segments) - start_pos) + start_pos - 1\n\n    # Map the positions against the segments with separator\n    start = segments.index(cleaned_segments[start_pos])\n    end = segments.index(cleaned_segments[end_pos]) + 1\n\n    # Return all segments joined\n    sliced_parts = segments[start:end]\n    return \"\".join(sliced_parts)", "response": "Slice out a segment of a string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef search_by_prefix(portal_type, prefix):\n    catalog = api.get_tool(\"uid_catalog\")\n    brains = catalog({\"portal_type\": portal_type})\n    # Filter brains with the same ID prefix\n    return filter(lambda brain: api.get_id(brain).startswith(prefix), brains)", "response": "Returns a list of brains which share the same portal_type and ID prefix"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of ids sharing the same portal type and prefix", "response": "def get_ids_with_prefix(portal_type, prefix):\n    \"\"\"Return a list of ids sharing the same portal type and prefix\n    \"\"\"\n    brains = search_by_prefix(portal_type, prefix)\n    ids = map(api.get_id, brains)\n    return ids"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_storage_key(portal_type, prefix=None):\n    key = portal_type.lower()\n    if prefix:\n        key = \"{}-{}\".format(key, prefix)\n    return key", "response": "Make a storage key for the number generator"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the sequence number of the given ID", "response": "def get_seq_number_from_id(id, id_template, prefix, **kw):\n    \"\"\"Return the sequence number of the given ID\n    \"\"\"\n    separator = kw.get(\"separator\", \"-\")\n    postfix = id.replace(prefix, \"\").strip(separator)\n    postfix_segments = postfix.split(separator)\n    seq_number = 0\n    possible_seq_nums = filter(lambda n: n.isalnum(), postfix_segments)\n    if possible_seq_nums:\n        seq_number = possible_seq_nums[-1]\n\n    # Check if this id has to be expressed as an alphanumeric number\n    seq_number = get_alpha_or_number(seq_number, id_template)\n    seq_number = to_int(seq_number)\n    return seq_number"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_alpha_or_number(number, template):\n    match = re.match(r\".*\\{alpha:(\\d+a\\d+d)\\}$\", template.strip())\n    if match and match.groups():\n        format = match.groups()[0]\n        return to_alpha(number, format)\n    return number", "response": "Returns an Alphanumber that represents the number passed in if it is already in the template. Otherwise returns the number passed in."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_counted_number(context, config, variables, **kw):\n    # This \"context\" is defined by the user in the Setup and can be actually\n    # anything. However, we assume it is something like \"sample\" or similar\n    ctx = config.get(\"context\")\n\n    # get object behind the context name (falls back to the current context)\n    obj = variables.get(ctx, context)\n\n    # get the counter type, which is either \"backreference\" or \"contained\"\n    counter_type = config.get(\"counter_type\")\n\n    # the counter reference is either the \"relationship\" for\n    # \"backreference\" or the meta type for contained objects\n    counter_reference = config.get(\"counter_reference\")\n\n    # This should be a list of existing items, including the current context\n    # object\n    seq_items = get_objects_in_sequence(obj, counter_type, counter_reference)\n\n    number = len(seq_items)\n    return number", "response": "Compute the number of items in the sequence type Counter"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a new persistent number with the number generator for the ID sequence type Generated", "response": "def get_generated_number(context, config, variables, **kw):\n    \"\"\"Generate a new persistent number with the number generator for the\n    sequence type \"Generated\"\n    \"\"\"\n    # separator where to split the ID\n    separator = kw.get('separator', '-')\n\n    # allow portal_type override\n    portal_type = get_type_id(context, **kw)\n\n    # The ID format for string interpolation, e.g. WS-{seq:03d}\n    id_template = config.get(\"form\", \"\")\n\n    # The split length defines where the key is splitted from the value\n    split_length = config.get(\"split_length\", 1)\n\n    # The prefix template is the static part of the ID\n    prefix_template = slice(id_template, separator=separator, end=split_length)\n\n    # get the number generator\n    number_generator = getUtility(INumberGenerator)\n\n    # generate the key for the number generator storage\n    prefix = prefix_template.format(**variables)\n\n    # normalize out any unicode characters like \u00d6, \u00c9, etc. from the prefix\n    prefix = api.normalize_filename(prefix)\n\n    # The key used for the storage\n    key = make_storage_key(portal_type, prefix)\n\n    # Handle flushed storage\n    if key not in number_generator:\n        max_num = 0\n        existing = get_ids_with_prefix(portal_type, prefix)\n        numbers = map(lambda id: get_seq_number_from_id(id, id_template, prefix), existing)\n        # figure out the highest number in the sequence\n        if numbers:\n            max_num = max(numbers)\n        # set the number generator\n        logger.info(\"*** SEEDING Prefix '{}' to {}\".format(prefix, max_num))\n        number_generator.set_number(key, max_num)\n\n    if not kw.get(\"dry_run\", False):\n        # Generate a new number\n        # NOTE Even when the number exceeds the given ID sequence format,\n        #      it will overflow gracefully, e.g.\n        #      >>> {sampleId}-R{seq:03d}'.format(sampleId=\"Water\", seq=999999)\n        #      'Water-R999999\u2018\n        number = number_generator.generate_number(key=key)\n    else:\n        # => This allows us to \"preview\" the next generated ID in the UI\n        # TODO Show the user the next generated number somewhere in the UI\n        number = number_generator.get(key, 1)\n\n    # Return an int or Alphanumber\n    return get_alpha_or_number(number, id_template)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a unique ID for the current portal type.", "response": "def generateUniqueId(context, **kw):\n    \"\"\" Generate pretty content IDs.\n    \"\"\"\n\n    # get the config for this portal type from the system setup\n    config = get_config(context, **kw)\n\n    # get the variables map for later string interpolation\n    variables = get_variables(context, **kw)\n\n    # The new generate sequence number\n    number = 0\n\n    # get the sequence type from the global config\n    sequence_type = config.get(\"sequence_type\", \"generated\")\n\n    # Sequence Type is \"Counter\", so we use the length of the backreferences or\n    # contained objects of the evaluated \"context\" defined in the config\n    if sequence_type in [\"counter\"]:\n        number = get_counted_number(context, config, variables, **kw)\n\n    # Sequence Type is \"Generated\", so the ID is constructed according to the\n    # configured split length\n    if sequence_type in [\"generated\"]:\n        number = get_generated_number(context, config, variables, **kw)\n\n    # store the new sequence number to the variables map for str interpolation\n    if isinstance(number, Alphanumber):\n        variables[\"alpha\"] = number\n    variables[\"seq\"] = to_int(number)\n\n    # The ID formatting template from user config, e.g. {sampleId}-R{seq:02d}\n    id_template = config.get(\"form\", \"\")\n\n    # Interpolate the ID template\n    try:\n        new_id = id_template.format(**variables)\n    except KeyError, e:\n        logger.error('KeyError: {} not in id_template {}'.format(\n            e, id_template))\n        raise\n    normalized_id = api.normalize_filename(new_id)\n    logger.info(\"generateUniqueId: {}\".format(normalized_id))\n\n    return normalized_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef renameAfterCreation(obj):\n    # Check if the _bika_id was already set\n    bika_id = getattr(obj, \"_bika_id\", None)\n    if bika_id is not None:\n        return bika_id\n    # Can't rename without a subtransaction commit when using portal_factory\n    transaction.savepoint(optimistic=True)\n    # The id returned should be normalized already\n    new_id = None\n    # Checking if an adapter exists for this content type. If yes, we will\n    # get new_id from adapter.\n    for name, adapter in getAdapters((obj, ), IIdServer):\n        if new_id:\n            logger.warn(('More than one ID Generator Adapter found for'\n                         'content type -> %s') % obj.portal_type)\n        new_id = adapter.generate_id(obj.portal_type)\n    if not new_id:\n        new_id = generateUniqueId(obj)\n\n    # TODO: This is a naive check just in current folder\n    # -> this should check globally for duplicate objects with same prefix\n    # N.B. a check like `search_by_prefix` each time would probably slow things\n    # down too much!\n    # -> A solution could be to store all IDs with a certain prefix in a storage\n    parent = api.get_parent(obj)\n    if new_id in parent.objectIds():\n        # XXX We could do the check in a `while` loop and generate a new one.\n        raise KeyError(\"The ID {} is already taken in the path {}\".format(\n            new_id, api.get_path(parent)))\n    # rename the object to the new id\n    parent.manage_renameObject(obj.id, new_id)\n\n    return new_id", "response": "Rename the content after it was created or added to the new id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nobjecting has been transitioned to a new state", "response": "def ObjectTransitionedEventHandler(obj, event):\n    \"\"\"Object has been transitioned to an new state\n    \"\"\"\n\n    # only snapshot supported objects\n    if not supports_snapshots(obj):\n        return\n\n    # default transition entry\n    entry = {\n        \"modified\": DateTime().ISO(),\n        \"action\": event.action,\n    }\n\n    # get the last history item\n    history = api.get_review_history(obj, rev=True)\n    if history:\n        entry = history[0]\n        # make transitions also a modification entry\n        timestamp = entry.pop(\"time\", DateTime())\n        entry[\"modified\"] = timestamp.ISO()\n        entry[\"action\"] = event.action\n\n    # take a new snapshot\n    take_snapshot(obj, **entry)\n\n    # reindex the object in the auditlog catalog\n    reindex_object(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nobjecting has been modified by the user", "response": "def ObjectModifiedEventHandler(obj, event):\n    \"\"\"Object has been modified\n    \"\"\"\n\n    # only snapshot supported objects\n    if not supports_snapshots(obj):\n        return\n\n    # take a new snapshot\n    take_snapshot(obj, action=\"edit\")\n\n    # reindex the object in the auditlog catalog\n    reindex_object(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ObjectInitializedEventHandler(obj, event):\n\n    # only snapshot supported objects\n    if not supports_snapshots(obj):\n        return\n\n    # object has already snapshots\n    if has_snapshots(obj):\n        return\n\n    # take a new snapshot\n    take_snapshot(obj, action=\"create\")", "response": "Object has been created\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the title if possible else return the Sample type.", "response": "def Title(self):\n        \"\"\" Return the title if possible, else return the Sample type.\n        Fall back on the instance's ID if there's no sample type or title.\n        \"\"\"\n        title = ''\n        if self.title:\n            title = self.title\n        else:\n            sampletype = self.getSampleType()\n            if sampletype:\n                title = sampletype.Title()\n        return safe_unicode(title).encode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary with the specification fields for each reservation service.", "response": "def getResultsRangeDict(self):\n        \"\"\"Return a dictionary with the specification fields for each\n           service. The keys of the dictionary are the keywords of each\n           analysis service. Each service contains a dictionary in which\n           each key is the name of the spec field:\n           specs['keyword'] = {'min': value,\n                               'max': value,\n                               'warnmin': value,\n                               ... }\n        \"\"\"\n        specs = {}\n        subfields = self.Schema()['ResultsRange'].subfields\n        for spec in self.getResultsRange():\n            keyword = spec['keyword']\n            specs[keyword] = {}\n            for key in subfields:\n                if key not in ['uid', 'keyword']:\n                    specs[keyword][key] = spec.get(key, '')\n        return specs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getSampleTypes(self, active_only=True):\n        catalog = api.get_tool(\"bika_setup_catalog\")\n        query = {\n            \"portal_type\": \"SampleType\",\n            # N.B. The `sortable_title` index sorts case sensitive. Since there\n            #      is no sort key for sample types, it makes more sense to sort\n            #      them alphabetically in the selection\n            \"sort_on\": \"title\",\n            \"sort_order\": \"ascending\"\n        }\n        results = catalog(query)\n        if active_only:\n            results = filter(api.is_active, results)\n        sampletypes = map(\n            lambda brain: (brain.UID, brain.Title), results)\n        return DisplayList(sampletypes)", "response": "Return all sample types in the system"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _objectdata_cache_key(func, obj):\n    uid = api.get_uid(obj)\n    modified = api.get_modification_date(obj).millis()\n    review_state = api.get_review_status(obj)\n    return \"{}-{}-{}\".format(uid, review_state, modified)", "response": "Cache Key for object data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_storage(obj):\n    annotation = IAnnotations(obj)\n    if annotation.get(SNAPSHOT_STORAGE) is None:\n        annotation[SNAPSHOT_STORAGE] = PersistentList()\n    return annotation[SNAPSHOT_STORAGE]", "response": "Get or create the audit log storage for the given object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_snapshot_count(obj):\n    try:\n        annotation = IAnnotations(obj)\n    except TypeError:\n        return 0\n    storage = annotation.get(SNAPSHOT_STORAGE, [])\n    return len(storage)", "response": "Returns the number of snapshots in the content object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_snapshot_by_version(obj, version=0):\n    if version < 0:\n        return None\n    snapshots = get_snapshots(obj)\n    if version > len(snapshots) - 1:\n        return None\n    return snapshots[version]", "response": "Get a snapshot by version"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting object schema data", "response": "def get_object_data(obj):\n    \"\"\"Get object schema data\n\n    NOTE: We RAM cache this data because it should only change when the object\n    was modified!\n\n    XXX: We need to set at least the modification date when we set fields in\n    Ajax Listing when we take a snapshot there!\n\n    :param obj: Content object\n    :returns: Dictionary of extracted schema data\n    \"\"\"\n\n    model = SuperModel(obj)\n    try:\n        data = model.to_dict()\n    except Exception as exc:\n        logger.error(\"Failed to get schema data for {}: {}\"\n                     .format(repr(obj), str(exc)))\n        data = {}\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_request_data(request=None):\n\n    if request is None:\n        # get the request\n        request = api.get_request()\n\n    # Happens in the test runner\n    if not request:\n        return {}\n\n    # Try to obtain the real IP address of the client\n    forwarded_for = request.get_header(\"X_FORWARDED_FOR\")\n    real_ip = request.get_header(\"X_REAL_IP\")\n    remote_address = request.get_header(\"REMOTE_ADDR\")\n\n    return {\n        \"comments\": request.form.get(\"comments\", \"\"),\n        \"remote_address\": forwarded_for or real_ip or remote_address,\n        \"user_agent\": request.get_header(\"HTTP_USER_AGENT\"),\n        \"referer\": request.get_header(\"HTTP_REFERER\"),\n    }", "response": "Get the request header and form data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting object metadata :param obj: Content object :returns: Dictionary of extracted object metadata", "response": "def get_object_metadata(obj, **kw):\n    \"\"\"Get object metadata\n\n    :param obj: Content object\n    :returns: Dictionary of extracted object metadata\n    \"\"\"\n\n    # inject metadata of volatile data\n    metadata = {\n        \"actor\": get_user_id(),\n        \"roles\": get_roles(),\n        \"action\": \"\",\n        \"review_state\": api.get_review_status(obj),\n        \"active\": api.is_active(obj),\n        \"snapshot_created\": DateTime().ISO(),\n        \"modified\": api.get_modification_date(obj).ISO(),\n        \"remote_address\": \"\",\n        \"user_agent\": \"\",\n        \"referer\": \"\",\n        \"comments\": \"\",\n    }\n\n    # Update request data\n    metadata.update(get_request_data())\n\n    # allow metadata overrides\n    metadata.update(kw)\n\n    return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntakes a snapshot of the passed in object and stores it in the object s storage.", "response": "def take_snapshot(obj, store=True, **kw):\n    \"\"\"Takes a snapshot of the passed in object\n\n    :param obj: Content object\n    :returns: New snapshot\n    \"\"\"\n    logger.debug(\"\ud83d\udcf7 Take new snapshot for {}\".format(repr(obj)))\n\n    # get the object data\n    snapshot = get_object_data(obj)\n\n    # get the metadata\n    metadata = get_object_metadata(obj, **kw)\n\n    # store the metadata\n    snapshot[\"__metadata__\"] = metadata\n\n    # convert the snapshot to JSON\n    data = json.dumps(snapshot)\n\n    # return immediately\n    if not store:\n        return snapshot\n\n    # get the snapshot storage\n    storage = get_storage(obj)\n\n    # store the snapshot data\n    storage.append(data)\n\n    # Mark the content as auditable\n    alsoProvides(obj, IAuditable)\n\n    return snapshot"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compare_snapshots(snapshot_a, snapshot_b, raw=False):\n    if not all(map(lambda x: isinstance(x, dict),\n                   [snapshot_a, snapshot_b])):\n        return {}\n\n    diffs = {}\n    for key_a, value_a in snapshot_a.iteritems():\n        # skip fieds starting with _ or __\n        if key_a.startswith(\"_\"):\n            continue\n        # get the value of the second snapshot\n        value_b = snapshot_b.get(key_a)\n        # get the diff between the two values\n        diff = diff_values(value_a, value_b, raw=raw)\n        if diff is not None:\n            diffs[key_a] = diff\n    return diffs", "response": "Returns a diff of two given snapshots"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compare_last_two_snapshots(obj, raw=False):\n\n    if get_snapshot_count(obj) < 2:\n        return {}\n\n    version = get_version(obj)\n\n    snap1 = get_snapshot_by_version(obj, version - 1)\n    snap2 = get_snapshot_by_version(obj, version)\n\n    return compare_snapshots(snap1, snap2, raw=raw)", "response": "Helper to compare the last two snapshots directly"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a human - readable diff between two values in a node tree.", "response": "def diff_values(value_a, value_b, raw=False):\n    \"\"\"Returns a human-readable diff between two values\n\n    :param value_a: First value to compare\n    :param value_b: Second value to compare\n    :param raw: True to compare the raw values, e.g. UIDs\n    :returns a list of diff tuples\n    \"\"\"\n\n    if not raw:\n        value_a = _process_value(value_a)\n        value_b = _process_value(value_b)\n\n    # No changes\n    if value_a == value_b:\n        return None\n\n    diffs = []\n    # N.B.: the choice for the tuple data structure is to enable in the future\n    # more granular diffs, e.g. the changed values within a dictionary etc.\n    diffs.append((value_a, value_b))\n    return diffs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _process_value(value):\n    if not value:\n        value = _(\"Not set\")\n    # XXX: bad data, e.g. in AS Method field\n    elif value == \"None\":\n        value = _(\"Not set\")\n    # 0 is detected as the portal UID\n    elif value == \"0\":\n        pass\n    elif api.is_uid(value):\n        value = _get_title_or_id_from_uid(value)\n    elif isinstance(value, (dict)):\n        value = json.dumps(sorted(value.items()), indent=1)\n    elif isinstance(value, (list, tuple)):\n        value = sorted(map(_process_value, value))\n        value = \"; \".join(value)\n    elif isinstance(value, unicode):\n        value = api.safe_unicode(value).encode(\"utf8\")\n    return str(value)", "response": "Convert the value into a human readable diff string\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the title or ID from the given UID", "response": "def _get_title_or_id_from_uid(uid):\n    \"\"\"Returns the title or ID from the given UID\n    \"\"\"\n    try:\n        obj = api.get_object_by_uid(uid)\n    except api.APIError:\n        return \"<Deleted {}>\".format(uid)\n    title_or_id = api.get_title(obj) or api.get_id(obj)\n    return title_or_id"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Title(self):\n        titlefield = self.Schema().getField('title')\n        if titlefield.widget.visible:\n            return safe_unicode(self.title).encode('utf-8')\n        else:\n            return safe_unicode(self.id).encode('utf-8')", "response": "Return the Batch ID if title is not defined"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the Client for which the current Batch is attached to", "response": "def getClient(self):\n        \"\"\"Retrieves the Client for which the current Batch is attached to\n           Tries to retrieve the Client from the Schema property, but if not\n           found, searches for linked ARs and retrieve the Client from the\n           first one. If the Batch has no client, returns None.\n        \"\"\"\n        client = self.Schema().getField('Client').get(self)\n        if client:\n            return client\n        client = self.aq_parent\n        if IClient.providedBy(client):\n            return client"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning all batch labels as a display list", "response": "def BatchLabelVocabulary(self):\n        \"\"\"Return all batch labels as a display list\n        \"\"\"\n        bsc = getToolByName(self, 'bika_setup_catalog')\n        ret = []\n        for p in bsc(portal_type='BatchLabel',\n                     is_active=True,\n                     sort_on='sortable_title'):\n            ret.append((p.UID, p.Title))\n        return DisplayList(ret)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn all the Analysis Requests brains linked to the Batch", "response": "def getAnalysisRequestsBrains(self, **kwargs):\n        \"\"\"Return all the Analysis Requests brains linked to the Batch\n        kargs are passed directly to the catalog.\n        \"\"\"\n        kwargs['getBatchUID'] = self.UID()\n        catalog = getToolByName(self, CATALOG_ANALYSIS_REQUEST_LISTING)\n        brains = catalog(kwargs)\n        return brains"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning all the Analysis Requests objects linked to the Batch kargs are passed directly to the catalog.", "response": "def getAnalysisRequests(self, **kwargs):\n        \"\"\"Return all the Analysis Requests objects linked to the Batch kargs\n        are passed directly to the catalog.\n        \"\"\"\n        brains = self.getAnalysisRequestsBrains(**kwargs)\n        return [b.getObject() for b in brains]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getValidCertifications(self):\n        certs = []\n        today = date.today()\n        for c in self.getCertifications():\n            validfrom = c.getValidFrom() if c else None\n            validto = c.getValidTo() if validfrom else None\n            if not validfrom or not validto:\n                continue\n            validfrom = validfrom.asdatetime().date()\n            validto = validto.asdatetime().date()\n            if (today >= validfrom and today <= validto):\n                certs.append(c)\n        return certs", "response": "Returns the certifications fully valid\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if the current instrument is not out for verification calibration and calibration in progress and if the latest QC is valid.", "response": "def isValid(self):\n        \"\"\" Returns if the current instrument is not out for verification, calibration,\n        out-of-date regards to its certificates and if the latest QC succeed\n        \"\"\"\n        return self.isOutOfDate() is False \\\n            and self.isQCValid() is True \\\n            and self.getDisposeUntilNextCalibrationTest() is False \\\n            and self.isValidationInProgress() is False \\\n            and self.isCalibrationInProgress() is False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the results of the last batch of QC Analyses assigned to this instrument are within the valid range.", "response": "def isQCValid(self):\n        \"\"\" Returns True if the results of the last batch of QC Analyses\n        performed against this instrument was within the valid range.\n\n        For a given Reference Sample, more than one Reference Analyses assigned\n        to this same instrument can be performed and the Results Capture Date\n        might slightly differ amongst them. Thus, this function gets the latest\n        QC Analysis performed, looks for siblings (through RefAnalysisGroupID)\n        and if the results for all them are valid, then returns True. If there\n        is one single Reference Analysis from the group with an out-of-range\n        result, the function returns False\n        \"\"\"\n        query = {\"portal_type\": \"ReferenceAnalysis\",\n                 \"getInstrumentUID\": self.UID(),\n                 \"sort_on\": \"getResultCaptureDate\",\n                 \"sort_order\": \"reverse\",\n                 \"sort_limit\": 1,}\n        brains = api.search(query, CATALOG_ANALYSIS_LISTING)\n        if len(brains) == 0:\n            # There are no Reference Analyses assigned to this instrument yet\n            return True\n\n        # Look for siblings. These are the QC Analyses that were created\n        # together with this last ReferenceAnalysis and for the same Reference\n        # Sample. If they were added through \"Add Reference Analyses\" in a\n        # Worksheet, they typically appear in the same slot.\n        group_id = brains[0].getReferenceAnalysesGroupID\n        query = {\"portal_type\": \"ReferenceAnalysis\",\n                 \"getInstrumentUID\": self.UID(),\n                 \"getReferenceAnalysesGroupID\": group_id,}\n        brains = api.search(query, CATALOG_ANALYSIS_LISTING)\n        for brain in brains:\n            results_range = brain.getResultsRange\n            if not results_range:\n                continue\n            # Is out of range?\n            out_of_range = is_out_of_range(brain)[0]\n            if out_of_range:\n                return False\n\n        # By default, in range\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd reference analyses to the Instrument", "response": "def addReferences(self, reference, service_uids):\n        \"\"\" Add reference analyses to reference\n        \"\"\"\n        # TODO Workflow - Analyses. Assignment of refanalysis to Instrument\n        addedanalyses = []\n        wf = getToolByName(self, 'portal_workflow')\n        bsc = getToolByName(self, 'bika_setup_catalog')\n        bac = getToolByName(self, 'bika_analysis_catalog')\n        ref_type = reference.getBlank() and 'b' or 'c'\n        ref_uid = reference.UID()\n        postfix = 1\n        for refa in reference.getReferenceAnalyses():\n            grid = refa.getReferenceAnalysesGroupID()\n            try:\n                cand = int(grid.split('-')[2])\n                if cand >= postfix:\n                    postfix = cand + 1\n            except:\n                pass\n        postfix = str(postfix).zfill(int(3))\n        refgid = 'I%s-%s' % (reference.id, postfix)\n        for service_uid in service_uids:\n            # services with dependents don't belong in references\n            service = bsc(portal_type='AnalysisService', UID=service_uid)[0].getObject()\n            calc = service.getCalculation()\n            if calc and calc.getDependentServices():\n                continue\n            ref_analysis = reference.addReferenceAnalysis(service)\n\n            # Set ReferenceAnalysesGroupID (same id for the analyses from\n            # the same Reference Sample and same Worksheet)\n            # https://github.com/bikalabs/Bika-LIMS/issues/931\n            ref_analysis.setReferenceAnalysesGroupID(refgid)\n            ref_analysis.setInstrument(self)\n            ref_analysis.reindexObject()\n            addedanalyses.append(ref_analysis)\n\n        # Set DisposeUntilNextCalibrationTest to False\n        if (len(addedanalyses) > 0):\n            self.getField('DisposeUntilNextCalibrationTest').set(self, False)\n\n        return addedanalyses"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the list of import data interfaces that this entry belongs to", "response": "def setImportDataInterface(self, values):\n        \"\"\" Return the current list of import data interfaces\n        \"\"\"\n        exims = self.getImportDataInterfacesList()\n        new_values = [value for value in values if value in exims]\n        if len(new_values) < len(values):\n            logger.warn(\"Some Interfaces weren't added...\")\n        self.Schema().getField('ImportDataInterface').set(self, new_values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisplays the value of a Picklist Widget.", "response": "def displayValue(self, vocab, value, widget):\n        \"\"\"Overwrite the Script (Python) `displayValue.py` located at\n           `Products.Archetypes.skins.archetypes` to handle the references\n           of our Picklist Widget (Methods) gracefully.\n           This method gets called by the `picklist.pt` template like this:\n\n           display python:context.displayValue(vocab, value, widget);\"\n        \"\"\"\n        # Taken from the Script (Python)\n        t = self.restrictedTraverse('@@at_utils').translate\n\n        # ensure we have strings, otherwise the `getValue` method of\n        # Products.Archetypes.utils will raise a TypeError\n        def to_string(v):\n            if isinstance(v, basestring):\n                return v\n            return api.get_title(v)\n\n        if isinstance(value, (list, tuple)):\n            value = map(to_string, value)\n\n        return t(vocab, value, widget)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nservicing triggered each time an item is iterated in folderitems. The use of this service prevents the extra-loops in child objects. :obj: the instance of the class to be foldered :item: dict containing the properties of the object to be used by the template :index: current index of the item", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Service triggered each time an item is iterated in folderitems.\n        The use of this service prevents the extra-loops in child objects.\n        :obj: the instance of the class to be foldered\n        :item: dict containing the properties of the object to be used by\n            the template\n        :index: current index of the item\n        \"\"\"\n        title = obj.Title()\n        description = obj.Description()\n        url = obj.absolute_url()\n\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n        item[\"Description\"] = description\n        item[\"Formula\"] = obj.getMinifiedFormula()\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef folderitem(self, obj, item, index):\n\n        fullname = obj.getFullname()\n        if fullname:\n            item[\"Fullname\"] = fullname\n            item[\"replace\"][\"Fullname\"] = get_link(\n                obj.absolute_url(), value=fullname)\n        else:\n            item[\"Fullname\"] = \"\"\n\n        default_department = obj.getDefaultDepartment()\n        if default_department:\n            item[\"replace\"][\"DefaultDepartment\"] = get_link(\n                default_department.absolute_url(),\n                value=default_department.Title())\n\n        departments = obj.getDepartments()\n        if departments:\n            links = map(\n                lambda o: get_link(o.absolute_url(),\n                                   value=o.Title(),\n                                   css_class=\"link\"),\n                departments)\n            item[\"replace\"][\"Departments\"] = \", \".join(links)\n\n        email = obj.getEmailAddress()\n        if email:\n            item[\"EmailAddress\"] = obj.getEmailAddress()\n            item[\"replace\"][\"EmailAddress\"] = get_email_link(\n                email, value=email)\n\n        item[\"BusinessPhone\"] = obj.getBusinessPhone()\n        item[\"Fax\"] = obj.getBusinessFax()\n        item[\"MobilePhone\"] = obj.getMobilePhone()\n\n        return item", "response": "This method is called when an item is iterated in folderitems. It returns the item that is used by the template."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ObjectModifiedEventHandler(obj, event):\n    if not hasattr(obj, 'portal_type'):\n        return\n\n    if obj.portal_type == 'Calculation':\n        pr = getToolByName(obj, 'portal_repository')\n        uc = getToolByName(obj, 'uid_catalog')\n        obj = uc(UID=obj.UID())[0].getObject()\n        version_id = obj.version_id if hasattr(obj, 'version_id') else 0\n\n        backrefs = obj.getBackReferences('MethodCalculation')\n        for i, target in enumerate(backrefs):\n            target = uc(UID=target.UID())[0].getObject()\n            pr.save(obj=target, comment=\"Calculation updated to version %s\" %\n                (version_id + 1,))\n            reference_versions = getattr(target, 'reference_versions', {})\n            reference_versions[obj.UID()] = version_id + 1\n            target.reference_versions = reference_versions\n\n    elif obj.portal_type == 'Contact':\n        # Verify that the Contact details are the same as the Plone user.\n        contact_username = obj.Schema()['Username'].get(obj)\n        if contact_username:\n            contact_email = obj.Schema()['EmailAddress'].get(obj)\n            contact_fullname = obj.Schema()['Fullname'].get(obj)\n            mt = getToolByName(obj, 'portal_membership')\n            member = mt.getMemberById(contact_username)\n            if member:\n                properties = {'username': contact_username,\n                              'email': contact_email,\n                              'fullname': contact_fullname}\n                member.setMemberProperties(properties)\n\n    elif obj.portal_type == 'AnalysisCategory':\n        # If the analysis category's Title is modified, we must\n        # re-index all services and analyses that refer to this title.\n        for i in [['Analysis', 'bika_analysis_catalog'],\n                  ['AnalysisService', 'bika_setup_catalog']]:\n            cat = getToolByName(obj, i[1])\n            brains = cat(portal_type=i[0], getCategoryUID=obj.UID())\n            for brain in brains:\n                brain.getObject().reindexObject(idxs=['getCategoryTitle'])", "response": "Object modified event handler."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncontrol if the viewlet should be rendered", "response": "def show(self):\n        \"\"\"Controls if the viewlet should be rendered\n        \"\"\"\n        url = self.request.getURL()\n        # XXX: Hack to show the viewlet only on the AR base_view\n        if not any(map(url.endswith, [\"base_view\", \"manage_results\"])):\n            return False\n        return self.attachments_view.user_can_add_attachments() or \\\n               self.attachments_view.user_can_update_attachments()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall always before render", "response": "def update(self):\n        \"\"\"Called always before render()\n        \"\"\"\n        super(AttachmentsViewlet, self).update()\n        self.attachments_view = self.get_attachments_view()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads Shimadzu GCMS - TQ8030 GC - MS - MS analysis results", "response": "def Import(context, request):\n    \"\"\" Read Shimadzu GCMS-TQ8030 GC/MS/MS analysis results\n    \"\"\"\n    form = request.form\n    # TODO form['file'] sometimes returns a list\n    infile = form['instrument_results_file'][0] if \\\n        isinstance(form['instrument_results_file'], list) \\\n        else form['instrument_results_file']\n    override = form['results_override']\n    artoapply = form['artoapply']\n    instrument = form.get('instrument', None)\n    errors = []\n    logs = []\n\n    # Load the most suitable parser according to file extension/options/etc...\n    parser = None\n    if not hasattr(infile, 'filename'):\n        errors.append(_(\"No file selected\"))\n    parser = GCMSQP2010SECSVParser(infile)\n\n    if parser:\n        # Load the importer\n        status = ['sample_received', 'attachment_due', 'to_be_verified']\n        if artoapply == 'received':\n            status = ['sample_received']\n        elif artoapply == 'received_tobeverified':\n            status = ['sample_received', 'attachment_due', 'to_be_verified']\n\n        over = [False, False]\n        if override == 'nooverride':\n            over = [False, False]\n        elif override == 'override':\n            over = [True, False]\n        elif override == 'overrideempty':\n            over = [True, True]\n\n        importer = GCMSQP2010SEImporter(parser=parser,\n                                        context=context,\n                                        allowed_ar_states=status,\n                                        allowed_analysis_states=None,\n                                        override=over,\n                                        instrument_uid=instrument)\n        tbex = ''\n        try:\n            importer.process()\n        except:\n            tbex = traceback.format_exc()\n        errors = importer.errors\n        logs = importer.logs\n        warns = importer.warns\n        if tbex:\n            errors.append(tbex)\n\n    results = {'errors': errors, 'log': logs, 'warns': warns}\n\n    return json.dumps(results)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_headerline(self, line):\n        if self._end_header is True:\n            # Header already processed\n            return 0\n\n        splitted = [token.strip() for token in line.split('\\t')]\n\n        # [Header]\n        if splitted[0] == self.HEADERTABLE_KEY:\n            if self.HEADERTABLE_KEY in self._header:\n                self.warn(\"Header [Header] Info already found. Discarding\",\n                          numline=self._numline, line=line)\n                return 0\n\n            self._header[self.HEADERTABLE_KEY] = []\n            for i in range(len(splitted) - 1):\n                if splitted[i + 1]:\n                    self._header[self.HEADERTABLE_KEY].append(splitted[i + 1])\n\n        # Data File Name, C:\\GCMSsolution\\Data\\October\\\n        # 1-16-02249-001_CD_10172016_2.qgd\n        elif splitted[0] == self.HEADERKEY_FILENAME:\n            if self.HEADERKEY_FILENAME in self._header:\n                self.warn(\"Header File Data Name already found. Discarding\",\n                          numline=self._numline, line=line)\n                return 0\n\n            if splitted[1]:\n                self._header[self.HEADERKEY_FILENAME] = splitted[1]\n            else:\n                self.warn(\"File Data Name not found or empty\",\n                          numline=self._numline, line=line)\n\n        # Output Date\t10/18/2016\n        elif splitted[0] == self.HEADERKEY_OUTPUTDATE:\n            if splitted[1]:\n                try:\n                    d = datetime.strptime(splitted[1], \"%m/%d/%Y\")\n                    self._header[self.HEADERKEY_OUTPUTDATE] = d\n                except ValueError:\n                    self.err(\"Invalid Output Date format\",\n                             numline=self._numline, line=line)\n            else:\n                self.warn(\"Output Date not found or empty\",\n                          numline=self._numline, line=line)\n                d = datetime.strptime(splitted[1], \"%m/%d/%Y\")\n\n        # Output Time\t12:04:11 PM\n        elif splitted[0] == self.HEADERKEY_OUTPUTTIME:\n            if splitted[1]:\n                try:\n                    d = datetime.strptime(splitted[1], \"%I:%M:%S %p\")\n                    self._header[self.HEADERKEY_OUTPUTTIME] = d\n                except ValueError:\n                    self.err(\"Invalid Output Time format\",\n                             numline=self._numline, line=line)\n            else:\n                self.warn(\"Output Time not found or empty\",\n                          numline=self._numline, line=line)\n                d = datetime.strptime(splitted[1], \"%I:%M %p\")\n\n        if line.startswith(self.QUANTITATIONRESULTS_KEY):\n            self._end_header = True\n            if len(self._header) == 0:\n                self.err(\"No header found\", numline=self._numline)\n                return -1\n            return 0\n\n        return 0", "response": "Parses the header line and sets the internal _header dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_quantitationesultsline(self, line):\n\n        # [MS Quantitative Results]\n        if line.startswith(self.QUANTITATIONRESULTS_KEY) \\\n                or line.startswith(self.QUANTITATIONRESULTS_NUMBEROFIDS) \\\n                or line.startswith(self.SIMILARITYSEARCHRESULTS_KEY) \\\n                or line.startswith(self.PEAK_TABLE_KEY):\n\n            # Nothing to do, continue\n            return 0\n\n        # # of IDs \\t23\n        if line.startswith(self.QUANTITATIONRESULTS_HEADER_ID_NUMBER):\n            self._quantitationresultsheader = [token.strip() for token\n                                               in line.split('\\t')\n                                               if token.strip()]\n            return 0\n\n        # 1 \\talpha-Pinene \\tTarget \\t0 \\t93.00 \\t7.738 \\t7.680 \\t7.795 \\t2.480\n        # \\t344488 \\t138926 \\t0.02604 \\tAuto \\t2\t\\t7.812\t\\tLinear \\t0 \\t0\n        # \\t4.44061e+008\t\\t278569 \\t0 \\t0 \\t38.94 \\t38.58 \\t0.00\t\\t98 \\t92.00\n        # \\t0 \\t0 \\t38.94 \\t38.58 \\t91.00 \\t0 \\t0 \\t38.93 \\t40.02 \\t0 \\t0 \\t0\n        # \\t0 \\t0 \\t0 \\t0 #\\t0 \\t0 \\t0 \\t0 \\t0 \\t0 \\t0 \\t0 \\t75.27 \\tmg \\t0.000\n        splitted = [token.strip() for token in line.split('\\t')]\n        ar_id = self._header['Data File Name'].split('\\\\')[-1].split('.')[0]\n        quantitation = {'DefaultResult': 'Conc.', 'AR': ar_id}\n        for colname in self._quantitationresultsheader:\n            quantitation[colname] = ''\n\n        for i in range(len(splitted)):\n            token = splitted[i]\n            if i < len(self._quantitationresultsheader):\n                colname = self._quantitationresultsheader[i]\n                if colname in self.QUANTITATIONRESULTS_NUMERICHEADERS:\n                    try:\n                        quantitation[colname] = float(token)\n                    except ValueError:\n                        self.warn(\n                            \"No valid number ${token} in column \"\n                            \"${index} (${column_name})\",\n                            mapping={\"token\": token,\n                                     \"index\": str(i + 1),\n                                     \"column_name\": colname},\n                            numline=self._numline, line=line)\n                        quantitation[colname] = token\n                else:\n                    quantitation[colname] = token\n\n                # val = re.sub(r\"\\W\", \"\", splitted[1])\n                # self._addRawResult(quantitation['AR'],\n                #                   values={val:quantitation},\n                #                   override=False)\n            elif token:\n                self.err(\"Orphan value in column ${index} (${token})\",\n                         mapping={\"index\": str(i+1),\n                                  \"token\": token},\n                         numline=self._numline, line=line)\n\n        result = quantitation[quantitation['DefaultResult']]\n        column_name = quantitation['DefaultResult']\n        result = self.zeroValueDefaultInstrumentResults(column_name,\n                                                        result, line)\n        quantitation[quantitation['DefaultResult']] = result\n\n        val = re.sub(r\"\\W\", \"\", splitted[1])\n        self._addRawResult(quantitation['AR'],\n                           values={val: quantitation},\n                           override=False)", "response": "Parses the line of a QUANTITATIONRESULTS output file and returns the entry number of the entry in the file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread analysis results from an XML string", "response": "def Import(context, request):\n    \"\"\" Read analysis results from an XML string\n    \"\"\"\n    errors = []\n    logs = []\n\n    # Do import stuff here\n    logs.append(\"Generic XML Import is not available\")\n\n    results = {'errors': errors, 'log': logs}\n    return json.dumps(results)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_storage_location():\n    location = api.portal.getSite()\n    if location.get('bika_setup', False):\n        location = location['bika_setup']\n    return location", "response": "get the storage location"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef storage(self):\n        annotation = get_portal_annotation()\n        if annotation.get(NUMBER_STORAGE) is None:\n            annotation[NUMBER_STORAGE]  = OIBTree()\n        return annotation[NUMBER_STORAGE]", "response": "get the counter storage"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the next consecutive number for a key", "response": "def get_number(self, key):\n        \"\"\" get the next consecutive number\n        \"\"\"\n        storage = self.storage\n\n        logger.debug(\"NUMBER before => %s\" % storage.get(key, '-'))\n        try:\n            logger.debug(\"*** consecutive number lock acquire ***\")\n            lock.acquire()\n            try:\n                counter = storage[key]\n                storage[key] = counter + 1\n            except KeyError:\n                storage[key] = 1\n        finally:\n            logger.debug(\"*** consecutive number lock release ***\")\n            self.storage._p_changed = True\n            lock.release()\n\n        logger.debug(\"NUMBER after => %s\" % storage.get(key, '-'))\n        return storage[key]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_number(self, key, value):\n        storage = self.storage\n\n        if not isinstance(value, int):\n            logger.error(\"set_number: Value must be an integer\")\n            return\n\n        try:\n            lock.acquire()\n            storage[key] = value\n        finally:\n            self.storage._p_changed = True\n            lock.release()\n\n        return storage[key]", "response": "set a key s value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the translations for the given domain and language.", "response": "def _gettext_catalog(self, domain, language):\n        \"\"\"\n        Overrides jarn.jsi18n.view.jsi18n\n        See:\n            https://github.com/collective/jarn.jsi18n/issues/1\n            https://github.com/collective/jarn.jsi18n/pull/2\n\n        :param domain: translation domain\n        :param language: iso code for language\n        :return: dict, with the translations for the domain and language\n        \"\"\"\n        td = queryUtility(ITranslationDomain, domain)\n        if td is None or language not in td._catalogs:\n            return\n\n        # N.B. Multiple PO files might be registered for one language, where\n        # usually the one at position 0 is the custom catalog with the\n        # overriding terms in it. Therefore, we need to reverse the order, so\n        # that the custom catalog wins.\n        _catalog = {}\n        for mo_path in reversed(td._catalogs[language]):\n            catalog = td._data[mo_path]._catalog\n            if catalog is None:\n                td._data[mo_path].reload()\n                catalog = td._data[mo_path]._catalog\n            catalog = catalog._catalog\n            for key, val in catalog.iteritems():\n                if key and val:\n                    _catalog[key] = val\n        return _catalog"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _addRawResult(self, resid, values={}, override=False):\n\n        if 'Date' in values and 'Time' in values:\n            try:\n                dtstr = '%s %s' % (values.get('Date')['Date'], values.get('Time')['Time'])\n                # 2/11/2005 13:33 PM\n                from datetime import datetime\n                dtobj = datetime.strptime(dtstr, '%d/%m/%Y %H:%M %p')\n                dateTime = dtobj.strftime(\"%Y%m%d %H:%M:%S\")\n            except:\n                pass\n            del values['Date']\n            del values['Time']\n\n        # Adding the date, time and calibration inside each analysis service result.\n        # I'm adding the calibration number here because it is the way we can avoid\n        # WINE-76 easly\n        for keyword in values.keys():\n            values[keyword]['DateTime'] = dateTime\n            values[keyword]['Calibration'] = self._calibration\n\n        # First, we must find if already exists a row with results for\n        # the same date, in order to take into account replicas, Mean\n        # and Standard Deviation\n        dtidx = values.get('Calibration',{}).get('Calibration',0)\n        rows = self.getRawResults().get(resid, [])\n        row, rows = self._extractrowbycalibration(rows, self._calibration)\n        is_std = values.get('Rep #',{}).get('Rep #','') == 'Sd'\n        is_mean = values.get('Rep #',{}).get('Rep #','') == 'Mean'\n        if is_std:\n            # Add the results of Standard Deviation. For each acode, add\n            # the Standard Result\n            del values['Rep #']\n            for key, value in values.iteritems():\n                row['Sd-%s' % key] = value\n        elif is_mean:\n            # Remove the # item and override with new values\n            row = values\n            del row['Rep #']\n        else:\n            # Override with new values\n            row = values\n        rows.append(row)\n        isfirst = True\n        for row in rows:\n            WinescanCSVParser._addRawResult(self, resid, row, isfirst)\n            isfirst = False", "response": "Add raw result to the table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getJSMinimumVolume(self, **kw):\n        default = self.Schema()['MinimumVolume'].get(self)\n        try:\n            mgdefault = default.split(' ', 1)\n            mgdefault = mg(float(mgdefault[0]), mgdefault[1])\n        except:\n            mgdefault = mg(0, 'ml')\n        try:\n            return str(mgdefault.ounit('ml'))\n        except:\n            pass\n        try:\n            return str(mgdefault.ounit('g'))\n        except:\n            pass\n        return str(default)", "response": "Try to convert the MinimumVolume to ml or g so that JS has an\n        easier time working with it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _sticker_templates_vocabularies(self):\n        admitted = self.getAdmittedStickers()\n        if not admitted:\n            return DisplayList()\n        voc = DisplayList()\n        stickers = getStickerTemplates()\n        for sticker in stickers:\n            if sticker.get('id') in admitted:\n                voc.add(sticker.get('id'), sticker.get('title'))\n        return voc", "response": "Returns the vocabulary to be used in the object s AdmittedStickerTemplates. admitted stickers and AdmittedStickerTemplates. small_default_cluster_names attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fail(self, message, status=500, **kw):\n        self.request.response.setStatus(status)\n        result = {\"success\": False, \"errors\": message, \"status\": status}\n        result.update(kw)\n        return result", "response": "Set a JSON error object and a status to the response"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_ajax_request(self):\n        # check if the method exists\n        func_arg = self.traverse_subpath[0]\n        func_name = \"ajax_{}\".format(func_arg)\n        func = getattr(self, func_name, None)\n\n        if func is None:\n            return self.fail(\"Invalid function\", status=400)\n\n        # Additional provided path segments after the function name are handled\n        # as positional arguments\n        args = self.traverse_subpath[1:]\n\n        # check mandatory arguments\n        func_sig = inspect.getargspec(func)\n        # positional arguments after `self` argument\n        required_args = func_sig.args[1:]\n\n        if len(args) < len(required_args):\n            return self.fail(\"Wrong signature, please use '{}/{}'\"\n                             .format(func_arg, \"/\".join(required_args)), 400)\n        return func(*args)", "response": "Handle requests ajax routes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the recipients of the report to the list of ARReports that are not already in the report", "response": "def set_report_recipients(self, report, recipients):\n        \"\"\"Set recipients to the reports w/o overwriting the old ones\n\n        :param reports: list of ARReports\n        :param recipients: list of name,email strings\n        \"\"\"\n        to_set = report.getRecipients()\n        for recipient in recipients:\n            if recipient not in to_set:\n                to_set.append(recipient)\n        report.setRecipients(to_set)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef publish(self, ar):\n        wf = api.get_tool(\"portal_workflow\")\n        status = wf.getInfoFor(ar, \"review_state\")\n        transitions = {\"verified\": \"publish\",\n                       \"published\": \"republish\"}\n        transition = transitions.get(status, \"prepublish\")\n        logger.info(\"AR Transition: {} -> {}\".format(status, transition))\n        try:\n            wf.doActionFor(ar, transition)\n            return True\n        except WorkflowException as e:\n            logger.debug(e)\n            return False", "response": "Set status to prepublished/published/republished"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses an email to an unicode name email tuple", "response": "def parse_email(self, email):\n        \"\"\"parse an email to an unicode name, email tuple\n        \"\"\"\n        splitted = safe_unicode(email).rsplit(\",\", 1)\n        if len(splitted) == 1:\n            return (False, splitted[0])\n        elif len(splitted) == 2:\n            return (splitted[0], splitted[1])\n        else:\n            raise ValueError(\"Could not parse email '{}'\".format(email))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new MIME Attachment object from the given filename and filedata.", "response": "def to_email_attachment(self, filename, filedata, **kw):\n        \"\"\"Create a new MIME Attachment\n\n        The Content-Type: header is build from the maintype and subtype of the\n        guessed filename mimetype. Additional parameters for this header are\n        taken from the keyword arguments.\n        \"\"\"\n        maintype = \"application\"\n        subtype = \"octet-stream\"\n\n        mime_type = mimetypes.guess_type(filename)[0]\n        if mime_type is not None:\n            maintype, subtype = mime_type.split(\"/\")\n\n        attachment = MIMEBase(maintype, subtype, **kw)\n        attachment.set_payload(filedata)\n        encoders.encode_base64(attachment)\n        attachment.add_header(\"Content-Disposition\",\n                              \"attachment; filename=%s\" % filename)\n        return attachment"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_email(self, recipients, subject, body, attachments=None):\n\n        recipient_pairs = map(self.parse_email, recipients)\n        template_context = {\n            \"recipients\": \"\\n\".join(\n                map(lambda p: formataddr(p), recipient_pairs))\n        }\n\n        body_template = Template(safe_unicode(body)).safe_substitute(\n            **template_context)\n\n        _preamble = \"This is a multi-part message in MIME format.\\n\"\n        _from = formataddr((self.email_from_name, self.email_from_address))\n        _subject = Header(s=safe_unicode(subject), charset=\"utf8\")\n        _body = MIMEText(body_template, _subtype=\"plain\", _charset=\"utf8\")\n\n        # Create the enclosing message\n        mime_msg = MIMEMultipart()\n        mime_msg.preamble = _preamble\n        mime_msg[\"Subject\"] = _subject\n        mime_msg[\"From\"] = _from\n        mime_msg.attach(_body)\n\n        # Attach attachments\n        for attachment in attachments:\n            mime_msg.attach(attachment)\n\n        success = []\n        # Send one email per recipient\n        for pair in recipient_pairs:\n            # N.B.: Headers are added additive, so we need to remove any\n            #       existing \"To\" headers\n            # No KeyError is raised if the key does not exist.\n            # https://docs.python.org/2/library/email.message.html#email.message.Message.__delitem__\n            del mime_msg[\"To\"]\n\n            # N.B. we use just the email here to prevent this Postfix Error:\n            # Recipient address rejected: User unknown in local recipient table\n            mime_msg[\"To\"] = pair[1]\n            msg_string = mime_msg.as_string()\n            sent = self.send(msg_string)\n            if not sent:\n                logger.error(\"Could not send email to {}\".format(pair))\n            success.append(sent)\n\n        if not all(success):\n            return False\n        return True", "response": "Prepare and send an email to the recipients."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send(self, msg_string, immediate=True):\n        try:\n            mailhost = api.get_tool(\"MailHost\")\n            mailhost.send(msg_string, immediate=immediate)\n        except SMTPException as e:\n            logger.error(e)\n            return False\n        except socket.error as e:\n            logger.error(e)\n            return False\n        return True", "response": "Send the email via the MailHost tool"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_status_message(self, message, level=\"info\"):\n        return self.context.plone_utils.addPortalMessage(message, level)", "response": "Add a portal status message"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the report data to be used in the template", "response": "def get_report_data(self, report):\n        \"\"\"Report data to be used in the template\n        \"\"\"\n        ar = report.getAnalysisRequest()\n        attachments = map(self.get_attachment_data, ar.getAttachment())\n        pdf = self.get_pdf(report)\n        filesize = \"{} Kb\".format(self.get_filesize(pdf))\n        filename = \"{}.pdf\".format(ar.getId())\n\n        return {\n            \"ar\": ar,\n            \"attachments\": attachments,\n            \"pdf\": pdf,\n            \"obj\": report,\n            \"uid\": api.get_uid(report),\n            \"filesize\": filesize,\n            \"filename\": filename,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_attachment_data(self, attachment):\n        f = attachment.getAttachmentFile()\n        attachment_type = attachment.getAttachmentType()\n        attachment_keys = attachment.getAttachmentKeys()\n        filename = f.filename\n        filesize = self.get_filesize(f)\n        mimetype = f.getContentType()\n        report_option = attachment.getReportOption()\n\n        return {\n            \"obj\": attachment,\n            \"attachment_type\": attachment_type,\n            \"attachment_keys\": attachment_keys,\n            \"file\": f,\n            \"uid\": api.get_uid(attachment),\n            \"filesize\": filesize,\n            \"filename\": filename,\n            \"mimetype\": mimetype,\n            \"report_option\": report_option,\n        }", "response": "Returns a dict of all the data for an attachment"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the list of recipients data for the given list of reports.", "response": "def get_recipients_data(self, reports):\n        \"\"\"Recipients data to be used in the template\n        \"\"\"\n        if not reports:\n            return []\n\n        recipients = []\n        recipient_names = []\n\n        for num, report in enumerate(reports):\n            # get the linked AR of this ARReport\n            ar = report.getAnalysisRequest()\n            # recipient names of this report\n            report_recipient_names = []\n            for recipient in self.get_recipients(ar):\n                name = recipient.get(\"Fullname\")\n                email = recipient.get(\"EmailAddress\")\n                record = {\n                    \"name\": name,\n                    \"email\": email,\n                    \"valid\": True,\n                }\n                if record not in recipients:\n                    recipients.append(record)\n                # remember the name of the recipient for this report\n                report_recipient_names.append(name)\n            recipient_names.append(report_recipient_names)\n\n        # recipient names, which all of the reports have in common\n        common_names = set(recipient_names[0]).intersection(*recipient_names)\n        # mark recipients not in common\n        for recipient in recipients:\n            if recipient.get(\"name\") not in common_names:\n                recipient[\"valid\"] = False\n        return recipients"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the list of data that will be used in the template.", "response": "def get_responsibles_data(self, reports):\n        \"\"\"Responsibles data to be used in the template\n        \"\"\"\n        if not reports:\n            return []\n\n        recipients = []\n        recipient_names = []\n\n        for num, report in enumerate(reports):\n            # get the linked AR of this ARReport\n            ar = report.getAnalysisRequest()\n\n            # recipient names of this report\n            report_recipient_names = []\n            responsibles = ar.getResponsible()\n            for manager_id in responsibles.get(\"ids\", []):\n                responsible = responsibles[\"dict\"][manager_id]\n                name = responsible.get(\"name\")\n                email = responsible.get(\"email\")\n                record = {\n                    \"name\": name,\n                    \"email\": email,\n                    \"valid\": True,\n                }\n                if record not in recipients:\n                    recipients.append(record)\n                # remember the name of the recipient for this report\n                report_recipient_names.append(name)\n            recipient_names.append(report_recipient_names)\n\n        # recipient names, which all of the reports have in common\n        common_names = set(recipient_names[0]).intersection(*recipient_names)\n        # mark recipients not in common\n        for recipient in recipients:\n            if recipient.get(\"name\") not in common_names:\n                recipient[\"valid\"] = False\n\n        return recipients"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the email address of the current object", "response": "def email_from_address(self):\n        \"\"\"Portal email\n        \"\"\"\n        lab_email = self.laboratory.getEmailAddress()\n        portal_email = self.portal.email_from_address\n        return lab_email or portal_email"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef email_from_name(self):\n        lab_from_name = self.laboratory.getName()\n        portal_from_name = self.portal.email_from_name\n        return lab_from_name or portal_from_name", "response": "Returns the laboratory email name from the name of the portal"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_total_size(self, *files):\n\n        # Recursive unpack an eventual list of lists\n        def iterate(item):\n            if isinstance(item, (list, tuple)):\n                for i in item:\n                    for ii in iterate(i):\n                        yield ii\n            else:\n                yield item\n\n        # Calculate the total size of the given objects starting with an\n        # initial size of 0\n        return reduce(lambda x, y: x + y,\n                      map(self.get_filesize, iterate(files)), 0)", "response": "Calculate the total size of the given files"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the objects from the UIDs given in the request", "response": "def get_reports(self):\n        \"\"\"Return the objects from the UIDs given in the request\n        \"\"\"\n        # Create a mapping of source ARs for copy\n        uids = self.request.form.get(\"uids\", [])\n        # handle 'uids' GET parameter coming from a redirect\n        if isinstance(uids, basestring):\n            uids = uids.split(\",\")\n        uids = filter(api.is_uid, uids)\n        unique_uids = OrderedDict().fromkeys(uids).keys()\n        return map(self.get_object_by_uid, unique_uids)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_attachments(self):\n        # Create a mapping of source ARs for copy\n        uids = self.request.form.get(\"attachment_uids\", [])\n        return map(self.get_object_by_uid, uids)", "response": "Return the objects from the UIDs given in the request"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the object by UID.", "response": "def get_object_by_uid(self, uid):\n        \"\"\"Get the object by UID\n        \"\"\"\n        logger.debug(\"get_object_by_uid::UID={}\".format(uid))\n        obj = api.get_object_by_uid(uid, None)\n        if obj is None:\n            logger.warn(\"!! No object found for UID #{} !!\")\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_filesize(self, f):\n        try:\n            filesize = float(f.get_size())\n            return float(\"%.2f\" % (filesize / 1024))\n        except (POSKeyError, TypeError, AttributeError):\n            return 0.0", "response": "Return the filesize of the PDF as a float"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the AR recipients in the same format like the AR Report expects in the records field Recipients", "response": "def get_recipients(self, ar):\n        \"\"\"Return the AR recipients in the same format like the AR Report\n        expects in the records field `Recipients`\n        \"\"\"\n        plone_utils = api.get_tool(\"plone_utils\")\n\n        def is_email(email):\n            if not plone_utils.validateSingleEmailAddress(email):\n                return False\n            return True\n\n        def recipient_from_contact(contact):\n            if not contact:\n                return None\n            email = contact.getEmailAddress()\n            return {\n                \"UID\": api.get_uid(contact),\n                \"Username\": contact.getUsername(),\n                \"Fullname\": to_utf8(contact.Title()),\n                \"EmailAddress\": email,\n            }\n\n        def recipient_from_email(email):\n            if not is_email(email):\n                return None\n            return {\n                \"UID\": \"\",\n                \"Username\": \"\",\n                \"Fullname\": email,\n                \"EmailAddress\": email,\n            }\n\n        # Primary Contacts\n        to = filter(None, [recipient_from_contact(ar.getContact())])\n        # CC Contacts\n        cc = filter(None, map(recipient_from_contact, ar.getCCContact()))\n        # CC Emails\n        cc_emails = map(lambda x: x.strip(), ar.getCCEmails().split(\",\"))\n        cc_emails = filter(None, map(recipient_from_email, cc_emails))\n\n        return to + cc + cc_emails"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ajax_recalculate_size(self):\n        reports = self.get_reports()\n        attachments = self.get_attachments()\n        total_size = self.get_total_size(reports, attachments)\n\n        return {\n            \"files\": len(reports) + len(attachments),\n            \"size\": \"%.2f\" % total_size,\n            \"limit\": self.max_email_size,\n            \"limit_exceeded\": total_size > self.max_email_size,\n        }", "response": "Recalculate the total size of the selected reports and attachments"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_action_for(self, context, request):\n        savepoint = transaction.savepoint()\n        workflow = getToolByName(context, 'portal_workflow')\n        uc = getToolByName(context, 'uid_catalog')\n\n        action = request.get('action', '')\n        if not action:\n            raise BadRequest(\"No action specified in request\")\n\n        ret = {\n            \"url\": router.url_for(\"doActionFor\", force_external=True),\n            \"success\": True,\n            \"error\": False,\n        }\n\n        data = read(context, request)\n        objects = data.get('objects', [])\n        if len(objects) == 0:\n            raise BadRequest(\"No matching objects found\")\n        for obj_dict in objects:\n            try:\n                obj = uc(UID=obj_dict['UID'])[0].getObject()\n                workflow.doActionFor(obj, action)\n                obj.reindexObject()\n            except Exception as e:\n                savepoint.rollback()\n                msg = \"Cannot execute '{0}' on {1} ({2})\".format(\n                    action, obj, e.message)\n                msg = msg.replace(\"${action_id}\", action)\n                raise BadRequest(msg)\n        return ret", "response": "This method performs the workflow transition on the values returned by the read API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_action_for_many(self, context, request):\n        savepoint = transaction.savepoint()\n        workflow = getToolByName(context, 'portal_workflow')\n        site_path = request['PATH_INFO'].replace(\"/@@API/doActionFor_many\", \"\")\n\n        obj_paths = json.loads(request.get('f', '[]'))\n        action = request.get('action', '')\n        if not action:\n            raise BadRequest(\"No action specified in request\")\n\n        ret = {\n            \"url\": router.url_for(\"doActionFor_many\", force_external=True),\n            \"success\": True,\n            \"error\": False,\n        }\n\n        for obj_path in obj_paths:\n            if not obj_path.startswith(\"/\"):\n                obj_path = \"/\" + obj_path\n            obj = context.restrictedTraverse(str(site_path + obj_path))\n            if obj_path.startswith(site_path):\n                obj_path = obj_path[len(site_path):]\n            try:\n                workflow.doActionFor(obj, action)\n                obj.reindexObject()\n            except Exception as e:\n                savepoint.rollback()\n                msg = \"Cannot execute '{0}' on {1} ({2})\".format(\n                    action, obj, e.message)\n                msg = msg.replace(\"${action_id}\", action)\n                raise BadRequest(msg)\n        return ret", "response": "This method handles the action for many objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns whether the current context from the request is a Worksheet", "response": "def is_worksheet_context():\n    \"\"\"Returns whether the current context from the request is a Worksheet\n    \"\"\"\n    request = api.get_request()\n    parents = request.get(\"PARENTS\", [])\n    portal_types_names = map(lambda p: getattr(p, \"portal_type\", None), parents)\n    if \"Worksheet\" in portal_types_names:\n        return True\n\n    # Check if the worksheet is declared in request explicitly\n    ws_uid = request.get(\"ws_uid\", \"\")\n    obj = api.get_object_by_uid(ws_uid, None)\n    if IWorksheet.providedBy(obj):\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef guard_assign(analysis):\n    # Only if the request was done from worksheet context.\n    if not is_worksheet_context():\n        return False\n\n    # Cannot assign if the Sample has not been received\n    if not analysis.isSampleReceived():\n        return False\n\n    # Cannot assign if the analysis has a worksheet assigned already\n    if analysis.getWorksheet():\n        return False\n\n    # Cannot assign if user does not have permissions to manage worksheets\n    return user_can_manage_worksheets()", "response": "Return whether the transition assign can be performed or not\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning whether the transition submit can be performed or not", "response": "def guard_submit(analysis):\n    \"\"\"Return whether the transition \"submit\" can be performed or not\n    \"\"\"\n    # Cannot submit without a result\n    if not analysis.getResult():\n        return False\n\n    # Cannot submit with interims without value\n    for interim in analysis.getInterimFields():\n        if not interim.get(\"value\", \"\"):\n            return False\n\n    # Cannot submit if attachment not set, but is required\n    if not analysis.getAttachment():\n        if analysis.getAttachmentOption() == 'r':\n            return False\n\n    # Check if can submit based on the Analysis Request state\n    if IRequestAnalysis.providedBy(analysis):\n        point_of_capture = analysis.getPointOfCapture()\n        # Cannot submit if the Sample has not been received\n        if point_of_capture == \"lab\" and not analysis.isSampleReceived():\n            return False\n        # Cannot submit if the Sample has not been sampled\n        if point_of_capture == \"field\" and not analysis.isSampleSampled():\n            return False\n\n    # Check if the current user can submit if is not assigned\n    if not analysis.bika_setup.getAllowToSubmitNotAssigned():\n        if not user_has_super_roles():\n            # Cannot submit if unassigned\n            if not analysis.getAnalyst():\n                return False\n            # Cannot submit if assigned analyst is not the current user\n            if analysis.getAnalyst() != api.get_current_user().getId():\n                return False\n\n    # Cannot submit unless all dependencies are submitted or can be submitted\n    for dependency in analysis.getDependencies():\n        if not is_submitted_or_submittable(dependency):\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef guard_multi_verify(analysis):\n    # Cannot multiverify if there is only one remaining verification\n    remaining_verifications = analysis.getNumberOfRemainingVerifications()\n    if remaining_verifications <= 1:\n        return False\n\n    # Cannot verify if the user submitted and self-verification is not allowed\n    if was_submitted_by_current_user(analysis):\n        if not analysis.isSelfVerificationEnabled():\n            return False\n\n    # Cannot verify if the user verified and multi verification is not allowed\n    if was_verified_by_current_user(analysis):\n        if not is_multi_verification_allowed(analysis):\n            return False\n\n    # Cannot verify if the user was last verifier and consecutive verification\n    # by same user is not allowed\n    if current_user_was_last_verifier(analysis):\n        if not is_consecutive_multi_verification_allowed(analysis):\n            return False\n\n    # Cannot verify unless all dependencies are verified or can be verified\n    for dependency in analysis.getDependencies():\n        if not is_verified_or_verifiable(dependency):\n            return False\n    return True", "response": "Return whether the transition multi_verify can be performed or not."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning whether the transition retract can be performed or not", "response": "def guard_retract(analysis):\n    \"\"\" Return whether the transition \"retract\" can be performed or not\n    \"\"\"\n    # Cannot retract if there are dependents that cannot be retracted\n    if not is_transition_allowed(analysis.getDependents(), \"retract\"):\n        return False\n\n    dependencies = analysis.getDependencies()\n    if not dependencies:\n        return True\n\n    # Cannot retract if all dependencies have been verified\n    if all(map(lambda an: IVerified.providedBy(an), dependencies)):\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef user_has_super_roles():\n    member = api.get_current_user()\n    super_roles = [\"LabManager\", \"Manager\"]\n    diff = filter(lambda role: role in super_roles, member.getRoles())\n    return len(diff) > 0", "response": "Return whether the current user belongs to superuser roles\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning whether the current user was the last verifier or not", "response": "def current_user_was_last_verifier(analysis):\n    \"\"\"Returns whether the current user was the last verifier or not\n    \"\"\"\n    verifiers = analysis.getVerificators()\n    return verifiers and verifiers[:-1] == api.get_current_user().getId()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning whether all analyses can be transitioned or not", "response": "def is_transition_allowed(analyses, transition_id):\n    \"\"\"Returns whether all analyses can be transitioned or not\n    \"\"\"\n    if not analyses:\n        return True\n    if not isinstance(analyses, list):\n        return is_transition_allowed([analyses], transition_id)\n    for analysis in analyses:\n        if not wf.isTransitionAllowed(analysis, transition_id):\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_submitted_or_submittable(analysis):\n    if ISubmitted.providedBy(analysis):\n        return True\n    if wf.isTransitionAllowed(analysis, \"submit\"):\n        return True\n    return False", "response": "Returns whether the analysis is submitted or has already been submitted"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning whether the analysis is verified or has already been verified", "response": "def is_verified_or_verifiable(analysis):\n    \"\"\"Returns whether the analysis is verifiable or has already been verified\n    \"\"\"\n    if IVerified.providedBy(analysis):\n        return True\n    if wf.isTransitionAllowed(analysis, \"verify\"):\n        return True\n    if wf.isTransitionAllowed(analysis, \"multi_verify\"):\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef isValidationInProgress(self):\n        today = DateTime()\n        down_from = self.getDownFrom()\n        down_to = self.getDownTo()\n\n        return down_from <= today <= down_to", "response": "Checks if the date is beteween a validation period"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getRemainingDaysInValidation(self):\n        delta = 0\n        today = DateTime()\n        down_from = self.getDownFrom() or today\n        down_to = self.getDownTo()\n\n        # one of the fields is not set, return 0 days\n        if not down_from or not down_to:\n            return 0\n        # down_from comes after down_to?\n        if down_from > down_to:\n            return 0\n        # calculate the time between today and down_to, even if down_from\n        # is in the future.\n        else:\n            delta = down_to - today\n\n        return int(math.ceil(delta))", "response": "Returns the days until the instrument returns from validation\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the UIDs of all the ancestors of the analysis request", "response": "def getAncestorsUIDs(instance):\n    \"\"\"Returns the UIDs of all the ancestors (Analysis Requests) this analysis\n    comes from\n    \"\"\"\n    request = instance.getRequest()\n    parents = map(lambda ar: api.get_uid(ar), request.getAncestors())\n    return [api.get_uid(request)] + parents"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hasUser(self):\n        username = self.getUsername()\n        if not username:\n            return False\n        user = api.get_user(username)\n        return user is not None", "response": "Check if contact has user"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _departmentsVoc(self):\n        query = {\n            \"portal_type\": \"Department\",\n            \"is_active\": True\n        }\n        results = api.search(query, \"bika_setup_catalog\")\n        items = map(lambda dept: (api.get_uid(dept), api.get_title(dept)),\n                    results)\n        dept_uids = map(api.get_uid, results)\n        # Currently assigned departments\n        depts = self.getDepartments()\n        # If one department assigned to the Lab Contact is disabled, it will\n        # be shown in the list until the department has been unassigned.\n        for dept in depts:\n            uid = api.get_uid(dept)\n            if uid in dept_uids:\n                continue\n            items.append((uid, api.get_title(dept)))\n\n        return api.to_display_list(items, sort_by=\"value\", allow_empty=False)", "response": "Vocabulary of available departments"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of the default items in the site.", "response": "def _defaultDepsVoc(self):\n        \"\"\"Vocabulary of all departments\n        \"\"\"\n        # Getting the assigned departments\n        deps = self.getDepartments()\n        items = []\n        for d in deps:\n            items.append((api.get_uid(d), api.get_title(d)))\n        return api.to_display_list(items, sort_by=\"value\", allow_empty=True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a department to the set of department objects", "response": "def addDepartment(self, dep):\n        \"\"\"Adds a department\n\n        :param dep: UID or department object\n        :returns: True when the department was added\n        \"\"\"\n        if api.is_uid(dep):\n            dep = api.get_object_by_uid(dep)\n        deps = self.getDepartments()\n        if dep not in deps:\n            return False\n        deps.append(dep)\n        self.setDepartments(deps)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves a department from the set of department objects", "response": "def removeDepartment(self, dep):\n        \"\"\"Removes a department\n\n        :param dep: UID or department object\n        :returns: True when the department was removed\n        \"\"\"\n        if api.is_uid(dep):\n            dep = api.get_object_by_uid(dep)\n        deps = self.getDepartments()\n        if dep not in deps:\n            return False\n        deps.remove(dep)\n        self.setDepartments(deps)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrename Bika Setup to just Setup", "response": "def rename_bika_setup():\n    \"\"\"\n    Rename Bika Setup to just Setup to avoid naming confusions for new users\n    \"\"\"\n    logger.info(\"Renaming Bika Setup...\")\n    bika_setup = api.get_bika_setup()\n    bika_setup.setTitle(\"Setup\")\n    bika_setup.reindexObject()\n    setup = api.get_portal().portal_setup\n    setup.runImportStepFromProfile('profile-bika.lims:default', 'controlpanel')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the result for the analysis passed in is out of range and if it is in the range.", "response": "def is_out_of_range(brain_or_object, result=_marker):\n    \"\"\"Checks if the result for the analysis passed in is out of range and/or\n    out of shoulders range.\n\n            min                                                   max\n            warn            min                   max             warn\n    \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7|---------------|=====================|---------------|\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n    ----- out-of-range -----><----- in-range ------><----- out-of-range -----\n             <-- shoulder --><----- in-range ------><-- shoulder -->\n\n    :param brain_or_object: A single catalog brain or content object\n    :param result: Tentative result. If None, use the analysis result\n    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain\n    :returns: Tuple of two elements. The first value is `True` if the result is\n    out of range and `False` if it is in range. The second value is `True` if\n    the result is out of shoulder range and `False` if it is in shoulder range\n    :rtype: (bool, bool)\n    \"\"\"\n    analysis = api.get_object(brain_or_object)\n    if not IAnalysis.providedBy(analysis) and \\\n            not IReferenceAnalysis.providedBy(analysis):\n        api.fail(\"{} is not supported. Needs to be IAnalysis or \"\n                 \"IReferenceAnalysis\".format(repr(analysis)))\n\n    if result is _marker:\n        result = api.safe_getattr(analysis, \"getResult\", None)\n    if not api.is_floatable(result):\n        # Result is empty/None or not a valid number\n        return False, False\n\n    result = api.to_float(result)\n\n    # Note that routine analyses, duplicates and reference analyses all them\n    # implement the function getResultRange:\n    # - For routine analyses, the function returns the valid range based on the\n    #   specs assigned during the creation process.\n    # - For duplicates, the valid range is the result of the analysis the\n    #   the duplicate was generated from +/- the duplicate variation.\n    # - For reference analyses, getResultRange returns the valid range as\n    #   indicated in the Reference Sample from which the analysis was created.\n    result_range = api.safe_getattr(analysis, \"getResultsRange\", None)\n    if not result_range:\n        # No result range defined or the passed in object does not suit\n        return False, False\n\n    # Maybe there is a custom adapter\n    adapters = getAdapters((analysis,), IResultOutOfRange)\n    for name, adapter in adapters:\n        ret = adapter(result=result, specification=result_range)\n        if not ret or not ret.get('out_of_range', False):\n            continue\n        if not ret.get('acceptable', True):\n            # Out of range + out of shoulders\n            return True, True\n        # Out of range, but in shoulders\n        return True, False\n\n    result_range = ResultsRangeDict(result_range)\n\n    # The assignment of result as default fallback for min and max guarantees\n    # the result will be in range also if no min/max values are defined\n    specs_min = api.to_float(result_range.min, result)\n    specs_max = api.to_float(result_range.max, result)\n\n    in_range = False\n    min_operator = result_range.min_operator\n    if min_operator == \"geq\":\n        in_range = result >= specs_min\n    else:\n        in_range = result > specs_min\n\n    max_operator = result_range.max_operator\n    if in_range:\n        if max_operator == \"leq\":\n            in_range = result <= specs_max\n        else:\n            in_range = result < specs_max\n\n    # If in range, no need to check shoulders\n    if in_range:\n        return False, False\n\n    # Out of range, check shoulders. If no explicit warn_min or warn_max have\n    # been defined, no shoulders must be considered for this analysis. Thus, use\n    # specs' min and max as default fallback values\n    warn_min = api.to_float(result_range.warn_min, specs_min)\n    warn_max = api.to_float(result_range.warn_max, specs_max)\n    in_shoulder = warn_min <= result <= warn_max\n    return True, not in_shoulder"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_formatted_interval(results_range, default=_marker):\n    if not isinstance(results_range, Mapping):\n        if default is not _marker:\n            return default\n        api.fail(\"Type not supported\")\n    results_range = ResultsRangeDict(results_range)\n    min_str = results_range.min if api.is_floatable(results_range.min) else None\n    max_str = results_range.max if api.is_floatable(results_range.max) else None\n    if min_str is None and max_str is None:\n        if default is not _marker:\n            return default\n        api.fail(\"Min and max values are not floatable or not defined\")\n\n    min_operator = results_range.min_operator\n    max_operator = results_range.max_operator\n    if max_str is None:\n        return \"{}{}\".format(MIN_OPERATORS.getValue(min_operator), min_str)\n    if min_str is None:\n        return \"{}{}\".format(MAX_OPERATORS.getValue(max_operator), max_str)\n\n    # Both values set. Return an interval\n    min_bracket = min_operator == 'geq' and '[' or '('\n    max_bracket = max_operator == 'leq' and ']' or ')'\n\n    return \"{}{};{}{}\".format(min_bracket, min_str, max_str, max_bracket)", "response": "Returns a string representation of the interval defined by the results_range passed in\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if the submit transition can be applied to the worksheet passed in.", "response": "def guard_submit(obj):\n    \"\"\"Returns if 'submit' transition can be applied to the worksheet passed in.\n    By default, the target state for the 'submit' transition for a worksheet is\n    'to_be_verified', so this guard returns true if all the analyses assigned\n    to the worksheet have already been submitted. Those analyses that are in a\n    non-valid state (cancelled, inactive) are dismissed in the evaluation, but\n    at least one analysis must be in an active state (and submitted) for this\n    guard to return True. Otherwise, always returns False.\n    Note this guard depends entirely on the current status of the children.\n    \"\"\"\n    analyses = obj.getAnalyses()\n    if not analyses:\n        # An empty worksheet cannot be submitted\n        return False\n\n    can_submit = False\n    for analysis in obj.getAnalyses():\n        # Dismiss analyses that are not active\n        if not api.is_active(analysis):\n            continue\n        # Dismiss analyses that have been rejected or retracted\n        if api.get_workflow_status_of(analysis) in [\"rejected\", \"retracted\"]:\n            continue\n        # Worksheet cannot be submitted if there is one analysis not submitted\n        can_submit = ISubmitted.providedBy(analysis)\n        if not can_submit:\n            # No need to look further\n            return False\n\n    # This prevents the submission of the worksheet if all its analyses are in\n    # a detached status (rejected, retracted or cancelled)\n    return can_submit"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef guard_retract(worksheet):\n    analyses = worksheet.getAnalyses()\n    detached = ['rejected', 'retracted']\n    num_detached = 0\n    for analysis in analyses:\n        if api.get_workflow_status_of(analysis) in detached:\n            num_detached += 1\n        elif not isTransitionAllowed(analysis, \"retract\"):\n            return False\n    return analyses and num_detached < len(analyses) or False", "response": "Returns whether the transition retract can be performed or not to the\n    passed in."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if the verify transition can be applied to the Worksheet passed in.", "response": "def guard_verify(obj):\n    \"\"\"Returns True if 'verify' transition can be applied to the Worksheet\n    passed in. This is, returns true if all the analyses assigned\n    have already been verified. Those analyses that are in an inactive state\n    (cancelled, inactive) are dismissed, but at least one analysis must be in\n    an active state (and verified), otherwise always return False.\n    Note this guard depends entirely on the current status of the children\n    :returns: true or false\n    \"\"\"\n\n    analyses = obj.getAnalyses()\n    if not analyses:\n        # An empty worksheet cannot be verified\n        return False\n\n    can_verify = False\n    for analysis in obj.getAnalyses():\n        # Dismiss analyses that are not active\n        if not api.is_active(analysis):\n            continue\n        # Dismiss analyses that have been rejected or retracted\n        if api.get_workflow_status_of(analysis) in [\"rejected\", \"retracted\"]:\n            continue\n        # Worksheet cannot be verified if there is one analysis not verified\n        can_verify = IVerified.providedBy(analysis)\n        if not can_verify:\n            # No need to look further\n            return False\n\n    # This prevents the verification of the worksheet if all its analyses are in\n    # a detached status (rejected, retracted or cancelled)\n    return can_verify"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef guard_rollback_to_open(worksheet):\n    for analysis in worksheet.getAnalyses():\n        if api.get_review_status(analysis) in [\"assigned\"]:\n            return True\n    return False", "response": "Return whether rollback_to_receive transition can be performed or not"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef folderitem(self, obj, item, index):\n        title = obj.Title()\n        description = obj.Description()\n        url = obj.absolute_url()\n\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n        item[\"Description\"] = description\n\n        manager = obj.getManager()\n        manager_name = obj.getManagerName()\n        manager_phone = obj.getManagerPhone()\n        manager_email = obj.getManagerEmail()\n\n        if manager:\n            item[\"Manager\"] = manager_name\n            item[\"replace\"][\"Manager\"] = get_link(\n                manager.absolute_url(), manager_name)\n        else:\n            item[\"Manager\"] = \"\"\n\n        if manager_email:\n            item[\"ManagerEmail\"] = manager_email\n            item[\"replace\"][\"ManagerEmail\"] = get_email_link(\n                manager_email, value=manager_email)\n        else:\n            item[\"ManagerEmail\"] = \"\"\n\n        item[\"ManagerPhone\"] = manager_phone\n\n        return item", "response": "This method is used to add the properties of the object to the item dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getContainers(self):\r\n        _containers = []\r\n        for container in self.bika_setup.bika_containers.objectValues():\r\n            containertype = container.getContainerType()\r\n            if containertype and containertype.UID() == self.UID():\r\n                _containers.append(container)\r\n        return _containers", "response": "Returns a list of all containers of this type"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the Dependent Services from the text of the calculation Formula", "response": "def setFormula(self, Formula=None):\n        \"\"\"Set the Dependent Services from the text of the calculation Formula\n        \"\"\"\n        bsc = getToolByName(self, 'bika_setup_catalog')\n        if Formula is None:\n            self.setDependentServices(None)\n            self.getField('Formula').set(self, Formula)\n        else:\n            keywords = re.compile(r\"\\[([^.^\\]]+)\\]\").findall(Formula)\n            brains = bsc(portal_type='AnalysisService',\n                         getKeyword=keywords)\n            services = [brain.getObject() for brain in brains]\n            self.getField('DependentServices').set(self, services)\n            self.getField('Formula').set(self, Formula)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a flat list of services who depend on this calculation.", "response": "def getCalculationDependants(self, deps=None):\n        \"\"\"Return a flat list of services who depend on this calculation.\n\n        This refers only to services who's Calculation UIDReferenceField have\n        the value set to point to this calculation.\n\n        It has nothing to do with the services referenced in the calculation's\n        Formula.\n        \"\"\"\n        if deps is None:\n            deps = []\n        backrefs = get_backreferences(self, 'AnalysisServiceCalculation')\n        services = map(get_object_by_uid, backrefs)\n        for service in services:\n            calc = service.getCalculation()\n            if calc and calc.UID() != self.UID():\n                calc.getCalculationDependants(deps)\n            deps.append(service)\n        return deps"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the globals dictionary for the formula calculation", "response": "def _getGlobals(self, **kwargs):\n        \"\"\"Return the globals dictionary for the formula calculation\n        \"\"\"\n        # Default globals\n        globs = {\n            \"__builtins__\": None,\n            \"all\": all,\n            \"any\": any,\n            \"bool\": bool,\n            \"chr\": chr,\n            \"cmp\": cmp,\n            \"complex\": complex,\n            \"divmod\": divmod,\n            \"enumerate\": enumerate,\n            \"float\": float,\n            \"format\": format,\n            \"frozenset\": frozenset,\n            \"hex\": hex,\n            \"int\": int,\n            \"len\": len,\n            \"list\": list,\n            \"long\": long,\n            \"math\": math,\n            \"max\": max,\n            \"min\": min,\n            \"oct\": oct,\n            \"ord\": ord,\n            \"pow\": pow,\n            \"range\": range,\n            \"reversed\": reversed,\n            \"round\": round,\n            \"str\": str,\n            \"sum\": sum,\n            \"tuple\": tuple,\n            \"xrange\": xrange,\n        }\n        # Update with keyword arguments\n        globs.update(kwargs)\n        # Update with additional Python libraries\n        for imp in self.getPythonImports():\n            mod = imp[\"module\"]\n            func = imp[\"function\"]\n            member = self._getModuleMember(mod, func)\n            if member is None:\n                raise ImportError(\n                    \"Could not find member {} of module {}\".format(\n                        func, mod))\n            globs[func] = member\n        return globs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the member object of a module.", "response": "def _getModuleMember(self, dotted_name, member):\n        \"\"\"Get the member object of a module.\n\n        :param dotted_name: The dotted name of the module, e.g. 'scipy.special'\n        :type dotted_name: string\n        :param member: The name of the member function, e.g. 'gammaincinv'\n        :type member: string\n        :returns: member object or None\n        \"\"\"\n        try:\n            mod = importlib.import_module(dotted_name)\n        except ImportError:\n            return None\n\n        members = dict(inspect.getmembers(mod))\n        return members.get(member)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef after_unassign(reference_analysis):\n    analysis_events.after_unassign(reference_analysis)\n    ref_sample = reference_analysis.aq_parent\n    ref_sample.manage_delObjects([reference_analysis.getId()])", "response": "Removes the reference analysis from the system\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, context, request):\n        savepoint = transaction.savepoint()\n        self.context = context\n        self.request = request\n        self.unused = [x for x in self.request.form.keys()]\n        self.used(\"form.submitted\")\n        self.used(\"__ac_name\")\n        self.used(\"__ac_password\")\n        ret = {\n            \"url\": router.url_for(\"update\", force_external=True),\n            \"success\": False,\n            \"error\": True,\n        }\n        obj = None\n        # Find out if the object can be retrieved via UID or via\n        # traversing.\n        if self.request.get('obj_uid', ''):\n            uc = getToolByName(self.context, 'uid_catalog')\n            brain = uc(UID=self.request.get('obj_uid', ''))\n            obj = brain[0].getObject() if brain else None\n        if self.request.get('obj_path', '') and not obj:\n            obj_path = self.request['obj_path'].split(\"?\")[0]\n            site_path = context.portal_url.getPortalObject().getPhysicalPath()\n            if site_path and isinstance(site_path, basestring):\n                site_path = site_path if site_path.startswith('/') else '/' + site_path\n                obj = context.restrictedTraverse(site_path + obj_path)\n            elif site_path and len(site_path) > 1:\n                site_path = site_path[1]\n                site_path = site_path if site_path.startswith('/') else '/' + site_path\n                obj = context.restrictedTraverse(site_path + obj_path)\n\n        if obj:\n            self.used('obj_uid')\n            self.used('obj_path')\n        else:\n            ret['success'] = False\n            ret['error'] = True\n            return ret\n\n        try:\n            fields = set_fields_from_request(obj, request)\n            if not fields:\n                return ret\n            for field in fields:\n                self.used(field)\n        except:\n            savepoint.rollback()\n            raise\n\n        ret['success'] = True\n        ret['error'] = False\n\n        if self.unused:\n            raise BadRequest(\"The following request fields were not used: %s.  Request aborted.\" % self.unused)\n\n        return ret", "response": "Update the object s properties"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the field is required", "response": "def require(self, fieldname, allow_blank=False):\n        \"\"\"fieldname is required\"\"\"\n        if self.request.form and fieldname not in self.request.form.keys():\n            raise Exception(\"Required field not found in request: %s\" % fieldname)\n        if self.request.form and (not self.request.form[fieldname] or allow_blank):\n            raise Exception(\"Required field %s may not have blank value\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove from list of unused fields", "response": "def used(self, fieldname):\n        \"\"\"fieldname is used, remove from list of unused fields\"\"\"\n        if fieldname in self.unused:\n            self.unused.remove(fieldname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_many(self, context, request):\n        self.context = context\n        self.request = request\n        self.unused = [x for x in self.request.form.keys()]\n        self.used(\"form.submitted\")\n        self.used(\"__ac_name\")\n        self.used(\"__ac_password\")\n        ret = {\n            \"url\": router.url_for(\"update_many\", force_external=True),\n            \"success\": False,\n            \"error\": True,\n            \"updates\": [],\n        }\n\n        input_values = json.loads(request.get('input_values', '[]'))\n        if not input_values:\n            raise BadRequest(\"missing input_values\")\n        site_path = request['PATH_INFO'].replace(\"/@@API/update_many\", \"\")\n\n        for obj_path, i in input_values.items():\n            savepoint = transaction.savepoint()\n            if not obj_path.startswith(\"/\"):\n                obj_path = \"/\" + obj_path\n            if obj_path.startswith(site_path):\n                obj_path = obj_path[len(site_path):]\n            obj = context.restrictedTraverse(str(site_path + obj_path))\n            this_ret = {\n                \"url\": router.url_for(\"update_many\", force_external=True),\n                \"success\": False,\n                \"error\": True,\n            }\n            try:\n                fields = set_fields_from_request(obj, i)\n                if not fields:\n                    this_ret['success'] = False\n                    this_ret['error'] = True\n                else:\n                    this_ret['success'] = True\n                    this_ret['error'] = False\n            except:\n                savepoint.rollback()\n                raise\n            ret['updates'].append(this_ret)\n        ret['success'] = True\n        ret['error'] = False\n        return ret", "response": "This method is used to update the values of a specific object in a specific context."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nservice triggered each time an item is iterated in folderitems. The use of this service prevents the extra-loops in child objects. :obj: the instance of the class to be foldered :item: dict containing the properties of the object to be used by the template :index: current index of the item", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Service triggered each time an item is iterated in folderitems.\n\n        The use of this service prevents the extra-loops in child objects.\n\n        :obj: the instance of the class to be foldered\n        :item: dict containing the properties of the object to be used by\n            the template\n        :index: current index of the item\n        \"\"\"\n        item = super(ReferenceAnalysesView, self).folderitem(obj, item, index)\n\n        if not item:\n            return None\n        item[\"Category\"] = obj.getCategoryTitle\n        ref_analysis = api.get_object(obj)\n        ws = ref_analysis.getWorksheet()\n        if not ws:\n            logger.warn(\n                \"No Worksheet found for ReferenceAnalysis {}\"\n                .format(obj.getId))\n        else:\n            item[\"Worksheet\"] = ws.Title()\n            anchor = \"<a href='%s'>%s</a>\" % (ws.absolute_url(), ws.Title())\n            item[\"replace\"][\"Worksheet\"] = anchor\n\n        # Add the analysis to the QC Chart\n        self.chart.add_analysis(obj)\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a mapping of Analysis Service -> Reference Results", "response": "def get_reference_results(self):\n        \"\"\"Return a mapping of Analysis Service -> Reference Results\n        \"\"\"\n        referenceresults = self.context.getReferenceResults()\n        return dict(map(lambda rr: (rr.get(\"uid\"), rr), referenceresults))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nservice triggered each time an item is iterated in folderitems. The use of this service prevents the extra-loops in child objects. :obj: the instance of the class to be foldered :item: dict containing the properties of the object to be used by the template :index: current index of the item", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Service triggered each time an item is iterated in folderitems.\n\n        The use of this service prevents the extra-loops in child objects.\n\n        :obj: the instance of the class to be foldered\n        :item: dict containing the properties of the object to be used by\n            the template\n        :index: current index of the item\n        \"\"\"\n\n        uid = api.get_uid(obj)\n        url = api.get_url(obj)\n        title = api.get_title(obj)\n\n        # get the category\n        if self.show_categories_enabled():\n            category = obj.getCategoryTitle()\n            if category not in self.categories:\n                self.categories.append(category)\n            item[\"category\"] = category\n\n        ref_results = self.get_reference_results()\n        ref_result = ref_results.get(uid)\n\n        item[\"Title\"] = title\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n        item[\"result\"] = ref_result.get(\"result\")\n        item[\"min\"] = ref_result.get(\"min\")\n        item[\"max\"] = ref_result.get(\"max\")\n\n        # Icons\n        after_icons = \"\"\n        if obj.getAccredited():\n            after_icons += get_image(\n                \"accredited.png\", title=_(\"Accredited\"))\n        if obj.getAttachmentOption() == \"r\":\n            after_icons += get_image(\n                \"attach_reqd.png\", title=_(\"Attachment required\"))\n        if obj.getAttachmentOption() == \"n\":\n            after_icons += get_image(\n                \"attach_no.png\", title=_(\"Attachment not permitted\"))\n        if after_icons:\n            item[\"after\"][\"Title\"] = after_icons\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies new properties to the item that is currently being rendered as a row in the list", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Applies new properties to the item (Client) that is currently being\n        rendered as a row in the list\n\n        :param obj: client to be rendered as a row in the list\n        :param item: dict representation of the client, suitable for the list\n        :param index: current position of the item within the list\n        :type obj: ATContentType/DexterityContentType\n        :type item: dict\n        :type index: int\n        :return: the dict representation of the item\n        :rtype: dict\n        \"\"\"\n\n        # XXX Refactor expiration to a proper place\n        # ---------------------------- 8< -------------------------------------\n        if item.get(\"review_state\", \"current\") == \"current\":\n            # Check expiry date\n            exdate = obj.getExpiryDate()\n            if exdate:\n                expirydate = DT2dt(exdate).replace(tzinfo=None)\n                if (datetime.today() > expirydate):\n                    # Trigger expiration\n                    self.workflow.doActionFor(obj, \"expire\")\n                    item[\"review_state\"] = \"expired\"\n                    item[\"obj\"] = obj\n\n        if self.contentFilter.get('review_state', '') \\\n           and item.get('review_state', '') == 'expired':\n            # This item must be omitted from the list\n            return None\n        # ---------------------------- >8 -------------------------------------\n\n        url = api.get_url(obj)\n        id = api.get_id(obj)\n\n        item[\"ID\"] = id\n        item[\"replace\"][\"ID\"] = get_link(url, value=id)\n        item[\"DateSampled\"] = self.ulocalized_time(\n            obj.getDateSampled(), long_format=True)\n        item[\"DateReceived\"] = self.ulocalized_time(obj.getDateReceived())\n        item[\"DateOpened\"] = self.ulocalized_time(obj.getDateOpened())\n        item[\"ExpiryDate\"] = self.ulocalized_time(obj.getExpiryDate())\n\n        # Icons\n        after_icons = ''\n        if obj.getBlank():\n            after_icons += get_image(\n                \"blank.png\", title=t(_(\"Blank\")))\n        if obj.getHazardous():\n            after_icons += get_image(\n                \"hazardous.png\", title=t(_(\"Hazardous\")))\n        if after_icons:\n            item[\"after\"][\"ID\"] = after_icons\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning all available instrument interfaces as a list of tuples. Each tuple is ( id_interface adapter )", "response": "def get_instrument_interfaces():\n    \"\"\"Returns all available instrument interfaces as a list of tuples. Each\n    tuple is (id_interface, adapter)\n    \"\"\"\n    interfaces = list()\n    for name, adapter in getAdapters((api.get_portal(),), IInstrumentInterface):\n        # We need a unique identifier for this instrument interface\n        id = \"{}.{}\".format(adapter.__module__, adapter.__class__.__name__)\n        interfaces.append((id, adapter))\n\n    # TODO Remove the following code once clasic instrument interfaces migrated\n    # Now grab the information (the old way)\n    curr_module = sys.modules[__name__]\n    for name, obj in inspect.getmembers(curr_module):\n        if hasattr(obj, '__name__'):\n            obj_name = obj.__name__.replace(__name__, \"\")\n            obj_name = obj_name and obj_name[1:] or \"\"\n            if obj_name in __all__:\n                interfaces.append((obj_name, obj))\n    return interfaces"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_import_interface(instrument_interface):\n    if IInstrumentImportInterface.providedBy(instrument_interface):\n        return True\n\n    # TODO Remove this once classic instrument interface migrated\n    if hasattr(instrument_interface, '__name__'):\n        obj_name = instrument_interface.__name__.replace(__name__, \"\")\n        if obj_name[1:] in __all__ and hasattr(instrument_interface, \"Import\"):\n            return True\n    return False", "response": "Returns whether the instrument interface passed in is for results import\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn whether the instrument interface passed in is for results export", "response": "def is_export_interface(instrument_interface):\n    \"\"\"Returns whether the instrument interface passed in is for results export\n    \"\"\"\n    if IInstrumentExportInterface.providedBy(instrument_interface):\n        return True\n\n    # TODO Remove this once classic instrument interface migrated\n    if hasattr(instrument_interface, '__name__'):\n        obj_name = instrument_interface.__name__.replace(__name__, \"\")\n        if obj_name[1:] in __all__ and hasattr(instrument_interface, \"Export\"):\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getExim(exim_id):\n    interfaces = filter(lambda i: i[0]==exim_id, get_instrument_interfaces())\n    return interfaces and interfaces[0][1] or None", "response": "Returns the instrument interface for the given exim_id passed in\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the parser to be used by default for the instrument id exim_id and infile passed in.", "response": "def get_automatic_parser(exim_id, infile):\n    \"\"\"Returns the parser to be used by default for the instrument id interface\n    and results file passed in.\n    \"\"\"\n    adapter = getExim(exim_id)\n    if IInstrumentAutoImportInterface.providedBy(adapter):\n        return adapter.get_automatic_parser(infile)\n\n    # TODO Remove this once classic instrument interface migrated\n    parser_func = filter(lambda i: i[0] == exim_id, PARSERS)\n    parser_func = parser_func and parser_func[0][1] or None\n    if not parser_func or not hasattr(adapter, parser_func):\n        return None\n    parser_func = getattr(adapter, parser_func)\n    return parser_func(infile)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling submission of a new object.", "response": "def handle_submit(self):\n        \"\"\"Handle form submission\n        \"\"\"\n        form = self.request.form\n        # Selected service UIDs\n        uids = form.get(\"uids\")\n        # reference sample -> selected services mapping\n        supported_services = form.get(\"SupportedServices\")\n        # service -> position mapping\n        positions = form.get(\"Position\")[0]\n        for uid in uids:\n            position = positions.get(uid)\n            if position == \"new\":\n                position = None\n            service_uids = supported_services.get(uid)\n            referencesample = api.get_object_by_uid(uid)\n            self.context.addReferenceAnalyses(\n                referencesample, service_uids, slot=position)\n        redirect_url = \"{}/{}\".format(\n            api.get_url(self.context), \"manage_results\")\n        self.request.response.redirect(redirect_url)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if edit is allowed", "response": "def is_edit_allowed(self):\n        \"\"\"Check if edit is allowed\n        \"\"\"\n        checkPermission = self.context.portal_membership.checkPermission\n        return checkPermission(EditWorksheet, self.context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_manage_allowed(self):\n        checkPermission = self.context.portal_membership.checkPermission\n        return checkPermission(ManageWorksheets, self.context)", "response": "Check if manage is allowed"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_assigned_services(self):\n        analyses = self.context.getAnalyses()\n        routine_analyses = filter(\n            lambda an: IRoutineAnalysis.providedBy(an), analyses)\n        services = map(lambda an: an.getAnalysisService(), routine_analyses)\n        return services", "response": "Get the current assigned services of this worksheet"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the current assigned services UIDs of this Worksheet", "response": "def get_assigned_services_uids(self):\n        \"\"\"Get the current assigned services UIDs of this Worksheet\n        \"\"\"\n        services = self.get_assigned_services()\n        uids = map(api.get_uid, services)\n        return list(set(uids))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_supported_services_uids(self, referencesample):\n        uids = referencesample.getSupportedServices(only_uids=True)\n        return list(set(uids))", "response": "Get the list of UIDs of the supported services of the reference sample"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_supported_services_choices(self, referencesample):\n        choices = []\n        assigned_services = self.get_assigned_services_uids()\n        for uid in self.get_supported_services_uids(referencesample):\n            service = api.get_object(uid)\n            title = api.get_title(service)\n            selected = uid in assigned_services\n            choices.append({\n                \"ResultValue\": uid,\n                \"ResultText\": title,\n                \"selected\": selected,\n            })\n        return choices", "response": "Create choices for supported services"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate choices for available positions", "response": "def make_position_choices(self):\n        \"\"\"Create choices for available positions\n        \"\"\"\n        choices = []\n        for pos in self.get_available_positions():\n            choices.append({\n                \"ResultValue\": pos,\n                \"ResultText\": pos,\n            })\n        return choices"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_available_positions(self):\n        available_positions = [\"new\"]\n        layout = self.context.getLayout()\n        used_positions = [int(slot[\"position\"]) for slot in layout]\n        if used_positions:\n            used = [\n                pos for pos in range(1, max(used_positions) + 1) if\n                pos not in used_positions]\n            available_positions.extend(used)\n        return available_positions", "response": "Return a list of empty slot numbers"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef folderitem(self, obj, item, index):\n        item = super(ReferenceSamplesView, self).folderitem(obj, item, index)\n\n        # ensure we have an object and not a brain\n        obj = api.get_object(obj)\n        url = api.get_url(obj)\n        title = api.get_title(obj)\n\n        item[\"Title\"] = title\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n        item[\"allow_edit\"] = self.get_editable_columns()\n\n        # Supported Services\n        supported_services_choices = self.make_supported_services_choices(obj)\n        item[\"choices\"][\"SupportedServices\"] = supported_services_choices\n\n        # Position\n        item[\"Position\"] = \"new\"\n        item[\"choices\"][\"Position\"] = self.make_position_choices()\n\n        return item", "response": "This method is called by the base class to add the properties of the object to the item dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef isItemAllowed(self, obj):\n        uid = api.get_uid(obj)\n        if not self.get_item_slot(uid):\n            logger.warning(\"Slot not assigned to item %s\" % uid)\n            return False\n        return BaseView.isItemAllowed(self, obj)", "response": "Returns True if the current analysis to be rendered is a row in the current layout. Otherwise False."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies new properties to the item that is currently being rendered as a row in the list.", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Applies new properties to the item (analysis) that is currently\n        being rendered as a row in the list.\n\n        :param obj: analysis to be rendered as a row in the list\n        :param item: dict representation of the analysis, suitable for the list\n        :param index: current position of the item within the list\n        :type obj: ATContentType/DexterityContentType\n        :type item: dict\n        :type index: int\n        :return: the dict representation of the item\n        :rtype: dict\n        \"\"\"\n        item = super(AnalysesView, self).folderitem(obj, item, index)\n        item_obj = api.get_object(obj)\n        uid = item[\"uid\"]\n\n        # Slot is the row position where all analyses sharing the same parent\n        # (eg. AnalysisRequest, SampleReference), will be displayed as a group\n        slot = self.get_item_slot(uid)\n        item[\"Pos\"] = slot\n\n        # The position string contains both the slot + the position of the\n        # analysis within the slot: \"position_sortkey\" will be used to sort all\n        # the analyses to be displayed in the list\n        str_position = self.uids_strpositions[uid]\n        item[\"pos_sortkey\"] = str_position\n\n        item[\"colspan\"] = {\"Pos\": 1}\n        item[\"Service\"] = item_obj.Title()\n        item[\"Category\"] = item_obj.getCategoryTitle()\n        item[\"DueDate\"] = self.ulocalized_time(item_obj, long_format=0)\n        item[\"class\"][\"Service\"] = \"service_title\"\n\n        # To prevent extra loops, we compute here the number of analyses to be\n        # rendered within each slot. This information will be useful later for\n        # applying rowspan to the first cell of each slot, that contains info\n        # about the parent of all the analyses contained in that slot (e.g\n        # Analysis Request ID, Sample Type, etc.)\n        rowspans = self.items_rowspans.get(slot, 0) + 1\n\n        remarks_enabled = self.is_analysis_remarks_enabled()\n        if remarks_enabled:\n            # Increase in one unit the rowspan, cause the comment field for\n            # this analysis will be rendered in a new row, below the row that\n            # displays the current item\n            rowspans = rowspans + 1\n        # We map this rowspan information in items_rowspan, that will be used\n        # later during the rendereing of slot headers (first cell of each row)\n        self.items_rowspans[slot] = rowspans\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef folderitems(self):\n        items = BaseView.folderitems(self)\n\n        # Fill empty positions from the layout with fake rows. The worksheet\n        # can be generated by making use of a WorksheetTemplate, so there is\n        # the chance that some slots of this worksheet being empty. We need to\n        # render a row still, at lest to display the slot number (Pos)\n        self.fill_empty_slots(items)\n\n        # Sort the items in accordance with the layout\n        items = sorted(items, key=itemgetter(\"pos_sortkey\"))\n\n        # Fill the slot header cells (first cell of each row). Each slot\n        # contains the analyses that belong to the same parent\n        # (AnalysisRequest, ReferenceSample), so the information about the\n        # parent must be displayed in the first cell of each slot.\n        self.fill_slots_headers(items)\n\n        return items", "response": "Returns an array of dictionaries each dictionary represents an analyzed entry in the list. The array returned is sorted by the position of each entry in the worksheet."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_uids_strpositions(self):\n        uids_positions = dict()\n        layout = self.context.getLayout()\n        layout = layout and layout or []\n        # Map the analysis uids with their positions.\n        occupied = []\n        next_positions = {}\n        for item in layout:\n            uid = item.get(\"analysis_uid\", \"\")\n            slot = int(item[\"position\"])\n            occupied.append(slot)\n            position = next_positions.get(slot, 1)\n            str_position = \"{:010}:{:010}\".format(slot, position)\n            next_positions[slot] = position + 1\n            uids_positions[uid] = str_position\n\n        # Fill empties\n        last_slot = max(occupied) if occupied else 1\n        empties = [num for num in range(1, last_slot) if num not in occupied]\n        for empty_slot in empties:\n            str_position = \"{:010}:{:010}\".format(empty_slot, 1)\n            uid = \"empty-{}\".format(empty_slot)\n            uids_positions[uid] = str_position\n\n        return uids_positions", "response": "Returns a dictionary with the positions of each analysis in accordance with the current layout."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_item_position(self, analysis_uid):\n        str_position = self.uids_strpositions.get(analysis_uid, \"\")\n        tokens = str_position.split(\":\")\n        if len(tokens) != 2:\n            return None\n        return [to_int(tokens[0]), to_int(tokens[1])]", "response": "Returns a list with the position for the analysis_uid passed in\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list with the position number of the slots that are occupied.", "response": "def _get_slots(self, empty_uid=False):\n        \"\"\"Returns a list with the position number of the slots that are\n        occupied (if empty_uid=False) or are empty (if empty_uid=True)\n\n        :param empty_uid: True exclude occupied slots. False excludes empties\n        :return: sorted list with slot numbers\n        \"\"\"\n        slots = list()\n        for uid, position in self.uids_strpositions.items():\n            if empty_uid and not uid.startswith(\"empty-\"):\n                continue\n            elif not empty_uid and uid.startswith(\"empty-\"):\n                continue\n            tokens = position.split(\":\")\n            slots.append(to_int(tokens[0]))\n        return sorted(list(set(slots)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fill_empty_slots(self, items):\n        for pos in self.get_empty_slots():\n            item = {\n                \"obj\": self.context,\n                \"id\": self.context.id,\n                \"uid\": self.context.UID(),\n                \"title\": self.context.Title(),\n                \"type_class\": \"blank-worksheet-row\",\n                \"url\": self.context.absolute_url(),\n                \"relative_url\": self.context.absolute_url(),\n                \"view_url\": self.context.absolute_url(),\n                \"path\": \"/\".join(self.context.getPhysicalPath()),\n                \"before\": {},\n                \"after\": {},\n                \"replace\": {\n                    \"Pos\": \"<span class='badge'>{}</span> {}\".format(\n                        pos, _(\"Reassignable Slot\"))\n                },\n                \"choices\": {},\n                \"class\": {},\n                \"state_class\": \"state-empty\",\n                \"allow_edit\": [],\n                \"Pos\": pos,\n                \"pos_sortkey\": \"{:010}:{:010}\".format(pos, 1),\n                \"Service\": \"\",\n                \"Attachments\": \"\",\n                \"state_title\": \"\",\n                \"disabled\": True,\n            }\n\n            items.append(item)", "response": "Append dicts to the items passed in for those slots that don t have any analysis assigned but the row needs to be rendered still."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fill_slots_headers(self, items):\n        prev_position = 0\n        for item in items:\n            item_position = item[\"Pos\"]\n            if item_position == prev_position:\n                item = self.skip_item_key(item, \"Pos\")\n                # head slot already filled\n                continue\n            if item.get(\"disabled\", False):\n                # empty slot\n                continue\n\n            # This is the first analysis found for the given position, add the\n            # slot info in there and apply a rowspan accordingly.\n            rowspan = self.items_rowspans.get(item_position, 1)\n            prev_position = item_position\n            item[\"rowspan\"] = {\"Pos\": rowspan}\n            item[\"replace\"][\"Pos\"] = self.get_slot_header(item)", "response": "Generates the header cell for each slot. For each slot the first cell displays information about the parent analyses that have in common."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef skip_item_key(self, item, key):\n        if \"skip\" in item:\n            item[\"skip\"].append(key)\n        else:\n            item[\"skip\"] = [key]\n        return item", "response": "Add the key to the item s skip list"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a slot header for the item", "response": "def get_slot_header(self, item):\n        \"\"\"Generates a slot header (the first cell of the row) for the item\n\n        :param item: the item for which the slot header is requested\n        :return: the html contents to be displayed in the first cell of a slot\n        \"\"\"\n        obj = item[\"obj\"]\n        obj = api.get_object(obj)\n\n        # Prepare the template data\n        data = {\n            \"obj\": obj,\n            \"item\": item,\n            \"position\": item[\"Pos\"],\n        }\n        # update the data\n        data.update(self.get_slot_header_data(obj))\n\n        template = ViewPageTemplateFile(\"../templates/slot_header.pt\")\n        return template(self, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_slot_header_data(self, obj):\n\n        item_obj = None\n        item_title = \"\"\n        item_url = \"\"\n        item_img = \"\"\n        item_img_url = \"\"\n        item_img_text = \"\"\n        additional_item_icons = []\n\n        parent_obj = None\n        parent_title = \"\"\n        parent_url = \"\"\n        parent_img = \"\"\n        parent_img_text = \"\"\n        additional_parent_icons = []\n\n        sample_type_obj = None\n        sample_type_title = \"\"\n        sample_type_url = \"\"\n        sample_type_img = \"\"\n        sample_type_img_text = \"\"\n\n        if IDuplicateAnalysis.providedBy(obj):\n            # item\n            request = obj.getRequest()\n            item_obj = request\n            item_title = api.get_id(request)\n            item_url = api.get_url(request)\n            item_img = \"duplicate.png\"\n            item_img_url = api.get_url(request)\n            item_img_text = t(_(\"Duplicate\"))\n            # additional item icons\n            additional_item_icons.append(\n                self.render_remarks_tag(request))\n            # parent\n            client = request.getClient()\n            parent_obj = client\n            parent_title = api.get_title(client)\n            parent_url = api.get_url(client)\n            parent_img = \"client.png\"\n            parent_img_text = t(_(\"Client\"))\n            # sample type\n            sample_type = request.getSampleType()\n            sample_type_title = request.getSampleTypeTitle()\n            sample_type_url = api.get_url(sample_type)\n            sample_type_img = \"sampletype.png\"\n            sample_type_img_text = t(_(\"Sample Type\"))\n        elif IReferenceAnalysis.providedBy(obj):\n            # item\n            sample = obj.getSample()\n            item_obj = sample\n            item_title = api.get_id(sample)\n            item_url = api.get_url(sample)\n            item_img_url = api.get_url(sample)\n            item_img = \"control.png\"\n            item_img_text = t(_(\"Control\"))\n            if obj.getReferenceType() == \"b\":\n                item_img = \"blank.png\"\n                item_img_text = t(_(\"Blank\"))\n            # parent\n            supplier = obj.getSupplier()\n            parent_obj = supplier\n            parent_title = api.get_title(supplier)\n            parent_url = api.get_url(supplier)\n            parent_img = \"supplier.png\"\n            parent_img_text = t(_(\"Supplier\"))\n        elif IRoutineAnalysis.providedBy(obj):\n            # item\n            request = obj.getRequest()\n            item_obj = request\n            item_title = api.get_id(request)\n            item_url = api.get_url(request)\n            item_img = \"sample.png\"\n            item_img_url = api.get_url(request)\n            item_img_text = t(_(\"Sample\"))\n            # additional item icons\n            additional_item_icons.append(\n                self.render_remarks_tag(request))\n            # parent\n            client = obj.getClient()\n            parent_obj = client\n            parent_title = api.get_title(client)\n            parent_url = api.get_url(client)\n            parent_img = \"client.png\"\n            parent_img_text = t(_(\"Client\"))\n            # sample type\n            sample_type = obj.getSampleType()\n            sample_type_title = obj.getSampleTypeTitle()\n            sample_type_url = api.get_url(sample_type)\n            sample_type_img = \"sampletype.png\"\n            sample_type_img_text = t(_(\"Sample Type\"))\n\n        return {\n            # item\n            \"item_obj\": item_obj,\n            \"item_title\": item_title,\n            \"item_url\": item_url,\n            \"item_img\": get_image(item_img, title=item_img_text),\n            \"item_img_url\": item_img_url,\n            \"additional_item_icons\": additional_item_icons,\n            # parent\n            \"parent_obj\": parent_obj,\n            \"parent_title\": parent_title,\n            \"parent_url\": parent_url,\n            \"parent_img\": get_image(parent_img, title=parent_img_text),\n            \"additional_parent_icons\": additional_parent_icons,\n            # sample type\n            \"sample_type_obj\": sample_type_obj,\n            \"sample_type_title\": sample_type_title,\n            \"sample_type_url\": sample_type_url,\n            \"sample_type_img\": get_image(\n                sample_type_img, title=sample_type_img_text),\n        }", "response": "Prepare the data for the slot header template"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrendering a remarks image icon", "response": "def render_remarks_tag(self, ar):\n        \"\"\"Renders a remarks image icon\n        \"\"\"\n        if not ar.getRemarks():\n            return \"\"\n\n        uid = api.get_uid(ar)\n        url = ar.absolute_url()\n        title = ar.Title()\n        tooltip = _(\"Remarks of {}\").format(title)\n\n        # Note: The 'href' is picked up by the overlay handler, see\n        #       bika.lims.worksheet.coffee\n        attrs = {\n            \"css_class\": \"slot-remarks\",\n            \"style\": \"cursor: pointer;\",\n            \"title\": tooltip,\n            \"uid\": uid,\n            \"href\": \"{}/base_view\".format(url),\n        }\n\n        return get_image(\"remarks_ico.png\", **attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns searchable data as Description", "response": "def Description(self):\n        \"\"\"Returns searchable data as Description\"\"\"\n        descr = \" \".join((self.getId(), self.aq_parent.Title()))\n        return safe_unicode(descr).encode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getDefaultMemberDiscount(self):\n        if hasattr(self, 'getMemberDiscountApplies'):\n            if self.getMemberDiscountApplies():\n                settings = self.bika_setup\n                return settings.getMemberDiscount()\n            else:\n                return \"0.00\"", "response": "Compute default member discount if it applies\n Arc"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getAnalysesNum(self):\n        an_nums = [0, 0, 0, 0]\n        for analysis in self.getAnalyses():\n            review_state = analysis.review_state\n            if review_state in ['retracted', 'rejected', 'cancelled']:\n                continue\n            if review_state == 'to_be_verified':\n                an_nums[3] += 1\n            elif review_state in ['published', 'verified']:\n                an_nums[0] += 1\n            else:\n                an_nums[2] += 1\n            an_nums[1] += 1\n        return an_nums", "response": "Returns an array with the number of analyses for the current AR in\n            different statuses like follows"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getResponsible(self):\n        managers = {}\n        for department in self.getDepartments():\n            manager = department.getManager()\n            if manager is None:\n                continue\n            manager_id = manager.getId()\n            if manager_id not in managers:\n                managers[manager_id] = {}\n                managers[manager_id]['salutation'] = safe_unicode(\n                    manager.getSalutation())\n                managers[manager_id]['name'] = safe_unicode(\n                    manager.getFullname())\n                managers[manager_id]['email'] = safe_unicode(\n                    manager.getEmailAddress())\n                managers[manager_id]['phone'] = safe_unicode(\n                    manager.getBusinessPhone())\n                managers[manager_id]['job_title'] = safe_unicode(\n                    manager.getJobTitle())\n                if manager.getSignature():\n                    managers[manager_id]['signature'] = \\\n                        '{}/Signature'.format(manager.absolute_url())\n                else:\n                    managers[manager_id]['signature'] = False\n                managers[manager_id]['departments'] = ''\n            mngr_dept = managers[manager_id]['departments']\n            if mngr_dept:\n                mngr_dept += ', '\n            mngr_dept += safe_unicode(department.Title())\n            managers[manager_id]['departments'] = mngr_dept\n        mngr_keys = managers.keys()\n        mngr_info = {'ids': mngr_keys, 'dict': managers}\n\n        return mngr_info", "response": "Return all manager info of responsible departments\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn all managers of responsible departments", "response": "def getManagers(self):\n        \"\"\"Return all managers of responsible departments\n        \"\"\"\n        manager_ids = []\n        manager_list = []\n        for department in self.getDepartments():\n            manager = department.getManager()\n            if manager is None:\n                continue\n            manager_id = manager.getId()\n            if manager_id not in manager_ids:\n                manager_ids.append(manager_id)\n                manager_list.append(manager)\n        return manager_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the earliest due date of the analyses this Analysis Request contains.", "response": "def getDueDate(self):\n        \"\"\"Returns the earliest due date of the analyses this Analysis Request\n        contains.\"\"\"\n        due_dates = map(lambda an: an.getDueDate, self.getAnalyses())\n        return due_dates and min(due_dates) or None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getLate(self):\n        for analysis in self.getAnalyses():\n            if analysis.review_state == \"retracted\":\n                continue\n            analysis_obj = api.get_object(analysis)\n            if analysis_obj.isLateAnalysis():\n                return True\n        return False", "response": "Return True if there is at least one late analysis in this Request\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns 0 1 or 2 to indicate Printed state.", "response": "def getPrinted(self):\n        \"\"\" returns \"0\", \"1\" or \"2\" to indicate Printed state.\n            0 -> Never printed.\n            1 -> Printed after last publish\n            2 -> Printed but republished afterwards.\n        \"\"\"\n        workflow = getToolByName(self, 'portal_workflow')\n        review_state = workflow.getInfoFor(self, 'review_state', '')\n        if review_state not in ['published']:\n            return \"0\"\n        report_list = sorted(self.objectValues('ARReport'),\n                             key=lambda report: report.getDatePublished())\n        if not report_list:\n            return \"0\"\n        last_report = report_list[-1]\n        if last_report.getDatePrinted():\n            return \"1\"\n        else:\n            for report in report_list:\n                if report.getDatePrinted():\n                    return \"2\"\n        return \"0\""}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the items to be billed", "response": "def getBillableItems(self):\n        \"\"\"Returns the items to be billed\n        \"\"\"\n        # Assigned profiles\n        profiles = self.getProfiles()\n        # Billable profiles which have a fixed price set\n        billable_profiles = filter(\n            lambda pr: pr.getUseAnalysisProfilePrice(), profiles)\n        # All services contained in the billable profiles\n        billable_profile_services = reduce(lambda a, b: a+b, map(\n            lambda profile: profile.getService(), billable_profiles), [])\n        # Keywords of the contained services\n        billable_service_keys = map(\n            lambda s: s.getKeyword(), set(billable_profile_services))\n        # The billable items contain billable profiles and single selected analyses\n        billable_items = billable_profiles\n        # Get the analyses to be billed\n        exclude_rs = [\"retracted\", \"rejected\"]\n        for analysis in self.getAnalyses(is_active=True):\n            if analysis.review_state in exclude_rs:\n                continue\n            if analysis.getKeyword in billable_service_keys:\n                continue\n            billable_items.append(api.get_object(analysis))\n        return billable_items"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getVATAmount(self):\n        has_client_discount = self.aq_parent.getMemberDiscountApplies()\n        VATAmount = self.getSubtotalVATAmount()\n        if has_client_discount:\n            discount = Decimal(self.getDefaultMemberDiscount())\n            return Decimal((1 - discount / 100) * VATAmount)\n        else:\n            return VATAmount", "response": "This method computes the VAT amount from ( subtotal - discount.) * VAT / 100 but\n        is the ID of the current analysis request."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an invoice for this analysis request.", "response": "def createInvoice(self, pdf):\n        \"\"\"Issue invoice\n        \"\"\"\n        client = self.getClient()\n        invoice = self.getInvoice()\n        if not invoice:\n            invoice = _createObjectByType(\"Invoice\", client, tmpID())\n        invoice.edit(\n            AnalysisRequest=self,\n            Client=client,\n            InvoiceDate=DateTime(),\n            InvoicePDF=pdf\n        )\n        invoice.processForm()\n        self.setInvoice(invoice)\n        return invoice"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef printInvoice(self, REQUEST=None, RESPONSE=None):\n        invoice = self.getInvoice()\n        invoice_url = invoice.absolute_url()\n        RESPONSE.redirect('{}/invoice_print'.format(invoice_url))", "response": "Print invoice to the invoice_print page"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the user that verified the whole Analysis Request.", "response": "def getVerifier(self):\n        \"\"\"Returns the user that verified the whole Analysis Request. Since the\n        verification is done automatically as soon as all the analyses it\n        contains are verified, this function returns the user that verified the\n        last analysis pending.\n        \"\"\"\n        wtool = getToolByName(self, 'portal_workflow')\n        mtool = getToolByName(self, 'portal_membership')\n\n        verifier = None\n        # noinspection PyBroadException\n        try:\n            review_history = wtool.getInfoFor(self, 'review_history')\n        except:  # noqa FIXME: remove blind except!\n            return 'access denied'\n\n        if not review_history:\n            return 'no history'\n        for items in review_history:\n            action = items.get('action')\n            if action != 'verify':\n                continue\n            actor = items.get('actor')\n            member = mtool.getMemberById(actor)\n            verifier = member.getProperty('fullname')\n            if verifier is None or verifier == '':\n                verifier = actor\n        return verifier"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the ids from users that have verified at least one analysis from this Analysis Request", "response": "def getVerifiersIDs(self):\n        \"\"\"Returns the ids from users that have verified at least one analysis\n        from this Analysis Request\n        \"\"\"\n        verifiers_ids = list()\n        for brain in self.getAnalyses():\n            verifiers_ids += brain.getVerificators\n        return list(set(verifiers_ids))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the list of lab contacts that have verified at least one one analysis from this Analysis Request", "response": "def getVerifiers(self):\n        \"\"\"Returns the list of lab contacts that have verified at least one\n        analysis from this Analysis Request\n        \"\"\"\n        contacts = list()\n        for verifier in self.getVerifiersIDs():\n            user = api.get_user(verifier)\n            contact = api.get_user_contact(user, [\"LabContact\"])\n            if contact:\n                contacts.append(contact)\n        return contacts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the UID of the contact associated with the authenticated user", "response": "def getContactUIDForUser(self):\n        \"\"\"get the UID of the contact associated with the authenticated user\n        \"\"\"\n        mt = getToolByName(self, 'portal_membership')\n        user = mt.getAuthenticatedMember()\n        user_id = user.getUserName()\n        pc = getToolByName(self, 'portal_catalog')\n        r = pc(portal_type='Contact',\n               getUsername=user_id)\n        if len(r) == 1:\n            return r[0].UID"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getQCAnalyses(self, qctype=None, review_state=None):\n        qcanalyses = []\n        suids = []\n        ans = self.getAnalyses()\n        wf = getToolByName(self, 'portal_workflow')\n        for an in ans:\n            an = an.getObject()\n            if an.getServiceUID() not in suids:\n                suids.append(an.getServiceUID())\n\n        def valid_dup(wan):\n            if wan.portal_type == 'ReferenceAnalysis':\n                return False\n            an_state = wf.getInfoFor(wan, 'review_state')\n            return \\\n                wan.portal_type == 'DuplicateAnalysis' \\\n                and wan.getRequestID() == self.id \\\n                and (review_state is None or an_state in review_state)\n\n        def valid_ref(wan):\n            if wan.portal_type != 'ReferenceAnalysis':\n                return False\n            an_state = wf.getInfoFor(wan, 'review_state')\n            an_reftype = wan.getReferenceType()\n            return wan.getServiceUID() in suids \\\n                and wan not in qcanalyses \\\n                and (qctype is None or an_reftype == qctype) \\\n                and (review_state is None or an_state in review_state)\n\n        for an in ans:\n            an = an.getObject()\n            ws = an.getWorksheet()\n            if not ws:\n                continue\n            was = ws.getAnalyses()\n            for wa in was:\n                if valid_dup(wa):\n                    qcanalyses.append(wa)\n                elif valid_ref(wa):\n                    qcanalyses.append(wa)\n\n        return qcanalyses", "response": "returns the QC analyses performed in the worksheet in which this AR is present."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getResultsRange(self):\n        specs_range = []\n        specification = self.getSpecification()\n        if specification:\n            specs_range = specification.getResultsRange()\n            specs_range = specs_range and specs_range or []\n\n        # Override with AR's custom ranges\n        ar_range = self.Schema().getField(\"ResultsRange\").get(self)\n        if not ar_range:\n            return specs_range\n\n        # Remove those analysis ranges that neither min nor max are floatable\n        an_specs = [an for an in ar_range if\n                    api.is_floatable(an.get('min', None)) or\n                    api.is_floatable(an.get('max', None))]\n        # Want to know which are the analyses that needs to be overriden\n        keywords = map(lambda item: item.get('keyword'), an_specs)\n        # Get rid of those analyses to be overriden\n        out_specs = [sp for sp in specs_range if sp['keyword'] not in keywords]\n        # Add manually set ranges\n        out_specs.extend(an_specs)\n        return map(lambda spec: ResultsRangeDict(spec), out_specs)", "response": "Returns the valid result ranges for the analyses in this Analysis\n            Request."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getSamplingWorkflowEnabled(self):\n        template = self.getTemplate()\n        if template:\n            return template.getSamplingRequired()\n        return self.bika_setup.getSamplingWorkflowEnabled()", "response": "Returns True if the sample of this Analysis Request has to be collected by the laboratory personnel\n        collected by the laboratory personnel\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getDepartments(self):\n        departments = list()\n        for analysis in self.getAnalyses(full_objects=True):\n            department = analysis.getDepartment()\n            if department and department not in departments:\n                departments.append(department)\n        return departments", "response": "Returns a list of the departments assigned to the Analyses\n        from this Analysis Request"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the results interpretation for this Analysis Request and department.", "response": "def getResultsInterpretationByDepartment(self, department=None):\n        \"\"\"Returns the results interpretation for this Analysis Request\n           and department. If department not set, returns the results\n           interpretation tagged as 'General'.\n\n        :returns: a dict with the following keys:\n            {'uid': <department_uid> or 'general', 'richtext': <text/plain>}\n        \"\"\"\n        uid = department.UID() if department else 'general'\n        rows = self.Schema()['ResultsInterpretationDepts'].get(self)\n        row = [row for row in rows if row.get('uid') == uid]\n        if len(row) > 0:\n            row = row[0]\n        elif uid == 'general' \\\n                and hasattr(self, 'getResultsInterpretation') \\\n                and self.getResultsInterpretation():\n            row = {'uid': uid, 'richtext': self.getResultsInterpretation()}\n        else:\n            row = {'uid': uid, 'richtext': ''}\n        return row"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the settings for the analysis service that matches with the uid provided.", "response": "def getAnalysisServiceSettings(self, uid):\n        \"\"\"Returns a dictionary with the settings for the analysis service that\n        match with the uid provided.\n\n        If there are no settings for the analysis service and\n        analysis requests:\n\n        1. looks for settings in AR's ARTemplate. If found, returns the\n           settings for the AnalysisService set in the Template\n        2. If no settings found, looks in AR's ARProfile. If found, returns the\n           settings for the AnalysisService from the AR Profile. Otherwise,\n           returns a one entry dictionary with only the key 'uid'\n        \"\"\"\n        sets = [s for s in self.getAnalysisServicesSettings()\n                if s.get('uid', '') == uid]\n\n        # Created by using an ARTemplate?\n        if not sets and self.getTemplate():\n            adv = self.getTemplate().getAnalysisServiceSettings(uid)\n            sets = [adv] if 'hidden' in adv else []\n\n        # Created by using an AR Profile?\n        if not sets and self.getProfiles():\n            adv = []\n            adv += [profile.getAnalysisServiceSettings(uid) for profile in\n                    self.getProfiles()]\n            sets = adv if 'hidden' in adv[0] else []\n\n        return sets[0] if sets else {'uid': uid}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getPartitions(self):\n        partitions = []\n        for analysis in self.getAnalyses(full_objects=True):\n            if analysis.getSamplePartition() not in partitions:\n                partitions.append(analysis.getSamplePartition())\n        return partitions", "response": "This functions returns the partitions from the analysis request s\n        analyses."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if the analysis service with the provided uid is hidden in results. Returns the hidden flag if it is set otherwise raises a ValueError.", "response": "def isAnalysisServiceHidden(self, uid):\n        \"\"\"Checks if the analysis service that match with the uid provided must\n        be hidden in results. If no hidden assignment has been set for the\n        analysis in this request, returns the visibility set to the analysis\n        itself.\n\n        Raise a TypeError if the uid is empty or None\n\n        Raise a ValueError if there is no hidden assignment in this request or\n        no analysis service found for this uid.\n        \"\"\"\n        if not uid:\n            raise TypeError('None type or empty uid')\n        sets = self.getAnalysisServiceSettings(uid)\n        if 'hidden' not in sets:\n            uc = getToolByName(self, 'uid_catalog')\n            serv = uc(UID=uid)\n            if serv and len(serv) == 1:\n                return serv[0].getObject().getRawHidden()\n            else:\n                raise ValueError('{} is not valid'.format(uid))\n        return sets.get('hidden', False)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getRejecter(self):\n        wtool = getToolByName(self, 'portal_workflow')\n        mtool = getToolByName(self, 'portal_membership')\n        # noinspection PyBroadException\n        try:\n            review_history = wtool.getInfoFor(self, 'review_history')\n        except:  # noqa FIXME: remove blind except!\n            return None\n        for items in review_history:\n            action = items.get('action')\n            if action != 'reject':\n                continue\n            actor = items.get('actor')\n            return mtool.getMemberById(actor)\n        return None", "response": "Returns the user who did the\n        rejection. Returns None if the Analysis Request has not been rejected."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getPrioritySortkey(self):\n        priority = self.getPriority()\n        created_date = self.created().ISO8601()\n        return '%s.%s' % (priority, created_date)", "response": "Returns the key used to sort the current Analysis\n        Request based on both its priority and creation date. On ASC sorting the oldest item with highest priority will be displayed. On DESC sorting the oldest item with highest priority will be displayed. On ASC sorting the oldest item with highest priority will be displayed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getObjectWorkflowStates(self):\n        workflow = getToolByName(self, 'portal_workflow')\n        states = {}\n        for w in workflow.getWorkflowsFor(self):\n            state = w._getWorkflowStateOf(self).id\n            states[w.state_var] = state\n        return states", "response": "This method returns a dictionary with the workflow id as key and workflow state as value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the ancestor(s) of this Analysis Request", "response": "def getAncestors(self, all_ancestors=True):\n        \"\"\"Returns the ancestor(s) of this Analysis Request\n        param all_ancestors: include all ancestors, not only the parent\n        \"\"\"\n        parent = self.getParentAnalysisRequest()\n        if not parent:\n            return list()\n        if not all_ancestors:\n            return [parent]\n        return [parent] + parent.getAncestors(all_ancestors=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the Analysis Requests that are descendant of this Analysis Request.", "response": "def getDescendants(self, all_descendants=False):\n        \"\"\"Returns the descendant Analysis Requests\n\n        :param all_descendants: recursively include all descendants\n        \"\"\"\n\n        # N.B. full objects returned here from\n        #      `Products.Archetypes.Referenceable.getBRefs`\n        #      -> don't add this method into Metadata\n        children = self.getBackReferences(\n            \"AnalysisRequestParentAnalysisRequest\")\n\n        descendants = []\n\n        # recursively include all children\n        if all_descendants:\n            for child in children:\n                descendants.append(child)\n                descendants += child.getDescendants(all_descendants=True)\n        else:\n            descendants = children\n\n        return descendants"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getDescendantsUIDs(self, all_descendants=False):\n        descendants = self.getDescendants(all_descendants=all_descendants)\n        return map(api.get_uid, descendants)", "response": "Returns the UIDs of the descendant Analysis Requests"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef isOpen(self):\n        for analysis in self.getAnalyses():\n            if not api.get_object(analysis).isOpen():\n                return False\n        return True", "response": "Returns whether all analyses from this Analysis Request are open"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the parent analysis request for this instance.", "response": "def setParentAnalysisRequest(self, value):\n        \"\"\"Sets a parent analysis request, making the current a partition\n        \"\"\"\n        self.Schema().getField(\"ParentAnalysisRequest\").set(self, value)\n        if not value:\n            noLongerProvides(self, IAnalysisRequestPartition)\n        else:\n            alsoProvides(self, IAnalysisRequestPartition)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the date received to this analysis request and to secondary analysis requests.", "response": "def setDateReceived(self, value):\n        \"\"\"Sets the date received to this analysis request and to secondary\n        analysis requests\n        \"\"\"\n        self.Schema().getField('DateReceived').set(self, value)\n        for secondary in self.getSecondaryAnalysisRequests():\n            secondary.setDateReceived(value)\n            secondary.reindexObject(idxs=[\"getDateReceived\", \"is_received\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef addAttachment(self, attachment):\n        if not isinstance(attachment, (list, tuple)):\n            attachment = [attachment]\n\n        original = self.getAttachment() or []\n\n        # Function addAttachment can accept brain, objects or uids\n        original = map(api.get_uid, original)\n        attachment = map(api.get_uid, attachment)\n\n        # Boil out attachments already assigned to this Analysis Request\n        attachment = filter(lambda at: at not in original, attachment)\n        if attachment:\n            original.extend(attachment)\n            self.setAttachment(original)", "response": "Adds an attachment or a list of attachments to the Analysis Request\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles submission of a new AR UIDs and add duplicate analyses to the context", "response": "def handle_submit(self):\n        \"\"\"Handle form submission\n        \"\"\"\n        form = self.request.form\n        # Selected AR UIDs\n        uids = form.get(\"uids\")\n        container_mapping = self.get_container_mapping()\n        for uid in uids:\n            src_pos = container_mapping[uid]\n            self.context.addDuplicateAnalyses(src_pos)\n        redirect_url = \"{}/{}\".format(\n            api.get_url(self.context), \"manage_results\")\n        self.request.response.redirect(redirect_url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a mapping of container to postition containing the current locale.", "response": "def get_container_mapping(self):\n        \"\"\"Returns a mapping of container -> postition\n        \"\"\"\n        layout = self.context.getLayout()\n        container_mapping = {}\n        for slot in layout:\n            if slot[\"type\"] != \"a\":\n                continue\n            position = slot[\"position\"]\n            container_uid = slot[\"container_uid\"]\n            container_mapping[container_uid] = position\n        return container_mapping"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_next_id_for(self, key):\n        portal_type = key.split(\"-\")[0]\n        config = get_config(None, portal_type=portal_type)\n        id_template = config.get(\"form\", \"\")\n        number = self.storage.get(key) + 1\n        spec = {\n            \"seq\": number,\n            \"alpha\": Alphanumber(number),\n            \"year\": get_current_year(),\n            \"parent_analysisrequest\": \"ParentAR\",\n            \"parent_ar_id\": \"ParentARId\",\n            \"sampleType\": key.replace(portal_type, \"\").strip(\"-\"),\n        }\n        return id_template.format(**spec)", "response": "Get a preview of the next number for the given key"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an integer from the number", "response": "def to_int(self, number, default=0):\n        \"\"\"Returns an integer\n        \"\"\"\n        try:\n            return int(number)\n        except (KeyError, ValueError):\n            return self.to_int(default, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_seed(self, key, value):\n        number_generator = getUtility(INumberGenerator)\n        return number_generator.set_number(key, self.to_int(value))", "response": "Set a number of the number generator\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresetting the number from which the next generated sequence start.", "response": "def seed(self):\n        \"\"\" Reset the number from which the next generated sequence start.\n            If you seed at 100, next seed will be 101\n        \"\"\"\n        form = self.request.form\n        prefix = form.get('prefix', None)\n        if prefix is None:\n            return 'No prefix provided'\n        seed = form.get('seed', None)\n        if seed is None:\n            return 'No seed provided'\n        if not seed.isdigit():\n            return 'Seed must be a digit'\n        seed = int(seed)\n        if seed < 0:\n            return 'Seed cannot be negative'\n\n        new_seq = self.set_seed(prefix, seed)\n        return 'IDServerView: \"%s\" seeded to %s' % (prefix, new_seq)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the dashboard_panels_visibility values.", "response": "def get_dashboard_registry_record():\n    \"\"\"\n    Return the 'bika.lims.dashboard_panels_visibility' values.\n    :return: A dictionary or None\n    \"\"\"\n    try:\n        registry = api.portal.get_registry_record(\n            'bika.lims.dashboard_panels_visibility')\n        return registry\n    except InvalidParameterError:\n        # No entry in the registry for dashboard panels roles.\n        # Maybe upgradestep 1.1.8 was not run?\n        logger.warn(\"Cannot find a record with name \"\n                    \"'bika.lims.dashboard_panels_visibility' in \"\n                    \"registry_record. Missed upgrade 1.1.8?\")\n    return dict()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setup_dashboard_panels_visibility_registry(section_name):\n    registry_info = get_dashboard_registry_record()\n    role_permissions_list = []\n    # Getting roles defined in the system\n    roles = []\n    acl_users = get_tool(\"acl_users\")\n    roles_tree = acl_users.portal_role_manager.listRoleIds()\n    for role in roles_tree:\n        roles.append(role)\n    # Set view permissions to each role as 'yes':\n    # \"role1,yes,role2,no,rol3,no\"\n    for role in roles:\n        role_permissions_list.append(role)\n        visible = 'no'\n        if role in ['LabManager', 'Manager']:\n            visible = 'yes'\n        role_permissions_list.append(visible)\n    role_permissions = ','.join(role_permissions_list)\n\n    # Set permissions string into dict\n    registry_info[get_unicode(section_name)] = get_unicode(role_permissions)\n    # Set new values to registry record\n    set_dashboard_registry_record(registry_info)\n    return registry_info", "response": "Sets the values for panels visibility in registry_records. By default only users with LabManager or Manager roles can see the panels."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of pairs as values that represents the role - permission view relation for the panel section passed in.", "response": "def get_dashboard_panels_visibility_by_section(section_name):\n    \"\"\"\n    Return a list of pairs as values that represents the role-permission\n    view relation for the panel section passed in.\n    :param section_name: the panels section id.\n    :return: a list of tuples.\n    \"\"\"\n    registry_info = get_dashboard_registry_record()\n    if section_name not in registry_info:\n        # Registry hasn't been set, do it at least for this section\n        registry_info = \\\n            setup_dashboard_panels_visibility_registry(section_name)\n\n    pairs = registry_info.get(section_name)\n    pairs = get_strings(pairs)\n    if pairs is None:\n        # In the registry, but with None value?\n        setup_dashboard_panels_visibility_registry(section_name)\n        return get_dashboard_panels_visibility_by_section(section_name)\n\n    pairs = pairs.split(',')\n    if len(pairs) == 0 or len(pairs) % 2 != 0:\n        # Non-valid or malformed value\n        setup_dashboard_panels_visibility_registry(section_name)\n        return get_dashboard_panels_visibility_by_section(section_name)\n\n    result = [\n        (pairs[i], pairs[i + 1]) for i in range(len(pairs)) if i % 2 == 0]\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the user is allowed to see the panel", "response": "def is_panel_visible_for_user(panel, user):\n    \"\"\"\n    Checks if the user is allowed to see the panel\n    :param panel: panel ID as string\n    :param user: a MemberData object\n    :return: Boolean\n    \"\"\"\n    roles = user.getRoles()\n    visibility = get_dashboard_panels_visibility_by_section(panel)\n    for pair in visibility:\n        if pair[0] in roles and pair[1] == 'yes':\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the dashboard cookie exists through bikasetup configuration. If it does not exist create it with default values.", "response": "def check_dashboard_cookie(self):\n        \"\"\"\n        Check if the dashboard cookie should exist through bikasetup\n        configuration.\n\n        If it should exist but doesn't exist yet, the function creates it\n        with all values as default.\n        If it should exist and already exists, it returns the value.\n        Otherwise, the function returns None.\n\n        :return: a dictionary of strings\n        \"\"\"\n        # Getting cookie\n        cookie_raw = self.request.get(DASHBOARD_FILTER_COOKIE, None)\n        # If it doesn't exist, create it with default values\n        if cookie_raw is None:\n            cookie_raw = self._create_raw_data()\n            self.request.response.setCookie(\n                DASHBOARD_FILTER_COOKIE,\n                json.dumps(cookie_raw),\n                quoted=False,\n                path='/')\n            return cookie_raw\n        return get_strings(json.loads(cookie_raw))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_filter_selected(self, selection_id, value):\n        selected = self.dashboard_cookie.get(selection_id)\n        return selected == value", "response": "Compares whether the value saved in the selected cookie is the same value as the value parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the user is the admin or a SiteAdmin user.", "response": "def is_admin_user(self):\n        \"\"\"\n        Checks if the user is the admin or a SiteAdmin user.\n        :return: Boolean\n        \"\"\"\n        user = api.user.get_current()\n        roles = user.getRoles()\n        return \"LabManager\" in roles or \"Manager\" in roles"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_raw_data(self):\n        result = {}\n        for section in self.get_sections():\n            result[section.get('id')] = 'all'\n        return result", "response": "Create a dictionary of all the data that is used for the first entry in the cookie."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a date range that suits with the passed in periodicity.", "response": "def get_date_range(self, periodicity=PERIODICITY_WEEKLY):\n        \"\"\"Returns a date range (date from, date to) that suits with the passed\n        in periodicity.\n\n        :param periodicity: string that represents the periodicity\n        :type periodicity: str\n        :return: A date range\n        :rtype: [(DateTime, DateTime)]\n        \"\"\"\n        today = datetime.date.today()\n        if periodicity == PERIODICITY_DAILY:\n            # Daily, load last 30 days\n            date_from = DateTime() - 30\n            date_to = DateTime() + 1\n            return date_from, date_to\n\n        if periodicity == PERIODICITY_MONTHLY:\n            # Monthly, load last 2 years\n            min_year = today.year - 1 if today.month == 12 else today.year - 2\n            min_month = 1 if today.month == 12 else today.month\n            date_from = DateTime(min_year, min_month, 1)\n            date_to = DateTime(today.year, today.month,\n                               monthrange(today.year, today.month)[1],\n                               23, 59, 59)\n            return date_from, date_to\n\n        if periodicity == PERIODICITY_QUARTERLY:\n            # Quarterly, load last 4 years\n            m = (((today.month - 1) / 3) * 3) + 1\n            min_year = today.year - 4 if today.month == 12 else today.year - 5\n            date_from = DateTime(min_year, m, 1)\n            date_to = DateTime(today.year, m + 2,\n                               monthrange(today.year, m + 2)[1], 23, 59,\n                               59)\n            return date_from, date_to\n        if periodicity == PERIODICITY_BIANNUAL:\n            # Biannual, load last 10 years\n            m = (((today.month - 1) / 6) * 6) + 1\n            min_year = today.year - 10 if today.month == 12 else today.year - 11\n            date_from = DateTime(min_year, m, 1)\n            date_to = DateTime(today.year, m + 5,\n                               monthrange(today.year, m + 5)[1], 23, 59,\n                               59)\n            return date_from, date_to\n\n        if periodicity in [PERIODICITY_YEARLY, PERIODICITY_ALL]:\n            # Yearly or All time, load last 15 years\n            min_year = today.year - 15 if today.month == 12 else today.year - 16\n            date_from = DateTime(min_year, 1, 1)\n            date_to = DateTime(today.year, 12, 31, 23, 59, 59)\n            return date_from, date_to\n\n        # Default Weekly, load last six months\n        year, weeknum, dow = today.isocalendar()\n        min_year = today.year if today.month > 6 else today.year - 1\n        min_month = today.month - 6 if today.month > 6 \\\n            else (today.month - 6) + 12\n        date_from = DateTime(min_year, min_month, 1)\n        date_to = DateTime() - dow + 7\n        return date_from, date_to"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an array with the sections to be displayed.", "response": "def get_sections(self):\n        \"\"\" Returns an array with the sections to be displayed.\n            Every section is a dictionary with the following structure:\n                {'id': <section_identifier>,\n                 'title': <section_title>,\n                'panels': <array of panels>}\n        \"\"\"\n        sections = []\n        user = api.user.get_current()\n        if is_panel_visible_for_user('analyses', user):\n            sections.append(self.get_analyses_section())\n        if is_panel_visible_for_user('analysisrequests', user):\n            sections.append(self.get_analysisrequests_section())\n        if is_panel_visible_for_user('worksheets', user):\n            sections.append(self.get_worksheets_section())\n        return sections"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_analysisrequests_section(self):\n        out = []\n        catalog = getToolByName(self.context, CATALOG_ANALYSIS_REQUEST_LISTING)\n        query = {'portal_type': \"AnalysisRequest\",\n                 'is_active': True}\n\n        # Check if dashboard_cookie contains any values to query\n        # elements by\n        query = self._update_criteria_with_filters(query, 'analysisrequests')\n\n        # Active Samples (All)\n        total = self.search_count(query, catalog.id)\n\n        # Sampling workflow enabled?\n        if self.context.bika_setup.getSamplingWorkflowEnabled():\n            # Samples awaiting to be sampled or scheduled\n            name = _('Samples to be sampled')\n            desc = _(\"To be sampled\")\n            purl = 'samples?samples_review_state=to_be_sampled'\n            query['review_state'] = ['to_be_sampled', ]\n            out.append(self._getStatistics(name, desc, purl, catalog, query, total))\n\n            # Samples awaiting to be preserved\n            name = _('Samples to be preserved')\n            desc = _(\"To be preserved\")\n            purl = 'samples?samples_review_state=to_be_preserved'\n            query['review_state'] = ['to_be_preserved', ]\n            out.append(self._getStatistics(name, desc, purl, catalog, query, total))\n\n            # Samples scheduled for Sampling\n            name = _('Samples scheduled for sampling')\n            desc = _(\"Sampling scheduled\")\n            purl = 'samples?samples_review_state=scheduled_sampling'\n            query['review_state'] = ['scheduled_sampling', ]\n            out.append(self._getStatistics(name, desc, purl, catalog, query, total))\n\n        # Samples awaiting for reception\n        name = _('Samples to be received')\n        desc = _(\"Reception pending\")\n        purl = 'analysisrequests?analysisrequests_review_state=sample_due'\n        query['review_state'] = ['sample_due', ]\n        out.append(self._getStatistics(name, desc, purl, catalog, query, total))\n\n        # Samples under way\n        name = _('Samples with results pending')\n        desc = _(\"Results pending\")\n        purl = 'analysisrequests?analysisrequests_review_state=sample_received'\n        query['review_state'] = ['attachment_due',\n                                 'sample_received', ]\n        out.append(self._getStatistics(name, desc, purl, catalog, query, total))\n\n        # Samples to be verified\n        name = _('Samples to be verified')\n        desc = _(\"To be verified\")\n        purl = 'analysisrequests?analysisrequests_review_state=to_be_verified'\n        query['review_state'] = ['to_be_verified', ]\n        out.append(self._getStatistics(name, desc, purl, catalog, query, total))\n\n        # Samples verified (to be published)\n        name = _('Samples verified')\n        desc = _(\"Verified\")\n        purl = 'analysisrequests?analysisrequests_review_state=verified'\n        query['review_state'] = ['verified', ]\n        out.append(self._getStatistics(name, desc, purl, catalog, query, total))\n\n        # Samples published\n        name = _('Samples published')\n        desc = _(\"Published\")\n        purl = 'analysisrequests?analysisrequests_review_state=published'\n        query['review_state'] = ['published', ]\n        out.append(self._getStatistics(name, desc, purl, catalog, query, total))\n\n        # Samples to be printed\n        if self.context.bika_setup.getPrintingWorkflowEnabled():\n            name = _('Samples to be printed')\n            desc = _(\"To be printed\")\n            purl = 'analysisrequests?analysisrequests_getPrinted=0'\n            query['getPrinted'] = '0'\n            query['review_state'] = ['published', ]\n            out.append(\n                self._getStatistics(name, desc, purl, catalog, query, total))\n\n        # Chart with the evolution of ARs over a period, grouped by\n        # periodicity\n        outevo = self.fill_dates_evo(catalog, query)\n        out.append({'type':         'bar-chart-panel',\n                    'name':         _('Evolution of Samples'),\n                    'class':        'informative',\n                    'description':  _('Evolution of Samples'),\n                    'data':         json.dumps(outevo),\n                    'datacolors':   json.dumps(self.get_colors_palette())})\n\n        return {'id': 'analysisrequests',\n                'title': _('Samples'),\n                'panels': out}", "response": "Returns the section related with Analysis\n            Requests that contains some informative panels that are being published and that contains some informative panels that are being published and that are being processed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_worksheets_section(self):\n        out = []\n        bc = getToolByName(self.context, CATALOG_WORKSHEET_LISTING)\n        query = {'portal_type': \"Worksheet\", }\n\n        # Check if dashboard_cookie contains any values to query\n        # elements by\n        query = self._update_criteria_with_filters(query, 'worksheets')\n\n        # Active Worksheets (all)\n        total = self.search_count(query, bc.id)\n\n        # Open worksheets\n        name = _('Results pending')\n        desc = _('Results pending')\n        purl = 'worksheets?list_review_state=open'\n        query['review_state'] = ['open', 'attachment_due']\n        out.append(self._getStatistics(name, desc, purl, bc, query, total))\n\n        # Worksheets to be verified\n        name = _('To be verified')\n        desc = _('To be verified')\n        purl = 'worksheets?list_review_state=to_be_verified'\n        query['review_state'] = ['to_be_verified', ]\n        out.append(self._getStatistics(name, desc, purl, bc, query, total))\n\n        # Worksheets verified\n        name = _('Verified')\n        desc = _('Verified')\n        purl = 'worksheets?list_review_state=verified'\n        query['review_state'] = ['verified', ]\n        out.append(self._getStatistics(name, desc, purl, bc, query, total))\n\n        # Chart with the evolution of WSs over a period, grouped by\n        # periodicity\n        outevo = self.fill_dates_evo(bc, query)\n        out.append({'type':         'bar-chart-panel',\n                    'name':         _('Evolution of Worksheets'),\n                    'class':        'informative',\n                    'description':  _('Evolution of Worksheets'),\n                    'data':         json.dumps(outevo),\n                    'datacolors':   json.dumps(self.get_colors_palette())})\n\n        return {'id': 'worksheets',\n                'title': _('Worksheets'),\n                'panels': out}", "response": "Returns the section related with Worksheets and informative panels"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _fill_dates_evo(self, query_json, catalog_name, periodicity):\n        outevoidx = {}\n        outevo = []\n        days = 1\n        if periodicity == PERIODICITY_YEARLY:\n            days = 336\n        elif periodicity == PERIODICITY_BIANNUAL:\n            days = 168\n        elif periodicity == PERIODICITY_QUARTERLY:\n            days = 84\n        elif periodicity == PERIODICITY_MONTHLY:\n            days = 28\n        elif periodicity == PERIODICITY_WEEKLY:\n            days = 7\n        elif periodicity == PERIODICITY_ALL:\n            days = 336\n\n        # Get the date range\n        date_from, date_to = self.get_date_range(periodicity)\n        query = json.loads(query_json)\n        if 'review_state' in query:\n            del query['review_state']\n        query['sort_on'] = 'created'\n        query['created'] = {'query': (date_from, date_to),\n                            'range': 'min:max'}\n\n        otherstate = _('Other status')\n        statesmap = self.get_states_map(query['portal_type'])\n        stats = statesmap.values()\n        stats.sort()\n        stats.append(otherstate)\n        statscount = {s: 0 for s in stats}\n        # Add first all periods, cause we want all segments to be displayed\n        curr = date_from.asdatetime()\n        end = date_to.asdatetime()\n        while curr < end:\n            currstr = self._getDateStr(periodicity, DateTime(curr))\n            if currstr not in outevoidx:\n                outdict = {'date': currstr}\n                for k in stats:\n                    outdict[k] = 0\n                outevo.append(outdict)\n                outevoidx[currstr] = len(outevo)-1\n            curr = curr + datetime.timedelta(days=days)\n\n        brains = search(query, catalog_name)\n        for brain in brains:\n            created = brain.created\n            state = brain.review_state\n            if state not in statesmap:\n                logger.warn(\"'%s' State for '%s' not available\" % (state, query['portal_type']))\n            state = statesmap[state] if state in statesmap else otherstate\n            created = self._getDateStr(periodicity, created)\n            statscount[state] += 1\n            if created in outevoidx:\n                oidx = outevoidx[created]\n                if state in outevo[oidx]:\n                    outevo[oidx][state] += 1\n                else:\n                    outevo[oidx][state] = 1\n            else:\n                # Create new row\n                currow = {'date': created,\n                          state: 1}\n                outevo.append(currow)\n\n        # Remove all those states for which there is no data\n        rstates = [k for k, v in statscount.items() if v == 0]\n        for o in outevo:\n            for r in rstates:\n                if r in o:\n                    del o[r]\n\n        # Sort available status by number of occurences descending\n        sorted_states = sorted(statscount.items(), key=itemgetter(1))\n        sorted_states = map(lambda item: item[0], sorted_states)\n        sorted_states.reverse()\n        return {'data': outevo, 'states': sorted_states}", "response": "Fills the outevo dictionary with the amount of items created at a given date and grouped by review_state."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _update_criteria_with_filters(self, query, section_name):\n        if self.dashboard_cookie is None:\n            return query\n        cookie_criteria = self.dashboard_cookie.get(section_name)\n        if cookie_criteria == 'mine':\n            query['Creator'] = self.member.getId()\n        return query", "response": "This method updates the query dictionary with the criteria stored in the dashboard cookie."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list with the services supported by this reference sample", "response": "def getSupportedServices(self, only_uids=True):\n        \"\"\"Return a list with the services supported by this reference sample,\n        those for which there is a valid results range assigned in reference\n        results\n        :param only_uids: returns a list of uids or a list of objects\n        :return: list of uids or AnalysisService objects\n        \"\"\"\n        uids = map(lambda range: range['uid'], self.getReferenceResults())\n        uids = filter(api.is_uid, uids)\n        if only_uids:\n            return uids\n        brains = api.search({'UID': uids}, 'uid_catalog')\n        return map(api.get_object, brains)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn all analyses linked to this reference sample for a service", "response": "def getReferenceAnalysesService(self, service_uid):\n        \"\"\" return all analyses linked to this reference sample for a service \"\"\"\n        analyses = []\n        for analysis in self.objectValues('ReferenceAnalysis'):\n            if analysis.getServiceUID() == service_uid:\n                analyses.append(analysis)\n        return analyses"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef addReferenceAnalysis(self, service):\n        if api.is_uid(service) or api.is_brain(service):\n            return self.addReferenceAnalysis(api.get_object(service))\n\n        if not IAnalysisService.providedBy(service):\n            return None\n\n        interim_fields = service.getInterimFields()\n        analysis = _createObjectByType(\"ReferenceAnalysis\", self, id=tmpID())\n        # Copy all the values from the schema\n        # TODO Add Service as a param in ReferenceAnalysis constructor and do\n        #      this logic there instead of here\n        discard = ['id', ]\n        keys = service.Schema().keys()\n        for key in keys:\n            if key in discard:\n                continue\n            if key not in analysis.Schema().keys():\n                continue\n            val = service.getField(key).get(service)\n            # Campbell's mental note:never ever use '.set()' directly to a\n            # field. If you can't use the setter, then use the mutator in order\n            # to give the value. We have realized that in some cases using\n            # 'set' when the value is a string, it saves the value\n            # as unicode instead of plain string.\n            # analysis.getField(key).set(analysis, val)\n            mutator_name = analysis.getField(key).mutator\n            mutator = getattr(analysis, mutator_name)\n            mutator(val)\n        analysis.setAnalysisService(service)\n        ref_type = self.getBlank() and 'b' or 'c'\n        analysis.setReferenceType(ref_type)\n        analysis.setInterimFields(interim_fields)\n        analysis.unmarkCreationFlag()\n        renameAfterCreation(analysis)\n        return analysis", "response": "Adds a new ReferenceAnalysis object based on this Sample and the type passed in and associates the newly created object to the Analysis Service passed in."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting all services for this Sample", "response": "def getServices(self):\n        \"\"\" get all services for this Sample \"\"\"\n        tool = getToolByName(self, REFERENCE_CATALOG)\n        services = []\n        for spec in self.getReferenceResults():\n            service = tool.lookupObject(spec['uid'])\n            services.append(service)\n        return services"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef isValid(self):\n        today = DateTime()\n        expiry_date = self.getExpiryDate()\n        if expiry_date and today > expiry_date:\n            return False\n        # TODO: Do We really need ExpiryDate + DateExpired? Any difference?\n        date_expired = self.getDateExpired()\n        if date_expired and today > date_expired:\n            return False\n\n        date_disposed = self.getDateDisposed()\n        if date_disposed and today > date_disposed:\n            return False\n\n        return True", "response": "Returns if the current Reference Sample is valid. This is only valid if the current Reference Sample has not been expired nor disposed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef before_unassign(analysis):\n    worksheet = analysis.getWorksheet()\n    if not worksheet:\n        return\n\n    # Removal of a routine analysis causes the removal of their duplicates\n    for dup in worksheet.get_duplicates_for(analysis):\n        doActionFor(dup, \"unassign\")", "response": "Function triggered before unassign transition takes place"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfunctions triggered before reject transition takes place", "response": "def before_reject(analysis):\n    \"\"\"Function triggered before 'unassign' transition takes place\n    \"\"\"\n    worksheet = analysis.getWorksheet()\n    if not worksheet:\n        return\n\n    # Rejection of a routine analysis causes the removal of their duplicates\n    for dup in worksheet.get_duplicates_for(analysis):\n        doActionFor(dup, \"unassign\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef after_retract(analysis):\n\n    # Retract our dependents (analyses that depend on this analysis)\n    cascade_to_dependents(analysis, \"retract\")\n\n    # Retract our dependencies (analyses this analysis depends on)\n    promote_to_dependencies(analysis, \"retract\")\n\n    # Rename the analysis to make way for it's successor.\n    # Support multiple retractions by renaming to *-0, *-1, etc\n    parent = analysis.aq_parent\n    keyword = analysis.getKeyword()\n\n    # Get only those that are analyses and with same keyword as the original\n    analyses = parent.getAnalyses(full_objects=True)\n    analyses = filter(lambda an: an.getKeyword() == keyword, analyses)\n    # TODO This needs to get managed by Id server in a nearly future!\n    new_id = '{}-{}'.format(keyword, len(analyses))\n\n    # Create a copy of the retracted analysis\n    an_uid = api.get_uid(analysis)\n    new_analysis = create_analysis(parent, analysis, id=new_id, RetestOf=an_uid)\n    new_analysis.setResult(\"\")\n    new_analysis.setResultCaptureDate(None)\n    new_analysis.reindexObject()\n    logger.info(\"Retest for {} ({}) created: {}\".format(\n        keyword, api.get_id(analysis), api.get_id(new_analysis)))\n\n    # Assign the new analysis to this same worksheet, if any.\n    worksheet = analysis.getWorksheet()\n    if worksheet:\n        worksheet.addAnalysis(new_analysis)\n\n    # Try to rollback the Analysis Request\n    if IRequestAnalysis.providedBy(analysis):\n        doActionFor(analysis.getRequest(), \"rollback_to_receive\")\n        reindex_request(analysis)", "response": "Function triggered after a retract transition for the passed analysis is performed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef after_reject(analysis):\n    # Remove from the worksheet\n    remove_analysis_from_worksheet(analysis)\n\n    # Reject our dependents (analyses that depend on this analysis)\n    cascade_to_dependents(analysis, \"reject\")\n\n    if IRequestAnalysis.providedBy(analysis):\n        # Try verify (for when remaining analyses are in 'verified')\n        doActionFor(analysis.getRequest(), \"verify\")\n\n        # Try submit (remaining analyses are in 'to_be_verified')\n        doActionFor(analysis.getRequest(), \"submit\")\n\n        # Try rollback (no remaining analyses or some not submitted)\n        doActionFor(analysis.getRequest(), \"rollback_to_receive\")\n        reindex_request(analysis)", "response": "Function triggered after the reject transition for the passed in analysis is performed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef after_verify(analysis):\n    # Mark this analysis as IVerified\n    alsoProvides(analysis, IVerified)\n\n    # Promote to analyses this analysis depends on\n    promote_to_dependencies(analysis, \"verify\")\n\n    # TODO: REFLEX TO REMOVE\n    # Do all the reflex rules process\n    if IRequestAnalysis.providedBy(analysis):\n        analysis._reflex_rule_process('verify')\n\n    # Promote transition to worksheet\n    ws = analysis.getWorksheet()\n    if ws:\n        doActionFor(ws, 'verify')\n        push_reindex_to_actions_pool(ws)\n\n    # Promote transition to Analysis Request\n    if IRequestAnalysis.providedBy(analysis):\n        doActionFor(analysis.getRequest(), 'verify')\n        reindex_request(analysis)", "response": "This function is triggered after a verify transition for the passed in analysis is performed. Promotes the transition to the Analysis Request and to\n    Worksheets and the Analysis Request and to\n    Worksheets are pushed to actions pool and reindex_request and reindex_request."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove the analysis passed in from the worksheet if assigned to any", "response": "def remove_analysis_from_worksheet(analysis):\n    \"\"\"Removes the analysis passed in from the worksheet, if assigned to any\n    \"\"\"\n    worksheet = analysis.getWorksheet()\n    if not worksheet:\n        return\n\n    analyses = filter(lambda an: an != analysis, worksheet.getAnalyses())\n    worksheet.setAnalyses(analyses)\n    worksheet.purgeLayout()\n    if analyses:\n        # Maybe this analysis was the only one that was not yet submitted or\n        # verified, so try to submit or verify the Worksheet to be aligned\n        # with the current states of the analyses it contains.\n        doActionFor(worksheet, \"submit\")\n        doActionFor(worksheet, \"verify\")\n    else:\n        # We've removed all analyses. Rollback to \"open\"\n        doActionFor(worksheet, \"rollback_to_open\")\n\n    # Reindex the Worksheet\n    idxs = [\"getAnalysesUIDs\"]\n    push_reindex_to_actions_pool(worksheet, idxs=idxs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove the action Quality Control from Reports", "response": "def remove_qc_reports(portal):\n    \"\"\"Removes the action Quality Control from Reports\n    \"\"\"\n    logger.info(\"Removing Reports > Quality Control ...\")\n    ti = portal.reports.getTypeInfo()\n    actions = map(lambda action: action.id, ti._actions)\n    for index, action in enumerate(actions, start=0):\n        if action == 'qualitycontrol':\n            ti.deleteActions([index])\n            break\n    logger.info(\"Removing Reports > Quality Control [DONE]\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef purge_portlets(portal):\n    logger.info(\"Purging portlets ...\")\n\n    def remove_portlets(context_portlet):\n        mapping = portal.restrictedTraverse(context_portlet)\n        for key in mapping.keys():\n            if key not in PORTLETS_TO_PURGE:\n                logger.info(\"Skipping portlet: '{}'\".format(key))\n                continue\n            logger.info(\"Removing portlet: '{}'\".format(key))\n            del mapping[key]\n\n    remove_portlets(\"++contextportlets++plone.leftcolumn\")\n    remove_portlets(\"++contextportlets++plone.rightcolumn\")\n\n    # Reimport the portlets profile\n    setup = portal.portal_setup\n    setup.runImportStepFromProfile(profile, 'portlets')\n    logger.info(\"Purging portlets [DONE]\")", "response": "Remove old portlets. Leave the Navigation portlet only\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setup_partitioning(portal):\n    logger.info(\"Setting up the enhanced partitioning system\")\n\n    # Add \"Create partition\" transition\n    add_create_partition_transition(portal)\n\n    # Add getAncestorsUIDs index in analyses catalog\n    add_partitioning_indexes(portal)\n\n    # Adds metadata columns for partitioning\n    add_partitioning_metadata(portal)\n\n    # Setup default ID formatting for partitions\n    set_partitions_id_formatting(portal)", "response": "Setups the enhanced partitioning system"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds the indexes for partitioning", "response": "def add_partitioning_indexes(portal):\n    \"\"\"Adds the indexes for partitioning\n    \"\"\"\n    logger.info(\"Adding partitioning indexes\")\n\n    add_index(portal, catalog_id=CATALOG_ANALYSIS_LISTING,\n              index_name=\"getAncestorsUIDs\",\n              index_attribute=\"getAncestorsUIDs\",\n              index_metatype=\"KeywordIndex\")\n\n    add_index(portal, catalog_id=CATALOG_ANALYSIS_REQUEST_LISTING,\n              index_name=\"isRootAncestor\",\n              index_attribute=\"isRootAncestor\",\n              index_metatype=\"BooleanIndex\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_partitioning_metadata(portal):\n    logger.info(\"Adding partitioning metadata\")\n\n    add_metadata(portal, CATALOG_ANALYSIS_REQUEST_LISTING,\n                 'getRawParentAnalysisRequest')\n\n    add_metadata(portal, CATALOG_ANALYSIS_REQUEST_LISTING,\n                 \"getDescendantsUIDs\")", "response": "Add columns required for partitioning machinery\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving sample_prep and sample_prep_complete transitions", "response": "def remove_sample_prep_workflow(portal):\n    \"\"\"Removes sample_prep and sample_prep_complete transitions\n    \"\"\"\n    # There is no need to walk through objects because of:\n    # https://github.com/senaite/senaite.core/blob/master/bika/lims/upgrade/v01_02_008.py#L187\n    logger.info(\"Removing 'sample_prep' related states and transitions ...\")\n    workflow_ids = [\"bika_sample_workflow\",\n                    \"bika_ar_workflow\",\n                    \"bika_analysis_workflow\"]\n    to_remove = [\"sample_prep\", \"sample_prep_complete\"]\n    wf_tool = api.get_tool(\"portal_workflow\")\n    for wf_id in workflow_ids:\n        workflow = wf_tool.getWorkflowById(wf_id)\n        for state_trans in to_remove:\n            if state_trans in workflow.transitions:\n                logger.info(\"Removing transition '{}' from '{}'\"\n                            .format(state_trans, wf_id))\n                workflow.transitions.deleteTransitions([state_trans])\n            if state_trans in workflow.states:\n                logger.info(\"Removing state '{}' from '{}'\"\n                            .format(state_trans, wf_id))\n                workflow.states.deleteStates([state_trans])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rebind_calculations(portal):\n    logger.info(\"Rebinding calculations to analyses ...\")\n    review_states = [\"sample_due\",\n                     \"attachment_due\",\n                     \"sample_received\",\n                     \"to_be_verified\"]\n\n    calcs = {}\n    brains = api.search(dict(portal_type=\"Calculation\"), \"bika_setup_catalog\")\n    for calc in brains:\n        calc = api.get_object(calc)\n        calc.setFormula(calc.getFormula())\n        calcs[api.get_uid(calc)] = calc\n\n    query = dict(review_state=review_states)\n    analyses = api.search(query, CATALOG_ANALYSIS_LISTING)\n    total = len(analyses)\n    for num, analysis in enumerate(analyses):\n        if num % 100 == 0:\n            logger.info(\"Rebinding calculations to analyses: {0}/{1}\"\n                        .format(num, total))\n        analysis = api.get_object(analysis)\n        calc = analysis.getCalculation()\n        if not calc:\n            continue\n\n        calc_uid = api.get_uid(calc)\n        if calc_uid not in calcs:\n            logger.warn(\"Calculation with UID {} not found!\".format(calc_uid))\n            continue\n\n        calc = calcs[calc_uid]\n        an_interims = analysis.getInterimFields()\n        an_interims_keys = map(lambda interim: interim.get('keyword'),\n                               an_interims)\n        calc_interims = filter(lambda interim: interim.get('keyword')\n                                            not in an_interims_keys,\n                            calc.getInterimFields())\n        analysis.setCalculation(calc)\n        analysis.setInterimFields(an_interims + calc_interims)", "response": "Rebinds calculations of active analyses to analyses. The analysis Calculation cannot resolve DependentServices."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reindex_multifiles(portal):\n    logger.info(\"Reindexing Multifiles ...\")\n\n    brains = api.search(dict(portal_type=\"Multifile\"), \"bika_setup_catalog\")\n    total = len(brains)\n\n    for num, brain in enumerate(brains):\n        if num % 100 == 0:\n            logger.info(\"Reindexing Multifile: {0}/{1}\".format(num, total))\n        obj = api.get_object(brain)\n        obj.reindexObject()", "response": "Reindex Multifiles to be searchable by the catalog"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_reference_sample_indexes(portal):\n    logger.info(\"Adding reference sample indexes\")\n\n    add_index(portal, catalog_id=\"bika_catalog\",\n              index_name=\"getSupportedServices\",\n              index_attribute=\"getSupportedServices\",\n              index_metatype=\"KeywordIndex\")\n\n    add_index(portal, catalog_id=\"bika_catalog\",\n              index_name=\"getBlank\",\n              index_attribute=\"getBlank\",\n              index_metatype=\"BooleanIndex\")\n\n    add_index(portal, catalog_id=\"bika_catalog\",\n              index_name=\"isValid\",\n              index_attribute=\"isValid\",\n              index_metatype=\"BooleanIndex\")", "response": "Adds the indexes for partitioning\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves the view Not requested analyses", "response": "def remove_not_requested_analyses_view(portal):\n    \"\"\"Remove the view 'Not requested analyses\" from inside AR\n    \"\"\"\n    logger.info(\"Removing 'Analyses not requested' view ...\")\n    ar_ptype = portal.portal_types.AnalysisRequest\n    ar_ptype._actions = filter(lambda act: act.id != \"analyses_not_requested\",\n                               ar_ptype.listActions())"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the catalogs from the site", "response": "def get_catalogs(portal):\n    \"\"\"Returns the catalogs from the site\n    \"\"\"\n    res = []\n    for object in portal.objectValues():\n        if ICatalogTool.providedBy(object):\n            res.append(object)\n        elif IZCatalog.providedBy(object):\n            res.append(object)\n    res.sort()\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds indexes for better worksheet handling", "response": "def add_worksheet_indexes(portal):\n    \"\"\"Add indexes for better worksheet handling\n    \"\"\"\n    logger.info(\"Adding worksheet indexes\")\n\n    add_index(portal, catalog_id=\"bika_analysis_catalog\",\n              index_name=\"getCategoryTitle\",\n              index_attribute=\"getCategoryTitle\",\n              index_metatype=\"FieldIndex\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving all bika_listing resources", "response": "def remove_bika_listing_resources(portal):\n    \"\"\"Remove all bika_listing resources\n    \"\"\"\n    logger.info(\"Removing bika_listing resouces\")\n\n    REMOVE_JS = [\n        \"++resource++bika.lims.js/bika.lims.bikalisting.js\",\n        \"++resource++bika.lims.js/bika.lims.bikalistingfilterbar.js\",\n    ]\n\n    REMOVE_CSS = [\n        \"bika_listing.css\",\n    ]\n\n    for js in REMOVE_JS:\n        logger.info(\"********** Unregistering JS %s\" % js)\n        portal.portal_javascripts.unregisterResource(js)\n\n    for css in REMOVE_CSS:\n        logger.info(\"********** Unregistering CSS %s\" % css)\n        portal.portal_css.unregisterResource(css)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hide_samples(portal):\n    logger.info(\"Removing Samples from navbar ...\")\n    if \"samples\" in portal:\n        portal.manage_delObjects([\"samples\"])\n\n    def remove_samples_action(content_type):\n        type_info = content_type.getTypeInfo()\n        actions = map(lambda action: action.id, type_info._actions)\n        for index, action in enumerate(actions, start=0):\n            if action == 'samples':\n                type_info.deleteActions([index])\n                break\n\n    def remove_actions_from_sample(sample):\n        type_info = sample.getTypeInfo()\n        idxs = [index for index, value in enumerate(type_info._actions)]\n        type_info.deleteActions(idxs)\n\n    logger.info(\"Removing Samples action view from inside Clients ...\")\n    for client in portal.clients.objectValues(\"Client\"):\n        remove_samples_action(client)\n\n    logger.info(\"Removing Samples action view from inside Batches ...\")\n    for batch in portal.batches.objectValues(\"Batch\"):\n        remove_samples_action(batch)\n\n    logger.info(\"Removing actions from inside Samples ...\")\n    for sample in api.search(dict(portal_type=\"Sample\"), \"bika_catalog\"):\n        remove_actions_from_sample(api.get_object(sample))\n    commit_transaction(portal)", "response": "Removes samples views from everywhere related indexes etc."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfixing inconsistencies between analyses and the ARs they belong to when is in a canceled invalidated or rejected state", "response": "def fix_ar_analyses_inconsistencies(portal):\n    \"\"\"Fixes inconsistencies between analyses and the ARs they belong to when\n    the AR is in a \"cancelled\", \"invalidated\" or \"rejected state\n    \"\"\"\n    def fix_analyses(request, status):\n        wf_id = \"bika_analysis_workflow\"\n        workflow = api.get_tool(\"portal_workflow\").getWorkflowById(wf_id)\n        review_states = ['assigned', 'unassigned', 'to_be_verified']\n        query = dict(portal_type=\"Analysis\",\n                     getRequestUID=api.get_uid(request),\n                     review_state=review_states)\n        for brain in api.search(query, CATALOG_ANALYSIS_LISTING):\n            analysis = api.get_object(brain)\n            # If the analysis is assigned to a worksheet, unassign first\n            ws = analysis.getWorksheet()\n            if ws:\n                remove_analysis_from_worksheet(analysis)\n                reindex_request(analysis)\n\n            # Force the new state\n            changeWorkflowState(analysis, wf_id, status)\n            workflow.updateRoleMappingsFor(analysis)\n            analysis.reindexObject(idxs=[\"review_state\", \"is_active\"])\n\n    def fix_ar_analyses(status, wf_state_id=\"review_state\"):\n        brains = api.search({wf_state_id: status},\n                            CATALOG_ANALYSIS_REQUEST_LISTING)\n        total = len(brains)\n        for num, brain in enumerate(brains):\n            if num % 100 == 0:\n                logger.info(\"Fixing inconsistent analyses from {} ARs: {}/{}\"\n                            .format(status, num, total))\n            fix_analyses(brain, status)\n\n    logger.info(\"Fixing Analysis Request - Analyses inconsistencies ...\")\n    pool = ActionHandlerPool.get_instance()\n    pool.queue_pool()\n    fix_ar_analyses(\"cancelled\")\n    fix_ar_analyses(\"invalid\")\n    fix_ar_analyses(\"rejected\")\n    pool.resume()\n    commit_transaction(portal)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd getProgressPercentage metadata to worksheets catalog", "response": "def add_worksheet_progress_percentage(portal):\n    \"\"\"Adds getProgressPercentage metadata to worksheets catalog\n    \"\"\"\n    add_metadata(portal, CATALOG_WORKSHEET_LISTING, \"getProgressPercentage\")\n    logger.info(\"Reindexing Worksheets ...\")\n    query = dict(portal_type=\"Worksheet\")\n    brains = api.search(query, CATALOG_WORKSHEET_LISTING)\n    total = len(brains)\n    for num, brain in enumerate(brains):\n        if num % 100 == 0:\n            logger.info(\"Reindexing open Worksheets: {}/{}\"\n                        .format(num, total))\n        worksheet = api.get_object(brain)\n        worksheet.reindexObject()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving indexes and metadata for getDepartmentUIDs", "response": "def remove_get_department_uids(portal):\n    \"\"\"Removes getDepartmentUIDs indexes and metadata\n    \"\"\"\n    logger.info(\"Removing filtering by department ...\")\n    del_index(portal, \"bika_catalog\", \"getDepartmentUIDs\")\n    del_index(portal, \"bika_setup_catalog\", \"getDepartmentUID\")\n    del_index(portal, CATALOG_ANALYSIS_REQUEST_LISTING, \"getDepartmentUIDs\")\n    del_index(portal, CATALOG_WORKSHEET_LISTING, \"getDepartmentUIDs\")\n    del_index(portal, CATALOG_ANALYSIS_LISTING, \"getDepartmentUID\")\n\n    del_metadata(portal, CATALOG_ANALYSIS_REQUEST_LISTING, \"getDepartmentUIDs\")\n    del_metadata(portal, CATALOG_WORKSHEET_LISTING, \"getDepartmentUIDs\")\n    del_metadata(portal, CATALOG_ANALYSIS_LISTING, \"getDepartmentUID\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef change_analysis_requests_id_formatting(portal, p_type=\"AnalysisRequest\"):\n    ar_id_format = dict(\n        form='{sampleType}-{seq:04d}',\n        portal_type='AnalysisRequest',\n        prefix='analysisrequest',\n        sequence_type='generated',\n        counter_type='',\n        split_length=1)\n\n    bs = portal.bika_setup\n    id_formatting = bs.getIDFormatting()\n    ar_format = filter(lambda id: id[\"portal_type\"] == p_type, id_formatting)\n    if p_type==\"AnalysisRequest\":\n        logger.info(\"Set ID Format for Analysis Request portal_type ...\")\n        if not ar_format or \"sample\" in ar_format[0][\"form\"]:\n            # Copy the ID formatting set for Sample\n            change_analysis_requests_id_formatting(portal, p_type=\"Sample\")\n            return\n        else:\n            logger.info(\"ID Format for Analysis Request already set: {} [SKIP]\"\n                        .format(ar_format[0][\"form\"]))\n            return\n    else:\n        ar_format = ar_format and ar_format[0].copy() or ar_id_format\n\n    # Set the Analysis Request ID Format\n    ar_id_format.update(ar_format)\n    ar_id_format[\"portal_type\"] =\"AnalysisRequest\"\n    ar_id_format[\"prefix\"] = \"analysisrequest\"\n    set_id_format(portal, ar_id_format)\n\n    # Find out the last ID for Sample and reseed AR to prevent ID already taken\n    # errors on AR creation\n    if p_type == \"Sample\":\n        number_generator = getUtility(INumberGenerator)\n        ar_keys = dict()\n        ar_keys_prev = dict()\n        for key, value in number_generator.storage.items():\n            if \"sample-\" in key:\n                ar_key = key.replace(\"sample-\", \"analysisrequest-\")\n                ar_keys[ar_key] = api.to_int(value, 0)\n            elif \"analysisrequest-\" in key:\n                ar_keys_prev[key] = api.to_int(value, 0)\n\n        for key, value in ar_keys.items():\n            if key in ar_keys_prev:\n                # Maybe this upgrade step has already been run, so we don't\n                # want the ar IDs to be reseeded again!\n                if value <= ar_keys_prev[key]:\n                    logger.info(\"ID for '{}' already seeded to '{}' [SKIP]\"\n                                .format(key, ar_keys_prev[key]))\n                    continue\n            logger.info(\"Seeding {} to {}\".format(key, value))\n            number_generator.set_number(key, value)", "response": "Applies the system s Sample ID Formatting to Analysis Request\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_id_format(portal, format):\n    bs = portal.bika_setup\n    if 'portal_type' not in format:\n        return\n    logger.info(\"Applying format {} for {}\".format(format.get('form', ''),\n                                                   format.get(\n                                                       'portal_type')))\n    portal_type = format['portal_type']\n    ids = list()\n    id_map = bs.getIDFormatting()\n    for record in id_map:\n        if record.get('portal_type', '') == portal_type:\n            continue\n        ids.append(record)\n    ids.append(format)\n    bs.setIDFormatting(ids)", "response": "Sets the id formatting in setup for the format provided\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving stale javascripts from the portal", "response": "def remove_stale_javascripts(portal):\n    \"\"\"Removes stale javascripts\n    \"\"\"\n    logger.info(\"Removing stale javascripts ...\")\n    for js in JAVASCRIPTS_TO_REMOVE:\n        logger.info(\"Unregistering JS %s\" % js)\n        portal.portal_javascripts.unregisterResource(js)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves stale CSS from the current page", "response": "def remove_stale_css(portal):\n    \"\"\"Removes stale CSS\n    \"\"\"\n    logger.info(\"Removing stale css ...\")\n    for css in CSS_TO_REMOVE:\n        logger.info(\"Unregistering CSS %s\" % css)\n        portal.portal_css.unregisterResource(css)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove stale indexes and metadata from bika_catalog.", "response": "def remove_stale_indexes_from_bika_catalog(portal):\n    \"\"\"Removes stale indexes and metadata from bika_catalog. Most of these\n    indexes and metadata were used for Samples, but they are no longer used.\n    \"\"\"\n    logger.info(\"Removing stale indexes and metadata from bika_catalog ...\")\n    cat_id = \"bika_catalog\"\n    indexes_to_remove = [\n        \"getAnalyst\",\n        \"getAnalysts\",\n        \"getAnalysisService\",\n        \"getClientOrderNumber\",\n        \"getClientReference\",\n        \"getClientSampleID\",\n        \"getContactTitle\",\n        \"getDateDisposed\",\n        \"getDateExpired\",\n        \"getDateOpened\",\n        \"getDatePublished\",\n        \"getInvoiced\",\n        \"getPreserver\",\n        \"getSamplePointTitle\",\n        \"getSamplePointUID\",\n        \"getSampler\",\n        \"getScheduledSamplingSampler\",\n        \"getSamplingDate\",\n        \"getWorksheetTemplateTitle\",\n        \"BatchUID\",\n    ]\n    metadata_to_remove = [\n        \"getAnalysts\",\n        \"getClientOrderNumber\",\n        \"getClientReference\",\n        \"getClientSampleID\",\n        \"getContactTitle\",\n        \"getSamplePointTitle\",\n        \"getAnalysisService\",\n        \"getDatePublished\",\n    ]\n    for index in indexes_to_remove:\n        del_index(portal, cat_id, index)\n\n    for metadata in metadata_to_remove:\n        del_metadata(portal, cat_id, metadata)\n    commit_transaction(portal)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fix_worksheet_status_inconsistencies(portal):\n    logger.info(\"Fixing worksheet inconsistencies ...\")\n    query = dict(portal_type=\"Worksheet\",\n                 review_state=[\"open\", \"to_be_verified\"])\n    brains = api.search(query, CATALOG_WORKSHEET_LISTING)\n    total = len(brains)\n    for num, brain in enumerate(brains):\n        success = False\n        if num % 100 == 0:\n            logger.info(\"Fixing worksheet inconsistencies: {}/{}\"\n                        .format(num, total))\n        # Note we don't check anything, WS guards for \"submit\" and \"verify\"\n        # will take care of checking if the status of contained analyses allows\n        # the transition.\n        worksheet = api.get_object(brain)\n        if api.get_workflow_status_of(worksheet) == \"open\":\n            success, msg = do_action_for(worksheet, \"submit\")\n        elif api.get_workflow_status_of(worksheet) == \"to_be_verified\":\n            success, msg = do_action_for(worksheet, \"verify\")\n\n        if success:\n            logger.info(\"Worksheet {} transitioned to 'to_be_verified'\"\n                        .format(worksheet.getId()))\n            success, msg = do_action_for(worksheet, \"verify\")\n            if success:\n                logger.info(\"Worksheet {} transitioned to 'verified'\"\n                            .format(worksheet.getId()))\n    commit_transaction(portal)", "response": "Walks through open worksheets and transition them to verified or to_be_verified if all the analyses are not in an open status\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apply_analysis_request_partition_interface(portal):\n    logger.info(\"Applying 'IAnalysisRequestPartition' marker interface ...\")\n    query = dict(portal_type=\"AnalysisRequest\", isRootAncestor=False)\n    brains = api.search(query, CATALOG_ANALYSIS_REQUEST_LISTING)\n    total = len(brains)\n    for num, brain in enumerate(brains):\n        if num % 100 == 0:\n            logger.info(\"Applying 'IAnalysisRequestPartition' interface: {}/{}\"\n                        .format(num, total))\n        ar = api.get_object(brain)\n        if IAnalysisRequestPartition.providedBy(ar):\n            continue\n        if ar.getParentAnalysisRequest():\n            alsoProvides(ar, IAnalysisRequestPartition)\n    commit_transaction(portal)", "response": "Walks trhough all AR - like partitions and applies the IAnalysisRequestPartition marker interface to them"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_ar_listing_catalog(portal):\n    cat_id = CATALOG_ANALYSIS_REQUEST_LISTING\n\n    catalog = api.get_tool(cat_id)\n\n    logger.info(\"Updating Indexes/Metadata of Catalog '{}'\".format(cat_id))\n\n    indexes_to_add = [\n        # name, attribute, metatype\n        (\"getClientID\", \"getClientID\", \"FieldIndex\"),\n        (\"is_active\", \"is_active\", \"BooleanIndex\"),\n        (\"is_received\", \"is_received\", \"BooleanIndex\"),\n    ]\n\n    metadata_to_add = [\n        \"getClientID\",\n    ]\n\n    for index in indexes_to_add:\n        add_index(portal, cat_id, *index)\n\n    for metadata in metadata_to_add:\n        refresh = metadata not in catalog.schema()\n        add_metadata(portal, cat_id, metadata, refresh_catalog=refresh)", "response": "Add Indexes and Metadata to bika_catalog_analysisrequest_listing\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_notify_on_sample_invalidation(portal):\n    setup = api.get_setup()\n\n    # NotifyOnARRetract --> NotifyOnSampleInvalidation\n    old_value = setup.__dict__.get(\"NotifyOnARRetract\", True)\n    setup.setNotifyOnSampleInvalidation(old_value)\n\n    # NotifyOnRejection --> NotifyOnSampleRejection\n    old_value = setup.__dict__.get(\"NotifyOnRejection\", False)\n    setup.setNotifyOnSampleRejection(old_value)", "response": "Update the NotifyOnARRetract and NotifyOnRejection fields in the API."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_secondary_id_formatting(portal):\n    secondary_id_format = dict(\n        form=\"{parent_ar_id}-S{secondary_count:02d}\",\n        portal_type=\"AnalysisRequestSecondary\",\n        prefix=\"analysisrequestsecondary\",\n        sequence_type=\"\")\n    set_id_format(portal, secondary_id_format)", "response": "Sets the default id formatting for secondary ARs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fix_calculation_version_inconsistencies(portal):\n    logger.info(\"Fix Calculation version inconsistencies ...\")\n    brains = api.search({\"portal_type\": \"Calculation\"}, \"bika_setup_catalog\")\n    total = len(brains)\n    for num, brain in enumerate(brains):\n        if num % 100 == 0:\n            logger.info(\"Fix Calculation version inconsistencies: {}/{}\"\n                        .format(num, total))\n        calc = api.get_object(brain)\n        version = getattr(calc, \"version_id\", None)\n        if version is None:\n            pr = api.get_tool(\"portal_repository\")\n            pr.save(obj=calc, comment=\"First version\")\n            logger.info(\"First version created for {}\".format(calc.Title()))\n    logger.info(\"Fix Calculation version inconsistencies [DONE]\")", "response": "Creates the first version of all Calculations that have not been yet edited"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves all existing invoices inside the client and removes the invoices folder with the invoice batches", "response": "def remove_invoices(portal):\n    \"\"\"Moves all existing invoices inside the client and removes the invoices\n    folder with the invoice batches\n    \"\"\"\n    logger.info(\"Unlink Invoices\")\n\n    invoices = portal.get(\"invoices\")\n    if invoices is None:\n        return\n\n    for batch in invoices.objectValues():\n        for invoice in batch.objectValues():\n            invoice_id = invoice.getId()\n            client = invoice.getClient()\n            if not client:\n                # invoice w/o a client -> remove\n                batch.manage_delObjects(invoice_id)\n                continue\n\n            if invoice_id in client.objectIds():\n                continue\n\n            # move invoices inside the client\n            cp = batch.manage_cutObjects(invoice_id)\n            client.manage_pasteObjects(cp)\n\n    # delete the invoices folder\n    portal.manage_delObjects(invoices.getId())"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list with the workflow ids bound to the type of the object passed in", "response": "def get_workflow_ids_for(brain_or_object):\n    \"\"\"Returns a list with the workflow ids bound to the type of the object\n    passed in\n    \"\"\"\n    portal_type = api.get_portal_type(brain_or_object)\n    wf_ids = workflow_ids_by_type.get(portal_type, None)\n    if wf_ids:\n        return wf_ids\n\n    workflow_ids_by_type[portal_type] = api.get_workflows_for(brain_or_object)\n    return workflow_ids_by_type[portal_type]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list with the actions supported by the workflows that this object is bound to.", "response": "def get_workflow_actions_for(brain_or_object):\n    \"\"\"Returns a list with the actions (transitions) supported by the workflows\n    the object pass in is bound to. Note it returns all actions, not only those\n    allowed for the object based on its current state and permissions.\n    \"\"\"\n    portal_type = api.get_portal_type(brain_or_object)\n    actions = actions_by_type.get(portal_type, None)\n    if actions:\n        return actions\n\n    # Retrieve the actions from the workflows this object is bound to\n    actions = []\n    wf_tool = api.get_tool(\"portal_workflow\")\n    for wf_id in get_workflow_ids_for(brain_or_object):\n        workflow = wf_tool.getWorkflowById(wf_id)\n        wf_actions = map(lambda action: action[0], workflow.transitions.items())\n        actions.extend(wf_actions)\n\n    actions = list(set(actions))\n    actions_by_type[portal_type] = actions\n    return actions"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list with the states supported by the workflows that this object is bound to", "response": "def get_workflow_states_for(brain_or_object):\n    \"\"\"Returns a list with the states supported by the workflows the object\n    passed in is bound to\n    \"\"\"\n    portal_type = api.get_portal_type(brain_or_object)\n    states = states_by_type.get(portal_type, None)\n    if states:\n        return states\n\n    # Retrieve the states from the workflows this object is bound to\n    states = []\n    wf_tool = api.get_tool(\"portal_workflow\")\n    for wf_id in get_workflow_ids_for(brain_or_object):\n        workflow = wf_tool.getWorkflowById(wf_id)\n        wf_states = map(lambda state: state[0], workflow.states.items())\n        states.extend(wf_states)\n\n    states = list(set(states))\n    states_by_type[portal_type] = states\n    return states"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef restore_review_history_for_affected_objects(portal):\n    logger.info(\"Restoring review_history ...\")\n    query = dict(portal_type=NEW_SENAITE_WORKFLOW_BINDINGS)\n    brains = api.search(query, UID_CATALOG)\n    total = len(brains)\n    done = 0\n    for num, brain in enumerate(brains):\n        if num % 100 == 0:\n            logger.info(\"Restoring review_history: {}/{}\"\n                        .format(num, total))\n\n        review_history = api.get_review_history(brain, rev=False)\n        if review_history:\n            # Nothing to do. The object already has the review history set\n            continue\n\n        # Object without review history. Set the review_history manually\n        restore_review_history_for(brain)\n\n        done += 1\n        if done % 1000 == 0:\n            commit_transaction(portal)\n\n    logger.info(\"Restoring review history: {} processed [DONE]\".format(done))", "response": "Applies the review history for objects that are bound to new senaite_* workflows\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef restore_review_history_for(brain_or_object):\n    # Get the review history. Note this comes sorted from oldest to newest\n    review_history = get_purged_review_history_for(brain_or_object)\n\n    obj = api.get_object(brain_or_object)\n    wf_tool = api.get_tool(\"portal_workflow\")\n    wf_ids = get_workflow_ids_for(brain_or_object)\n    wfs = map(lambda wf_id: wf_tool.getWorkflowById(wf_id), wf_ids)\n    wfs = filter(lambda wf: wf.state_var == \"review_state\", wfs)\n    if not wfs:\n        logger.error(\"No valid workflow found for {}\".format(api.get_id(obj)))\n    else:\n        # It should not be possible to have more than one workflow with same\n        # state_variable here. Anyhow, we don't care in this case (we only want\n        # the object to have a review history).\n        workflow = wfs[0]\n        create_action = False\n        for history in review_history:\n            action_id = history[\"action\"]\n            if action_id is None:\n                if create_action:\n                    # We don't want multiple creation events, we only stick to\n                    # one workflow, so if this object had more thone one wf\n                    # bound in the past, we still want only one creation action\n                    continue\n                create_action = True\n\n            # Change status and reindex\n            wf_tool.setStatusOf(workflow.id, obj, history)\n            indexes = [\"review_state\", \"is_active\"]\n            obj.reindexObject(idxs=indexes)", "response": "Restores the review history for the given brain or object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_purged_review_history_for(brain_or_object):\n    history = review_history_cache.get(api.get_uid(brain_or_object), [])\n\n    # Boil out those actions not supported by object's current workflow\n    available_actions = get_workflow_actions_for(brain_or_object)\n    history = filter(lambda action: action[\"action\"] in available_actions\n                                    or action[\"action\"] is None, history)\n\n    # Boil out those states not supported by object's current workflow\n    available_states = get_workflow_states_for(brain_or_object)\n    history = filter(lambda act: act[\"review_state\"] in available_states,\n                     history)\n\n    # If no meaning history found, create a default one for initial state\n    if not history:\n        history = create_initial_review_history(brain_or_object)\n    return history", "response": "Returns the review history for the object passed in but filtered by the workflow currently bound\n    to the object plus those actions that are None for initial state"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfills the review_history_cache dict with the values of the objects to be bound to new workflow and the keys are the uids of the objects to be bound to new workflow and the values are their current review_history", "response": "def cache_affected_objects_review_history(portal):\n    \"\"\"Fills the review_history_cache dict. The keys are the uids of the objects\n    to be bound to new workflow and the values are their current review_history\n    \"\"\"\n    logger.info(\"Caching review_history ...\")\n    query = dict(portal_type=NEW_SENAITE_WORKFLOW_BINDINGS)\n    brains = api.search(query, UID_CATALOG)\n    total = len(brains)\n    for num, brain in enumerate(brains):\n        if num % 100 == 0:\n            logger.info(\"Caching review_history: {}/{}\"\n                        .format(num, total))\n        review_history = get_review_history_for(brain)\n        review_history_cache[api.get_uid(brain)] = review_history"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the review history list for the given object.", "response": "def get_review_history_for(brain_or_object):\n    \"\"\"Returns the review history list for the given object. If there is no\n    review history for the object, it returns a default review history\n    \"\"\"\n    workflow_history = api.get_object(brain_or_object).workflow_history\n    if not workflow_history:\n        # No review_history for this object!. This object was probably\n        # migrated to 1.3.0 before review_history was handled in this\n        # upgrade step.\n        # https://github.com/senaite/senaite.core/issues/1270\n        return create_initial_review_history(brain_or_object)\n\n    review_history = []\n    for wf_id, histories in workflow_history.items():\n        for history in histories:\n            hist = history.copy()\n            if \"inactive_state\" in history:\n                hist[\"review_state\"] = history[\"inactive_state\"]\n            elif \"cancellation_state\" in history:\n                hist[\"review_state\"] = history[\"cancellation_state\"]\n            review_history.append(hist)\n\n    # Sort by time (from oldest to newest)\n    return sorted(review_history, key=lambda st: st.get(\"time\"))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new review history for the given object.", "response": "def create_initial_review_history(brain_or_object):\n    \"\"\"Creates a new review history for the given object\n    \"\"\"\n    obj = api.get_object(brain_or_object)\n\n    # It shouldn't be necessary to walk-through all workflows from this object,\n    # cause there are no objects with more than one workflow bound in 1.3.\n    # Nevertheless, one never knows if there is an add-on that does.\n    review_history = list()\n    wf_tool = api.get_tool(\"portal_workflow\")\n    for wf_id in api.get_workflows_for(obj):\n        wf = wf_tool.getWorkflowById(wf_id)\n        if not hasattr(wf, \"initial_state\"):\n            logger.warn(\"No initial_state attr for workflow '{}': {}'\"\n                        .format(wf_id, repr(obj)))\n        # If no initial_state found for this workflow and object, set\n        # \"registered\" as default. This upgrade step is smart enough to generate\n        # a new review_state for new workflow bound to this object later,\n        # if the current state does not match with any of the newly available.\n        # Hence, is totally safe to set the initial state here to \"registered\"\n        initial_state = getattr(wf, \"initial_state\", \"registered\")\n        initial_review_history = {\n            'action': None,\n            'actor': obj.Creator(),\n            'comments': 'Default review_history (by 1.3 upgrade step)',\n            'review_state': initial_state,\n            'time': obj.created(),\n        }\n        if hasattr(wf, \"state_variable\"):\n            initial_review_history[wf.state_variable] = initial_state\n        else:\n            # This is totally weird, but as per same reasoning above (re\n            # initial_state), not having a \"state_variable\" won't cause any\n            # problem later. The logic responsible of the reassignment of\n            # initial review states after new workflows are bound will safely\n            # assign the default state_variable (review_state)\n            logger.warn(\"No state_variable attr for workflow '{}': {}\"\n                        .format(wf_id, repr(obj)))\n\n        review_history.append(initial_review_history)\n\n    # Sort by time (from oldest to newest)\n    return sorted(review_history, key=lambda st: st.get(\"time\"))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresorting the client actions to the end of the main page", "response": "def resort_client_actions(portal):\n    \"\"\"Resorts client action views\n    \"\"\"\n    sorted_actions = [\n        \"edit\",\n        \"contacts\",\n        \"view\", # this redirects to analysisrequests\n        \"analysisrequests\",\n        \"batches\",\n        \"samplepoints\",\n        \"profiles\",\n        \"templates\",\n        \"specs\",\n        \"orders\",\n        \"reports_listing\"\n    ]\n    type_info = portal.portal_types.getTypeInfo(\"Client\")\n    actions = filter(lambda act: act.id in sorted_actions, type_info._actions)\n    missing = filter(lambda act: act.id not in sorted_actions, type_info._actions)\n\n    # Sort the actions\n    actions = sorted(actions, key=lambda act: sorted_actions.index(act.id))\n    if missing:\n        # Move the actions not explicitily sorted to the end\n        actions.extend(missing)\n\n    # Reset the actions to type info\n    type_info._actions = actions"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef init_auditlog(portal):\n    # reindex the auditlog folder to display the icon right in the setup\n    portal.bika_setup.auditlog.reindexObject()\n\n    # Initialize contents for audit logging\n    start = time.time()\n    uid_catalog = api.get_tool(\"uid_catalog\")\n    brains = uid_catalog()\n    total = len(brains)\n\n    logger.info(\"Initializing {} objects for the audit trail...\".format(total))\n    for num, brain in enumerate(brains):\n        # Progress notification\n        if num and num % 1000 == 0:\n            transaction.commit()\n            logger.info(\"{}/{} ojects initialized for audit logging\"\n                        .format(num, total))\n        # End progress notification\n        if num + 1 == total:\n            end = time.time()\n            duration = float(end-start)\n            logger.info(\"{} ojects initialized for audit logging in {:.2f}s\"\n                        .format(total, duration))\n\n        if api.get_portal_type(brain) in SKIP_TYPES_FOR_AUDIT_LOG:\n            continue\n\n        obj = api.get_object(brain)\n\n        if not supports_snapshots(obj):\n            continue\n\n        if has_snapshots(obj):\n            continue\n\n        # Take one snapshot per review history item\n        rh = api.get_review_history(obj, rev=False)\n        for item in rh:\n            actor = item.get(\"actor\")\n            user = get_user(actor)\n            if user:\n                # remember the roles of the actor\n                item[\"roles\"] = get_roles(user)\n            # The review history contains the variable \"time\" which we will set\n            # as the \"modification\" time\n            timestamp = item.pop(\"time\", DateTime())\n            item[\"time\"] = timestamp.ISO()\n            item[\"modified\"] = timestamp.ISO()\n            item[\"remote_address\"] = None\n            take_snapshot(obj, **item)", "response": "Initialize the contents for the audit log"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_log_action(portal):\n    logger.info(\"Removing Log Tab ...\")\n    portal_types = api.get_tool(\"portal_types\")\n    for name in portal_types.listContentTypes():\n        ti = portal_types[name]\n        actions = map(lambda action: action.id, ti._actions)\n        for index, action in enumerate(actions):\n            if action == \"log\":\n                logger.info(\"Removing Log Action for {}\".format(name))\n                ti.deleteActions([index])\n                break\n    logger.info(\"Removing Log Tab [DONE]\")", "response": "Removes the old Log action from types\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reindex_sortable_title(portal):\n    catalogs = [\n        \"bika_catalog\",\n        \"bika_setup_catalog\",\n        \"portal_catalog\",\n    ]\n    for catalog_name in catalogs:\n        logger.info(\"Reindexing sortable_title for {} ...\".format(catalog_name))\n        handler = ZLogHandler(steps=100)\n        catalog = api.get_tool(catalog_name)\n        catalog.reindexIndex(\"sortable_title\", None, pghandler=handler)\n        commit_transaction(portal)", "response": "Reindex sortable_title from some catalogs\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the localized time in string format", "response": "def ulocalized_time(self, time, context, request):\n        \"\"\"Returns the localized time in string format\n        \"\"\"\n        value = ut(time, long_format=self.show_time, time_only=False,\n                   context=context, request=request)\n        return value or \"\""}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the localized time in string format but in GMT + 0", "response": "def ulocalized_gmt0_time(self, time, context, request):\n        \"\"\"Returns the localized time in string format, but in GMT+0\n        \"\"\"\n        value = get_date(context, time)\n        if not value:\n            return \"\"\n        # DateTime is stored with TimeZone, but DateTimeWidget omits TZ\n        value = value.toZone(\"GMT+0\")\n        return self.ulocalized_time(value, context, request)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_currency_symbol(self):\n        currency = self.context.bika_setup.getCurrency()\n        locale = locales.getLocale(\"en\")\n        locale_currency = locale.numbers.currencies.get(currency)\n        if locale_currency is None:\n            return \"$\"\n        return locale_currency.symbol", "response": "Returns the locale currency symbol"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format_price(self, price):\n        return u\"{} {}{}{:02d}\".format(\n            self.currency_symbol,\n            price[0],\n            self.decimal_mark,\n            price[1],\n        )", "response": "Formats the price with the set decimal mark and correct currency"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format_maxtime(self, maxtime):\n        minutes = maxtime.get(\"minutes\", \"0\")\n        hours = maxtime.get(\"hours\", \"0\")\n        days = maxtime.get(\"days\", \"0\")\n        # days, hours, minutes\n        return u\"{}: {} {}: {} {}: {}\".format(\n            _(\"days\"), days, _(\"hours\"), hours, _(\"minutes\"), minutes)", "response": "Formats the max time record to a days hours minutes string"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef folderitem(self, obj, item, index):\n\n        cat = obj.getCategoryTitle()\n        cat_order = self.an_cats_order.get(cat)\n        if self.do_cats:\n            # category groups entries\n            item[\"category\"] = cat\n            if (cat, cat_order) not in self.categories:\n                self.categories.append((cat, cat_order))\n\n        # Category\n        category = obj.getCategory()\n        if category:\n            title = category.Title()\n            url = category.absolute_url()\n            item[\"Category\"] = title\n            item[\"replace\"][\"Category\"] = get_link(url, value=title)\n\n        # Calculation\n        calculation = obj.getCalculation()\n        if calculation:\n            title = calculation.Title()\n            url = calculation.absolute_url()\n            item[\"Calculation\"] = title\n            item[\"replace\"][\"Calculation\"] = get_link(url, value=title)\n\n        # Methods\n        methods = obj.getMethods()\n        if methods:\n            links = map(\n                lambda m: get_link(\n                    m.absolute_url(), value=m.Title(), css_class=\"link\"),\n                methods)\n            item[\"replace\"][\"Methods\"] = \", \".join(links)\n\n        # Max time allowed\n        maxtime = obj.MaxTimeAllowed\n        if maxtime:\n            item[\"MaxTimeAllowed\"] = self.format_maxtime(maxtime)\n\n        # Price\n        item[\"Price\"] = self.format_price(obj.Price)\n\n        # Duplicate Variation\n        dup_variation = obj.DuplicateVariation\n        if dup_variation:\n            item[\"DuplicateVariation\"] = self.format_duplication_variation(\n                dup_variation)\n\n        # Icons\n        after_icons = \"\"\n        if obj.getAccredited():\n            after_icons += get_image(\n                \"accredited.png\", title=_(\"Accredited\"))\n        if obj.getAttachmentOption() == \"r\":\n            after_icons += get_image(\n                \"attach_reqd.png\", title=_(\"Attachment required\"))\n        if obj.getAttachmentOption() == \"n\":\n            after_icons += get_image(\n                \"attach_no.png\", title=_(\"Attachment not permitted\"))\n        if after_icons:\n            item[\"after\"][\"Title\"] = after_icons\n\n        return item", "response": "This method is called by the folderitem service triggered each time an item is iterated in folderitems. It adds the category calculation and price to the item dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsort by Analysis Category", "response": "def folderitems(self, full_objects=False, classic=True):\n        \"\"\"Sort by Categories\n        \"\"\"\n        bsc = getToolByName(self.context, \"bika_setup_catalog\")\n        self.an_cats = bsc(\n            portal_type=\"AnalysisCategory\",\n            sort_on=\"sortable_title\")\n        self.an_cats_order = dict([\n            (b.Title, \"{:04}\".format(a))\n            for a, b in enumerate(self.an_cats)])\n        items = super(AnalysisServicesView, self).folderitems()\n        if self.do_cats:\n            self.categories = map(lambda x: x[0],\n                                  sorted(self.categories, key=lambda x: x[1]))\n        else:\n            self.categories.sort()\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of SuperModel items", "response": "def get_items(self):\n        \"\"\"Returns a list of SuperModel items\n        \"\"\"\n        uids = self.get_uids()\n        if not uids:\n            return [SuperModel(self.context)]\n        items = map(lambda uid: SuperModel(uid), uids)\n        return self._resolve_number_of_copies(items)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an array with the templates of stickers available.", "response": "def getAvailableTemplates(self):\n        \"\"\"Returns an array with the templates of stickers available.\n\n        Each array item is a dictionary with the following structure:\n\n            {'id': <template_id>,\n            'title': <teamplate_title>,\n            'selected: True/False'}\n        \"\"\"\n        # Getting adapters for current context. those adapters will return\n        # the desired sticker templates for the current context:\n        try:\n            adapters = getAdapters((self.context, ), IGetStickerTemplates)\n        except ComponentLookupError:\n            logger.info(\"No IGetStickerTemplates adapters found.\")\n            adapters = None\n        templates = []\n        if adapters is not None:\n            # Gather all templates\n            for name, adapter in adapters:\n                templates += adapter(self.request)\n        if templates:\n            return templates\n        # If there are no adapters, get all sticker templates in the system\n        seltemplate = self.getSelectedTemplate()\n        for temp in getStickerTemplates(filter_by_type=self.filter_by_type):\n            out = temp\n            out[\"selected\"] = temp.get(\"id\", \"\") == seltemplate\n            templates.append(out)\n        return templates"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getSelectedTemplate(self, default=\"Code_39_40x20mm.pt\"):\n        # Default sticker\n        bs_template = self.context.bika_setup.getAutoStickerTemplate()\n        size = self.request.get(\"size\", \"\")\n\n        if self.filter_by_type:\n            templates = getStickerTemplates(filter_by_type=self.filter_by_type)\n            # Get the first sticker\n            bs_template = templates[0].get(\"id\", \"\") if templates else \"\"\n        elif size == \"small\":\n            bs_template = self.context.bika_setup.getSmallStickerTemplate()\n        elif size == \"large\":\n            bs_template = self.context.bika_setup.getLargeStickerTemplate()\n        rq_template = self.request.get(\"template\", bs_template)\n        # Check if the template exists. If not, fallback to default's\n        # 'prefix' is also the resource folder's name\n        prefix = \"\"\n        templates_dir = \"\"\n        if rq_template.find(\":\") >= 0:\n            prefix, rq_template = rq_template.split(\":\")\n            templates_dir = self._getStickersTemplatesDirectory(prefix)\n        else:\n            this_dir = os.path.dirname(os.path.abspath(__file__))\n            templates_dir = os.path.join(this_dir, \"templates/stickers/\")\n            if self.filter_by_type:\n                templates_dir = templates_dir + \"/\" + self.filter_by_type\n        if not os.path.isfile(os.path.join(templates_dir, rq_template)):\n            rq_template = default\n        return \"%s:%s\" % (prefix, rq_template) if prefix else rq_template", "response": "Returns the id of the selected template."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlooking for the CSS file from the selected template and return its contents.", "response": "def getSelectedTemplateCSS(self):\n        \"\"\"Looks for the CSS file from the selected template and return its\n           contents.\n\n        If the selected template is default.pt, looks for a file named\n        default.css in the stickers path and return its contents. If no CSS\n        file found, retrns an empty string\n        \"\"\"\n        template = self.getSelectedTemplate()\n        # Look for the CSS\n        content = \"\"\n        if template.find(\":\") >= 0:\n            # A template from another add-on\n            prefix, template = template.split(\":\")\n            templates_dir = self._getStickersTemplatesDirectory(prefix)\n            css = \"{0}.css\".format(template[:-3])\n            if css in os.listdir(templates_dir):\n                path = \"%s/%s.css\" % (templates_dir, template[:-3])\n                if os.path.isfile(path):\n                    with open(path, \"r\") as content_file:\n                        content = content_file.read()\n        else:\n            this_dir = os.path.dirname(os.path.abspath(__file__))\n            templates_dir = os.path.join(this_dir, \"templates/stickers/\")\n            # Only use the directory asked in \"filter_by_type\"\n            if self.filter_by_type:\n                templates_dir = templates_dir + \"/\" + self.filter_by_type\n            path = \"%s/%s.css\" % (templates_dir, template[:-3])\n            if os.path.isfile(path):\n                with open(path, \"r\") as content_file:\n                    content = content_file.read()\n        return content"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrendering the next available sticker.", "response": "def renderItem(self, item):\n        \"\"\"Renders the next available sticker.\n\n        Uses the template specified in the request ('template' parameter) by\n        default. If no template defined in the request, uses the default\n        template set by default in Setup > Stickers.\n\n        If the template specified doesn't exist, uses the default bika.lims'\n        Code_128_1x48mm.pt template (was sticker_small.pt).\n        \"\"\"\n        self.current_item = item\n        templates_dir = \"templates/stickers\"\n        embedt = self.getSelectedTemplate()\n        if embedt.find(\":\") >= 0:\n            prefix, embedt = embedt.split(\":\")\n            templates_dir = self._getStickersTemplatesDirectory(prefix)\n        elif self.filter_by_type:\n            templates_dir = \"/\".join([templates_dir, self.filter_by_type])\n        fullpath = os.path.join(templates_dir, embedt)\n\n        try:\n            embed = ViewPageTemplateFile(fullpath)\n            return embed(self, item=item)\n        except Exception:\n            exc = traceback.format_exc()\n            msg = \"<div class='error'>{} - {} '{}':<pre>{}</pre></div>\".format(\n                item.id, _(\"Failed to load sticker\"), embedt, exc)\n            return msg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse in stickers_preview. pt", "response": "def getItemsURL(self):\n        \"\"\"Used in stickers_preview.pt\n        \"\"\"\n        req_items = self.get_uids()\n        req_items = req_items or [api.get_uid(self.context)]\n        req = \"{}?items={}\".format(self.request.URL, \",\".join(req_items))\n        return req"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the paths for the css and pt files containing the css and pt files for the stickers that depend on the filter_by_type.", "response": "def _getStickersTemplatesDirectory(self, resource_name):\n        \"\"\"Returns the paths for the directory containing the css and pt files\n        for the stickers deppending on the filter_by_type.\n\n        :param resource_name: The name of the resource folder.\n        :type resource_name: string\n        :returns: a string as a path\n        \"\"\"\n        templates_dir =\\\n            queryResourceDirectory(\"stickers\", resource_name).directory\n        if self.filter_by_type:\n            templates_dir = templates_dir + \"/\" + self.filter_by_type\n        return templates_dir"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a pdf stream with the stickers", "response": "def pdf_from_post(self):\n        \"\"\"Returns a pdf stream with the stickers\n        \"\"\"\n        html = self.request.form.get(\"html\")\n        style = self.request.form.get(\"style\")\n        reporthtml = \"<html><head>{0}</head><body>{1}</body></html>\"\n        reporthtml = reporthtml.format(style, html)\n        reporthtml = safe_unicode(reporthtml).encode(\"utf-8\")\n        pdf_fn = tempfile.mktemp(suffix=\".pdf\")\n        pdf_file = createPdf(htmlreport=reporthtml, outfile=pdf_fn)\n        return pdf_file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_copies_count(self):\n        setup = api.get_setup()\n        default_num = setup.getDefaultNumberOfCopies()\n        request_num = self.request.form.get(\"copies_count\")\n        return to_int(request_num, default_num)", "response": "Return the copies_count number request parameter\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the list of duplicate analyses that share the same Request and are included in the same Worksheet as the current analysis.", "response": "def getSiblings(self, retracted=False):\n        \"\"\"\n        Return the list of duplicate analyses that share the same Request and\n        are included in the same Worksheet as the current analysis. The current\n        duplicate is excluded from the list.\n        :param retracted: If false, retracted/rejected siblings are dismissed\n        :type retracted: bool\n        :return: list of siblings for this analysis\n        :rtype: list of IAnalysis\n        \"\"\"\n        worksheet = self.getWorksheet()\n        requestuid = self.getRequestUID()\n        if not requestuid or not worksheet:\n            return []\n\n        siblings = []\n        retracted_states = [STATE_RETRACTED, STATE_REJECTED]\n        analyses = worksheet.getAnalyses()\n        for analysis in analyses:\n            if analysis.UID() == self.UID():\n                # Exclude me from the list\n                continue\n\n            if not IRequestAnalysis.providedBy(analysis):\n                # Exclude analyses that do not have an analysis request\n                # associated\n                continue\n\n            if analysis.getRequestUID() != requestuid:\n                # Exclude those analyses that does not belong to the same\n                # analysis request I belong to\n                continue\n\n            if retracted is False and in_state(analysis, retracted_states):\n                # Exclude retracted analyses\n                continue\n\n            siblings.append(analysis)\n\n        return siblings"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the valid result range for this analysis duplicate based on the result and duplicate variation set in the original analysis.", "response": "def getResultsRange(self):\n        \"\"\"Returns the valid result range for this analysis duplicate, based on\n        both on the result and duplicate variation set in the original analysis\n\n        A Duplicate will be out of range if its result does not match with the\n        result for the parent analysis plus the duplicate variation in % as the\n        margin error.\n        :return: A dictionary with the keys min and max\n        :rtype: dict\n        \"\"\"\n        specs = ResultsRangeDict()\n        analysis = self.getAnalysis()\n        if not analysis:\n            return specs\n\n        result = analysis.getResult()\n        if not api.is_floatable(result):\n            return specs\n\n        specs.min = specs.max = result\n        result = api.to_float(result)\n        dup_variation = analysis.getDuplicateVariation()\n        dup_variation = api.to_float(dup_variation)\n        if not dup_variation:\n            return specs\n\n        margin = abs(result) * (dup_variation / 100.0)\n        specs.min = str(result - margin)\n        specs.max = str(result + margin)\n        return specs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget users belonging to specified roles", "response": "def getusers(self, context, request):\n        \"\"\"/@@API/getusers: Return users belonging to specified roles\n\n        Required parameters:\n\n            - roles: The role of which users to return\n\n        {\n            runtime: Function running time.\n            error: true or string(message) if error. false if no error.\n            success: true or string(message) if success. false if no success.\n            users: list of dictionaries: {fullname: fullname, userid: userid}\n        }\n        \n        >>> portal = layer['portal']\n        >>> portal_url = portal.absolute_url()\n        >>> from plone.app.testing import SITE_OWNER_NAME\n        >>> from plone.app.testing import SITE_OWNER_PASSWORD\n\n        >>> browser = layer['getBrowser'](portal, loggedIn=True, username=SITE_OWNER_NAME, password=SITE_OWNER_PASSWORD)\n        >>> browser.open(portal_url+\"/@@API/getusers?roles:list=Manager&roles:list=Analyst\")\n        >>> browser.contents\n        '{...test_labmanager1...}'\n        >>> browser.contents\n        '{...test_analyst1...}'\n        \n        >>> browser.open(portal_url+\"/@@API/getusers\")\n        >>> browser.contents\n        'No roles specified'\n        \"\"\"\n        roles = request.get('roles','')\n\n        if len(roles) == 0:\n            raise BadRequest(\"No roles specified\")\n        \n        mtool = getToolByName(context, 'portal_membership') \n        users = []\n        for user in mtool.searchForMembers(roles=roles):\n            uid = user.getId()\n            fullname = user.getProperty('fullname')\n            if not fullname:\n                fullname = uid\n            users.append({'fullname':fullname, 'userid': uid})\n            \n        ret = {\n            \"url\": router.url_for(\"remove\", force_external=True),\n            \"success\": True,\n            \"error\": False,\n            'users': users,\n        }\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getCatalogDefinitions():\n    final = {}\n    analysis_request = bika_catalog_analysisrequest_listing_definition\n    analysis = bika_catalog_analysis_listing_definition\n    autoimportlogs = bika_catalog_autoimportlogs_listing_definition\n    worksheet = bika_catalog_worksheet_listing_definition\n    report = bika_catalog_report_definition\n    # Merging the catalogs\n    final.update(analysis_request)\n    final.update(analysis)\n    final.update(autoimportlogs)\n    final.update(worksheet)\n    final.update(report)\n    return final", "response": "Returns a dictionary with catalogs definitions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getCatalog(instance, field='UID'):\n    uid = instance.UID()\n    if 'workflow_skiplist' in instance.REQUEST and \\\n        [x for x in instance.REQUEST['workflow_skiplist']\n         if x.find(uid) > -1]:\n        return None\n    else:\n        # grab the first catalog we are indexed in.\n        # we're only indexed in one.\n        at = getToolByName(instance, 'archetype_tool')\n        plone = instance.portal_url.getPortalObject()\n        catalog_name = instance.portal_type in at.catalog_map \\\n            and at.catalog_map[instance.portal_type][0] or 'portal_catalog'\n        catalog = getToolByName(plone, catalog_name)\n        return catalog", "response": "Returns the first catalog that stores objects of the passed in type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _merge_catalog_definitions(dict1, dict2):\n    if not dict2:\n        return dict1.copy()\n\n    outdict = {}\n    # Use dict1 as a template\n    for k, v in dict1.items():\n        if k not in dict2 and isinstance(v, dict):\n            outdict[k] = v.copy()\n            continue\n        if k not in dict2 and isinstance(v, list):\n            outdict[k] = v[:]\n            continue\n        if k == 'indexes':\n            sdict1 = v.copy()\n            sdict2 = dict2[k].copy()\n            sdict1.update(sdict2)\n            outdict[k] = sdict1\n            continue\n        if k in ['types', 'columns']:\n            list1 = v\n            list2 = dict2[k]\n            outdict[k] = list(set(list1 + list2))\n            continue\n        if isinstance(v, dict):\n            sdict1 = v.copy()\n            sdict2 = dict2[k].copy()\n            outdict[k] = _merge_catalog_definitions(sdict1, sdict2)\n\n    # Now, add the rest of keys from dict2 that don't exist in dict1\n    for k, v in dict2.items():\n        if k in outdict:\n            continue\n        outdict[k] = v.copy()\n    return outdict", "response": "Merges two dictionaries containing the same structure as the main template of the Bika LIMS\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _map_content_types(archetype_tool, catalogs_definition):\n    # This will be a dictionari like {'content_type':['catalog_id', ...]}\n    ct_map = {}\n    # This list will contain the atalog ids to be rebuild\n    to_reindex = []\n    # getting the dictionary of mapped content_types in the catalog\n    map_types = archetype_tool.catalog_map\n    for catalog_id in catalogs_definition.keys():\n        catalog_info = catalogs_definition.get(catalog_id, {})\n        # Mapping the catalog with the defined types\n        types = catalog_info.get('types', [])\n        for t in types:\n            tmp_l = ct_map.get(t, [])\n            tmp_l.append(catalog_id)\n            ct_map[t] = tmp_l\n    # Mapping\n    for t in ct_map.keys():\n        catalogs_list = ct_map[t]\n        # Getting the previus mapping\n        perv_catalogs_list = archetype_tool.catalog_map.get(t, [])\n        # If the mapping has changed, update it\n        set1 = set(catalogs_list)\n        set2 = set(perv_catalogs_list)\n        if set1 != set2:\n            archetype_tool.setCatalogsByType(t, catalogs_list)\n            # Adding to reindex only the catalogs that have changed\n            to_reindex = to_reindex + list(set1 - set2) + list(set2 - set1)\n    return to_reindex", "response": "Maps content types to the corresponding content types in the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a catalog definition it updates the indexes, columns and content_type definitions of the catalog. :portal: the Plone site object :catalog_id: a string as the catalog id :catalog_definition: a dictionary like { 'types': ['ContentType', ...], 'indexes': { 'UID': 'FieldIndex', ... }, 'columns': [ 'Title', ... ] }", "response": "def _setup_catalog(portal, catalog_id, catalog_definition):\n    \"\"\"\n    Given a catalog definition it updates the indexes, columns and content_type\n    definitions of the catalog.\n    :portal: the Plone site object\n    :catalog_id: a string as the catalog id\n    :catalog_definition: a dictionary like\n        {\n            'types':   ['ContentType', ...],\n            'indexes': {\n                'UID': 'FieldIndex',\n                ...\n            },\n            'columns': [\n                'Title',\n                ...\n            ]\n        }\n    \"\"\"\n\n    reindex = False\n    catalog = getToolByName(portal, catalog_id, None)\n    if catalog is None:\n        logger.warning('Could not find the %s tool.' % (catalog_id))\n        return False\n    # Indexes\n    indexes_ids = catalog_definition.get('indexes', {}).keys()\n    # Indexing\n    for idx in indexes_ids:\n        # The function returns if the index needs to be reindexed\n        indexed = _addIndex(catalog, idx, catalog_definition['indexes'][idx])\n        reindex = True if indexed else reindex\n    # Removing indexes\n    in_catalog_idxs = catalog.indexes()\n    to_remove = list(set(in_catalog_idxs)-set(indexes_ids))\n    for idx in to_remove:\n        # The function returns if the index has been deleted\n        desindexed = _delIndex(catalog, idx)\n        reindex = True if desindexed else reindex\n    # Columns\n    columns_ids = catalog_definition.get('columns', [])\n    for col in columns_ids:\n        created = _addColumn(catalog, col)\n        reindex = True if created else reindex\n    # Removing columns\n    in_catalog_cols = catalog.schema()\n    to_remove = list(set(in_catalog_cols)-set(columns_ids))\n    for col in to_remove:\n        # The function returns if the index has been deleted\n        desindexed = _delColumn(catalog, col)\n        reindex = True if desindexed else reindex\n    return reindex"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _addIndex(catalog, index, indextype):\n    if index not in catalog.indexes():\n        try:\n            if indextype == 'ZCTextIndex':\n                addZCTextIndex(catalog, index)\n            else:\n                catalog.addIndex(index, indextype)\n            logger.info('Catalog index %s added to %s.' % (index, catalog.id))\n            return True\n        except:\n            logger.error(\n                'Catalog index %s error while adding to %s.'\n                % (index, catalog.id))\n    return False", "response": "This function indexes the index element into the catalog if it doesn t already exist."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _addColumn(cat, col):\n    # First check if the metadata column already exists\n    if col not in cat.schema():\n        try:\n            cat.addColumn(col)\n            logger.info('Column %s added to %s.' % (col, cat.id))\n            return True\n        except:\n            logger.error(\n                'Catalog column %s error while adding to %s.' % (col, cat.id))\n    return False", "response": "This function adds a column to the acatalog."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _delIndex(catalog, index):\n    if index in catalog.indexes():\n        try:\n            catalog.delIndex(index)\n            logger.info(\n                'Catalog index %s deleted from %s.' % (index, catalog.id))\n            return True\n        except:\n            logger.error(\n                'Catalog index %s error while deleting from %s.'\n                % (index, catalog.id))\n    return False", "response": "This function deletes the index element from the catalog."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _cleanAndRebuildIfNeeded(portal, cleanrebuild):\n    for cat in cleanrebuild:\n        catalog = getToolByName(portal, cat)\n        if catalog:\n            if hasattr(catalog, \"softClearFindAndRebuild\"):\n                catalog.softClearFindAndRebuild()\n            else:\n                catalog.clearFindAndRebuild()\n        else:\n            logger.warning('%s do not found' % cat)", "response": "Clean and rebuild the given catalogs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the line of headers into a list of strings.", "response": "def parse_line_headers(self, line):\n        \"\"\"We must build headers carefully: there are multiple blank values\n        in the header row, and the instrument may just add more for all\n        we know.\n        \"\"\"\n        headers = line.split(\",\")\n        for i, v in enumerate(headers):\n            if v:\n                headers[i] = v\n            else:\n                headers[i] = str(i)\n        self.headers = headers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the data line into a dictionary for the importer", "response": "def parse_data_line(self, line):\n            \"\"\" Parses the data line into a dictionary for the importer\n            \"\"\"\n            it = self._generate(line)\n            reader = csv.DictReader(it, fieldnames=self.headers)\n            values = reader.next()\n            values['DefaultResult'] = 'ResidualError'\n            values['LineName'] = re.sub(r'\\W', '', values['LineName'].strip())\n            values['Concentration'] = values['Cc'].strip()\n            values['StandardDeviation'] = values['SD'].strip()\n            values['ResidualError'] = values['RSD'].strip()\n            values['NetIntensity'] = values['Net_Intensity'].strip().split('/')\n            values['Remarks'] = ''\n            values['TestLine'] = ''\n            self._addRawResult(self._resid, {values['LineName']: values}, False)\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all items in the analysis specification view.", "response": "def folderitems(self):\n        \"\"\"TODO: Refactor to non-classic mode\n        \"\"\"\n        items = super(AnalysisSpecificationView, self).folderitems()\n        self.categories.sort()\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef folderitem(self, obj, item, index):\n        # ensure we have an object and not a brain\n        obj = api.get_object(obj)\n        url = api.get_url(obj)\n        title = api.get_title(obj)\n        keyword = obj.getKeyword()\n\n        # get the category\n        if self.show_categories_enabled():\n            category = obj.getCategoryTitle()\n            if category not in self.categories:\n                self.categories.append(category)\n            item[\"category\"] = category\n\n        item[\"Title\"] = title\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n        item[\"choices\"][\"min_operator\"] = self.min_operator_choices\n        item[\"choices\"][\"max_operator\"] = self.max_operator_choices\n        item[\"allow_edit\"] = self.get_editable_columns()\n        item[\"required\"] = self.get_required_columns()\n\n        spec = self.specification.get(keyword, {})\n        item[\"selected\"] = spec and True or False\n        item[\"min_operator\"] = spec.get(\"min_operator\", \"geq\")\n        item[\"min\"] = spec.get(\"min\", \"\")\n        item[\"max_operator\"] = spec.get(\"max_operator\", \"leq\")\n        item[\"max\"] = spec.get(\"max\", \"\")\n        item[\"warn_min\"] = spec.get(\"warn_min\", \"\")\n        item[\"warn_max\"] = spec.get(\"warn_max\", \"\")\n        item[\"hidemin\"] = spec.get(\"hidemin\", \"\")\n        item[\"hidemax\"] = spec.get(\"hidemax\", \"\")\n        item[\"rangecomment\"] = spec.get(\"rangecomment\", \"\")\n\n        # Add methods\n        methods = obj.getMethods()\n        if methods:\n            links = map(\n                lambda m: get_link(\n                    m.absolute_url(), value=m.Title(), css_class=\"link\"),\n                methods)\n            item[\"replace\"][\"Methods\"] = \", \".join(links)\n        else:\n            item[\"methods\"] = \"\"\n\n        # Icons\n        after_icons = \"\"\n        if obj.getAccredited():\n            after_icons += get_image(\n                \"accredited.png\", title=_(\"Accredited\"))\n        if obj.getAttachmentOption() == \"r\":\n            after_icons += get_image(\n                \"attach_reqd.png\", title=_(\"Attachment required\"))\n        if obj.getAttachmentOption() == \"n\":\n            after_icons += get_image(\n                \"attach_no.png\", title=_(\"Attachment not permitted\"))\n        if after_icons:\n            item[\"after\"][\"Title\"] = after_icons\n\n        return item", "response": "This method is called every time an item is iterated in folderitems. It adds the properties of the object to the item dict."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess the AnalysisSpecsResultsField form.", "response": "def process_form(self, instance, field, form, empty_marker=None,\n                     emptyReturnsMarker=False):\n        \"\"\"Return a list of dictionaries fit for AnalysisSpecsResultsField\n           consumption.\n\n        If neither hidemin nor hidemax are specified, only services which have\n        float()able entries in result,min and max field will be included. If\n        hidemin and/or hidemax specified, results might contain empty min\n        and/or max fields.\n        \"\"\"\n        values = []\n\n        # selected services\n        service_uids = form.get(\"uids\", [])\n\n        if not service_uids:\n            # Inject empty fields for the validator\n            values = [dict.fromkeys(field.getSubfields())]\n\n        for uid in service_uids:\n            s_min = self._get_spec_value(form, uid, \"min\")\n            s_max = self._get_spec_value(form, uid, \"max\")\n\n            if not s_min and not s_max:\n                # If user has not set value neither for min nor max, omit this\n                # record. Otherwise, since 'min' and 'max' are defined as\n                # mandatory subfields, the following message will appear after\n                # submission: \"Specifications is required, please correct.\"\n                continue\n\n            # TODO: disallow this case in the UI\n            if s_min and s_max:\n                if float(s_min) > float(s_max):\n                    logger.warn(\"Min({}) > Max({}) is not allowed\"\n                                .format(s_min, s_max))\n                    continue\n\n            min_operator = self._get_spec_value(\n                form, uid, \"min_operator\", check_floatable=False)\n            max_operator = self._get_spec_value(\n                form, uid, \"max_operator\", check_floatable=False)\n\n            service = api.get_object_by_uid(uid)\n            values.append({\n                \"keyword\": service.getKeyword(),\n                \"uid\": uid,\n                \"min_operator\": min_operator,\n                \"min\": s_min,\n                \"max_operator\": max_operator,\n                \"max\": s_max,\n                \"warn_min\": self._get_spec_value(form, uid, \"warn_min\"),\n                \"warn_max\": self._get_spec_value(form, uid, \"warn_max\"),\n                \"hidemin\": self._get_spec_value(form, uid, \"hidemin\"),\n                \"hidemax\": self._get_spec_value(form, uid, \"hidemax\"),\n                \"rangecomment\": self._get_spec_value(form, uid, \"rangecomment\",\n                                                     check_floatable=False)})\n        return values, {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setDefaults(self, instance):\n    for field in self.values():\n\n        # ## bika addition: we fire adapters for IAcquireFieldDefaults.\n        # If IAcquireFieldDefaults returns None, this signifies \"ignore\" return.\n        # First adapter found with non-None result, wins.\n        value = None\n        if shasattr(field, 'acquire'):\n            adapters = {}\n            for adapter in getAdapters((instance,), IAcquireFieldDefaults):\n                sort_val = getattr(adapter[1], 'sort', 1000)\n                if sort_val not in adapters:\n                    adapters[sort_val] = []\n                adapters[sort_val].append(adapter)\n            if adapters:\n                keys = sorted(adapters.keys())\n                keys.reverse()\n                adapter = adapters[keys[0]]\n                _value = adapter[0][1](field)\n                if _value is not None:\n                    value = _value\n\n        if field.getName().lower() == 'id':\n            continue\n\n        # If our adapter reflects a value for a reference field, it will\n        # be permitted.\n        if field.type == \"reference\" and not value:\n            continue\n\n        default = value if value else field.getDefault(instance)\n\n        # always set defaults on writable fields\n        mutator = field.getMutator(instance)\n        if mutator is None:\n            continue\n\n        args = (default,)\n        kw = {'field': field.__name__,\n              '_initializing_': True}\n        if shasattr(field, 'default_content_type'):\n            # specify a mimetype if the mutator takes a mimetype argument if\n            # the schema supplies a default, we honour that, otherwise we use\n            # the site property\n            default_content_type = field.default_content_type\n            if default_content_type is None:\n                default_content_type = getDefaultContentType(instance)\n            kw['mimetype'] = default_content_type\n\n        mapply(mutator, *args, **kw)", "response": "Set the default values of the fields of the current object to the values of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_action_to_ancestors(analysis_request, transition_id):\n    parent_ar = analysis_request.getParentAnalysisRequest()\n    if parent_ar:\n        do_action_for(parent_ar, transition_id)", "response": "Promotes the transitiion passed in to ancestors"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_action_to_descendants(analysis_request, transition_id):\n    for partition in analysis_request.getDescendants(all_descendants=False):\n        do_action_for(partition, transition_id)", "response": "Cascades the transition passed in to the descendant partitions\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_action_to_analyses(analysis_request, transition_id, all_analyses=False):\n    analyses = list()\n    if all_analyses:\n        analyses = analysis_request.getAnalyses(full_objects=True)\n    else:\n        analyses = analysis_request.objectValues(\"Analysis\")\n    for analysis in analyses:\n        do_action_for(analysis, transition_id)", "response": "Cascades the transition to the analyses of the analysis request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getAnalysisServicesDisplayList(self):\n        ''' Returns a Display List with the active Analysis Services\n            available. The value is the keyword and the title is the\n            text to be displayed.\n        '''\n        bsc = getToolByName(self, 'bika_setup_catalog')\n        items = [('', '')] + [(o.getObject().Keyword, o.Title) for o in\n                                bsc(portal_type = 'AnalysisService',\n                                    is_active = True)]\n        items.sort(lambda x, y: cmp(x[1].lower(), y[1].lower()))\n        return DisplayList(list(items))", "response": "Returns a Display List with the active Analysis Services\n available. The value is the keyword and the title is the title of the Analysis Services\n text to be displayed."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a new id for the given portal_type", "response": "def generate_id(self, portal_type, batch_size = None):\n        \"\"\" Generate a new id for 'portal_type'\n        \"\"\"\n        plone = getSite()\n        portal_id = plone.getId()\n\n        if portal_type == 'News Item':\n            portal_type = 'NewsItem'\n\n        idserver_url = os.environ.get('IDServerURL')\n        try:\n            if batch_size:\n                # GET\n                f = urllib.urlopen('%s/%s%s?%s' % (\n                        idserver_url,\n                        portal_id,\n                        portal_type,\n                        urllib.urlencode({'batch_size': batch_size}))\n                        )\n            else:\n                f = urllib.urlopen('%s/%s%s' % (\n                    idserver_url, portal_id, portal_type\n                    )\n                )\n            id = f.read()\n            f.close()\n        except:\n            from sys import exc_info\n            info = exc_info()\n            import zLOG; zLOG.LOG('INFO', 0, '', 'generate_id raised exception: %s, %s \\n idserver_url: %s' % (info[0], info[1], idserver_url))\n            raise IDServerUnavailable(_('ID Server unavailable'))\n        return id"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getAnalyses(self, contentFilter=None, **kwargs):\n        # contentFilter and kwargs are combined.  They both exist for\n        # compatibility between the two signatures; kwargs has been added\n        # to be compatible with how getAnalyses() is used everywhere else.\n        cf = contentFilter if contentFilter else {}\n        cf.update(kwargs)\n        analyses = []\n        for ar in self.getAnalysisRequests():\n            analyses.extend(ar.getAnalyses(**cf))\n        return analyses", "response": "return list of all analyses against this sample"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef disposal_date(self):\n        date_sampled = self.getDateSampled()\n        if not date_sampled:\n            return None\n\n        # TODO Preservation - preservation's retention period has priority over\n        # sample type's preservation period\n\n        retention_period = self.getSampleType().getRetentionPeriod() or {}\n        retention_period_delta = timedelta(\n            days=int(retention_period.get(\"days\", 0)),\n            hours=int(retention_period.get(\"hours\", 0)),\n            minutes=int(retention_period.get(\"minutes\", 0))\n        )\n        return dt2DT(DT2dt(date_sampled) + retention_period_delta)", "response": "Returns the date the retention period ends for this sample based on the sample s date and the sample s retention period from the Sample Type. Returns None if the sample has not been collected yet."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the linked Plone User or None if no User exists", "response": "def getUser(self):\n        \"\"\"Returns the linked Plone User or None\n        \"\"\"\n        username = self.getUsername()\n        if not username:\n            return None\n        user = api.user.get(userid=username)\n        return user"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlinking the user to the Contact", "response": "def setUser(self, user_or_username):\n        \"\"\"Link the user to the Contact\n\n        :returns: True if OK, False if the User could not be linked\n        :rtype: bool\n        \"\"\"\n        user = None\n        userid = None\n\n        # Handle User IDs (strings)\n        if isinstance(user_or_username, types.StringTypes):\n            userid = user_or_username\n            user = api.user.get(userid)\n        # Handle User Objects (MemberData/PloneUser)\n        if hasattr(user_or_username, \"getId\"):\n            userid = user_or_username.getId()\n            user = user_or_username\n\n        # Not a valid user\n        if user is None:\n            return False\n\n        # Link the User\n        return self._linkUser(user)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unlinkUser(self, delete=False):\n        userid = self.getUsername()\n        user = self.getUser()\n        if user:\n            logger.debug(\"Unlinking User '{}' from Contact '{}'\".format(\n                userid, self.Title()))\n\n            # Unlink the User\n            if not self._unlinkUser():\n                return False\n\n            # Also remove the Plone User (caution)\n            if delete:\n                logger.debug(\"Removing Plone User '{}'\".format(userid))\n                api.user.delete(username=userid)\n\n            return True\n        return False", "response": "Unlink the User from the Contact"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlink a user to the current Contact and update the User properties and the User properties and the User properties of the current Contact and the User properties of the current Contact and the User properties of the current Contact and the User properties of the current Contact and the User properties of the current Contact.", "response": "def _linkUser(self, user):\n        \"\"\"Set the UID of the current Contact in the User properties and update\n        all relevant own properties.\n        \"\"\"\n        KEY = \"linked_contact_uid\"\n\n        username = user.getId()\n        contact = self.getContactByUsername(username)\n\n        # User is linked to another contact (fix in UI)\n        if contact and contact.UID() != self.UID():\n            raise ValueError(\"User '{}' is already linked to Contact '{}'\"\n                             .format(username, contact.Title()))\n\n        # User is linked to multiple other contacts (fix in Data)\n        if isinstance(contact, list):\n            raise ValueError(\"User '{}' is linked to multiple Contacts: '{}'\"\n                             .format(username, \",\".join(\n                                 map(lambda x: x.Title(), contact))))\n\n        # XXX: Does it make sense to \"remember\" the UID as a User property?\n        tool = user.getTool()\n        try:\n            user.getProperty(KEY)\n        except ValueError:\n            logger.info(\"Adding User property {}\".format(KEY))\n            tool.manage_addProperty(KEY, \"\", \"string\")\n\n        # Set the UID as a User Property\n        uid = self.UID()\n        user.setMemberProperties({KEY: uid})\n        logger.info(\"Linked Contact UID {} to User {}\".format(\n            user.getProperty(KEY), username))\n\n        # Set the Username\n        self.setUsername(user.getId())\n\n        # Update the Email address from the user\n        self.setEmailAddress(user.getProperty(\"email\"))\n\n        # somehow the `getUsername` index gets out of sync\n        self.reindexObject()\n\n        # N.B. Local owner role and client group applies only to client\n        #      contacts, but not lab contacts.\n        if IClient.providedBy(self.aq_parent):\n            # Grant local Owner role\n            self._addLocalOwnerRole(username)\n            # Add user to \"Clients\" group\n            self._addUserToGroup(username, group=\"Clients\")\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _unlinkUser(self):\n        KEY = \"linked_contact_uid\"\n\n        # Nothing to do if no user is linked\n        if not self.hasUser():\n            return False\n\n        user = self.getUser()\n        username = user.getId()\n\n        # Unset the UID from the User Property\n        user.setMemberProperties({KEY: \"\"})\n        logger.info(\"Unlinked Contact UID from User {}\"\n                    .format(user.getProperty(KEY, \"\")))\n\n        # Unset the Username\n        self.setUsername(None)\n\n        # Unset the Email\n        self.setEmailAddress(None)\n\n        # somehow the `getUsername` index gets out of sync\n        self.reindexObject()\n\n        # N.B. Local owner role and client group applies only to client\n        #      contacts, but not lab contacts.\n        if IClient.providedBy(self.aq_parent):\n            # Revoke local Owner role\n            self._delLocalOwnerRole(username)\n            # Remove user from \"Clients\" group\n            self._delUserFromGroup(username, group=\"Clients\")\n\n        return True", "response": "Remove the UID of the current Contact in the User properties and update all relevant own properties."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd user to the group", "response": "def _addUserToGroup(self, username, group=\"Clients\"):\n        \"\"\"Add user to the goup\n        \"\"\"\n        portal_groups = api.portal.get_tool(\"portal_groups\")\n        group = portal_groups.getGroupById('Clients')\n        group.addMember(username)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving user from group", "response": "def _delUserFromGroup(self, username, group=\"Clients\"):\n        \"\"\"Remove user from the group\n        \"\"\"\n        portal_groups = api.portal.get_tool(\"portal_groups\")\n        group = portal_groups.getGroupById(group)\n        group.removeMember(username)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _addLocalOwnerRole(self, username):\n        parent = self.getParent()\n        if parent.portal_type == \"Client\":\n            parent.manage_setLocalRoles(username, [\"Owner\", ])\n            # reindex object security\n            self._recursive_reindex_object_security(parent)", "response": "Add local owner role from parent object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _delLocalOwnerRole(self, username):\n        parent = self.getParent()\n        if parent.portal_type == \"Client\":\n            parent.manage_delLocalRoles([username])\n            # reindex object security\n            self._recursive_reindex_object_security(parent)", "response": "Remove local owner role from parent object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _recursive_reindex_object_security(self, obj):\n        if hasattr(aq_base(obj), \"objectValues\"):\n            for obj in obj.objectValues():\n                self._recursive_reindex_object_security(obj)\n\n        logger.debug(\"Reindexing object security for {}\".format(repr(obj)))\n        obj.reindexObjectSecurity()", "response": "Reindex object security after user linking"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nflag a function as deprecated. A warning will be emitted. :param comment: A human-friendly string, such as 'This function will be removed soon' :type comment: string :param replacement: The function to be used instead :type replacement: string or function", "response": "def deprecated(comment=None, replacement=None):\n    \"\"\"Flags a function as deprecated. A warning will be emitted.\n\n    :param comment: A human-friendly string, such as 'This  function\n                    will be removed soon'\n    :type comment: string\n    :param replacement: The function to be used instead\n    :type replacement: string or function\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            message = \"Call to deprecated function '{}.{}'\".format(\n                func.__module__,\n                func.__name__)\n            if replacement and isinstance(replacement, str):\n                message += \". Use '{}' instead\".format(replacement)\n            elif replacement:\n                message += \". Use '{}.{}' instead\".format(\n                 replacement.__module__,\n                 replacement.__name__)\n            if comment:\n                message += \". {}\".format(comment)\n            warnings.simplefilter('always', DeprecationWarning)\n            warnings.warn(message, category=DeprecationWarning, stacklevel=2)\n            warnings.simplefilter('default', DeprecationWarning)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_field_visibility_mode(self, field):\n        fallback_mode = (\"hidden\", \"hidden\")\n        widget = field.widget\n\n        # TODO This needs to be done differently\n        # Check where the field has to be located\n        layout = widget.isVisible(self.context, \"header_table\")\n        if layout in [\"invisible\", \"hidden\"]:\n            return fallback_mode\n\n        # Check permissions. We want to display field (either in view or edit\n        # modes) only if the current user has enough privileges.\n        if field.checkPermission(\"edit\", self.context):\n            mode = \"edit\"\n            sm = getSecurityManager()\n            if not sm.checkPermission(ModifyPortalContent, self.context):\n                logger.warn(\"Permission '{}' granted for the edition of '{}', \"\n                            \"but 'Modify portal content' not granted\"\n                            .format(field.write_permission, field.getName()))\n        elif field.checkPermission(\"view\", self.context):\n            mode = \"view\"\n        else:\n            return fallback_mode\n\n        # Check if the field needs to be displayed or not, even if the user has\n        # the permissions for edit or view. This may depend on criteria other\n        # than permissions (e.g. visibility depending on a setup setting, etc.)\n        if widget.isVisible(self.context, mode, field=field) != \"visible\":\n            if mode == \"view\":\n                return fallback_mode\n            # The field cannot be rendered in edit mode, but maybe can be\n            # rendered in view mode.\n            mode = \"view\"\n            if widget.isVisible(self.context, mode, field=field) != \"visible\":\n                return fallback_mode\n        return (mode, layout)", "response": "Returns the visibility mode of the current field."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getInstruments(self):\n        query = {\"portal_type\": \"Instrument\", \"is_active\": True}\n\n        if self.getRestrictToMethod():\n            query.update({\n                \"getMethodUIDs\": {\n                    \"query\": api.get_uid(self.getRestrictToMethod()),\n                    \"operator\": \"or\",\n                }\n            })\n\n        instruments = api.search(query, \"bika_setup_catalog\")\n        items = map(lambda i: (i.UID, i.Title), instruments)\n\n        instrument = self.getInstrument()\n        if instrument:\n            instrument_uids = map(api.get_uid, instruments)\n            if api.get_uid(instrument) not in instrument_uids:\n                items.append(\n                    (api.get_uid(instrument), api.get_title(instrument)))\n\n        items.sort(lambda x, y: cmp(x[1], y[1]))\n        items.insert(0, (\"\", _(\"No instrument\")))\n\n        return DisplayList(list(items))", "response": "Get the allowed instruments"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _getMethodsVoc(self):\n        methods = api.search({\n            \"portal_type\": \"Method\",\n            \"is_active\": True\n        }, \"bika_setup_catalog\")\n\n        items = map(lambda m: (api.get_uid(m), api.get_title(m)), methods)\n        items.sort(lambda x, y: cmp(x[1], y[1]))\n        items.insert(0, (\"\", _(\"Not specified\")))\n\n        return DisplayList(list(items))", "response": "Return the registered methods as DisplayList\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nredirects with a message", "response": "def redirect(self, redirect_url=None, message=None, level=\"info\"):\n        \"\"\"Redirect with a message\n        \"\"\"\n        if redirect_url is None:\n            redirect_url = self.back_url\n        if message is not None:\n            self.add_status_message(message, level)\n        return self.request.response.redirect(redirect_url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_uids(self):\n        uids = self.get_uids_from_request()\n        if not uids and api.is_object(self.context):\n            uids = [api.get_uid(self.context)]\n        return uids", "response": "Returns a uids list of the objects this action must be performed against to."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_uids_from_request(self):\n        uids = self.request.get(\"uids\", \"\")\n        if isinstance(uids, basestring):\n            uids = uids.split(\",\")\n        unique_uids = collections.OrderedDict().fromkeys(uids).keys()\n        return filter(api.is_uid, unique_uids)", "response": "Returns a list of uids from the request"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the action to be taken from the request. Returns None if no action is found action is found", "response": "def get_action(self):\n        \"\"\"Returns the action to be taken from the request. Returns None if no\n        action is found\n        \"\"\"\n        action = self.request.get(\"workflow_action_id\", None)\n        action = self.request.get(\"workflow_action\", action)\n        if not action:\n            return None\n\n        # A condition in the form causes Plone to sometimes send two actions\n        # This usually happens when the previous action was not managed properly\n        # and the request was not able to complete, so the previous form value\n        # is kept, together with the new one.\n        if type(action) in (list, tuple):\n            actions = list(set(action))\n            if len(actions) > 0:\n                logger.warn(\"Multiple actions in request: {}. Fallback to '{}'\"\n                            .format(repr(actions), actions[-1]))\n            action = actions[-1]\n        return action"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming the workflow transition passed in and returns the list of objects that have been successfully transitioned", "response": "def do_action(self, action, objects):\n        \"\"\"Performs the workflow transition passed in and returns the list of\n        objects that have been successfully transitioned\n        \"\"\"\n        transitioned = []\n        ActionHandlerPool.get_instance().queue_pool()\n        for obj in objects:\n            obj = api.get_object(obj)\n            success, message = do_action_for(obj, action)\n            if success:\n                transitioned.append(obj)\n        ActionHandlerPool.get_instance().resume()\n        return transitioned"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef success(self, objects, message=None):\n        if self.is_context_only(objects):\n            return self.redirect(message=_(\"Changes saved.\"))\n\n        ids = map(api.get_id, objects)\n        if not message:\n            message = _(\"Saved items: {}\").format(\", \".join(ids))\n        return self.redirect(message=message)", "response": "Redirects the user to success page with informative message"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a value from the request s form for the given uid", "response": "def get_form_value(self, form_key, object_brain_uid, default=None):\n        \"\"\"Returns a value from the request's form for the given uid, if any\n        \"\"\"\n        if form_key not in self.request.form:\n            return default\n\n        uid = object_brain_uid\n        if not api.is_uid(uid):\n            uid = api.get_uid(object_brain_uid)\n\n        values = self.request.form.get(form_key)\n        if isinstance(values, list):\n            if len(values) == 0:\n                return default\n            if len(values) > 1:\n                logger.warn(\"Multiple set of values for {}\".format(form_key))\n            values = values[0]\n\n        return values.get(uid, default)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_versioned_references_for(self, instance):\n        vrefs = []\n\n        # Retrieve the referenced objects\n        refs = instance.getRefs(relationship=self.relationship)\n\n        ref_versions = getattr(instance, REFERENCE_VERSIONS, None)\n        # No versions stored, return the original references\n        if ref_versions is None:\n            return refs\n\n        for ref in refs:\n            uid = api.get_uid(ref)\n            # get the linked version to the reference\n            version = ref_versions.get(uid)\n            # append the versioned reference\n            vrefs.append(self.retrieve_version(ref, version))\n\n        return vrefs", "response": "Returns the versioned references for the given instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef retrieve_version(self, obj, version):\n        current_version = getattr(obj, VERSION_ID, None)\n\n        if current_version is None:\n            # No initial version\n            return obj\n\n        if str(current_version) == str(version):\n            # Same version\n            return obj\n\n        # Retrieve the object from the repository\n        pr = api.get_tool(\"portal_repository\")\n        # bypass permission check to AccessPreviousVersions\n        result = pr._retrieve(\n            obj, selector=version, preserve=(), countPurged=True)\n        return result.object", "response": "Retrieve the version of the object in the repository"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_backreferences_for(self, instance):\n        rc = api.get_tool(\"reference_catalog\")\n        backreferences = rc.getReferences(instance, self.relationship)\n        return map(lambda ref: ref.targetUID, backreferences)", "response": "Returns the backreferences for the given instance"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef link_version(self, source, target):\n        if not hasattr(target, VERSION_ID):\n            # no initial version of this object!\n            logger.warn(\"No iniatial version found for '{}'\"\n                        .format(repr(target)))\n            return\n\n        if not hasattr(source, REFERENCE_VERSIONS):\n            source.reference_versions = {}\n\n        target_uid = api.get_uid(target)\n        # store the current version of the target on the source\n        source.reference_versions[target_uid] = target.version_id\n        # persist changes that occured referenced versions\n        source._p_changed = 1", "response": "Link the current version of the target object on the source object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unlink_version(self, source, target):\n        if not hasattr(source, REFERENCE_VERSIONS):\n            return\n        target_uid = api.get_uid(target)\n        if target_uid in source.reference_versions[target_uid]:\n            # delete the version\n            del source.reference_versions[target_uid]\n            # persist changes that occured referenced versions\n            source._p_changed = 1\n        else:\n            logger.warn(\"No version link found on '{}' -> '{}'\"\n                        .format(repr(source), repr(target)))", "response": "Unlink the current version of the target from the source"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a new reference to the current version of the current version", "response": "def add_reference(self, source, target, **kwargs):\n        \"\"\"Add a new reference\n        \"\"\"\n        # Tweak keyword arguments for addReference\n        addRef_kw = kwargs.copy()\n        addRef_kw.setdefault(\"referenceClass\", self.referenceClass)\n        if \"schema\" in addRef_kw:\n            del addRef_kw[\"schema\"]\n        uid = api.get_uid(target)\n        rc = api.get_tool(\"reference_catalog\")\n        # throws IndexError if uid is invalid\n        rc.addReference(source, uid, self.relationship, **addRef_kw)\n        # link the version of the reference\n        self.link_version(source, target)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef del_reference(self, source, target, **kwargs):\n        rc = api.get_tool(\"reference_catalog\")\n        uid = api.get_uid(target)\n        rc.deleteReference(source, uid, self.relationship)\n        # unlink the version of the reference\n        self.link_version(source, target)", "response": "Remove a reference from the reference catalog"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the value of the related object in the specified instance.", "response": "def set(self, instance, value, **kwargs):\n        \"\"\"Set (multi-)references\n        \"\"\"\n        value = self.preprocess_value(value)\n        existing_uids = self.get_backreferences_for(instance)\n\n        if not value and not existing_uids:\n            logger.warning(\"Field and value is empty!\")\n            return\n\n        if not self.multiValued and len(value) > 1:\n            raise ValueError(\"Multiple values given for single valued field {}\"\n                             .format(repr(self)))\n\n        set_uids = []\n        for val in value:\n            if api.is_uid(val):\n                set_uids.append(val)\n            elif api.is_object(val):\n                set_uids.append(api.get_uid(val))\n            else:\n                logger.error(\"Target has no UID: %s/%s\" % (val, value))\n\n        sub = filter(lambda uid: uid not in set_uids, existing_uids)\n        add = filter(lambda uid: uid not in existing_uids, set_uids)\n\n        for uid in set(existing_uids + set_uids):\n            # The object to link\n            target = api.get_object(uid)\n            # Add reference to object\n            if uid in add:\n                __traceback_info__ = (instance, uid, value, existing_uids)\n                self.add_reference(instance, target, **kwargs)\n            # Delete reference to object\n            elif uid in sub:\n                self.del_reference(instance, target, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(self, instance, aslist=False, **kwargs):\n        refs = self.get_versioned_references_for(instance)\n\n        if not self.multiValued:\n            if len(refs) > 1:\n                logger.warning(\"Found {} references for non-multivalued \"\n                               \"reference field '{}' of {}\".format(\n                                   len(refs), self.getName(), repr(instance)))\n            if not aslist:\n                if refs:\n                    refs = refs[0]\n                else:\n                    refs = None\n\n        if not self.referencesSortable or not hasattr(\n                aq_base(instance), \"at_ordered_refs\"):\n            return refs\n\n        refs = instance.at_ordered_refs\n        order = refs[self.relationship]\n        if order is None:\n            return refs\n\n        by_uid = dict(map(lambda ob: (api.get_uid(ob), ob), refs))\n        return [by_uid[uid] for uid in order if uid in by_uid]", "response": "Get the list of related objects for this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_submit(self):\n        wst_uid = self.request.form.get(\"getWorksheetTemplate\")\n        if not wst_uid:\n            return False\n\n        layout = self.context.getLayout()\n        wst = api.get_object_by_uid(wst_uid)\n\n        self.request[\"context_uid\"] = api.get_uid(self.context)\n        self.context.applyWorksheetTemplate(wst)\n\n        if len(self.context.getLayout()) == len(layout):\n            return False\n        return True", "response": "Handle submission of a new record."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef folderitems(self):\n        items = super(AddAnalysesView, self).folderitems(classic=False)\n        return items", "response": "Return folderitems as brains\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef folderitem(self, obj, item, index):\n        DueDate = obj.getDueDate\n\n        item[\"getDateReceived\"] = self.ulocalized_time(obj.getDateReceived)\n        item[\"getDueDate\"] = self.ulocalized_time(DueDate)\n\n        if DueDate and DueDate < DateTime():\n            item[\"after\"][\"DueDate\"] = get_image(\n                \"late.png\", title=t(_(\"Late Analysis\")))\n\n        # Add Priority column\n        priority_sort_key = obj.getPrioritySortkey\n        if not priority_sort_key:\n            # Default priority is Medium = 3.\n            # The format of PrioritySortKey is <priority>.<created>\n            priority_sort_key = \"3.%s\" % obj.created.ISO8601()\n\n        priority = priority_sort_key.split(\".\")[0]\n        priority_text = t(PRIORITIES.getValue(priority))\n        html = \"<div title='{}' class='priority-ico priority-{}'><div>\"\n        item[\"replace\"][\"Priority\"] = html.format(priority_text, priority)\n\n        return item", "response": "This method is called when an item is iterated in folderitems. It adds the appropriate properties to the item dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nservice triggered each time an item is iterated in folderitems. The use of this service prevents the extra-loops in child objects. :obj: the instance of the class to be foldered :item: dict containing the properties of the object to be used by the template :index: current index of the item", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Service triggered each time an item is iterated in folderitems.\n\n        The use of this service prevents the extra-loops in child objects.\n\n        :obj: the instance of the class to be foldered\n        :item: dict containing the properties of the object to be used by\n            the template\n        :index: current index of the item\n        \"\"\"\n\n        title = obj.Title()\n        description = obj.Description()\n        url = obj.absolute_url()\n\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n        item[\"Description\"] = description\n\n        sample_types = obj.getSampleTypes()\n        if sample_types:\n            links = map(\n                lambda st: get_link(st.absolute_url(),\n                                    value=st.Title(),\n                                    css_class=\"link\"),\n                sample_types)\n            item[\"replace\"][\"SampleTypes\"] = \", \".join(links)\n        else:\n            item[\"SampleTypes\"] = \"\"\n\n        parent = obj.aq_parent\n        if parent.portal_type == \"Client\":\n            item[\"Owner\"] = parent.aq_parent.Title()\n            item[\"replace\"][\"Owner\"] = get_link(\n                parent.absolute_url(), value=parent.getName())\n        else:\n            item[\"Owner\"] = self.context.bika_setup.laboratory.Title()\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the title or ID from the given UID", "response": "def get_title_or_id_from_uid(uid):\n    \"\"\"Returns the title or ID from the given UID\n    \"\"\"\n    obj = api.get_object_by_uid(uid, default=None)\n    if obj is None:\n        return \"\"\n    title_or_id = api.get_title(obj) or api.get_id(obj)\n    return title_or_id"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_meta_value_for(snapshot, key, default=None):\n    metadata = get_snapshot_metadata(snapshot)\n    return metadata.get(key, default)", "response": "Returns the metadata value for the given key"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the actor s fullname of the snapshot", "response": "def get_fullname(snapshot):\n    \"\"\"Get the actor's fullname of the snapshot\n    \"\"\"\n    actor = get_actor(snapshot)\n    properties = api.get_user_properties(actor)\n    return properties.get(\"fullname\", actor)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef snapshot_created(instance):\n    last_snapshot = get_last_snapshot(instance)\n    snapshot_created = get_created(last_snapshot)\n    return api.to_date(snapshot_created)", "response": "Returns the last snapshot created date"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef isCalibrationInProgress(self):\n        today = DateTime()\n        down_from = self.getDownFrom()\n        down_to = self.getDownTo()\n\n        return down_from <= today <= down_to", "response": "Checks if the current date is between a calibration period."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the current list of task types.", "response": "def getTaskTypes(self):\n        \"\"\" Return the current list of task types\n        \"\"\"\n        types = [\n            ('Calibration', safe_unicode(_('Calibration')).encode('utf-8')),\n            ('Enhancement', safe_unicode(_('Enhancement')).encode('utf-8')),\n            ('Preventive', safe_unicode(_('Preventive')).encode('utf-8')),\n            ('Repair', safe_unicode(_('Repair')).encode('utf-8')),\n            ('Validation', safe_unicode(_('Validation')).encode('utf-8')),\n        ]\n\n        return DisplayList(types)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the data line and add the result to the self. _header.", "response": "def parse_data_line(self, sline):\n        \"\"\"\n        Parse the data line. If an AS was selected it can distinguish between data rows and information rows.\n        :param sline: a split data line to parse\n        :returns: the number of rows to jump and parse the next data line or return the code error -1\n        \"\"\"\n\n        temp_line = sline\n        sline_ = ''.join(temp_line)\n        sline_ = sline_.split('\\t')\n        headerdict = {}\n        datadict = {}\n        for idx, result in enumerate(sline_):\n            if self._columns[idx] in self._includedcolumns:\n                datadict[self._columns[idx]] = {'Result': result, 'DefaultResult': 'Result'}\n            else:\n                headerdict[self._columns[idx]] = result\n        resid = headerdict['Sample ID']\n        print(datadict)\n        self._addRawResult(resid, datadict, False)\n        self._header = headerdict\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the current used analyses positions", "response": "def get_slots(self):\n        \"\"\"Return the current used analyses positions\n        \"\"\"\n        positions = map(\n            lambda uid: self.get_item_slot(uid), self.get_analyses_uids())\n        return map(lambda pos: str(pos), sorted(set(positions)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the full name of the person.", "response": "def getFullname(self):\n        \"\"\"Person's Fullname\n        \"\"\"\n\n        fn = self.getFirstname()\n        mi = self.getMiddleinitial()\n        md = self.getMiddlename()\n        sn = self.getSurname()\n        fullname = \"\"\n        if fn or sn:\n            if mi and md:\n                fullname = \"%s %s %s %s\" % (\n                    self.getFirstname(),\n                    self.getMiddleinitial(),\n                    self.getMiddlename(),\n                    self.getSurname())\n            elif mi:\n                fullname = \"%s %s %s\" % (\n                    self.getFirstname(),\n                    self.getMiddleinitial(),\n                    self.getSurname())\n            elif md:\n                fullname = \"%s %s %s\" % (\n                    self.getFirstname(),\n                    self.getMiddlename(),\n                    self.getSurname())\n            else:\n                fullname = '%s %s' % (self.getFirstname(), self.getSurname())\n        return fullname.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the currency Symbol", "response": "def get_currency_symbol(self):\n        \"\"\"Get the currency Symbol\n        \"\"\"\n        locale = locales.getLocale('en')\n        setup = api.get_setup()\n        currency = setup.getCurrency()\n        return locale.numbers.currencies[currency].symbol"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the log view of the requested analysis", "response": "def analysis_log_view(self):\n        \"\"\"Get the log view of the requested analysis\n        \"\"\"\n        service = self.get_analysis_or_service()\n        if not self.can_view_logs_of(service):\n            return None\n        view = api.get_view(\"auditlog\", context=service, request=self.request)\n        view.update()\n        view.before_render()\n        return view"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_record_value(request, uid, keyword, default=None):\n    value = request.get(keyword)\n    if not value:\n        return default\n    if not isinstance(value, list):\n        return default\n    return value[0].get(uid, default) or default", "response": "Returns the value for the keyword and uid from the request"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _toIntList(numstr, acceptX=0):\n    res = []\n    # Converting and removing invalid characters\n    for i in numstr:\n        if i in string.digits and i not in string.letters:\n            res.append(int(i))\n\n    # Converting control number into ISBN\n    if acceptX and (numstr[-1] in 'Xx'):\n        res.append(10)\n    return res", "response": "Convert an ans string to a list removing all invalid characters and adding 10 to the end of the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _sumLists(a, b):\n    val = 0\n    for i in map(lambda a, b: a * b, a, b):\n        val += i\n    return val", "response": "Sums the list of lists a and b."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_parent_objects(self, context):\n        parent_object = api.get_parent(context)\n        portal_type = api.get_portal_type(context)\n        return parent_object.objectValues(portal_type)", "response": "Return all objects of the same type from the parent object"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nqueries the parent objects of the same type from the parent objects.", "response": "def query_parent_objects(self, context, query=None):\n        \"\"\"Return the objects of the same type from the parent object\n\n        :param query: Catalog query to narrow down the objects\n        :type query: dict\n        :returns: Content objects of the same portal type in the parent\n        \"\"\"\n\n        # return the object values if we have no catalog query\n        if query is None:\n            return self.get_parent_objects(context)\n\n        # avoid undefined reference of catalog in except...\n        catalog = None\n\n        # try to fetch the results via the catalog\n        try:\n            catalogs = api.get_catalogs_for(context)\n            catalog = catalogs[0]\n            return map(api.get_object, catalog(query))\n        except (IndexError, UnicodeDecodeError, ParseError, APIError) as e:\n            # fall back to the object values of the parent\n            logger.warn(\"UniqueFieldValidator: Catalog query {} failed \"\n                        \"for catalog {} ({}) -> returning object values of {}\"\n                        .format(query, repr(catalog), str(e),\n                                repr(api.get_parent(context))))\n            return self.get_parent_objects(context)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_catalog_query(self, context, field, value):\n\n        # get the catalogs for the context\n        catalogs = api.get_catalogs_for(context)\n        # context not in any catalog?\n        if not catalogs:\n            logger.warn(\"UniqueFieldValidator: Context '{}' is not assigned\"\n                        \"to any catalog!\".format(repr(context)))\n            return None\n\n        # take the first catalog\n        catalog = catalogs[0]\n\n        # Check if the field accessor is indexed\n        field_index = field.getName()\n        accessor = field.getAccessor(context)\n        if accessor:\n            field_index = accessor.__name__\n\n        # return if the field is not indexed\n        if field_index not in catalog.indexes():\n            return None\n\n        # build a catalog query\n        query = {\n            \"portal_type\": api.get_portal_type(context),\n            \"path\": {\n                \"query\": api.get_parent_path(context),\n                \"depth\": 1,\n            }\n        }\n        query[field_index] = value\n        logger.info(\"UniqueFieldValidator:Query={}\".format(query))\n        return query", "response": "Create a catalog query for the field\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_service(self, request, uid):\n        spec_min = get_record_value(request, uid, \"min\")\n        spec_max = get_record_value(request, uid, \"max\")\n        error = get_record_value(request, uid, \"error\", \"0\")\n        warn_min = get_record_value(request, uid, \"warn_min\")\n        warn_max = get_record_value(request, uid, \"warn_max\")\n\n        if not spec_min and not spec_max:\n            # Neither min nor max values have been set, dismiss\n            return None\n\n        if not api.is_floatable(spec_min):\n            return \"'Min' value must be numeric\"\n        if not api.is_floatable(spec_max):\n            return \"'Max' value must be numeric\"\n        if api.to_float(spec_min) > api.to_float(spec_max):\n            return \"'Max' value must be above 'Min' value\"\n        if not api.is_floatable(error) or 0.0 < api.to_float(error) > 100:\n            return \"% Error must be between 0 and 100\"\n\n        if warn_min:\n            if not api.is_floatable(warn_min):\n                return \"'Warn Min' value must be numeric or empty\"\n            if api.to_float(warn_min) > api.to_float(spec_min):\n                return \"'Warn Min' value must be below 'Min' value\"\n\n        if warn_max:\n            if not api.is_floatable(warn_max):\n                return \"'Warn Max' value must be numeric or empty\"\n            if api.to_float(warn_max) < api.to_float(spec_max):\n                return \"'Warn Max' value must be above 'Max' value\"\n        return None", "response": "Validates the specs values from the request for the service uid. Returns a non - translated message if the validation fails."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_service(self, request, uid):\n\n        result = get_record_value(request, uid, 'result')\n        if not result:\n            # No result set for this service, dismiss\n            return None\n\n        if not api.is_floatable(result):\n            return \"Expected result value must be numeric\"\n\n        spec_min = get_record_value(request, uid, \"min\", result)\n        spec_max = get_record_value(request, uid, \"max\", result)\n        error = get_record_value(request, uid, \"error\", \"0\")\n        if not api.is_floatable(spec_min):\n            return \"'Min' value must be numeric\"\n        if not api.is_floatable(spec_max):\n            return \"'Max' value must be numeric\"\n        if api.to_float(spec_min) > api.to_float(result):\n            return \"'Min' value must be below the expected result\"\n        if api.to_float(spec_max) < api.to_float(result):\n            return \"'Max' value must be above the expected result\"\n        if not api.is_floatable(error) or 0.0 < api.to_float(error) > 100:\n            return \"% Error must be between 0 and 100\"\n        return None", "response": "Validates the specs values from request for the service uid. Returns a non - translated message if the validation failed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Title(self):\n        field = self.getField(\"Name\")\n        field = field and field.get(self) or \"\"\n        return safe_unicode(field).encode(\"utf-8\")", "response": "Return the name of the Organisation\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting an address for printing", "response": "def getPrintAddress(self):\n        \"\"\"Get an address for printing\n        \"\"\"\n        address_lines = []\n\n        addresses = [\n            self.getPostalAddress(),\n            self.getPhysicalAddress(),\n            self.getBillingAddress(),\n        ]\n\n        for address in addresses:\n            city = address.get(\"city\", \"\")\n            zip = address.get(\"zip\", \"\")\n            state = address.get(\"state\", \"\")\n            country = address.get(\"country\", \"\")\n\n            if city:\n                address_lines = [\n                    address[\"address\"].strip(),\n                    \"{} {}\".format(city, zip).strip(),\n                    \"{} {}\".format(state, country).strip(),\n                ]\n                break\n\n        return address_lines"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating an object in Bika LIMS", "response": "def create(container, portal_type, *args, **kwargs):\n    \"\"\"Creates an object in Bika LIMS\n\n    This code uses most of the parts from the TypesTool\n    see: `Products.CMFCore.TypesTool._constructInstance`\n\n    :param container: container\n    :type container: ATContentType/DexterityContentType/CatalogBrain\n    :param portal_type: The portal type to create, e.g. \"Client\"\n    :type portal_type: string\n    :param title: The title for the new content object\n    :type title: string\n    :returns: The new created object\n    \"\"\"\n    from bika.lims.utils import tmpID\n    if kwargs.get(\"title\") is None:\n        kwargs[\"title\"] = \"New {}\".format(portal_type)\n\n    # generate a temporary ID\n    tmp_id = tmpID()\n\n    # get the fti\n    types_tool = get_tool(\"portal_types\")\n    fti = types_tool.getTypeInfo(portal_type)\n\n    if fti.product:\n        obj = _createObjectByType(portal_type, container, tmp_id)\n    else:\n        # newstyle factory\n        factory = getUtility(IFactory, fti.factory)\n        obj = factory(tmp_id, *args, **kwargs)\n        if hasattr(obj, '_setPortalTypeName'):\n            obj._setPortalTypeName(fti.getId())\n        notify(ObjectCreatedEvent(obj))\n        # notifies ObjectWillBeAddedEvent, ObjectAddedEvent and\n        # ContainerModifiedEvent\n        container._setObject(tmp_id, obj)\n        # we get the object here with the current object id, as it might be\n        # renamed already by an event handler\n        obj = container._getOb(obj.getId())\n\n    # handle AT Content\n    if is_at_content(obj):\n        obj.processForm()\n\n    # Edit after processForm; processForm does AT unmarkCreationFlag.\n    obj.edit(**kwargs)\n\n    # explicit notification\n    modified(obj)\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a portal tool by name", "response": "def get_tool(name, context=None, default=_marker):\n    \"\"\"Get a portal tool by name\n\n    :param name: The name of the tool, e.g. `portal_catalog`\n    :type name: string\n    :param context: A portal object\n    :type context: ATContentType/DexterityContentType/CatalogBrain\n    :returns: Portal Tool\n    \"\"\"\n\n    # Try first with the context\n    if context is not None:\n        try:\n            context = get_object(context)\n            return getToolByName(context, name)\n        except (APIError, AttributeError) as e:\n            # https://github.com/senaite/bika.lims/issues/396\n            logger.warn(\"get_tool::getToolByName({}, '{}') failed: {} \"\n                        \"-> falling back to plone.api.portal.get_tool('{}')\"\n                        .format(repr(context), name, repr(e), name))\n            return get_tool(name, default=default)\n\n    # Try with the plone api\n    try:\n        return ploneapi.portal.get_tool(name)\n    except InvalidParameterError:\n        if default is not _marker:\n            return default\n        fail(\"No tool named '%s' found.\" % name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_object(brain_or_object):\n    if is_portal(brain_or_object):\n        return True\n    if is_at_content(brain_or_object):\n        return True\n    if is_dexterity_content(brain_or_object):\n        return True\n    if is_brain(brain_or_object):\n        return True\n    return False", "response": "Checks if the passed in object is a supported portal content object\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the full content object by the uid", "response": "def get_object(brain_object_uid, default=_marker):\n    \"\"\"Get the full content object\n\n    :param brain_object_uid: A catalog brain or content object or uid\n    :type brain_object_uid: PortalObject/ATContentType/DexterityContentType\n    /CatalogBrain/basestring\n    :returns: The full object\n    \"\"\"\n    if is_uid(brain_object_uid):\n        return get_object_by_uid(brain_object_uid)\n    if not is_object(brain_object_uid):\n        if default is _marker:\n            fail(\"{} is not supported.\".format(repr(brain_object_uid)))\n        return default\n    if is_brain(brain_object_uid):\n        return brain_object_uid.getObject()\n    return brain_object_uid"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_folderish(brain_or_object):\n    if hasattr(brain_or_object, \"is_folderish\"):\n        if callable(brain_or_object.is_folderish):\n            return brain_or_object.is_folderish()\n        return brain_or_object.is_folderish\n    return IFolderish.providedBy(get_object(brain_or_object))", "response": "Checks if the passed in object is folderish"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_portal_type(brain_or_object):\n    if not is_object(brain_or_object):\n        fail(\"{} is not supported.\".format(repr(brain_or_object)))\n    return brain_or_object.portal_type", "response": "Get the portal type for this object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_schema(brain_or_object):\n    obj = get_object(brain_or_object)\n    if is_portal(obj):\n        fail(\"get_schema can't return schema of portal root\")\n    if is_dexterity_content(obj):\n        pt = get_tool(\"portal_types\")\n        fti = pt.getTypeInfo(obj.portal_type)\n        return fti.lookupSchema()\n    if is_at_content(obj):\n        return obj.Schema()\n    fail(\"{} has no Schema.\".format(brain_or_object))", "response": "Get the schema of the content object\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_fields(brain_or_object):\n    obj = get_object(brain_or_object)\n    schema = get_schema(obj)\n    if is_dexterity_content(obj):\n        names = schema.names()\n        fields = map(lambda name: schema.get(name), names)\n        return dict(zip(names, fields))\n    return dict(zip(schema.keys(), schema.fields()))", "response": "Get the list of fields from the object\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the Plone ID for this object", "response": "def get_id(brain_or_object):\n    \"\"\"Get the Plone ID for this object\n\n    :param brain_or_object: A single catalog brain or content object\n    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain\n    :returns: Plone ID\n    :rtype: string\n    \"\"\"\n    if is_brain(brain_or_object) and base_hasattr(brain_or_object, \"getId\"):\n        return brain_or_object.getId\n    return get_object(brain_or_object).getId()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_title(brain_or_object):\n    if is_brain(brain_or_object) and base_hasattr(brain_or_object, \"Title\"):\n        return brain_or_object.Title\n    return get_object(brain_or_object).Title()", "response": "Get the Title for this object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_description(brain_or_object):\n    if is_brain(brain_or_object) \\\n            and base_hasattr(brain_or_object, \"Description\"):\n        return brain_or_object.Description\n    return get_object(brain_or_object).Description()", "response": "Get the Title for this object"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the absolute URL for this object", "response": "def get_url(brain_or_object):\n    \"\"\"Get the absolute URL for this object\n\n    :param brain_or_object: A single catalog brain or content object\n    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain\n    :returns: Absolute URL\n    :rtype: string\n    \"\"\"\n    if is_brain(brain_or_object) and base_hasattr(brain_or_object, \"getURL\"):\n        return brain_or_object.getURL()\n    return get_object(brain_or_object).absolute_url()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_object_by_uid(uid, default=_marker):\n\n    # nothing to do here\n    if not uid:\n        if default is not _marker:\n            return default\n        fail(\"get_object_by_uid requires UID as first argument; got {} instead\"\n             .format(uid))\n\n    # we defined the portal object UID to be '0'::\n    if uid == '0':\n        return get_portal()\n\n    brain = get_brain_by_uid(uid)\n\n    if brain is None:\n        if default is not _marker:\n            return default\n        fail(\"No object found for UID {}\".format(uid))\n\n    return get_object(brain)", "response": "Find an object by the UID"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nqueries a brain by a given UID", "response": "def get_brain_by_uid(uid, default=None):\n    \"\"\"Query a brain by a given UID\n\n    :param uid: The UID of the object to find\n    :type uid: string\n    :returns: ZCatalog brain or None\n    \"\"\"\n    if not is_uid(uid):\n        return default\n\n    # we try to find the object with the UID catalog\n    uc = get_tool(\"uid_catalog\")\n\n    # try to find the object with the reference catalog first\n    brains = uc(UID=uid)\n    if len(brains) != 1:\n        return default\n    return brains[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_object_by_path(path, default=_marker):\n\n    # nothing to do here\n    if not path:\n        if default is not _marker:\n            return default\n        fail(\"get_object_by_path first argument must be a path; {} received\"\n             .format(path))\n\n    portal = get_portal()\n    portal_path = get_path(portal)\n    portal_url = get_url(portal)\n\n    # ensure we have a physical path\n    if path.startswith(portal_url):\n        request = get_request()\n        path = \"/\".join(request.physicalPathFromURL(path))\n\n    if not path.startswith(portal_path):\n        if default is not _marker:\n            return default\n        fail(\"Not a physical path inside the portal.\")\n\n    if path == portal_path:\n        return portal\n\n    return portal.restrictedTraverse(path, default)", "response": "Find an object by a given physical path or absolute_url."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the physical path of the object", "response": "def get_path(brain_or_object):\n    \"\"\"Calculate the physical path of this object\n\n    :param brain_or_object: A single catalog brain or content object\n    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain\n    :returns: Physical path of the object\n    :rtype: string\n    \"\"\"\n    if is_brain(brain_or_object):\n        return brain_or_object.getPath()\n    return \"/\".join(get_object(brain_or_object).getPhysicalPath())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_parent_path(brain_or_object):\n    if is_portal(brain_or_object):\n        return get_path(get_portal())\n    if is_brain(brain_or_object):\n        path = get_path(brain_or_object)\n        return path.rpartition(\"/\")[0]\n    return get_path(get_object(brain_or_object).aq_parent)", "response": "Calculate the physical parent path of this object"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlocates the parent object of the content or catalog brain or content object", "response": "def get_parent(brain_or_object, catalog_search=False):\n    \"\"\"Locate the parent object of the content/catalog brain\n\n    The `catalog_search` switch uses the `portal_catalog` to do a search return\n    a brain instead of the full parent object. However, if the search returned\n    no results, it falls back to return the full parent object.\n\n    :param brain_or_object: A single catalog brain or content object\n    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain\n    :param catalog_search: Use a catalog query to find the parent object\n    :type catalog_search: bool\n    :returns: parent object\n    :rtype: ATContentType/DexterityContentType/PloneSite/CatalogBrain\n    \"\"\"\n\n    if is_portal(brain_or_object):\n        return get_portal()\n\n    # Do a catalog search and return the brain\n    if catalog_search:\n        parent_path = get_parent_path(brain_or_object)\n\n        # parent is the portal object\n        if parent_path == get_path(get_portal()):\n            return get_portal()\n\n        # get the catalog tool\n        pc = get_portal_catalog()\n\n        # query for the parent path\n        results = pc(path={\n            \"query\": parent_path,\n            \"depth\": 0})\n\n        # No results fallback: return the parent object\n        if not results:\n            return get_object(brain_or_object).aq_parent\n\n        # return the brain\n        return results[0]\n\n    return get_object(brain_or_object).aq_parent"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsearching for objects in a ZCatalog tree.", "response": "def search(query, catalog=_marker):\n    \"\"\"Search for objects.\n\n    :param query: A suitable search query.\n    :type query: dict\n    :param catalog: A single catalog id or a list of catalog ids\n    :type catalog: str/list\n    :returns: Search results\n    :rtype: List of ZCatalog brains\n    \"\"\"\n\n    # query needs to be a dictionary\n    if not isinstance(query, dict):\n        fail(\"Catalog query needs to be a dictionary\")\n\n    # Portal types to query\n    portal_types = query.get(\"portal_type\", [])\n    # We want the portal_type as a list\n    if not isinstance(portal_types, (tuple, list)):\n        portal_types = [portal_types]\n\n    # The catalogs used for the query\n    catalogs = []\n\n    # The user did **not** specify a catalog\n    if catalog is _marker:\n        # Find the registered catalogs for the queried portal types\n        for portal_type in portal_types:\n            # Just get the first registered/default catalog\n            catalogs.append(get_catalogs_for(\n                portal_type, default=\"portal_catalog\")[0])\n    else:\n        # User defined catalogs\n        if isinstance(catalog, (list, tuple)):\n            catalogs.extend(map(get_tool, catalog))\n        else:\n            catalogs.append(get_tool(catalog))\n\n    # Cleanup: Avoid duplicate catalogs\n    catalogs = list(set(catalogs)) or [get_portal_catalog()]\n\n    # We only support **single** catalog queries\n    if len(catalogs) > 1:\n        fail(\"Multi Catalog Queries are not supported!\")\n\n    return catalogs[0](query)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the attribute value of the object", "response": "def safe_getattr(brain_or_object, attr, default=_marker):\n    \"\"\"Return the attribute value\n\n    :param brain_or_object: A single catalog brain or content object\n    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain\n    :param attr: Attribute name\n    :type attr: str\n    :returns: Attribute value\n    :rtype: obj\n    \"\"\"\n    try:\n        value = getattr(brain_or_object, attr, _marker)\n        if value is _marker:\n            if default is not _marker:\n                return default\n            fail(\"Attribute '{}' not found.\".format(attr))\n        if callable(value):\n            return value()\n        return value\n    except Unauthorized:\n        if default is not _marker:\n            return default\n        fail(\"You are not authorized to access '{}' of '{}'.\".format(\n            attr, repr(brain_or_object)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the revision history for the given brain or context.", "response": "def get_revision_history(brain_or_object):\n    \"\"\"Get the revision history for the given brain or context.\n\n    :param brain_or_object: A single catalog brain or content object\n    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain\n    :returns: Workflow history\n    :rtype: obj\n    \"\"\"\n    obj = get_object(brain_or_object)\n    chv = ContentHistoryView(obj, safe_getattr(obj, \"REQUEST\", None))\n    return chv.fullHistory()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_workflows_for(brain_or_object):\n    workflow = ploneapi.portal.get_tool(\"portal_workflow\")\n    if isinstance(brain_or_object, basestring):\n        return workflow.getChainFor(brain_or_object)\n    obj = get_object(brain_or_object)\n    return workflow.getChainFor(obj)", "response": "Returns the assigned workflows for the given brain or context"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_workflow_status_of(brain_or_object, state_var=\"review_state\"):\n    # Try to get the state from the catalog brain first\n    if is_brain(brain_or_object):\n        if state_var in brain_or_object.schema():\n            return brain_or_object[state_var]\n\n    # Retrieve the sate from the object\n    workflow = get_tool(\"portal_workflow\")\n    obj = get_object(brain_or_object)\n    return workflow.getInfoFor(ob=obj, name=state_var, default='')", "response": "Get the current workflow status of the given object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_creation_date(brain_or_object):\n    created = getattr(brain_or_object, \"created\", None)\n    if created is None:\n        fail(\"Object {} has no creation date \".format(\n             repr(brain_or_object)))\n    if callable(created):\n        return created()\n    return created", "response": "Get the creation date of the brain or content object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_modification_date(brain_or_object):\n    modified = getattr(brain_or_object, \"modified\", None)\n    if modified is None:\n        fail(\"Object {} has no modification date \".format(\n             repr(brain_or_object)))\n    if callable(modified):\n        return modified()\n    return modified", "response": "Get the modification date of the brain or content object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_catalogs_for(brain_or_object, default=\"portal_catalog\"):\n    archetype_tool = get_tool(\"archetype_tool\", None)\n    if not archetype_tool:\n        # return the default catalog\n        return [get_tool(default)]\n\n    catalogs = []\n\n    # get the registered catalogs for portal_type\n    if is_object(brain_or_object):\n        catalogs = archetype_tool.getCatalogsByType(\n            get_portal_type(brain_or_object))\n    if isinstance(brain_or_object, basestring):\n        catalogs = archetype_tool.getCatalogsByType(brain_or_object)\n\n    if not catalogs:\n        return [get_tool(default)]\n    return catalogs", "response": "Get all registered catalogs for the given brain or content object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_transitions_for(brain_or_object):\n    workflow = get_tool('portal_workflow')\n    transitions = []\n    instance = get_object(brain_or_object)\n    for wfid in get_workflows_for(brain_or_object):\n        wf = workflow[wfid]\n        tlist = wf.getTransitionsFor(instance)\n        transitions.extend([t for t in tlist if t not in transitions])\n    return transitions", "response": "List available workflow transitions for the given brain or content object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_transition_for(brain_or_object, transition):\n    if not isinstance(transition, basestring):\n        fail(\"Transition type needs to be string, got '%s'\" % type(transition))\n    obj = get_object(brain_or_object)\n    try:\n        ploneapi.content.transition(obj, transition)\n    except ploneapi.exc.InvalidParameterError as e:\n        fail(\"Failed to perform transition '{}' on {}: {}\".format(\n             transition, obj, str(e)))\n    return obj", "response": "Performs a workflow transition for the passed in object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_roles_for_permission(permission, brain_or_object):\n    obj = get_object(brain_or_object)\n    allowed = set(rolesForPermissionOn(permission, obj))\n    return sorted(allowed)", "response": "Returns a list of granted roles for the given permission on the object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if the passed in object is versionable.", "response": "def is_versionable(brain_or_object, policy='at_edit_autoversion'):\n    \"\"\"Checks if the passed in object is versionable.\n\n    :param brain_or_object: A single catalog brain or content object\n    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain\n    :returns: True if the object is versionable\n    :rtype: bool\n    \"\"\"\n    pr = get_tool(\"portal_repository\")\n    obj = get_object(brain_or_object)\n    return pr.supportsPolicy(obj, 'at_edit_autoversion') \\\n        and pr.isVersionable(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_version(brain_or_object):\n    obj = get_object(brain_or_object)\n    if not is_versionable(obj):\n        return None\n    return getattr(aq_base(obj), \"version_id\", 0)", "response": "Get the version of the current object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the view by name", "response": "def get_view(name, context=None, request=None):\n    \"\"\"Get the view by name\n\n    :param name: The name of the view\n    :type name: str\n    :param context: The context to query the view\n    :type context: ATContentType/DexterityContentType/CatalogBrain\n    :param request: The request to query the view\n    :type request: HTTPRequest object\n    :returns: HTTP Request\n    :rtype: Products.Five.metaclass View object\n    \"\"\"\n    context = context or get_portal()\n    request = request or get_request() or None\n    return getMultiAdapter((get_object(context), request), name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the group object for the given groupname", "response": "def get_group(group_or_groupname):\n    \"\"\"Return Plone Group\n\n    :param group_or_groupname: Plone group or the name of the group\n    :type groupname:  GroupData/str\n    :returns: Plone GroupData\n    \"\"\"\n    if not group_or_groupname:\n\n        return None\n    if hasattr(group_or_groupname, \"_getGroup\"):\n        return group_or_groupname\n    gtool = get_tool(\"portal_groups\")\n    return gtool.getGroupById(group_or_groupname)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns Plone User or MemberData object", "response": "def get_user(user_or_username):\n    \"\"\"Return Plone User\n\n    :param user_or_username: Plone user or user id\n    :returns: Plone MemberData\n    \"\"\"\n    user = None\n    if isinstance(user_or_username, MemberData):\n        user = user_or_username\n    if isinstance(user_or_username, basestring):\n        user = get_member_by_login_name(get_portal(), user_or_username, False)\n    return user"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning User Properties :param user_or_username: Plone group identifier :returns: Plone MemberData", "response": "def get_user_properties(user_or_username):\n    \"\"\"Return User Properties\n\n    :param user_or_username: Plone group identifier\n    :returns: Plone MemberData\n    \"\"\"\n    user = get_user(user_or_username)\n    if user is None:\n        return {}\n    if not callable(user.getUser):\n        return {}\n    out = {}\n    plone_user = user.getUser()\n    for sheet in plone_user.listPropertysheets():\n        ps = plone_user.getPropertysheet(sheet)\n        out.update(dict(ps.propertyItems()))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_users_by_roles(roles=None):\n    if not isinstance(roles, (tuple, list)):\n        roles = [roles]\n    mtool = get_tool(\"portal_membership\")\n    return mtool.searchForMembers(roles=roles)", "response": "Search Plone users by their roles"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_user_contact(user, contact_types=['Contact', 'LabContact']):\n    if not user:\n        return None\n\n    query = {'portal_type': contact_types, 'getUsername': user.id}\n    brains = search(query, catalog='portal_catalog')\n    if not brains:\n        return None\n\n    if len(brains) > 1:\n        # Oops, the user has multiple contacts assigned, return None\n        contacts = map(lambda c: c.Title, brains)\n        err_msg = \"User '{}' is bound to multiple Contacts '{}'\"\n        err_msg = err_msg.format(user.id, ','.join(contacts))\n        logger.error(err_msg)\n        return None\n\n    return get_object(brains[0])", "response": "Returns the contact associated to a Plone user"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the client of the contact of a Plone user", "response": "def get_user_client(user_or_contact):\n    \"\"\"Returns the client of the contact of a Plone user\n\n    If the user passed in has no contact or does not belong to any client,\n    returns None.\n\n    :param: Plone user or contact\n    :returns: Client the contact of the Plone user belongs to\n    \"\"\"\n    if not user_or_contact or ILabContact.providedBy(user_or_contact):\n        # Lab contacts cannot belong to a client\n        return None\n\n    if not IContact.providedBy(user_or_contact):\n        contact = get_user_contact(user_or_contact, contact_types=['Contact'])\n        if IContact.providedBy(contact):\n            return get_user_client(contact)\n        return None\n\n    client = get_parent(user_or_contact)\n    if client and IClient.providedBy(client):\n        return client\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a cache key for a common brain or content object", "response": "def get_cache_key(brain_or_object):\n    \"\"\"Generate a cache key for a common brain or object\n\n    :param brain_or_object: A single catalog brain or content object\n    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain\n    :returns: Cache Key\n    :rtype: str\n    \"\"\"\n    key = [\n        get_portal_type(brain_or_object),\n        get_id(brain_or_object),\n        get_uid(brain_or_object),\n        # handle different domains gracefully\n        get_url(brain_or_object),\n        # Return the microsecond since the epoch in GMT\n        get_modification_date(brain_or_object).micros(),\n    ]\n    return \"-\".join(map(lambda x: str(x), key))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnormalizes the id :param string: A string to normalize :type string: str :returns: Normalized ID :rtype: str", "response": "def normalize_id(string):\n    \"\"\"Normalize the id\n\n    :param string: A string to normalize\n    :type string: str\n    :returns: Normalized ID\n    :rtype: str\n    \"\"\"\n    if not isinstance(string, basestring):\n        fail(\"Type of argument must be string, found '{}'\"\n             .format(type(string)))\n    # get the id nomalizer utility\n    normalizer = getUtility(IIDNormalizer).normalize\n    return normalizer(string)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef normalize_filename(string):\n    if not isinstance(string, basestring):\n        fail(\"Type of argument must be string, found '{}'\"\n             .format(type(string)))\n    # get the file nomalizer utility\n    normalizer = getUtility(IFileNameNormalizer).normalize\n    return normalizer(string)", "response": "Normalizes the filename of the\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if the passed in uid is a valid UID", "response": "def is_uid(uid, validate=False):\n    \"\"\"Checks if the passed in uid is a valid UID\n\n    :param uid: The uid to check\n    :param validate: If False, checks if uid is a valid 23 alphanumeric uid. If\n    True, also verifies if a brain exists for the uid passed in\n    :type uid: string\n    :return: True if a valid uid\n    :rtype: bool\n    \"\"\"\n    if not isinstance(uid, basestring):\n        return False\n    if uid == '0':\n        return True\n    if len(uid) != 32:\n        return False\n    if not UID_RX.match(uid):\n        return False\n    if not validate:\n        return True\n\n    # Check if a brain for this uid exists\n    uc = get_tool('uid_catalog')\n    brains = uc(UID=uid)\n    if brains:\n        assert (len(brains) == 1)\n    return len(brains) > 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntries to convert the passed in value to a valid DateTime or default.", "response": "def to_date(value, default=None):\n    \"\"\"Tries to convert the passed in value to Zope's DateTime\n\n    :param value: The value to be converted to a valid DateTime\n    :type value: str, DateTime or datetime\n    :return: The DateTime representation of the value passed in or default\n    \"\"\"\n    if isinstance(value, DateTime):\n        return value\n    if not value:\n        if default is None:\n            return None\n        return to_date(default)\n    try:\n        if isinstance(value, str) and '.' in value:\n            # https://docs.plone.org/develop/plone/misc/datetime.html#datetime-problems-and-pitfalls\n            return DateTime(value, datefmt='international')\n        return DateTime(value)\n    except (TypeError, ValueError, DateTimeError):\n        return to_date(default)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the total number of minutes in a cluster.", "response": "def to_minutes(days=0, hours=0, minutes=0, seconds=0, milliseconds=0,\n               round_to_int=True):\n    \"\"\"Returns the computed total number of minutes\n    \"\"\"\n    total = float(days)*24*60 + float(hours)*60 + float(minutes) + \\\n        float(seconds)/60 + float(milliseconds)/1000/60\n    return int(round(total)) if round_to_int else total"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_dhm_format(days=0, hours=0, minutes=0, seconds=0, milliseconds=0):\n    minutes = to_minutes(days=days, hours=hours, minutes=minutes,\n                         seconds=seconds, milliseconds=milliseconds)\n    delta = timedelta(minutes=int(round(minutes)))\n    d = delta.days\n    h = delta.seconds // 3600\n    m = (delta.seconds // 60) % 60\n    m = m and \"{}m \".format(str(m)) or \"\"\n    d = d and \"{}d \".format(str(d)) or \"\"\n    if m and d:\n        h = \"{}h \".format(str(h))\n    else:\n        h = h and \"{}h \".format(str(h)) or \"\"\n    return \"\".join([d, h, m]).strip()", "response": "Returns a representation of time in xd yh zm format\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntries to convert the value to int.", "response": "def to_int(value, default=_marker):\n    \"\"\"Tries to convert the value to int.\n    Truncates at the decimal point if the value is a float\n\n    :param value: The value to be converted to an int\n    :return: The resulting int or default\n    \"\"\"\n    if is_floatable(value):\n        value = to_float(value)\n    try:\n        return int(value)\n    except (TypeError, ValueError):\n        if default is None:\n            return default\n        if default is not _marker:\n            return to_int(default)\n        fail(\"Value %s cannot be converted to int\" % repr(value))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_float(value, default=_marker):\n    if not is_floatable(value):\n        if default is not _marker:\n            return to_float(default)\n        fail(\"Value %s is not floatable\" % repr(value))\n    return float(value)", "response": "Converts the passed in value to a floatable number\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_searchable_text_metadata(value):\n    if not value:\n        return u\"\"\n    if value is Missing.Value:\n        return u\"\"\n    if is_uid(value):\n        return u\"\"\n    if isinstance(value, (bool)):\n        return u\"\"\n    if isinstance(value, (list, tuple)):\n        for v in value:\n            return to_searchable_text_metadata(v)\n    if isinstance(value, (dict)):\n        for k, v in value.items():\n            return to_searchable_text_metadata(v)\n    if is_date(value):\n        return value.strftime(\"%Y-%m-%d\")\n    if not isinstance(value, basestring):\n        value = str(value)\n    return safe_unicode(value)", "response": "Parse the given metadata value to searchable text."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_display_list(pairs, sort_by=\"key\", allow_empty=True):\n    dl = DisplayList()\n\n    if isinstance(pairs, basestring):\n        pairs = [pairs, pairs]\n    for pair in pairs:\n        # pairs is a list of lists -> add each pair\n        if isinstance(pair, (tuple, list)):\n            dl.add(*pair)\n        # pairs is just a single pair -> add it and stop\n        if isinstance(pair, basestring):\n            dl.add(*pairs)\n            break\n\n    # add the empty option\n    if allow_empty:\n        dl.add(\"\", \"\")\n\n    # sort by key/value\n    if sort_by == \"key\":\n        dl = dl.sortedByKey()\n    elif sort_by == \"value\":\n        dl = dl.sortedByValue()\n\n    return dl", "response": "Create a Plone DisplayList from a list of key value pairs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef has_permission(self, permission, obj=None):\n        if not permission:\n            logger.warn(\"None permission is not allowed\")\n            return False\n        if obj is None:\n            return check_permission(permission, self.context)\n        return check_permission(permission, api.get_object(obj))", "response": "Returns True if the current user has rights for the permission passed in\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn if the user can edit the analysis passed in by the current user.", "response": "def is_analysis_edition_allowed(self, analysis_brain):\n        \"\"\"Returns if the analysis passed in can be edited by the current user\n\n        :param analysis_brain: Brain that represents an analysis\n        :return: True if the user can edit the analysis, otherwise False\n        \"\"\"\n        if not self.context_active:\n            # The current context must be active. We cannot edit analyses from\n            # inside a deactivated Analysis Request, for instance\n            return False\n\n        analysis_obj = api.get_object(analysis_brain)\n        if analysis_obj.getPointOfCapture() == 'field':\n            # This analysis must be captured on field, during sampling.\n            if not self.has_permission(EditFieldResults, analysis_obj):\n                # Current user cannot edit field analyses.\n                return False\n\n        elif not self.has_permission(EditResults, analysis_obj):\n            # The Point of Capture is 'lab' and the current user cannot edit\n            # lab analyses.\n            return False\n\n        # Check if the user is allowed to enter a value to to Result field\n        if not self.has_permission(FieldEditAnalysisResult, analysis_obj):\n            return False\n\n        # Is the instrument out of date?\n        # The user can assign a result to the analysis if it does not have any\n        # instrument assigned or the instrument assigned is valid.\n        if not self.is_analysis_instrument_valid(analysis_brain):\n            # return if it is allowed to enter a manual result\n            return analysis_obj.getManualEntryOfResults()\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_result_edition_allowed(self, analysis_brain):\n\n        # Always check general edition first\n        if not self.is_analysis_edition_allowed(analysis_brain):\n            return False\n\n        # Get the ananylsis object\n        obj = api.get_object(analysis_brain)\n\n        if not obj.getDetectionLimitOperand():\n            # This is a regular result (not a detection limit)\n            return True\n\n        # Detection limit selector is enabled in the Analysis Service\n        if obj.getDetectionLimitSelector():\n            # Manual detection limit entry is *not* allowed\n            if not obj.getAllowManualDetectionLimit():\n                return False\n\n        return True", "response": "Checks if the edition of the result field is allowed for the user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_uncertainty_edition_allowed(self, analysis_brain):\n\n        # Only allow to edit the uncertainty if result edition is allowed\n        if not self.is_result_edition_allowed(analysis_brain):\n            return False\n\n        # Get the ananylsis object\n        obj = api.get_object(analysis_brain)\n\n        # Manual setting of uncertainty is not allowed\n        if not obj.getAllowManualUncertainty():\n            return False\n\n        # Result is a detection limit -> uncertainty setting makes no sense!\n        if obj.getDetectionLimitOperand() in [LDL, UDL]:\n            return False\n\n        return True", "response": "Checks if the edition of the result field is allowed for the current user"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_analysis_instrument_valid(self, analysis_brain):\n        if analysis_brain.meta_type == 'ReferenceAnalysis':\n            # If this is a ReferenceAnalysis, there is no need to check the\n            # validity of the instrument, because this is a QC analysis and by\n            # definition, it has the ability to promote an instrument to a\n            # valid state if the result is correct.\n            return True\n        instrument = self.get_instrument(analysis_brain)\n        return not instrument or instrument.isValid()", "response": "Return if the analysis passed in is from ReferenceAnalysis type returns True\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_object(self, brain_or_object_or_uid):\n        if api.is_uid(brain_or_object_or_uid):\n            return api.get_object_by_uid(brain_or_object_or_uid, default=None)\n        if api.is_object(brain_or_object_or_uid):\n            return api.get_object(brain_or_object_or_uid)\n        return None", "response": "Get the full content object. Returns None if the param passed in is\n        is not a valid object or not found"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_methods_vocabulary(self, analysis_brain):\n        uids = analysis_brain.getAllowedMethodUIDs\n        query = {'portal_type': 'Method',\n                 'is_active': True,\n                 'UID': uids}\n        brains = api.search(query, 'bika_setup_catalog')\n        if not brains:\n            return [{'ResultValue': '', 'ResultText': _('None')}]\n        return map(lambda brain: {'ResultValue': brain.UID,\n                                  'ResultText': brain.Title}, brains)", "response": "Returns a vocabulary with all the methods available for the passed in analysis"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_instruments_vocabulary(self, analysis_brain):\n        if not analysis_brain.getInstrumentEntryOfResults:\n            # Instrument entry of results for this analysis is not allowed\n            return list()\n\n        # If the analysis is a QC analysis, display all instruments, including\n        # those uncalibrated or for which the last QC test failed.\n        meta_type = analysis_brain.meta_type\n        uncalibrated = meta_type == 'ReferenceAnalysis'\n        if meta_type == 'DuplicateAnalysis':\n            base_analysis_type = analysis_brain.getAnalysisPortalType\n            uncalibrated = base_analysis_type == 'ReferenceAnalysis'\n\n        uids = analysis_brain.getAllowedInstrumentUIDs\n        query = {'portal_type': 'Instrument',\n                 'is_active': True,\n                 'UID': uids}\n        brains = api.search(query, 'bika_setup_catalog')\n        vocab = [{'ResultValue': '', 'ResultText': _('None')}]\n        for brain in brains:\n            instrument = self.get_object(brain)\n            if uncalibrated and not instrument.isOutOfDate():\n                # Is a QC analysis, include instrument also if is not valid\n                vocab.append({'ResultValue': instrument.UID(),\n                              'ResultText': instrument.Title()})\n            if instrument.isValid():\n                # Only add the 'valid' instruments: certificate\n                # on-date and valid internal calibration tests\n                vocab.append({'ResultValue': instrument.UID(),\n                              'ResultText': instrument.Title()})\n        return vocab", "response": "Returns a vocabulary with the valid and active instruments available for the analysis passed in."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef isItemAllowed(self, obj):\n        if not obj:\n            return False\n\n        # Does the user has enough privileges to see retracted analyses?\n        if obj.review_state == 'retracted' and \\\n                not self.has_permission(ViewRetractedAnalyses):\n            return False\n        return True", "response": "Checks if the passed in Analysis is allowed in the list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the item with the contents of the object.", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Prepare a data item for the listing.\n\n        :param obj: The catalog brain or content object\n        :param item: Listing item (dictionary)\n        :param index: Index of the listing item\n        :returns: Augmented listing data item\n        \"\"\"\n\n        item['Service'] = obj.Title\n        item['class']['service'] = 'service_title'\n        item['service_uid'] = obj.getServiceUID\n        item['Keyword'] = obj.getKeyword\n        item['Unit'] = format_supsub(obj.getUnit) if obj.getUnit else ''\n        item['retested'] = obj.getRetestOfUID and True or False\n        item['class']['retested'] = 'center'\n\n        # Append info link before the service\n        # see: bika.lims.site.coffee for the attached event handler\n        item[\"before\"][\"Service\"] = get_link(\n            \"analysisservice_info?service_uid={}&analysis_uid={}\"\n            .format(obj.getServiceUID, obj.UID),\n            value=\"<span class='glyphicon glyphicon-info-sign'></span>\",\n            css_class=\"service_info\")\n\n        # Note that getSampleTypeUID returns the type of the Sample, no matter\n        # if the sample associated to the analysis is a regular Sample (routine\n        # analysis) or if is a Reference Sample (Reference Analysis). If the\n        # analysis is a duplicate, it returns the Sample Type of the sample\n        # associated to the source analysis.\n        item['st_uid'] = obj.getSampleTypeUID\n\n        # Fill item's category\n        self._folder_item_category(obj, item)\n        # Fill item's row class\n        self._folder_item_css_class(obj, item)\n        # Fill result and/or result options\n        self._folder_item_result(obj, item)\n        # Fill calculation and interim fields\n        self._folder_item_calculation(obj, item)\n        # Fill method\n        self._folder_item_method(obj, item)\n        # Fill instrument\n        self._folder_item_instrument(obj, item)\n        # Fill analyst\n        self._folder_item_analyst(obj, item)\n        # Fill attachments\n        self._folder_item_attachments(obj, item)\n        # Fill uncertainty\n        self._folder_item_uncertainty(obj, item)\n        # Fill Detection Limits\n        self._folder_item_detection_limits(obj, item)\n        # Fill Specifications\n        self._folder_item_specifications(obj, item)\n        # Fill Due Date and icon if late/overdue\n        self._folder_item_duedate(obj, item)\n        # Fill verification criteria\n        self._folder_item_verify_icons(obj, item)\n        # Fill worksheet anchor/icon\n        self._folder_item_assigned_worksheet(obj, item)\n        # Fill reflex analysis icons\n        self._folder_item_reflex_icons(obj, item)\n        # Fill hidden field (report visibility)\n        self._folder_item_report_visibility(obj, item)\n        # Renders additional icons to be displayed\n        self._folder_item_fieldicons(obj)\n        # Renders remarks toggle button\n        self._folder_item_remarks(obj, item)\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the category to the item passed in", "response": "def _folder_item_category(self, analysis_brain, item):\n        \"\"\"Sets the category to the item passed in\n\n        :param analysis_brain: Brain that represents an analysis\n        :param item: analysis' dictionary counterpart that represents a row\n        \"\"\"\n        if not self.show_categories:\n            return\n\n        cat = analysis_brain.getCategoryTitle\n        item[\"category\"] = cat\n        cat_order = self.analysis_categories_order.get(cat)\n        if (cat, cat_order) not in self.categories:\n            self.categories.append((cat, cat_order))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the suitable css class name(s) to table_row_class from the passed in depending on the properties of the analysis object that represents an analysis object.", "response": "def _folder_item_css_class(self, analysis_brain, item):\n        \"\"\"Sets the suitable css class name(s) to `table_row_class` from the\n        item passed in, depending on the properties of the analysis object\n\n        :param analysis_brain: Brain that represents an analysis\n        :param item: analysis' dictionary counterpart that represents a row\n        \"\"\"\n        meta_type = analysis_brain.meta_type\n\n        # Default css names for table_row_class\n        css_names = item.get('table_row_class', '').split()\n        css_names.extend(['state-{}'.format(analysis_brain.review_state),\n                          'type-{}'.format(meta_type.lower())])\n\n        if meta_type == 'ReferenceAnalysis':\n            css_names.append('qc-analysis')\n\n        elif meta_type == 'DuplicateAnalysis':\n            if analysis_brain.getAnalysisPortalType == 'ReferenceAnalysis':\n                css_names.append('qc-analysis')\n\n        item['table_row_class'] = ' '.join(css_names)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the analysis due date to the item passed in.", "response": "def _folder_item_duedate(self, analysis_brain, item):\n        \"\"\"Set the analysis' due date to the item passed in.\n\n        :param analysis_brain: Brain that represents an analysis\n        :param item: analysis' dictionary counterpart that represents a row\n        \"\"\"\n\n        # Note that if the analysis is a Reference Analysis, `getDueDate`\n        # returns the date when the ReferenceSample expires. If the analysis is\n        # a duplicate, `getDueDate` returns the due date of the source analysis\n        due_date = analysis_brain.getDueDate\n        if not due_date:\n            return None\n        due_date_str = self.ulocalized_time(due_date, long_format=0)\n        item['DueDate'] = due_date_str\n\n        # If the Analysis is late/overdue, display an icon\n        capture_date = analysis_brain.getResultCaptureDate\n        capture_date = capture_date or DateTime()\n        if capture_date > due_date:\n            # The analysis is late or overdue\n            img = get_image('late.png', title=t(_(\"Late Analysis\")),\n                            width='16px', height='16px')\n            item['replace']['DueDate'] = '{} {}'.format(due_date_str, img)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the analysis result to the item passed in.", "response": "def _folder_item_result(self, analysis_brain, item):\n        \"\"\"Set the analysis' result to the item passed in.\n\n        :param analysis_brain: Brain that represents an analysis\n        :param item: analysis' dictionary counterpart that represents a row\n        \"\"\"\n\n        item[\"Result\"] = \"\"\n\n        if not self.has_permission(ViewResults, analysis_brain):\n            # If user has no permissions, don\"t display the result but an icon\n            img = get_image(\"to_follow.png\", width=\"16px\", height=\"16px\")\n            item[\"before\"][\"Result\"] = img\n            return\n\n        result = analysis_brain.getResult\n        capture_date = analysis_brain.getResultCaptureDate\n        capture_date_str = self.ulocalized_time(capture_date, long_format=0)\n\n        item[\"Result\"] = result\n        item[\"CaptureDate\"] = capture_date_str\n        item[\"result_captured\"] = capture_date_str\n\n        # Edit mode enabled of this Analysis\n        if self.is_analysis_edition_allowed(analysis_brain):\n            # Allow to set Remarks\n            item[\"allow_edit\"].append(\"Remarks\")\n\n            # Set the results field editable\n            if self.is_result_edition_allowed(analysis_brain):\n                item[\"allow_edit\"].append(\"Result\")\n\n            # Prepare result options\n            choices = analysis_brain.getResultOptions\n            if choices:\n                # N.B.we copy here the list to avoid persistent changes\n                choices = copy(choices)\n                # By default set empty as the default selected choice\n                choices.insert(0, dict(ResultValue=\"\", ResultText=\"\"))\n                item[\"choices\"][\"Result\"] = choices\n\n        if not result:\n            return\n\n        obj = self.get_object(analysis_brain)\n        formatted_result = obj.getFormattedResult(\n            sciformat=int(self.scinot), decimalmark=self.dmk)\n        item[\"formatted_result\"] = formatted_result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the analysis calculation and interims to the item passed in.", "response": "def _folder_item_calculation(self, analysis_brain, item):\n        \"\"\"Set the analysis' calculation and interims to the item passed in.\n\n        :param analysis_brain: Brain that represents an analysis\n        :param item: analysis' dictionary counterpart that represents a row\n        \"\"\"\n\n        is_editable = self.is_analysis_edition_allowed(analysis_brain)\n        # Set interim fields. Note we add the key 'formatted_value' to the list\n        # of interims the analysis has already assigned.\n        interim_fields = analysis_brain.getInterimFields or list()\n        for interim_field in interim_fields:\n            interim_keyword = interim_field.get('keyword', '')\n            if not interim_keyword:\n                continue\n            interim_value = interim_field.get('value', '')\n            interim_formatted = formatDecimalMark(interim_value, self.dmk)\n            interim_field['formatted_value'] = interim_formatted\n            item[interim_keyword] = interim_field\n            item['class'][interim_keyword] = 'interim'\n\n            # Note: As soon as we have a separate content type for field\n            #       analysis, we can solely rely on the field permission\n            #       \"senaite.core: Field: Edit Analysis Result\"\n            if is_editable:\n                if self.has_permission(FieldEditAnalysisResult, analysis_brain):\n                    item['allow_edit'].append(interim_keyword)\n\n            # Add this analysis' interim fields to the interim_columns list\n            interim_hidden = interim_field.get('hidden', False)\n            if not interim_hidden:\n                interim_title = interim_field.get('title')\n                self.interim_columns[interim_keyword] = interim_title\n\n        item['interimfields'] = interim_fields\n        self.interim_fields[analysis_brain.UID] = interim_fields\n\n        # Set calculation\n        calculation_uid = analysis_brain.getCalculationUID\n        has_calculation = calculation_uid and True or False\n        item['calculation'] = has_calculation"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _folder_item_method(self, analysis_brain, item):\n\n        is_editable = self.is_analysis_edition_allowed(analysis_brain)\n        method_title = analysis_brain.getMethodTitle\n        item['Method'] = method_title or ''\n        if is_editable:\n            method_vocabulary = self.get_methods_vocabulary(analysis_brain)\n            if method_vocabulary:\n                item['Method'] = analysis_brain.getMethodUID\n                item['choices']['Method'] = method_vocabulary\n                item['allow_edit'].append('Method')\n                self.show_methodinstr_columns = True\n        elif method_title:\n            item['replace']['Method'] = get_link(analysis_brain.getMethodURL,\n                                                 method_title)\n            self.show_methodinstr_columns = True", "response": "Fills the analysis method to the item passed in."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _folder_item_instrument(self, analysis_brain, item):\n        item['Instrument'] = ''\n        if not analysis_brain.getInstrumentEntryOfResults:\n            # Manual entry of results, instrument is not allowed\n            item['Instrument'] = _('Manual')\n            item['replace']['Instrument'] = \\\n                '<a href=\"#\">{}</a>'.format(t(_('Manual')))\n            return\n\n        # Instrument can be assigned to this analysis\n        is_editable = self.is_analysis_edition_allowed(analysis_brain)\n        self.show_methodinstr_columns = True\n        instrument = self.get_instrument(analysis_brain)\n        if is_editable:\n            # Edition allowed\n            voc = self.get_instruments_vocabulary(analysis_brain)\n            if voc:\n                # The service has at least one instrument available\n                item['Instrument'] = instrument.UID() if instrument else ''\n                item['choices']['Instrument'] = voc\n                item['allow_edit'].append('Instrument')\n                return\n\n        if instrument:\n            # Edition not allowed\n            instrument_title = instrument and instrument.Title() or ''\n            instrument_link = get_link(instrument.absolute_url(),\n                                       instrument_title)\n            item['Instrument'] = instrument_title\n            item['replace']['Instrument'] = instrument_link\n            return", "response": "Fills the analysis instrument to the item passed in."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _folder_item_uncertainty(self, analysis_brain, item):\n\n        item[\"Uncertainty\"] = \"\"\n\n        if not self.has_permission(ViewResults, analysis_brain):\n            return\n\n        result = analysis_brain.getResult\n\n        obj = self.get_object(analysis_brain)\n        formatted = format_uncertainty(obj, result, decimalmark=self.dmk,\n                                       sciformat=int(self.scinot))\n        if formatted:\n            item[\"Uncertainty\"] = formatted\n        else:\n            item[\"Uncertainty\"] = obj.getUncertainty(result)\n\n        if self.is_uncertainty_edition_allowed(analysis_brain):\n            item[\"allow_edit\"].append(\"Uncertainty\")", "response": "Fills the analysis uncertainty to the item passed in."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfills the analysis detection limits to the item passed in.", "response": "def _folder_item_detection_limits(self, analysis_brain, item):\n        \"\"\"Fills the analysis' detection limits to the item passed in.\n\n        :param analysis_brain: Brain that represents an analysis\n        :param item: analysis' dictionary counterpart that represents a row\n        \"\"\"\n        item[\"DetectionLimitOperand\"] = \"\"\n\n        if not self.is_analysis_edition_allowed(analysis_brain):\n            # Return immediately if the we are not in edit mode\n            return\n\n        # TODO: Performance, we wake-up the full object here\n        obj = self.get_object(analysis_brain)\n\n        # No Detection Limit Selection\n        if not obj.getDetectionLimitSelector():\n            return None\n\n        # Show Detection Limit Operand Selector\n        item[\"DetectionLimitOperand\"] = obj.getDetectionLimitOperand()\n        item[\"allow_edit\"].append(\"DetectionLimitOperand\")\n        self.columns[\"DetectionLimitOperand\"][\"toggle\"] = True\n\n        # Prepare selection list for LDL/UDL\n        choices = [\n            {\"ResultValue\": \"\", \"ResultText\": \"\"},\n            {\"ResultValue\": LDL, \"ResultText\": LDL},\n            {\"ResultValue\": UDL, \"ResultText\": UDL}\n        ]\n        # Set the choices to the item\n        item[\"choices\"][\"DetectionLimitOperand\"] = choices"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the results range to the item passed in", "response": "def _folder_item_specifications(self, analysis_brain, item):\n        \"\"\"Set the results range to the item passed in\"\"\"\n        # Everyone can see valid-ranges\n        item['Specification'] = ''\n        results_range = analysis_brain.getResultsRange\n        if not results_range:\n            return\n\n        # Display the specification interval\n        item[\"Specification\"] = get_formatted_interval(results_range, \"\")\n\n        # Show an icon if out of range\n        out_range, out_shoulders = is_out_of_range(analysis_brain)\n        if not out_range:\n            return\n        # At least is out of range\n        img = get_image(\"exclamation.png\", title=_(\"Result out of range\"))\n        if not out_shoulders:\n            img = get_image(\"warning.png\", title=_(\"Result in shoulder range\"))\n        self._append_html_element(item, \"Result\", img)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the analysis verification icons to the item passed in.", "response": "def _folder_item_verify_icons(self, analysis_brain, item):\n        \"\"\"Set the analysis' verification icons to the item passed in.\n\n        :param analysis_brain: Brain that represents an analysis\n        :param item: analysis' dictionary counterpart that represents a row\n        \"\"\"\n        submitter = analysis_brain.getSubmittedBy\n        if not submitter:\n            # This analysis hasn't yet been submitted, no verification yet\n            return\n\n        if analysis_brain.review_state == 'retracted':\n            # Don't display icons and additional info about verification\n            return\n\n        verifiers = analysis_brain.getVerificators\n        in_verifiers = submitter in verifiers\n        if in_verifiers:\n            # If analysis has been submitted and verified by the same person,\n            # display a warning icon\n            msg = t(_(\"Submitted and verified by the same user: {}\"))\n            icon = get_image('warning.png', title=msg.format(submitter))\n            self._append_html_element(item, 'state_title', icon)\n\n        num_verifications = analysis_brain.getNumberOfRequiredVerifications\n        if num_verifications > 1:\n            # More than one verification required, place an icon and display\n            # the number of verifications done vs. total required\n            done = analysis_brain.getNumberOfVerifications\n            pending = num_verifications - done\n            ratio = float(done) / float(num_verifications) if done > 0 else 0\n            ratio = int(ratio * 100)\n            scale = ratio == 0 and 0 or (ratio / 25) * 25\n            anchor = \"<a href='#' title='{} &#13;{} {}' \" \\\n                     \"class='multi-verification scale-{}'>{}/{}</a>\"\n            anchor = anchor.format(t(_(\"Multi-verification required\")),\n                                   str(pending),\n                                   t(_(\"verification(s) pending\")),\n                                   str(scale),\n                                   str(done),\n                                   str(num_verifications))\n            self._append_html_element(item, 'state_title', anchor)\n\n        if analysis_brain.review_state != 'to_be_verified':\n            # The verification of analysis has already been done or first\n            # verification has not been done yet. Nothing to do\n            return\n\n        # Check if the user has \"Bika: Verify\" privileges\n        if not self.has_permission(TransitionVerify):\n            # User cannot verify, do nothing\n            return\n\n        username = api.get_current_user().id\n        if username not in verifiers:\n            # Current user has not verified this analysis\n            if submitter != username:\n                # Current user is neither a submitter nor a verifier\n                return\n\n            # Current user is the same who submitted the result\n            if analysis_brain.isSelfVerificationEnabled:\n                # Same user who submitted can verify\n                title = t(_(\"Can verify, but submitted by current user\"))\n                html = get_image('warning.png', title=title)\n                self._append_html_element(item, 'state_title', html)\n                return\n\n            # User who submitted cannot verify\n            title = t(_(\"Cannot verify, submitted by current user\"))\n            html = get_image('submitted-by-current-user.png', title=title)\n            self._append_html_element(item, 'state_title', html)\n            return\n\n        # This user verified this analysis before\n        multi_verif = self.context.bika_setup.getTypeOfmultiVerification()\n        if multi_verif != 'self_multi_not_cons':\n            # Multi verification by same user is not allowed\n            title = t(_(\"Cannot verify, was verified by current user\"))\n            html = get_image('submitted-by-current-user.png', title=title)\n            self._append_html_element(item, 'state_title', html)\n            return\n\n        # Multi-verification by same user, but non-consecutively, is allowed\n        if analysis_brain.getLastVerificator != username:\n            # Current user was not the last user to verify\n            title = t(\n                _(\"Can verify, but was already verified by current user\"))\n            html = get_image('warning.png', title=title)\n            self._append_html_element(item, 'state_title', html)\n            return\n\n        # Last user who verified is the same as current user\n        title = t(_(\"Cannot verify, last verified by current user\"))\n        html = get_image('submitted-by-current-user.png', title=title)\n        self._append_html_element(item, 'state_title', html)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _folder_item_assigned_worksheet(self, analysis_brain, item):\n        if not IAnalysisRequest.providedBy(self.context):\n            # We want this icon to only appear if the context is an AR\n            return\n\n        analysis_obj = self.get_object(analysis_brain)\n        worksheet = analysis_obj.getWorksheet()\n        if not worksheet:\n            # No worksheet assigned. Do nothing\n            return\n\n        title = t(_(\"Assigned to: ${worksheet_id}\",\n                    mapping={'worksheet_id': safe_unicode(worksheet.id)}))\n        img = get_image('worksheet.png', title=title)\n        anchor = get_link(worksheet.absolute_url(), img)\n        self._append_html_element(item, 'state_title', anchor)", "response": "Adds an icon to the item dict if the analysis is assigned to a worksheet and if the icon is suitable for the current context"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _folder_item_reflex_icons(self, analysis_brain, item):\n        if not analysis_brain.getIsReflexAnalysis:\n            # Do nothing\n            return\n        img = get_image('reflexrule.png',\n                        title=t(_('It comes form a reflex rule')))\n        self._append_html_element(item, 'Service', img)", "response": "Adds an icon to the item dictionary if the analysis has been\n        automatically generated due to a reflex rule."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the visibility of the analysis in results report", "response": "def _folder_item_report_visibility(self, analysis_brain, item):\n        \"\"\"Set if the hidden field can be edited (enabled/disabled)\n\n        :analysis_brain: Brain that represents an analysis\n        :item: analysis' dictionary counterpart to be represented as a row\"\"\"\n        # Users that can Add Analyses to an Analysis Request must be able to\n        # set the visibility of the analysis in results report, also if the\n        # current state of the Analysis Request (e.g. verified) does not allow\n        # the edition of other fields. Note that an analyst has no privileges\n        # by default to edit this value, cause this \"visibility\" field is\n        # related with results reporting and/or visibility from the client\n        # side. This behavior only applies to routine analyses, the visibility\n        # of QC analyses is managed in publish and are not visible to clients.\n        if 'Hidden' not in self.columns:\n            return\n\n        full_obj = self.get_object(analysis_brain)\n        item['Hidden'] = full_obj.getHidden()\n        if self.has_permission(FieldEditAnalysisHidden, obj=full_obj):\n            item['allow_edit'].append('Hidden')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nresolve if field - specific icons must be displayed for the object passed in.", "response": "def _folder_item_fieldicons(self, analysis_brain):\n        \"\"\"Resolves if field-specific icons must be displayed for the object\n        passed in.\n\n        :param analysis_brain: Brain that represents an analysis\n        \"\"\"\n        full_obj = self.get_object(analysis_brain)\n        uid = api.get_uid(full_obj)\n        for name, adapter in getAdapters((full_obj,), IFieldIcons):\n            alerts = adapter()\n            if not alerts or uid not in alerts:\n                continue\n            alerts = alerts[uid]\n            if uid not in self.field_icons:\n                self.field_icons[uid] = alerts\n                continue\n            self.field_icons[uid].extend(alerts)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrenders the Remarks field for the passed in analysis.", "response": "def _folder_item_remarks(self, analysis_brain, item):\n        \"\"\"Renders the Remarks field for the passed in analysis\n\n        If the edition of the analysis is permitted, adds the field into the\n        list of editable fields.\n\n        :param analysis_brain: Brain that represents an analysis\n        :param item: analysis' dictionary counterpart that represents a row\n        \"\"\"\n\n        if self.analysis_remarks_enabled():\n            item[\"Remarks\"] = analysis_brain.getRemarks\n\n        if self.is_analysis_edition_allowed(analysis_brain):\n            item[\"allow_edit\"].extend([\"Remarks\"])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nappends an html value after or before the element in the item dict that represents an analysis row.", "response": "def _append_html_element(self, item, element, html, glue=\"&nbsp;\",\n                             after=True):\n        \"\"\"Appends an html value after or before the element in the item dict\n\n        :param item: dictionary that represents an analysis row\n        :param element: id of the element the html must be added thereafter\n        :param html: element to append\n        :param glue: glue to use for appending\n        :param after: if the html content must be added after or before\"\"\"\n        position = after and 'after' or 'before'\n        item[position] = item.get(position, {})\n        original = item[position].get(element, '')\n        if not original:\n            item[position][element] = html\n            return\n        item[position][element] = glue.join([original, html])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse the default Plone sortable_text index lower - case", "response": "def sortable_title(instance):\n    \"\"\"Uses the default Plone sortable_text index lower-case\n    \"\"\"\n    title = plone_sortable_title(instance)\n    if safe_callable(title):\n        title = title()\n    return title.lower()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sortable_sortkey_title(instance):\n    title = sortable_title(instance)\n    if safe_callable(title):\n        title = title()\n\n    sort_key = instance.getSortKey()\n    if sort_key is None:\n        sort_key = 999999\n\n    return \"{:010.3f}{}\".format(sort_key, title)", "response": "Returns a sortable title as a mxin of sortkey + lowercase sortable_title\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating hook for ARTemplateAnalysesView", "response": "def update(self):\n        \"\"\"Update hook\n        \"\"\"\n        super(ARTemplateAnalysesView, self).update()\n        self.allow_edit = self.is_edit_allowed()\n        self.configuration = self.get_configuration()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_settings(self):\n        settings = self.context.getAnalysisServicesSettings()\n        mapping = dict(map(lambda s: (s.get(\"uid\"), s), settings))\n        return mapping", "response": "Returns a mapping of UID - > setting"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a mapping of UID - > configuration", "response": "def get_configuration(self):\n        \"\"\"Returns a mapping of UID -> configuration\n        \"\"\"\n        mapping = {}\n        settings = self.get_settings()\n        for record in self.context.getAnalyses():\n            uid = record.get(\"service_uid\")\n            setting = settings.get(uid, {})\n            config = {\n                \"partition\": record.get(\"partition\"),\n                \"hidden\": setting.get(\"hidden\", False),\n            }\n            mapping[uid] = config\n        return mapping"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting the price with the set decimal mark and correct currency", "response": "def format_price(self, price):\n        \"\"\"Formats the price with the set decimal mark and correct currency\n        \"\"\"\n        return u\"{} {}{}{:02d}\".format(\n            self.get_currency_symbol(),\n            price[0],\n            self.get_decimal_mark(),\n            price[1],\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of items in the folder.", "response": "def folderitems(self):\n        \"\"\"TODO: Refactor to non-classic mode\n        \"\"\"\n        items = super(ARTemplateAnalysesView, self).folderitems()\n        self.categories.sort()\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nservicing triggered each time an item is iterated in folderitems. The use of this service prevents the extra-loops in child objects. :obj: the instance of the class to be foldered :item: dict containing the properties of the object to be used by the template :index: current index of the item", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Service triggered each time an item is iterated in folderitems.\n\n        The use of this service prevents the extra-loops in child objects.\n\n        :obj: the instance of the class to be foldered\n        :item: dict containing the properties of the object to be used by\n            the template\n        :index: current index of the item\n        \"\"\"\n        # ensure we have an object and not a brain\n        obj = api.get_object(obj)\n        uid = api.get_uid(obj)\n        url = api.get_url(obj)\n        title = api.get_title(obj)\n\n        # get the category\n        category = obj.getCategoryTitle()\n        item[\"category\"] = category\n        if category not in self.categories:\n            self.categories.append(category)\n\n        config = self.configuration.get(uid, {})\n        partition = config.get(\"partition\", \"part-1\")\n        hidden = config.get(\"hidden\", False)\n\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n        item[\"Price\"] = self.format_price(obj.Price)\n        item[\"allow_edit\"] = self.get_editable_columns()\n        item[\"required\"].append(\"Partition\")\n        item[\"choices\"][\"Partition\"] = self.partition_choices\n        item[\"Partition\"] = partition\n        item[\"Hidden\"] = hidden\n        item[\"selected\"] = uid in self.configuration\n\n        # Add methods\n        methods = obj.getMethods()\n        if methods:\n            links = map(\n                lambda m: get_link(\n                    m.absolute_url(), value=m.Title(), css_class=\"link\"),\n                methods)\n            item[\"replace\"][\"Methods\"] = \", \".join(links)\n        else:\n            item[\"methods\"] = \"\"\n\n        # Icons\n        after_icons = \"\"\n        if obj.getAccredited():\n            after_icons += get_image(\n                \"accredited.png\", title=t(_(\"Accredited\")))\n        if obj.getAttachmentOption() == \"r\":\n            after_icons += get_image(\n                \"attach_reqd.png\", title=t(_(\"Attachment required\")))\n        if obj.getAttachmentOption() == \"n\":\n            after_icons += get_image(\n                \"attach_no.png\", title=t(_('Attachment not permitted')))\n        if after_icons:\n            item[\"after\"][\"Title\"] = after_icons\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing the form and return a list of dictionaries fit for ARTemplate / Analyses field consumption.", "response": "def process_form(self, instance, field, form, empty_marker=None,\n                     emptyReturnsMarker=False):\n        \"\"\"Return a list of dictionaries fit for ARTemplate/Analyses field\n           consumption.\n        \"\"\"\n        value = []\n\n        # selected services\n        service_uids = form.get(\"uids\", [])\n        # defined partitions\n        partitions = form.get(\"Partition\", [])\n        partitions = partitions and partitions[0] or {}\n        # hidden services\n        hidden_services = form.get(\"Hidden\", {})\n\n        # get the service objects\n        services = map(api.get_object_by_uid, service_uids)\n        # get dependencies\n        dependencies = map(lambda s: s.getServiceDependencies(), services)\n        dependencies = list(itertools.chain.from_iterable(dependencies))\n        # Merge dependencies and services\n        services = set(services + dependencies)\n\n        # get the profile\n        profile_uid = form.get(\"AnalysisProfile_uid\")\n        if profile_uid:\n            profile = api.get_object_by_uid(profile_uid)\n            # update the services with those from the profile\n            services.update(profile.getService())\n\n        as_settings = []\n        for service in services:\n            service_uid = api.get_uid(service)\n            value.append({\n                \"service_uid\": service_uid,\n                \"partition\": partitions.get(service_uid, \"part-1\")\n            })\n\n            hidden = hidden_services.get(service_uid, \"\") == \"on\"\n            as_settings.append({\"uid\": service_uid, \"hidden\": hidden})\n\n        # set the analysis services settings\n        instance.setAnalysisServicesSettings(as_settings)\n\n        # This returns the value for the Analyses Schema Field\n        return value, {}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(self):\n\n        wb = load_workbook(self.getInputFile())\n        sheet = wb.worksheets[0]\n\n        sample_id = None\n        cnt = 0\n        for row in sheet.rows:\n\n            # sampleid is only present in first row of each sample.\n            # any rows above the first sample id are ignored.\n            if row[1].value:\n                sample_id = row[1].value\n            if not sample_id:\n                continue\n\n            # If no keyword is present, this row is skipped.\n            if len(row) < 7 or not isinstance(row[6].value, basestring):\n                continue\n            # keyword is stripped of non-word characters\n            keyword = re.sub(r\"\\W\", \"\", row[6].value)\n\n            # result is floatable or error\n            result = row[4].value\n            try:\n                float(result)\n            except ValueError:\n                self.log(\n                    \"Error in sample '\" + sample_id + \"': \" + \"Result for '\" +\n                    keyword + \"' is not a number \" + \"(\" + result + \").\")\n                continue\n\n            # Compose dict for importer.  No interim values, just a result.\n            rawdict = {\n                'DefaultResult': 'Result',\n                'Result': result,\n            }\n            result = rawdict[rawdict['DefaultResult']]\n            column_name = rawdict['DefaultResult']\n            cnt += 1\n            result = self.get_result(column_name, result, cnt)\n            rawdict[rawdict['DefaultResult']] = result\n            self._addRawResult(sample_id,\n                               values={keyword: rawdict},\n                               override=False)\n        return True", "response": "Parse the data from the file and add the results to the internal list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef before_render(self):\n        super(BatchFolderContentsView, self).before_render()\n\n        if self.context.portal_type == \"BatchFolder\":\n            self.request.set(\"disable_border\", 1)", "response": "Before template render hook\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall before the listing renders", "response": "def update(self):\n        \"\"\"Called before the listing renders\n        \"\"\"\n        super(BatchFolderContentsView, self).update()\n\n        if self.on_batch_folder() and self.can_add_batches():\n            self.context_actions[_(\"Add\")] = {\n                \"url\": \"createObject?type_name=Batch\",\n                \"permission\": \"Add portal content\",\n                \"icon\": \"++resource++bika.lims.images/add.png\"}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef folderitem(self, obj, item, index):\n\n        obj = api.get_object(obj)\n        url = \"{}/analysisrequests\".format(api.get_url(obj))\n        bid = api.get_id(obj)\n        cbid = obj.getClientBatchID()\n        title = api.get_title(obj)\n        client = obj.getClient()\n        created = api.get_creation_date(obj)\n        date = obj.getBatchDate()\n\n        item[\"BatchID\"] = bid\n        item[\"ClientBatchID\"] = cbid\n        item[\"replace\"][\"BatchID\"] = get_link(url, bid)\n        item[\"Title\"] = title\n        item[\"replace\"][\"Title\"] = get_link(url, title)\n        item[\"created\"] = self.ulocalized_time(created, long_format=True)\n        item[\"BatchDate\"] = self.ulocalized_time(date, long_format=True)\n\n        if client:\n            client_url = api.get_url(client)\n            client_name = client.getName()\n            client_id = client.getClientID()\n            item[\"Client\"] = client_name\n            item[\"ClientID\"] = client_id\n            item[\"replace\"][\"Client\"] = get_link(client_url, client_name)\n            item[\"replace\"][\"ClientID\"] = get_link(client_url, client_id)\n\n        return item", "response": "Applies new properties to the item that is currently being\n        rendered as a row in the list\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sorted_analyses(self, analyses):\n        analyses = sorted(analyses, key=lambda an: an.getRequestID())\n\n        def sorted_by_sortkey(objs):\n            return sorted(objs, key=lambda an: an.getPrioritySortkey())\n\n        # Now, we need the analyses within a request ID to be sorted by\n        # sortkey (sortable_title index), so it will appear in the same\n        # order as they appear in Analyses list from AR view\n        current_sample_id = None\n        current_analyses = []\n        sorted_analyses = []\n        for analysis in analyses:\n            sample_id = analysis.getRequestID()\n            if sample_id and current_sample_id != sample_id:\n                # Sort the brains we've collected until now, that\n                # belong to the same Analysis Request\n                current_analyses = sorted_by_sortkey(current_analyses)\n                sorted_analyses.extend(current_analyses)\n                current_sample_id = sample_id\n                current_analyses = []\n\n            # Now we are inside the same AR\n            current_analyses.append(analysis)\n            continue\n\n        # Sort the last set of brains we've collected\n        current_analyses = sorted_by_sortkey(current_analyses)\n        sorted_analyses.extend(current_analyses)\n        return sorted_analyses", "response": "Sort the analyses by AR ID ascending and subsorted by priority\n        sortkey within the AR"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the current list of maintenance types", "response": "def getMaintenanceTypes(self):\n        \"\"\" Return the current list of maintenance types\n        \"\"\"\n        types = [('Preventive',safe_unicode(_('Preventive')).encode('utf-8')),\n                 ('Repair', safe_unicode(_('Repair')).encode('utf-8')),\n                 ('Enhancement', safe_unicode(_('Enhancement')).encode('utf-8'))]\n        return DisplayList(types)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove the duplicate from the system", "response": "def after_unassign(duplicate_analysis):\n    \"\"\"Removes the duplicate from the system\n    \"\"\"\n    analysis_events.after_unassign(duplicate_analysis)\n    parent = duplicate_analysis.aq_parent\n    logger.info(\"Removing duplicate '{}' from '{}'\"\n                .format(duplicate_analysis.getId(), parent.getId()))\n    parent.manage_delObjects([duplicate_analysis.getId()])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef after_retract(duplicate_analysis):\n    # Rename the analysis to make way for it's successor.\n    # Support multiple retractions by renaming to *-0, *-1, etc\n    parent = duplicate_analysis.aq_parent\n    keyword = duplicate_analysis.getKeyword()\n    analyses = filter(lambda an: an.getKeyword() == keyword,\n                      parent.objectValues(\"DuplicateAnalysis\"))\n\n    # Rename the retracted duplicate\n    # https://docs.plone.org/develop/plone/content/rename.html\n    # _verifyObjectPaste permission check must be cancelled\n    parent._verifyObjectPaste = str\n    retracted_id = '{}-{}'.format(keyword, len(analyses))\n    # Make sure all persistent objects have _p_jar attribute\n    transaction.savepoint(optimistic=True)\n    parent.manage_renameObject(duplicate_analysis.getId(), retracted_id)\n    delattr(parent, '_verifyObjectPaste')\n\n    # Find out the slot position of the duplicate in the worksheet\n    worksheet = duplicate_analysis.getWorksheet()\n    if not worksheet:\n        logger.warn(\"Duplicate {} has been retracted, but without worksheet\"\n                    .format(duplicate_analysis.getId()))\n        return\n\n    dest_slot = worksheet.get_slot_position_for(duplicate_analysis)\n    if not dest_slot:\n        logger.warn(\"Duplicate {} has been retracted, but not found in any\"\n                    \"slot of worksheet {}\"\n                    .format(duplicate_analysis.getId(), worksheet.getId()))\n        return\n\n    # Create a copy (retest) of the duplicate and assign to worksheet\n    ref_gid = duplicate_analysis.getReferenceAnalysesGroupID()\n    retest = _createObjectByType(\"DuplicateAnalysis\", worksheet, tmpID())\n    copy_analysis_field_values(duplicate_analysis, retest)\n    retest.setAnalysis(duplicate_analysis.getAnalysis())\n    retest.setRetestOf(duplicate_analysis)\n    retest.setReferenceAnalysesGroupID(ref_gid)\n    retest.setResult(duplicate_analysis.getResult())\n    worksheet.addToLayout(retest, dest_slot)\n    worksheet.setAnalyses(worksheet.getAnalyses() + [retest, ])\n\n    # Reindex\n    retest.reindexObject(idxs=[\"getAnalyst\", \"getWorksheetUID\", \"isRetest\",\n                               \"getReferenceAnalysesGroupID\"])\n    worksheet.reindexObject(idxs=[\"getAnalysesUIDs\"])\n\n    # Try to rollback the worksheet to prevent inconsistencies\n    doActionFor(worksheet, \"rollback_to_open\")", "response": "Function triggered after a retract transition for the duplicate passed in is performed. The duplicate transitions to retracted state and a new\n    is returned."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a single line of header lines into a list of tuples.", "response": "def parse_headerline(self, line):\n        \"\"\" Parses header lines\n\n            Header example:\n            Batch Info,2013-03-20T07:11:09.9053262-07:00,2013-03-20T07:12:55.5280967-07:00,2013-03-20T07:11:07.1047817-07:00,,,,,,,,,,,,,,\n            Batch Data Path,D:\\MassHunter\\Data\\130129\\QuantResults\\130129LS.batch.bin,,,,,,,,,,,,,,,,\n            Analysis Time,3/20/2013 7:11 AM,Analyst Name,Administrator,,,,,,,,,,,,,,\n            Report Time,3/20/2013 7:12 AM,Reporter Name,Administrator,,,,,,,,,,,,,,\n            Last Calib Update,3/20/2013 7:11 AM,Batch State,Processed,,,,,,,,,,,,,,\n            ,,,,,,,,,,,,,,,,,\n        \"\"\"\n        if self._end_header == True:\n            # Header already processed\n            return 0\n\n        if line.startswith(self.SEQUENCETABLE_KEY):\n            self._end_header = True\n            if len(self._header) == 0:\n                self.err(\"No header found\", numline=self._numline)\n                return -1\n            return 0\n\n        splitted = [token.strip() for token in line.split(',')]\n\n        # Batch Info,2013-03-20T07:11:09.9053262-07:00,2013-03-20T07:12:55.5280967-07:00,2013-03-20T07:11:07.1047817-07:00,,,,,,,,,,,,,,\n        if splitted[0] == self.HEADERKEY_BATCHINFO:\n            if self.HEADERKEY_BATCHINFO in self._header:\n                self.warn(\"Header Batch Info already found. Discarding\",\n                          numline=self._numline, line=line)\n                return 0\n\n            self._header[self.HEADERKEY_BATCHINFO] = []\n            for i in range(len(splitted) - 1):\n                if splitted[i + 1]:\n                    self._header[self.HEADERKEY_BATCHINFO].append(splitted[i + 1])\n\n        # Batch Data Path,D:\\MassHunter\\Data\\130129\\QuantResults\\130129LS.batch.bin,,,,,,,,,,,,,,,,\n        elif splitted[0] == self.HEADERKEY_BATCHDATAPATH:\n            if self.HEADERKEY_BATCHDATAPATH in self._header:\n                self.warn(\"Header Batch Data Path already found. Discarding\",\n                          numline=self._numline, line=line)\n                return 0;\n\n            if splitted[1]:\n                self._header[self.HEADERKEY_BATCHDATAPATH] = splitted[1]\n            else:\n                self.warn(\"Batch Data Path not found or empty\",\n                          numline=self._numline, line=line)\n\n        # Analysis Time,3/20/2013 7:11 AM,Analyst Name,Administrator,,,,,,,,,,,,,,\n        elif splitted[0] == self.HEADERKEY_ANALYSISTIME:\n            if splitted[1]:\n                try:\n                    d = datetime.strptime(splitted[1], \"%m/%d/%Y %I:%M %p\")\n                    self._header[self.HEADERKEY_ANALYSISTIME] = d\n                except ValueError:\n                    self.err(\"Invalid Analysis Time format\",\n                             numline=self._numline, line=line)\n            else:\n                self.warn(\"Analysis Time not found or empty\",\n                          numline=self._numline, line=line)\n\n            if splitted[2] and splitted[2] == self.HEADERKEY_ANALYSTNAME:\n                if splitted[3]:\n                    self._header[self.HEADERKEY_ANALYSTNAME] = splitted[3]\n                else:\n                    self.warn(\"Analyst Name not found or empty\",\n                              numline=self._numline, line=line)\n            else:\n                self.err(\"Analyst Name not found\",\n                         numline=self._numline, line=line)\n\n        # Report Time,3/20/2013 7:12 AM,Reporter Name,Administrator,,,,,,,,,,,,,,\n        elif splitted[0] == self.HEADERKEY_REPORTTIME:\n            if splitted[1]:\n                try:\n                    d = datetime.strptime(splitted[1], \"%m/%d/%Y %I:%M %p\")\n                    self._header[self.HEADERKEY_REPORTTIME] = d\n                except ValueError:\n                    self.err(\"Invalid Report Time format\",\n                         numline=self._numline, line=line)\n            else:\n                self.warn(\"Report time not found or empty\",\n                          numline=self._numline, line=line)\n\n\n            if splitted[2] and splitted[2] == self.HEADERKEY_REPORTERNAME:\n                if splitted[3]:\n                    self._header[self.HEADERKEY_REPORTERNAME] = splitted[3]\n                else:\n                    self.warn(\"Reporter Name not found or empty\",\n                              numline=self._numline, line=line)\n\n            else:\n                self.err(\"Reporter Name not found\",\n                         numline=self._numline, line=line)\n\n\n        # Last Calib Update,3/20/2013 7:11 AM,Batch State,Processed,,,,,,,,,,,,,,\n        elif splitted[0] == self.HEADERKEY_LASTCALIBRATION:\n            if splitted[1]:\n                try:\n                    d = datetime.strptime(splitted[1], \"%m/%d/%Y %I:%M %p\")\n                    self._header[self.HEADERKEY_LASTCALIBRATION] = d\n                except ValueError:\n                    self.err(\"Invalid Last Calibration time format\",\n                             numline=self._numline, line=line)\n\n            else:\n                self.warn(\"Last Calibration time not found or empty\",\n                          numline=self._numline, line=line)\n\n\n            if splitted[2] and splitted[2] == self.HEADERKEY_BATCHSTATE:\n                if splitted[3]:\n                    self._header[self.HEADERKEY_BATCHSTATE] = splitted[3]\n                else:\n                    self.warn(\"Batch state not found or empty\",\n                              numline=self._numline, line=line)\n\n            else:\n                self.err(\"Batch state not found\",\n                         numline=self._numline, line=line)\n\n\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_sequencetableline(self, line):\n\n        # Sequence Table,,,,,,,,,,,,,,,,,\n        # prerunrespchk.d,prerunrespchk,Vial 3,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,\n        # mid_respchk.d,mid_respchk,Vial 3,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,\n        # ,,,,,,,,,,,,,,,,,\n        if line.startswith(self.SEQUENCETABLE_KEY) \\\n            or line.startswith(self.SEQUENCETABLE_PRERUN) \\\n            or line.startswith(self.SEQUENCETABLE_MIDRUN) \\\n            or self._end_sequencetable == True:\n\n            # Nothing to do, continue\n            return 0\n\n        # Data File,Sample Name,Position,Inj Vol,Level,Sample Type,Acq Method File,,,,,,,,,,,\n        if line.startswith(self.SEQUENCETABLE_HEADER_DATAFILE):\n            self._sequencesheader = [token.strip() for token in line.split(',') if token.strip()]\n            return 0\n\n        # post_respchk.d,post_respchk,Vial 3,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,\n        # Quantitation Results,,,,,,,,,,,,,,,,,\n        if line.startswith(self.SEQUENCETABLE_POSTRUN) \\\n            or line.startswith(self.QUANTITATIONRESULTS_KEY) \\\n            or line.startswith(self.COMMAS):\n            self._end_sequencetable = True\n            if len(self._sequences) == 0:\n                self.err(\"No Sequence Table found\", linenum=self._numline)\n                return -1\n\n            # Jumps 2 lines:\n            # Data File,Sample Name,Position,Inj Vol,Level,Sample Type,Acq Method File,,,,,,,,,,,\n            # prerunrespchk.d,prerunrespchk,Vial 3,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,\n            return 2\n\n        # DSS_Nist_L1.d,DSS_Nist_L1,P1-A2,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,\n        splitted = [token.strip() for token in line.split(',')]\n        sequence = {}\n        for colname in self._sequencesheader:\n            sequence[colname] = ''\n\n        for i in range(len(splitted)):\n            token = splitted[i]\n            if i < len(self._sequencesheader):\n                colname = self._sequencesheader[i]\n                if token and colname in self.SEQUENCETABLE_NUMERICHEADERS:\n                    try:\n                        sequence[colname] = float(token)\n                    except ValueError:\n                        self.warn(\n                            \"No valid number ${token} in column ${index} (${column_name})\",\n                            mapping={\"token\": token,\n                                     \"index\": str(i + 1),\n                                     \"column_name\": colname},\n                            numline=self._numline, line=line)\n                        sequence[colname] = token\n                else:\n                    sequence[colname] = token\n            elif token:\n                self.err(\"Orphan value in column ${index} (${token})\",\n                         mapping={\"index\": str(i+1),\n                                  \"token\": token},\n                         numline=self._numline, line=line)\n        self._sequences.append(sequence)", "response": "Parses the sequence table lines and returns a list of tuples."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing quantitationesults lines from a DSS Nist line.", "response": "def parse_quantitationesultsline(self, line):\n        \"\"\" Parses quantitation result lines\n\n            Quantitation results example:\n            Quantitation Results,,,,,,,,,,,,,,,,,\n            Target Compound,25-OH D3+PTAD+MA,,,,,,,,,,,,,,,,\n            Data File,Compound,ISTD,Resp,ISTD Resp,Resp Ratio, Final Conc,Exp Conc,Accuracy,,,,,,,,,\n            prerunrespchk.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,5816,274638,0.0212,0.9145,,,,,,,,,,,\n            DSS_Nist_L1.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,6103,139562,0.0437,1.6912,,,,,,,,,,,\n            DSS_Nist_L2.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,11339,135726,0.0835,3.0510,,,,,,,,,,,\n            DSS_Nist_L3.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,15871,141710,0.1120,4.0144,,,,,,,,,,,\n            mid_respchk.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,4699,242798,0.0194,0.8514,,,,,,,,,,,\n            DSS_Nist_L3-r002.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,15659,129490,0.1209,4.3157,,,,,,,,,,,\n            UTAK_DS_L1-r001.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,29846,132264,0.2257,7.7965,,,,,,,,,,,\n            UTAK_DS_L1-r002.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,28696,141614,0.2026,7.0387,,,,,,,,,,,\n            post_respchk.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,5022,231748,0.0217,0.9315,,,,,,,,,,,\n            ,,,,,,,,,,,,,,,,,\n            Target Compound,25-OH D2+PTAD+MA,,,,,,,,,,,,,,,,\n            Data File,Compound,ISTD,Resp,ISTD Resp,Resp Ratio, Final Conc,Exp Conc,Accuracy,,,,,,,,,\n            prerunrespchk.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,6222,274638,0.0227,0.8835,,,,,,,,,,,\n            DSS_Nist_L1.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,1252,139562,0.0090,0.7909,,,,,,,,,,,\n            DSS_Nist_L2.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,3937,135726,0.0290,0.9265,,,,,,,,,,,\n            DSS_Nist_L3.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,826,141710,0.0058,0.7697,,,,,,,,,,,\n            mid_respchk.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,7864,242798,0.0324,0.9493,,,,,,,,,,,\n            DSS_Nist_L3-r002.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,853,129490,0.0066,0.7748,,,,,,,,,,,\n            UTAK_DS_L1-r001.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,127496,132264,0.9639,7.1558,,,,,,,,,,,\n            UTAK_DS_L1-r002.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,135738,141614,0.9585,7.1201,,,,,,,,,,,\n            post_respchk.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,6567,231748,0.0283,0.9219,,,,,,,,,,,\n            ,,,,,,,,,,,,,,,,,\n        \"\"\"\n\n        # Quantitation Results,,,,,,,,,,,,,,,,,\n        # prerunrespchk.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,5816,274638,0.0212,0.9145,,,,,,,,,,,\n        # mid_respchk.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,4699,242798,0.0194,0.8514,,,,,,,,,,,\n        # post_respchk.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,6567,231748,0.0283,0.9219,,,,,,,,,,,\n        # ,,,,,,,,,,,,,,,,,\n        if line.startswith(self.QUANTITATIONRESULTS_KEY) \\\n            or line.startswith(self.QUANTITATIONRESULTS_PRERUN) \\\n            or line.startswith(self.QUANTITATIONRESULTS_MIDRUN) \\\n            or line.startswith(self.QUANTITATIONRESULTS_POSTRUN) \\\n            or line.startswith(self.COMMAS):\n\n            # Nothing to do, continue\n            return 0\n\n        # Data File,Compound,ISTD,Resp,ISTD Resp,Resp Ratio, Final Conc,Exp Conc,Accuracy,,,,,,,,,\n        if line.startswith(self.QUANTITATIONRESULTS_HEADER_DATAFILE):\n            self._quantitationresultsheader = [token.strip() for token in line.split(',') if token.strip()]\n            return 0\n\n        # Target Compound,25-OH D3+PTAD+MA,,,,,,,,,,,,,,,,\n        if line.startswith(self.QUANTITATIONRESULTS_TARGETCOMPOUND):\n            # New set of Quantitation Results\n            splitted = [token.strip() for token in line.split(',')]\n            if not splitted[1]:\n                self.warn(\"No Target Compound found\",\n                          numline=self._numline, line=line)\n            return 0\n\n        # DSS_Nist_L1.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,1252,139562,0.0090,0.7909,,,,,,,,,,,\n        splitted = [token.strip() for token in line.split(',')]\n        quantitation = {}\n        for colname in self._quantitationresultsheader:\n            quantitation[colname] = ''\n\n        for i in range(len(splitted)):\n            token = splitted[i]\n            if i < len(self._quantitationresultsheader):\n                colname = self._quantitationresultsheader[i]\n                if token and colname in self.QUANTITATIONRESULTS_NUMERICHEADERS:\n                    try:\n                        quantitation[colname] = float(token)\n                    except ValueError:\n                        self.warn(\n                            \"No valid number ${token} in column ${index} (${column_name})\",\n                            mapping={\"token\": token,\n                                     \"index\": str(i + 1),\n                                     \"column_name\": colname},\n                            numline=self._numline, line=line)\n                        quantitation[colname] = token\n                else:\n                    quantitation[colname] = token\n            elif token:\n                self.err(\"Orphan value in column ${index} (${token})\",\n                         mapping={\"index\": str(i+1),\n                                  \"token\": token},\n                         numline=self._numline, line=line)\n\n        if self.QUANTITATIONRESULTS_COMPOUNDCOLUMN in quantitation:\n            compound = quantitation[self.QUANTITATIONRESULTS_COMPOUNDCOLUMN]\n\n            # Look for sequence matches and populate rawdata\n            datafile = quantitation.get(self.QUANTITATIONRESULTS_HEADER_DATAFILE, '')\n            if not datafile:\n                self.err(\"No Data File found for quantitation result\",\n                         numline=self._numline, line=line)\n\n            else:\n                seqs = [sequence for sequence in self._sequences \\\n                        if sequence.get('Data File', '') == datafile]\n                if len(seqs) == 0:\n                    self.err(\"No sample found for quantitative result ${data_file}\",\n                             mapping={\"data_file\": datafile},\n                             numline=self._numline, line=line)\n                elif len(seqs) > 1:\n                    self.err(\"More than one sequence found for quantitative result: ${data_file}\",\n                             mapping={\"data_file\": datafile},\n                             numline=self._numline, line=line)\n                else:\n                    objid = seqs[0].get(self.SEQUENCETABLE_HEADER_SAMPLENAME, '')\n                    if objid:\n                        quantitation['DefaultResult'] = 'Final Conc'\n                        quantitation['Remarks'] = _(\"Autoimport\")\n                        rows = self.getRawResults().get(objid, [])\n                        raw = rows[0] if len(rows) > 0 else {}\n                        raw[compound] = quantitation\n                        self._addRawResult(objid, raw, True)\n                    else:\n                        self.err(\"No valid sequence for ${data_file}\",\n                                 mapping={\"data_file\": datafile},\n                                 numline=self._numline, line=line)\n        else:\n            self.err(\"Value for column '${column}' not found\",\n                     mapping={\"column\": self.QUANTITATIONRESULTS_COMPOUNDCOLUMN},\n                     numline=self._numline, line=line)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render_template_partitions(self):\n        Analyses = self.context.Schema()['Analyses'].get(self.context)\n        Parts = self.context.Schema()['Partitions'].get(self.context)\n        if not Parts:\n            # default value copied in from content/artemplate.py\n            Parts = [{'part_id': 'part-1',\n                      'Container': '',\n                      'Preservation': '',\n                      'container_uid': '',\n                      'preservation_uid': ''}]\n        parts = []\n        not_found = set()\n        for Part in Parts:\n            part = {\n                'part_id': Part.get(\"part_id\", \"part-1\"),\n                'container_titles': Part.get(\"Container\", \"\"),\n                'container': Part.get(\"container_uid\", \"\"),\n                'preservation_titles': Part.get(\"Preservation\", \"\"),\n                'preservation': Part.get(\"preservation_uid\", \"\"),\n                'services': [],\n                'service_titles': [],\n            }\n            for analysis in Analyses:\n                uid = analysis['service_uid']\n                partiton = analysis['partition']\n                if partiton == part['part_id']:\n                    part['services'].append(uid)\n                    part['service_titles'].append(uid)\n                    not_found.discard(analysis['service_uid'])\n                else:\n                    if uid in part['services']:\n                        part['services'].remove(uid)\n                    if uid in part['service_titles']:\n                        part['service_titles'].remove(uid)\n                    not_found.add(analysis['service_uid'])\n\n            parts.append(part)\n\n        # all others go into the first part.  Mostly this will be due to\n        # partition info not being defined?\n        for uid in not_found:\n            if uid not in part['services']:\n                parts[0]['services'].append(uid)\n            if uid not in part['service_titles']:\n               parts[0]['service_titles'].append(uid)\n\n        return parts", "response": "This method renders the Partitions for this AR Template. It returns a list of dictionaries that contains the contents of the Partitions for the AR Template."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_form(self, instance, field, form, empty_marker=None,\n                     emptyReturnsMarker=False):\n        \"\"\"A typed in value takes precedence over a selected value.\n        \"\"\"\n\n        name = field.getName()\n        otherName = \"%s_other\" % name\n        value = form.get(otherName, empty_marker)\n        regex = field.widget.field_regex\n\n        # validate the custom value against the given regex\n        if value and not re.match(regex, value):\n            value = None\n        # If value is an empty string we check if the selection box\n        # has a usable value.\n        if value is empty_marker or not value:\n            value = form.get(name, empty_marker)\n        if value is empty_marker:\n            return empty_marker\n        if not value and emptyReturnsMarker:\n            return empty_marker\n        return value, {}", "response": "A typed in value takes precedence over a selected value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef searchResults(self, REQUEST=None, used=None, **kw):\n    if REQUEST and REQUEST.get('getRequestUID') \\\n            and self.id == CATALOG_ANALYSIS_LISTING:\n\n        # Fetch all analyses that have the request UID passed in as an ancestor,\n        # cause we want Primary ARs to always display the analyses from their\n        # derived ARs (if result is not empty)\n\n        request = REQUEST.copy()\n        orig_uid = request.get('getRequestUID')\n\n        # If a list of request uid, retrieve them sequentially to make the\n        # masking process easier\n        if isinstance(orig_uid, list):\n            results = list()\n            for uid in orig_uid:\n                request['getRequestUID'] = [uid]\n                results += self.searchResults(REQUEST=request, used=used, **kw)\n            return results\n\n        # Get all analyses, those from descendant ARs included\n        del request['getRequestUID']\n        request['getAncestorsUIDs'] = orig_uid\n        results = self.searchResults(REQUEST=request, used=used, **kw)\n\n        # Masking\n        primary = filter(lambda an: an.getParentUID == orig_uid, results)\n        derived = filter(lambda an: an.getParentUID != orig_uid, results)\n        derived_keys = map(lambda an: an.getKeyword, derived)\n        results = filter(lambda an: an.getKeyword not in derived_keys, primary)\n        return results + derived\n\n    # Normal search\n    return self._catalog.searchResults(REQUEST, used, **kw)", "response": "Search the catalog for the available analyses."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_AnalysisRequest(self, instance):\n        # - If AR is sample_due: receive it before proceeding.\n        wf = getToolByName(self.context, 'portal_workflow')\n        if wf.getInfoFor(instance, 'review_state') == 'sample_due':\n            try:\n                wf.doActionFor(instance, 'receive')\n            except WorkflowException:\n                pass\n        # - If AR belongs to Batch, redirect to the BatchBook view.\n        batch = instance.getBatch()\n        if batch:\n            return batch.absolute_url() + \"/batchbook\"\n        #  - if permission/workflow permit: go to AR manage_results.\n        mtool = getToolByName(self.context, 'portal_membership')\n        if mtool.checkPermission(EditResults, instance):\n            return instance.absolute_url() + '/manage_results'\n        # - For other ARs, just redirect to the view screen.\n        return instance.absolute_url()", "response": "This method handles the analysis request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_Sample(self, instance):\n        ars = instance.getAnalysisRequests()\n        if len(ars) == 1:\n            return self.handle_AnalysisRequest(ars[0])\n        else:\n            return instance.absolute_url()", "response": "If this sample has a single AR, go there.\n        If the sample has 0 or >1 ARs, go to the sample's view URL."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_sample_type_default_stickers(portal):\n    # Getting all sticker templates\n    stickers = getStickerTemplates()\n    sticker_ids = []\n    for sticker in stickers:\n        sticker_ids.append(sticker.get('id'))\n    def_small_template = portal.bika_setup.getSmallStickerTemplate()\n    def_large_template = portal.bika_setup.getLargeStickerTemplate()\n    # Getting all Sample Type objects\n    catalog = api.get_tool('bika_setup_catalog')\n    brains = catalog(portal_type='SampleType')\n    for brain in brains:\n        obj = api.get_object(brain)\n        if obj.getAdmittedStickers() is not None:\n            continue\n        obj.setAdmittedStickers(sticker_ids)\n        obj.setDefaultLargeSticker(def_large_template)\n        obj.setDefaultSmallSticker(def_small_template)", "response": "Sets the admitted stickers and their default stickers to every sample\n    type"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, instance, **kwargs):\n        catalog = getToolByName(instance, CATALOG_ANALYSIS_LISTING)\n        query = dict(\n            [(k, v) for k, v in kwargs.items() if k in catalog.indexes()])\n        query[\"portal_type\"] = \"Analysis\"\n        query[\"getRequestUID\"] = api.get_uid(instance)\n        analyses = catalog(query)\n        if not kwargs.get(\"full_objects\", False):\n            return analyses\n\n        return map(api.get_object, analyses)", "response": "Returns a list of Analyses assigned to this AR\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the items of an AR to the given items.", "response": "def set(self, instance, items, prices=None, specs=None, hidden=None, **kw):\n        \"\"\"Set/Assign Analyses to this AR\n\n        :param items: List of Analysis objects/brains, AnalysisService\n                      objects/brains and/or Analysis Service uids\n        :type items: list\n        :param prices: Mapping of AnalysisService UID -> price\n        :type prices: dict\n        :param specs: List of AnalysisService UID -> Result Range mappings\n        :type specs: list\n        :param hidden: List of AnalysisService UID -> Hidden mappings\n        :type hidden: list\n        :returns: list of new assigned Analyses\n        \"\"\"\n        # This setter returns a list of new set Analyses\n        new_analyses = []\n\n        # Current assigned analyses\n        analyses = instance.objectValues(\"Analysis\")\n\n        # Analyses which are in a non-open state must be retained, except those\n        # that are in a registered state (the sample has not been received)\n        non_open_analyses = filter(lambda an: not an.isOpen(), analyses)\n        non_open_analyses = filter(\n            lambda an: api.get_workflow_status_of(an) != \"registered\",\n            non_open_analyses)\n\n        # Prevent removing all analyses\n        #\n        # N.B.: Non-open analyses are rendered disabled in the HTML form.\n        #       Therefore, their UIDs are not included in the submitted UIDs.\n        if not items and not non_open_analyses:\n            logger.warn(\"Not allowed to remove all Analyses from AR.\")\n            return new_analyses\n\n        # Bail out if the items is not a list type\n        if not isinstance(items, (list, tuple)):\n            raise TypeError(\n                \"Items parameter must be a tuple or list, got '{}'\".format(\n                    type(items)))\n\n        # Bail out if the AR is inactive\n        if not api.is_active(instance):\n            raise Unauthorized(\"Inactive ARs can not be modified\"\n                               .format(AddAnalysis))\n\n        # Bail out if the user has not the right permission\n        if not check_permission(AddAnalysis, instance):\n            raise Unauthorized(\"You do not have the '{}' permission\"\n                               .format(AddAnalysis))\n\n        # Convert the items to a valid list of AnalysisServices\n        services = filter(None, map(self._to_service, items))\n\n        # Calculate dependencies\n        # FIXME Infinite recursion error possible here, if the formula includes\n        #       the Keyword of the Service that includes the Calculation\n        dependencies = map(lambda s: s.getServiceDependencies(), services)\n        dependencies = list(itertools.chain.from_iterable(dependencies))\n\n        # Merge dependencies and services\n        services = set(services + dependencies)\n\n        # Modify existing AR specs with new form values of selected analyses.\n        self._update_specs(instance, specs)\n\n        # Create a mapping of Service UID -> Hidden status\n        if hidden is None:\n            hidden = []\n        hidden = dict(map(lambda d: (d.get(\"uid\"), d.get(\"hidden\")), hidden))\n\n        # Ensure we have a prices dictionary\n        if prices is None:\n            prices = dict()\n\n        # CREATE/MODIFY ANALYSES\n\n        for service in services:\n            service_uid = api.get_uid(service)\n            keyword = service.getKeyword()\n\n            # Create the Analysis if it doesn't exist\n            if shasattr(instance, keyword):\n                analysis = instance._getOb(keyword)\n            else:\n                analysis = create_analysis(instance, service)\n                new_analyses.append(analysis)\n\n            # set the hidden status\n            analysis.setHidden(hidden.get(service_uid, False))\n\n            # Set the price of the Analysis\n            analysis.setPrice(prices.get(service_uid, service.getPrice()))\n\n        # DELETE ANALYSES\n\n        # Service UIDs\n        service_uids = map(api.get_uid, services)\n\n        # Analyses IDs to delete\n        delete_ids = []\n\n        # Assigned Attachments\n        assigned_attachments = []\n\n        for analysis in analyses:\n            service_uid = analysis.getServiceUID()\n\n            # Skip if the Service is selected\n            if service_uid in service_uids:\n                continue\n\n            # Skip non-open Analyses\n            if analysis in non_open_analyses:\n                continue\n\n            # Remember assigned attachments\n            # https://github.com/senaite/senaite.core/issues/1025\n            assigned_attachments.extend(analysis.getAttachment())\n            analysis.setAttachment([])\n\n            # If it is assigned to a worksheet, unassign it before deletion.\n            worksheet = analysis.getWorksheet()\n            if worksheet:\n                worksheet.removeAnalysis(analysis)\n\n            # Unset the partition reference\n            # TODO Remove in >v1.3.0 - This is kept for backwards-compatibility\n            part = analysis.getSamplePartition()\n            if part:\n                # From this partition, remove the reference to the current\n                # analysis that is going to be removed to prevent inconsistent\n                # states (Sample Partitions referencing to Analyses that do not\n                # exist anymore\n                an_uid = api.get_uid(analysis)\n                part_ans = part.getAnalyses() or []\n                part_ans = filter(\n                    lambda an: api.get_uid(an) != an_uid, part_ans)\n                part.setAnalyses(part_ans)\n            # Unset the Analysis-to-Partition reference\n            analysis.setSamplePartition(None)\n            delete_ids.append(analysis.getId())\n\n        if delete_ids:\n            # Note: subscriber might promote the AR\n            instance.manage_delObjects(ids=delete_ids)\n\n        # Remove orphaned attachments\n        for attachment in assigned_attachments:\n            # only delete attachments which are no further linked\n            if not attachment.getLinkedAnalyses():\n                logger.info(\n                    \"Deleting attachment: {}\".format(attachment.getId()))\n                attachment_id = api.get_id(attachment)\n                api.get_parent(attachment).manage_delObjects(attachment_id)\n\n        return new_analyses"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_services(self, full_objects=False):\n        bsc = api.get_tool(\"bika_setup_catalog\")\n        brains = bsc(portal_type=\"AnalysisService\")\n        if full_objects:\n            return map(api.get_object, brains)\n        return brains", "response": "Fetch and return analysis service objects"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _to_service(self, thing):\n\n        # Convert UIDs to objects\n        if api.is_uid(thing):\n            thing = api.get_object_by_uid(thing, None)\n\n        # Bail out if the thing is not a valid object\n        if not api.is_object(thing):\n            logger.warn(\"'{}' is not a valid object!\".format(repr(thing)))\n            return None\n\n        # Ensure we have an object here and not a brain\n        obj = api.get_object(thing)\n\n        if IAnalysisService.providedBy(obj):\n            return obj\n\n        if IAnalysis.providedBy(obj):\n            return obj.getAnalysisService()\n\n        # An object, but neither an Analysis nor AnalysisService?\n        # This should never happen.\n        portal_type = api.get_portal_type(obj)\n        logger.error(\"ARAnalysesField doesn't accept objects from {} type. \"\n                     \"The object will be dismissed.\".format(portal_type))\n        return None", "response": "Convert the object to an Analysis Service or None if the object is not a valid object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_specs(self, instance, specs):\n\n        if specs is None:\n            return\n\n        # N.B. we copy the records here, otherwise the spec will be written to\n        #      the attached specification of this AR\n        rr = {item[\"keyword\"]: item.copy()\n              for item in instance.getResultsRange()}\n        for spec in specs:\n            keyword = spec.get(\"keyword\")\n            if keyword in rr:\n                # overwrite the instance specification only, if the specific\n                # analysis spec has min/max values set\n                if all([spec.get(\"min\"), spec.get(\"max\")]):\n                    rr[keyword].update(spec)\n                else:\n                    rr[keyword] = spec\n            else:\n                rr[keyword] = spec\n        return instance.setResultsRange(rr.values())", "response": "Update the AR specifications with the list of specs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getTotalPrice(self):\n        price, vat = self.getAnalysisProfilePrice(), self.getVATAmount()\n        return float(price) + float(vat)", "response": "Computes the final price using the VATAmount and the subtotal price"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove the Analysis Service passed in from the Analysis Profile. Returns True if the Analysis Service has been removed successfully.", "response": "def remove_service(self, service):\n        \"\"\"Removes the service passed in from the services offered by the\n        current Profile. If the Analysis Service passed in is not assigned to\n        this Analysis Profile, returns False.\n        :param service: the service to be removed from this Analysis Profile\n        :type service: AnalysisService\n        :return: True if the AnalysisService has been removed successfully\n        \"\"\"\n        obj = api.get_object(service)\n        uid = api.get_uid(obj)\n\n        # Remove the service from the referenced services\n        services = self.getService()\n        num_services = len(services)\n        services.remove(obj)\n        self.setService(services)\n        removed = len(services) < num_services\n\n        # Remove the service from the settings map\n        settings = self.getAnalysisServicesSettings()\n        settings = [item for item in settings if item.get('uid', '') != uid]\n        self.setAnalysisServicesSettings(settings)\n\n        return removed"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert xlsx to CSV format", "response": "def xlsx_to_csv(self, infile, worksheet=0, delimiter=\",\"):\n        \"\"\" Convert xlsx to easier format first, since we want to use the\n        convenience of the CSV library\n        \"\"\"\n        wb = load_workbook(self.getInputFile())\n        sheet = wb.worksheets[worksheet]\n        buffer = StringIO()\n\n        # extract all rows\n        for n, row in enumerate(sheet.rows):\n            line = []\n            for cell in row:\n                value = cell.value\n                if type(value) in types.StringTypes:\n                    value = value.encode(\"utf8\")\n                if value is None:\n                    value = \"\"\n                line.append(str(value))\n            print >>buffer, delimiter.join(line)\n        buffer.seek(0)\n        return buffer"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(self):\n        tree = ET.parse(self.getInputFile())\n        root = tree.getroot()\n        # Building Assay dictionary to query names by id from test results line\n        for as_ref in root.find(\"Assays\").findall(\"AssayRef\"):\n            self._assays[as_ref.get(\"KEY_AssayRef\")] = as_ref.get(\"ID\")\n\n        # Building Instruments dictionary to get Serial number by id\n        for ins in root.find(\"Instruments\").findall(\"Instrument\"):\n            self._instruments[ins.get(\"KEY_InstrumentData\")] = \\\n                ins.get(\"SerialNumber\")\n\n        for t_req in root.iter(\"TestRequest\"):\n            t_res = t_req.find(\"TestResult\")\n            if len(t_res) == 0 or not t_res.get(\"Valid\", \"false\") == \"true\":\n                continue\n            res_id = t_req.get(\"SampleID\")\n            test_name = self._assays.get(t_req.get(\"KEY_AssayRef\"))\n            test_name = format_keyword(test_name)\n            result = t_res.get(\"Value\")\n            if not result:\n                continue\n            result = result.split(\" cps/ml\")[0]\n            detected = t_res.get(\"Detected\")\n\n            # SOME ADDITIONAL DATA\n            # Getting instrument serial number from 'Run' element which is\n            # parent of 'TestRequest' elements\n            ins_serial = t_req.getParent().get(\"KEY_InstrumentData\")\n\n            # Matrix is important for calculation.\n            matrix = t_req.get(\"Matrix\")\n            # For now, using EasyQDirector as keyword, but this is not the\n            # right way. test_name must be used.\n            values = {\n                'EasyQDirector': {\n                    \"DefaultResult\": \"Result\",\n                    \"Result\": result,\n                    \"Detected\": detected,\n                    \"Matrix\": matrix,\n                    \"Instrument\": ins_serial\n                }\n            }\n            self._addRawResult(res_id, values)", "response": "parse the data file and add it to the internal dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_data_line(self, sline):\n        # if there are less values founded than headers, it's an error\n        if len(sline[4:]) != len(self._columns):\n            self.err(\"One data line has the wrong number of items\")\n            return -1\n\n        resid = sline[0]\n        analysis = {}\n        date = sline[3]\n        if sline[0] in self.extrainfolist:\n            return 0\n        for idx, result in enumerate(sline[4:]):\n            if result != '':\n                analysis[self._columns[idx]] = {\n                    'result': result,\n                    'DefaultResult': 'result',\n                    'DateTime': date,\n                }\n        self._addRawResult(resid, analysis, False)\n        return 0", "response": "Parses the data line and builds the dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nservicing triggered each time an item is iterated in folderitems. The use of this service prevents the extra-loops in child objects. :obj: the instance of the class to be foldered :item: dict containing the properties of the object to be used by the template :index: current index of the item", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Service triggered each time an item is iterated in folderitems.\n\n        The use of this service prevents the extra-loops in child objects.\n\n        :obj: the instance of the class to be foldered\n        :item: dict containing the properties of the object to be used by\n            the template\n        :index: current index of the item\n        \"\"\"\n\n        name = obj.getFullname()\n        url = obj.absolute_url()\n\n        item[\"replace\"][\"getFullname\"] = get_link(url, value=name)\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getSampleTypeTitles(self):\n        sample_types = self.getSampleTypes()\n        sample_type_titles = map(lambda obj: obj.Title(), sample_types)\n\n        # N.B. This is used only for search purpose, because the catalog does\n        #      not add an entry to the Keywordindex for an empty list.\n        #\n        # => This \"empty\" category allows to search for values with a certain\n        #    sample type set OR with no sample type set.\n        #    (see bika.lims.browser.analysisrequest.add2.get_sampletype_info)\n        if not sample_type_titles:\n            return [\"\"]\n        return sample_type_titles", "response": "Returns a list of sample type titles"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setSampleTypes(self, value, **kw):\n        bsc = getToolByName(self, 'bika_setup_catalog')\n        # convert value to objects\n        if value and type(value) == str:\n            value = [bsc(UID=value)[0].getObject(), ]\n        elif value and type(value) in (list, tuple) and type(value[0]) == str:\n            value = [bsc(UID=uid)[0].getObject() for uid in value if uid]\n        if not type(value) in (list, tuple):\n            value = [value, ]\n        # Find all SampleTypes that were removed\n        existing = self.Schema()['SampleTypes'].get(self)\n        removed = existing and [s for s in existing if s not in value] or []\n        added = value and [s for s in value if s not in existing] or []\n        ret = self.Schema()['SampleTypes'].set(self, value)\n\n        # finally be sure that we aren't trying to set None values here.\n        removed = [x for x in removed if x]\n        added = [x for x in added if x]\n\n        for st in removed:\n            samplepoints = st.getSamplePoints()\n            if self in samplepoints:\n                samplepoints.remove(self)\n                st.setSamplePoints(samplepoints)\n\n        for st in added:\n            st.setSamplePoints(list(st.getSamplePoints()) + [self, ])\n\n        return ret", "response": "Set the SampleTypes attribute of the object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef before_render(self):\n        # We want to display the nav tabs, so we do NOT want disable_border in\n        # the request. Thus, do not call super.before_render\n\n        mtool = api.get_tool(\"portal_membership\")\n        if not mtool.checkPermission(AddAnalysisSpec, self.context):\n            del self.context_actions[_(\"Add\")]", "response": "Before template render hook\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an Alphanumber object that represents the number in accordance with the format specified.", "response": "def to_alpha(number, format, alphabet=ALPHABET, default=_marker):\n    \"\"\"Returns an Alphanumber object that represents the number in accordance\n    with the format specified.\n    :param number: a number representation used to create the Alphanumber\n    :param format: the format to use. eg. '2a3d' for 2 chars and 3 digits\n    :param alphabet: alphabet to use\n    :type number: int, string, Alphanumber, float\n    :type format: string\n    :type alphabet: string\n    \"\"\"\n    match = re.match(r\"^(\\d+)a(\\d+)d\", format)\n    if not match or not match.groups() or len(match.groups()) != 2:\n        if default is not _marker:\n            return default\n        raise ValueError(\"Format not supported: {}\".format(format))\n    matches = match.groups()\n    num_chars = int(matches[0])\n    num_digits = int(matches[1])\n    try:\n        return Alphanumber(number=number, num_chars=num_chars,\n                           num_digits=num_digits, alphabet=alphabet)\n    except ValueError as e:\n        if default is not _marker:\n            return default\n        raise e"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert an alphanumeric code to a decimal number.", "response": "def to_decimal(alpha_number, alphabet=ALPHABET, default=_marker):\n    \"\"\"Converts an alphanumeric code (e.g AB12) to an integer\n    :param alpha_number: representation of an alphanumeric code\n    :param alphabet: alphabet to use when alpha_number is a non-int string\n    :type number: int, string, Alphanumber, float\n    :type alphabet: string\n    \"\"\"\n    num = api.to_int(alpha_number, default=None)\n    if num is not None:\n        return num\n\n    alpha_number = str(alpha_number)\n    regex = re.compile(r\"([A-Z]+)(\\d+)\", re.IGNORECASE)\n    matches = re.findall(regex, alpha_number)\n    if not matches:\n        if default is not _marker:\n            return default\n        raise ValueError(\"Not a valid alpha number: {}\".format(alpha_number))\n\n    alpha = matches[0][0]\n    number = int(matches[0][1])\n    max_num = 10 ** len(matches[0][1]) - 1\n    len_alphabet = len(alphabet)\n    for pos_char, alpha_char in enumerate(reversed(alpha)):\n        index_char = alphabet.find(alpha_char)\n        number += (index_char * max_num * len_alphabet ** pos_char)\n\n    return number"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parts(self):\n        def get_alpha(alpha_index, alphabet):\n            if alpha_index >= len(alphabet):\n                lead = get_alpha(alpha_index / len(alphabet), alphabet)\n                trail = alphabet[alpha_index % len(alphabet)]\n                return \"{}{}\".format(lead, trail)\n            return alphabet[alpha_index]\n\n        max_digits = 10 ** self.num_digits - 1\n        alpha_index = abs(self.number) / max_digits\n        alpha_number = abs(self.number) % max_digits\n        # Note the 1 digit leap e.g. AA99 + 1 == AB01 (not AB00)\n        if not alpha_number and abs(self.number):\n            alpha_number = max_digits\n            alpha_index -= 1\n\n        alpha = get_alpha(alpha_index, self.alphabet)\n        if len(alpha) > self.num_chars:\n            raise ValueError(\"Out of bounds. Requires {} chars, {} set\"\n                             .format(len(alpha), self.num_chars))\n\n        return alpha, alpha_number", "response": "Returns the alphanumeric parts of this AlphanumTaxonomy."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the content menu factories are available.", "response": "def contentmenu_factories_available(self):\n    \"\"\"These types will have their Add New... factories dropdown menu removed.\n    \"\"\"\n    if hasattr(self._addContext(), 'portal_type') \\\n    and self._addContext().portal_type in [\n        'ARImport',\n        'Batch',\n        'Client',\n        'AnalysisRequest',\n        'Worksheet',\n        'AnalysisCategory',\n        'AnalysisProfile',\n        'ARTemplate',\n        'AnalysisService',\n        'AnalysisSpec',\n        'Attachment',\n        'Calculation',\n        'Instrument',\n        'LabContact',\n        'Laboratory',\n        'Method',\n        'Department',\n        'ReferenceDefinition',\n        'ReportFolder',\n        'SampleType',\n        'SamplePoint',\n        'StorageLocation',\n        'WorksheetTemplate',\n        'LabProduct',\n        'ReferenceSample',\n        'Preservation'\n    ]:\n        return False\n    else:\n        itemsToAdd = self._itemsToAdd()\n        showConstrainOptions = self._showConstrainOptions()\n        if self._addingToParent() and not self.context_state.is_default_page():\n            return False\n        return (len(itemsToAdd) > 0 or showConstrainOptions)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the filesize of the PDF", "response": "def get_filesize(self, pdf):\n        \"\"\"Compute the filesize of the PDF\n        \"\"\"\n        try:\n            filesize = float(pdf.get_size())\n            return filesize / 1024\n        except (POSKeyError, TypeError):\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef folderitem(self, obj, item, index):\n\n        ar = obj.getAnalysisRequest()\n        review_state = api.get_workflow_status_of(ar)\n        status_title = review_state.capitalize().replace(\"_\", \" \")\n\n        item[\"replace\"][\"AnalysisRequest\"] = get_link(\n            ar.absolute_url(), value=ar.Title()\n        )\n\n        pdf = self.get_pdf(obj)\n        filesize = self.get_filesize(pdf)\n        if filesize > 0:\n            url = \"{}/download_pdf\".format(obj.absolute_url())\n            item[\"replace\"][\"PDF\"] = get_link(\n                url, value=\"PDF\", target=\"_blank\")\n\n        item[\"State\"] = _BMF(status_title)\n        item[\"state_class\"] = \"state-{}\".format(review_state)\n        item[\"FileSize\"] = \"{:.2f} Kb\".format(filesize)\n        fmt_date = self.localize_date(obj.created())\n        item[\"Date\"] = fmt_date\n        item[\"PublishedBy\"] = self.user_fullname(obj.Creator())\n\n        contained_ars = obj.getField(\"ContainedAnalysisRequests\").get(obj)\n        ar_icon_url = \"{}/{}\".format(\n            self.portal_url,\n            \"++resource++bika.lims.images/analysisrequest.png\"\n        )\n        ars = []\n        item[\"ContainedAnalysisRequests\"] = \"\"\n        for num, ar in enumerate(contained_ars):\n            ars.append(\n                \"<a href='{url}' target='_blank' title='{ar_id}'>\"\n                \"<img src='{ar_icon_url}' title='{ar_id}'/>\"\n                \"</a>\".format(\n                    url=ar.absolute_url(),\n                    ar_id=ar.getId(),\n                    ar_icon_url=ar_icon_url)\n            )\n        item[\"replace\"][\"ContainedAnalysisRequests\"] = \" \".join(ars)\n\n        # Metadata\n        metadata = obj.getField(\"Metadata\").get(obj) or {}\n        template = metadata.get(\"template\", \"\")\n        paperformat = metadata.get(\"paperformat\", \"\")\n        orientation = metadata.get(\"orientation\", \"\")\n        item[\"Metadata\"] = \"\"\n        if all([template, paperformat, orientation]):\n            item[\"replace\"][\"Metadata\"] = \" \".join([\n                \"<abbr title='{}'>\u2139</abbr>\".format(template),\n                \"<abbr title='{}'>\u21f2</abbr>\".format(paperformat),\n                \"<abbr title='{}'>\u21ba</abbr>\".format(orientation),\n            ])\n\n        # N.B. There is a bug in the current publication machinery, so that\n        # only the primary contact get stored in the Attachment as recipient.\n        #\n        # However, we're interested to show here the full list of recipients,\n        # so we use the recipients of the containing AR instead.\n        recipients = []\n\n        for recipient in self.get_recipients(ar):\n            email = safe_unicode(recipient[\"EmailAddress\"])\n            fullname = safe_unicode(recipient[\"Fullname\"])\n            if email:\n                value = u\"<a href='mailto:{}'>{}</a>\".format(email, fullname)\n                recipients.append(value)\n            else:\n                message = _(\"No email address set for this contact\")\n                value = u\"<span title='{}' class='text text-danger'>\" \\\n                        u\"\u26a0 {}</span>\".format(message, fullname)\n                recipients.append(value)\n\n        item[\"replace\"][\"Recipients\"] = \", \".join(recipients)\n\n        # No recipient with email set preference found in the AR, so we also\n        # flush the Recipients data from the Attachment\n        if not recipients:\n            item[\"Recipients\"] = \"\"\n\n        return item", "response": "Augment folder listing item\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a single line of a header line and save the values in the internal dict.", "response": "def parse_headerline(self, line):\n        #Process incoming header line\n        \"\"\"11/03/2014 14:46:46\n        PANalytical\n        Results quantitative - Omnian 2013,\n\n        Selected archive:,Omnian 2013\n        Number of results selected:,4\n        \"\"\"\n        \n        # Save each header field (that we know) and its own value in the dict        \n        if line.startswith('Results quantitative'):\n            line = to_unicode(line)\n            if len(self._header) == 0:\n                self.err(\"Unexpected header format\", numline=self._numline)\n                return -1\n\n            line = line.replace(',', \"\")\n            splitted = line.split(' - ')\n            self._header['Quantitative'] = splitted[1]\n            return 1\n\n        if line.startswith('Selected archive'):\n            if len(self._header) == 0:\n                self.err(\"No header found\", numline=self._numline)\n                return -1\n\n            splitted = self.splitLine(line)\n            if len(splitted) > 1:\n                self._header['Archive'] = splitted[1].replace('\"', '').strip()\n            else:\n                self.warn('Unexpected header format', numline=self._numline)\n            return 0\n\n        if line.startswith('Number of'):\n            if len(self._header) == 0:\n                self.err(\"No header found\", numline=self._numline)\n                return -1\n\n            splitted = self.splitLine(line)\n            if len(splitted) > 1:\n                self._header['NumResults'] = splitted[1].replace('\"', '').strip()\n            else:\n                self.warn('Unexpected header format', numline=self._numline)\n            return 0\n\n        if line.startswith('Seq.'):\n            if len(self._header) == 0:\n                self.err(\"No header found\", numline=self._numline)\n                return -1\n            #Grab column names\n            self._columns = line.split(',')\n            self._end_header = True\n            return 1\n\n        else:\n            self._header['Date'] = line\n            return 1"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parseline(self, line):\n        sline = line.split(SEPARATOR)\n        segment = sline[0]\n        handlers = {\n            SEGMENT_HEADER: self._handle_header,\n            SEGMENT_EOF: self._handle_eof,\n            SEGMENT_RESULT: self._handle_result_line,\n            SEGMENT_OBSERVATION_ORDER: self._handle_new_record\n        }\n        handler = handlers.get(segment)\n        if handler:\n            return handler(sline)\n        return 0", "response": "Parse a single line of ISO - 8601 data into a dictionary of key - value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _handle_result_line(self, sline):\n        as_kw = sline[3]\n        a_result = str(sline[5].split('^')[0])\n        self._cur_values[as_kw] = {\n            'DefaultResult': 'Result',\n            'Result': a_result\n        }\n        return 0", "response": "Parses the data line and adds the result to the dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _submit_results(self):\n        if self._cur_res_id and self._cur_values:\n            # Setting DefaultResult just because it is obligatory.\n            self._addRawResult(self._cur_res_id, self._cur_values)\n            self._reset()", "response": "Submits the current values as a Raw Result and resets everything."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_data(self, sline):\n        if sline[0] == '':\n            return 0\n        for idx, label in enumerate(self._labels_values[sline[0]]):\n            if label != '':\n                self._addRawResult(label.split(' ')[0], {self.analysiskey: sline[1:][idx]}, False)\n        return 0", "response": "This function parses the data line of the section into a dictionary of the appropriate class."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all users of the portal", "response": "def get_users(self):\n        \"\"\"Get all users of the portal\n        \"\"\"\n        # We make use of the existing controlpanel `@@usergroup-userprefs`\n        # view logic to make sure we get all users from all plugins (e.g. LDAP)\n        users_view = UsersOverviewControlPanel(self.context, self.request)\n        return users_view.doSearch(\"\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_user_properties(self):\n\n        user = self.context.getUser()\n\n        # No User linked, nothing to do\n        if user is None:\n            return {}\n\n        out = {}\n        plone_user = user.getUser()\n        userid = plone_user.getId()\n        for sheet in plone_user.listPropertysheets():\n            ps = plone_user.getPropertysheet(sheet)\n            out.update(dict(ps.propertyItems()))\n\n        portal = api.get_portal()\n        mtool = getToolByName(self.context, 'portal_membership')\n\n        out[\"id\"] = userid\n        out[\"portrait\"] = mtool.getPersonalPortrait(id=userid)\n        out[\"edit_url\"] = \"{}/@@user-information?userid={}\".format(\n            portal.absolute_url(), userid)\n\n        return out", "response": "Return the properties of the User"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef linkable_users(self):\n\n        # Only users with at nost these roles are displayed\n        linkable_roles = {\"Authenticated\", \"Member\", \"Client\"}\n\n        out = []\n        for user in self.get_users():\n            userid = user.get(\"id\", None)\n\n            if userid is None:\n                continue\n\n            # Skip users which are already linked to a Contact\n            contact = Contact.getContactByUsername(userid)\n            labcontact = LabContact.getContactByUsername(userid)\n\n            if contact or labcontact:\n                continue\n            if self.is_contact():\n                # Checking Plone user belongs to Client group only. Otherwise,\n                # weird things could happen (a client contact assigned to a\n                # user with labman privileges, different contacts from\n                # different clients assigned to the same user, etc.)\n                user_roles = security.get_roles(user=userid)\n                if not linkable_roles.issuperset(set(user_roles)):\n                    continue\n            userdata = {\n                \"userid\": userid,\n                \"email\": user.get(\"email\"),\n                \"fullname\": user.get(\"title\"),\n            }\n\n            # filter out users which do not match the searchstring\n            if self.searchstring:\n                s = self.searchstring.lower()\n                if not any(\n                        map(lambda v: re.search(s, str(v).lower()),\n                            userdata.values())):\n                    continue\n\n            # update data (maybe for later use)\n            userdata.update(user)\n\n            # Append the userdata for the results\n            out.append(userdata)\n\n        out.sort(lambda x, y: cmp(x[\"fullname\"], y[\"fullname\"]))\n        return out", "response": "Search Plone users which are not linked to a contact or lab contact or lab contact."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlink an existing user to the current Contact", "response": "def _link_user(self, userid):\n        \"\"\"Link an existing user to the current Contact\n        \"\"\"\n        # check if we have a selected user from the search-list\n        if userid:\n            try:\n                self.context.setUser(userid)\n                self.add_status_message(\n                    _(\"User linked to this Contact\"), \"info\")\n            except ValueError, e:\n                self.add_status_message(e, \"error\")\n        else:\n            self.add_status_message(\n                _(\"Please select a User from the list\"), \"info\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a status message to the portal message", "response": "def add_status_message(self, message, severity=\"info\"):\n        \"\"\"Set a portal message\n        \"\"\"\n        self.context.plone_utils.addPortalMessage(message, severity)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_user(self):\n\n        def error(field, message):\n            if field:\n                message = \"%s: %s\" % (field, message)\n            self.context.plone_utils.addPortalMessage(message, 'error')\n            return self.request.response.redirect(\n                self.context.absolute_url() + \"/login_details\")\n\n        form = self.request.form\n        contact = self.context\n\n        password = safe_unicode(form.get('password', '')).encode('utf-8')\n        username = safe_unicode(form.get('username', '')).encode('utf-8')\n        confirm = form.get('confirm', '')\n        email = safe_unicode(form.get('email', '')).encode('utf-8')\n\n        if not username:\n            return error('username', PMF(\"Input is required but not given.\"))\n\n        if not email:\n            return error('email', PMF(\"Input is required but not given.\"))\n\n        reg_tool = self.context.portal_registration\n        # properties = self.context.portal_properties.site_properties\n        # if properties.validate_email:\n        #     password = reg_tool.generatePassword()\n        # else:\n        if password != confirm:\n            return error('password', PMF(\"Passwords do not match.\"))\n\n        if not password:\n            return error('password', PMF(\"Input is required but not given.\"))\n\n        if not confirm:\n            return error('password', PMF(\"Passwords do not match.\"))\n\n        if len(password) < 5:\n            return error('password', PMF(\"Passwords must contain at least 5 \"\n                                         \"characters.\"))\n        for user in self.get_users():\n            userid = user.get(\"id\", None)\n            if userid is None:\n                continue\n            user_obj = api.get_user(userid)\n            if user_obj.getUserName() == username:\n                msg = \"Username {} already exists, please, choose \" \\\n                      \"another one.\".format(username)\n                return error(None, msg)\n\n        try:\n            reg_tool.addMember(username,\n                               password,\n                               properties={\n                                   'username': username,\n                                   'email': email,\n                                   'fullname': username})\n        except ValueError, msg:\n            return error(None, msg)\n\n        # set the user to the contact\n        contact.setUser(username)\n\n        # Additional groups for LabContact users only!\n        # -> This is not visible in the Client Contact Form\n        if \"groups\" in self.request and self.request[\"groups\"]:\n            groups = self.request[\"groups\"]\n            if not type(groups) in (list, tuple):\n                groups = [groups, ]\n            for group in groups:\n                group = self.portal_groups.getGroupById(group)\n                group.addMember(username)\n\n        if self.request.get('mail_me', 0):\n            try:\n                reg_tool.registeredNotify(username)\n            except Exception:\n                transaction.abort()\n                message = _(\"SMTP server disconnected. User creation aborted.\")\n                return error(None, message)\n\n        message = _(\"Member registered and linked to the current Contact.\")\n        self.context.plone_utils.addPortalMessage(message, 'info')\n        return self.request.response.redirect(\n            self.context.absolute_url() + \"/login_details\")", "response": "Create a new user in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _cache_key_select_analysisservice(method, self, allow_blank,\n                                      multiselect, style=None):\n    \"\"\"\n    This function returns the key used to decide if method select_analysisservice has to be recomputed\n    \"\"\"\n    key = update_timer(), allow_blank, multiselect, style\n    return key", "response": "This function returns the key used to decide if the method select_analysisservice has to be recomputed based on the parameters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _cache_key_select_analyst(method, self, allow_blank=False, style=None):\n    key = update_timer(),allow_blank, style\n    return key", "response": "This function returns the key used to decide if the method select_analyst has to be recomputed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _cache_key_select_daterange(method, self, field_id, field_title, style=None):\n    key = update_timer(), field_id, field_title, style\n    return key", "response": "This function returns the key used to decide if select_daterange has to be recomputed based on the method and the parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _cache_key_select_sample_type(method, self, allow_blank=True, multiselect=False, style=None):\n    key = update_timer(), allow_blank, multiselect, style\n    return key", "response": "This function returns the key used to decide if the method select_sample_type has to be recomputed based on the current time."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getResultsRange(self):\n        specs = ResultsRangeDict(result=\"\")\n        sample = self.getSample()\n        if not sample:\n            return specs\n\n        service_uid = self.getServiceUID()\n        sample_range = sample.getResultsRangeDict()\n        return sample_range.get(service_uid, specs)", "response": "Returns the valid result range for this reference analysis based on the results ranges defined in the Reference Sample and the results of the parent Reference Sample."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef folderitems(self):\n        wf = api.get_tool('portal_workflow')\n        schema = self.context.Schema()\n\n        ars = []\n\n        for o in schema.getField('InheritedObjects').get(self.context):\n            if o.portal_type == 'AnalysisRequest':\n                if o not in ars:\n                    ars.append(o)\n            elif o.portal_type == 'Batch':\n                for ar in o.getAnalysisRequests(is_active=True):\n                    if ar not in ars:\n                        ars.append(ar)\n\n        for ar in self.context.getAnalysisRequests(is_active=True):\n            if ar not in ars:\n                ars.append(ar)\n\n        self.categories = []\n        analyses = {}\n        items = []\n        distinct = []  # distinct analyses (each one a different service)\n        keywords = []\n        for ar in ars:\n            analyses[ar.id] = []\n            for analysis in ar.getAnalyses(full_objects=True):\n                analyses[ar.id].append(analysis)\n                if analysis.getKeyword() not in keywords:\n                    # we use a keyword check, because versioned services are !=.\n                    keywords.append(analysis.getKeyword())\n                    distinct.append(analysis)\n\n            batchlink = \"\"\n            batch = ar.getBatch()\n            if batch:\n                batchlink = \"<a href='%s'>%s</a>\" % (\n                    batch.absolute_url(), batch.Title())\n\n            arlink = \"<a href='%s'>%s</a>\" % (\n                ar.absolute_url(), ar.Title())\n\n            subgroup = ar.Schema()['SubGroup'].get(ar)\n            sub_title = subgroup.Title() if subgroup else 'No Subgroup'\n            sub_sort = subgroup.getSortKey() if subgroup else '1'\n            sub_class = re.sub(r\"[^A-Za-z\\w\\d\\-\\_]\", '', sub_title)\n\n            if [sub_sort, sub_title] not in self.categories:\n                self.categories.append([sub_sort, sub_title])\n\n            review_state = wf.getInfoFor(ar, 'review_state')\n            state_title = wf.getTitleForStateOnType(\n                review_state, 'AnalysisRequest')\n\n            item = {\n                'obj': ar,\n                'id': ar.id,\n                'uid': ar.UID(),\n                'category': sub_title,\n                'title': ar.Title(),\n                'type_class': 'contenttype-AnalysisRequest',\n                'url': ar.absolute_url(),\n                'relative_url': ar.absolute_url(),\n                'view_url': ar.absolute_url(),\n                'created': self.ulocalized_time(ar.created(), long_format=1),\n                'sort_key': ar.created(),\n                'replace': {\n                    'Batch': batchlink,\n                    'AnalysisRequest': arlink,\n                },\n                'before': {},\n                'after': {},\n                'choices': {},\n                'class': {'Batch': 'Title'},\n                'state_class': 'state-active subgroup_{0}'.format(sub_class) if sub_class else 'state-active',\n                'allow_edit': [],\n                'Batch': '',\n                'SamplePoint': ar.getSamplePoint().Title() if ar.getSamplePoint() else '',\n                'SampleType': ar.getSampleType().Title() if ar.getSampleType() else '',\n                'ClientOrderNumber': ar.getClientOrderNumber(),\n                'AnalysisRequest': '',\n                'state_title': state_title,\n            }\n            items.append(item)\n\n        unitstr = '<em class=\"discreet\" style=\"white-space:nowrap;\">%s</em>'\n        checkPermission = getSecurityManager().checkPermission\n\n        # Insert columns for analyses\n        for d_a in distinct:\n            keyword = d_a.getKeyword()\n            short = d_a.getShortTitle()\n            title = d_a.Title()\n            self.columns[keyword] = {\n                'title':  short if short else title,\n                'sortable': True\n            }\n            self.review_states[0]['columns'].insert(\n                len(self.review_states[0]['columns']) - 1, keyword)\n\n            # Insert values for analyses\n            for i, item in enumerate(items):\n                for analysis in analyses[item['id']]:\n                    if keyword not in items[i]:\n                        items[i][keyword] = ''\n                    if analysis.getKeyword() != keyword:\n                        continue\n\n                    edit = checkPermission(EditResults, analysis)\n                    calculation = analysis.getCalculation()\n                    if self.allow_edit and edit and not calculation:\n                        items[i]['allow_edit'].append(keyword)\n                        if not self.insert_submit_button:\n                            self.insert_submit_button = True\n\n                    value = analysis.getResult()\n                    items[i][keyword] = value\n                    items[i]['class'][keyword] = ''\n\n                    if value or (edit and not calculation):\n\n                        unit = unitstr % d_a.getUnit()\n                        items[i]['after'][keyword] = unit\n\n                if keyword not in items[i]['class']:\n                    items[i]['class'][keyword] = 'empty'\n        if self.insert_submit_button:\n            transitions = self.review_states[0].get('custom_transitions', [])\n            transitions.append({\n                'id': 'submit',\n                'title': _('Submit')\n            })\n            self.review_states[0]['custom_transitions'] = transitions\n\n        self.categories.sort()\n        self.categories = [x[1] for x in self.categories]\n\n        items = sorted(items, key=itemgetter(\"sort_key\"))\n\n        return items", "response": "Accumulate a list of all AnalysisRequest objects contained in this Batch as well as those which are inherited."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of items in the folder.", "response": "def folderitems(self):\n        \"\"\"TODO: Refactor to non-classic mode\n        \"\"\"\n        items = super(ServicesView, self).folderitems()\n        self.categories.sort()\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef folderitem(self, obj, item, index):\n        # ensure we have an object and not a brain\n        obj = api.get_object(obj)\n        uid = api.get_uid(obj)\n        url = api.get_url(obj)\n        title = api.get_title(obj)\n\n        # get the category\n        if self.show_categories_enabled():\n            category = obj.getCategoryTitle()\n            if category not in self.categories:\n                self.categories.append(category)\n            item[\"category\"] = category\n\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n        item[\"selected\"] = False\n        item[\"selected\"] = uid in self.selected_services_uids\n\n        # Add methods\n        methods = obj.getMethods()\n        if methods:\n            links = map(\n                lambda m: get_link(\n                    m.absolute_url(), value=m.Title(), css_class=\"link\"),\n                methods)\n            item[\"replace\"][\"Methods\"] = \", \".join(links)\n        else:\n            item[\"methods\"] = \"\"\n\n        calculation = obj.getCalculation()\n        if calculation:\n            title = calculation.Title()\n            url = calculation.absolute_url()\n            item[\"Calculation\"] = title\n            item[\"replace\"][\"Calculation\"] = get_link(url, value=title)\n        else:\n            item[\"Calculation\"] = \"\"\n\n        # Icons\n        after_icons = \"\"\n        if obj.getAccredited():\n            after_icons += get_image(\n                \"accredited.png\", title=_(\"Accredited\"))\n        if obj.getAttachmentOption() == \"r\":\n            after_icons += get_image(\n                \"attach_reqd.png\", title=_(\"Attachment required\"))\n        if obj.getAttachmentOption() == \"n\":\n            after_icons += get_image(\n                \"attach_no.png\", title=_(\"Attachment not permitted\"))\n        if after_icons:\n            item[\"after\"][\"Title\"] = after_icons\n\n        return item", "response": "This method is called by the folderitem service triggered each time an item is iterated in folderitems. It adds the template properties of the object to be used by the item."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_form(self, instance, field, form, empty_marker=None,\n                     emptyReturnsMarker=False, validating=True):\n        \"\"\"Return UIDs of the selected services\n        \"\"\"\n        service_uids = form.get(\"uids\", [])\n        return service_uids, {}", "response": "Process the form and return the UIDs of the selected services"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Services(self, field, show_select_column=True):\n\n        instance = getattr(self, \"instance\", field.aq_parent)\n        table = api.get_view(\"table_analyses_services\",\n                             context=instance,\n                             request=self.REQUEST)\n        # Call listing hooks\n        table.update()\n        table.before_render()\n        return table.ajax_contents_table()", "response": "Render Analyses Services Listing Table\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the user object", "response": "def get_user(user=None):\n    \"\"\"Get the user object\n\n    :param user: A user id, memberdata object or None for the current user\n    :returns: Plone User (PlonePAS) / Propertied User (PluggableAuthService)\n    \"\"\"\n    if user is None:\n        # Return the current authenticated user\n        user = getSecurityManager().getUser()\n    elif isinstance(user, MemberData):\n        # MemberData wrapped user -> get the user object\n        user = user.getUser()\n    elif isinstance(user, basestring):\n        # User ID -> get the user\n        user = get_member_by_login_name(get_portal(), user, False)\n        if user:\n            user = user.getUser()\n    return user"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the group object", "response": "def get_group(group):\n    \"\"\"Return the group\n\n    :param group: The group name/id\n    :returns: Group\n    \"\"\"\n    portal_groups = get_tool(\"portal_groups\")\n    if isinstance(group, basestring):\n        group = portal_groups.getGroupById(group)\n    elif isinstance(group, GroupData):\n        group = group\n    return group"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the groups of the user", "response": "def get_groups(user=None):\n    \"\"\"Return the groups of the user\n\n    :param user: A user id, memberdata object or None for the current user\n    :returns: List of groups\n    \"\"\"\n    portal_groups = get_tool(\"portal_groups\")\n    user = get_user(user)\n    if user is None:\n        return []\n    return portal_groups.getGroupsForPrincipal(user)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding the user to the group", "response": "def add_group(group, user=None):\n    \"\"\"Add the user to the group\n    \"\"\"\n    user = get_user(user)\n\n    if user is None:\n        raise ValueError(\"User '{}' not found\".format(repr(user)))\n\n    if isinstance(group, basestring):\n        group = [group]\n    elif isinstance(group, GroupData):\n        group = [group]\n\n    portal_groups = get_tool(\"portal_groups\")\n    for group in map(get_group, group):\n        if group is None:\n            continue\n        portal_groups.addPrincipalToGroup(get_user_id(user), group.getId())\n\n    return get_groups(user)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwalking through analyses and sets the formula of the dependent services field of the analyses that are not yet in the calculations and the DependentServices field of the analyses that are not in the calculations.", "response": "def fix_broken_calculations():\n    \"\"\"Walks-through calculations associated to undergoing analyses and\n    resets the value for DependentServices field\"\"\"\n\n    logger.info(\"Fixing broken calculations (re-assignment of dependents)...\")\n\n    # Fetch only the subset of analyses that are undergoing.\n    # Analyses that have been verified or published cannot be updated, so there\n    # is no sense to check their calculations\n    review_states = [\n         'attachment_due',\n         'not_requested',\n         'rejected',\n         'retracted',\n         'sample_due',\n         'sample_prep',\n         'sample_received',\n         'sample_received',\n         'sample_registered',\n         'sampled',\n         'to_be_preserved',\n         'to_be_sampled',\n    ]\n    uc = api.get_tool('uid_catalog')\n    catalog = get_tool(CATALOG_ANALYSIS_LISTING)\n    brains = catalog(portal_type='Analysis', review_state=review_states)\n    for brain in brains:\n        analysis = brain.getObject()\n        calculation = analysis.getCalculation()\n        if not calculation:\n            continue\n\n        dependents = calculation.getDependentServices()\n        # We don't want eventualities such as [None,]\n        dependents = filter(None, dependents)\n        if not dependents:\n            # Assign the formula again to the calculation. Note the function\n            # setFormula inferes the dependent services (and stores them) by\n            # inspecting the keywords set in the formula itself.\n            # So, instead of doing this job here, we just let setFormula to work\n            # for us.\n            formula = calculation.getFormula()\n            calculation.setFormula(formula)\n            deps = calculation.getDependentServices()\n            if not deps:\n                # Ok, this calculation does not depend on the result of other\n                # analyses, so we can omit this one, he is already ok\n                continue\n\n            deps = [dep.getKeyword() for dep in deps]\n            deps = ', '.join(deps)\n            arid = analysis.getRequestID()\n            logger.info(\"Dependents for {}.{}.{}: {}\".format(arid,\n                                                          analysis.getKeyword(),\n                                                          calculation.Title(),\n                                                          deps))\n\n            # Set the calculation to the analysis again (field Calculation is an\n            # HistoryAwareReferenceField in Analyses that inherits from\n            # AbstractRoutineAnalysis\n            analysis.setCalculation(calculation)\n\n            # Check if all is ok\n            an_deps =  analysis.getCalculation().getDependentServices()\n            if not an_deps:\n                # Maybe the version of the calculation is an old one. If so, we\n                # need to use the last version, cause HistoryAwareReferenceField\n                # will always point to the version assigned to the calculation\n                # that was associated to the analysis.\n                uid = calculation.UID()\n                target_version = analysis.reference_versions[uid]\n                last_calc = uc(UID=uid)\n                if not last_calc:\n                    # This should not happen\n                    logger.warn(\"No calculation found for %s\" % uid)\n                    continue\n                last_calc = last_calc[0].getObject()\n                if last_calc.version_id != target_version:\n                    # Ok, this is another version. We have no choice here... we\n                    # need to assign the latest version...\n                    analysis.reference_versions[uid]=last_calc.version_id\n\n            # Just in case\n            analysis.reindexObject()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert all ReferenceField s values into UIDReferenceFields.", "response": "def UpgradeReferenceFields():\n    \"\"\"Convert all ReferenceField's values into UIDReferenceFields.\n\n    These are not touched: HistoryAware to be removed:\n      - Analysis.Calculation: HistoryAwareReferenceField (rel=\n      AnalysisCalculation)\n      - DuplicateAnalysis.Calculation: HistoryAwareReferenceField (rel=\n      DuplicateAnalysisCalculation)\n      - RejectAnalysis.Calculation: HistoryAwareReferenceField (rel=\n      RejectAnalysisCalculation)\n\n    These are not touched: They are deprecated and will be removed:\n      - AnalysisRequest.Profile: ReferenceField (rel=AnalysisRequestProfile)\n      - LabContact.Department ReferenceField (rel=LabContactDepartment)\n\n    The remaining fields are listed below.\n    \"\"\"\n\n    # Change these carefully\n    # they were made slowly with love\n    # still they may be wrong.\n\n    for portal_type, fields in [\n        # portal_type\n        ['ARReport', [\n            ('AnalysisRequest', 'ARReportAnalysisRequest')\n        ]],\n\n        ['Analysis', [\n            # AbstractBaseAnalysis\n            ('Category', 'AnalysisCategory'),\n            ('Department', 'AnalysisDepartment'),\n            ('Instrument', 'AnalysisInstrument'),\n            ('Method', 'AnalysisMethod'),\n            # AbstractAnalysis\n            ('AnalysisService', 'AnalysisAnalysisService'),\n            ('Attachment', 'AnalysisAttachment'),\n            # AbstractRoutineAnalysis\n            ('OriginalReflexedAnalysis', 'AnalysisOriginalReflexedAnalysis'),\n            ('ReflexAnalysisOf', 'AnalysisReflexAnalysisOf'),\n            ('SamplePartition', 'AnalysisSamplePartition')\n        ]],\n\n        ['ReferenceAnalysis', [\n            # AbstractBaseAnalysis\n            ('Category', 'AnalysisCategory'),\n            ('Department', 'AnalysisDepartment'),\n            ('Instrument', 'AnalysisInstrument'),\n            ('Method', 'AnalysisMethod'),\n            # AbstractAnalysis\n            ('AnalysisService', 'AnalysisAnalysisService'),\n            ('Attachment', 'AnalysisAttachment'),\n        ]],\n\n        ['DuplicateAnalysis', [\n            # AbstractBaseAnalysis\n            ('Category', 'AnalysisCategory'),\n            ('Department', 'AnalysisDepartment'),\n            ('Instrument', 'AnalysisInstrument'),\n            ('Method', 'AnalysisMethod'),\n            # AbstractAnalysis\n            ('AnalysisService', 'AnalysisAnalysisService'),\n            ('Attachment', 'AnalysisAttachment'),\n            # AbstractRoutineAnalysis\n            ('OriginalReflexedAnalysis', 'AnalysisOriginalReflexedAnalysis'),\n            ('ReflexAnalysisOf', 'AnalysisReflexAnalysisOf'),\n            ('SamplePartition', 'AnalysisSamplePartition'),\n            # DuplicateAnalysis\n            ('Analysis', 'DuplicateAnalysisAnalysis'),\n        ]],\n\n        ['AnalysisService', [\n            # AbstractBaseAnalysis\n            ('Category', 'AnalysisCategory'),\n            ('Department', 'AnalysisDepartment'),\n            ('Instrument', 'AnalysisInstrument'),\n            ('Method', 'AnalysisMethod'),\n            # AnalysisService\n            ('Calculation', 'AnalysisServiceCalculation'),\n            ('Container', 'AnalysisServiceContainer'),\n            ('Instruments', 'AnalysisServiceInstruments'),\n            ('Methods', 'AnalysisServiceMethods'),\n            ('Preservation', 'AnalysisServicePreservation'),\n\n            # Backward compatibility with < 1.0.0\n            ('Calculation', 'AnalysisServiceCalculation'),\n            ('Category',    'AnalysisServiceAnalysisCategory'),\n            ('Department',  'AnalysisServiceDepartment'),\n            ('Instrument',  'AnalysisServiceInstrument'),\n            ('Instruments', 'AnalysisServiceInstruments'),\n            ('Method',      'AnalysisServiceMethod'),\n            ('Methods',     'AnalysisServiceMethods'),\n            ('Preservation','AnalysisServicePreservation'),\n            ('Container',   'AnalysisServiceContainer'),\n        ]],\n\n        ['AnalysisRequest', [\n            ('Contact', 'AnalysisRequestContact'),\n            ('Sample', 'AnalysisRequestSample'),\n        ]],\n\n        ['AnalysisSpec', [\n            ('SampleType', 'AnalysisSpecSampleType')\n        ]],\n\n        ['Calculation', [\n            ('DependentServices', 'CalculationDependentServices'),\n            ('DependentServices', 'CalculationAnalysisService'),\n        ]],\n\n        ['Instrument', [\n            ('Analyses', 'InstrumentAnalyses'),\n            ('Method', 'InstrumentMethod'),\n        ]],\n\n        ['Method', [\n            ('Calculation', 'MethodCalculation'),\n        ]],\n\n        ['SamplePartition', [\n            ('Analyses', 'SamplePartitionAnalysis'),\n        ]],\n\n        ['Worksheet', [\n            ('WorksheetTemplate', 'WorksheetAnalysisTemplate')\n        ]]]:\n\n        logger.info(\"Migrating references for portal_type: %s\" % portal_type)\n\n        for fieldname, relation in fields:\n            if is_UIDReferenceField(portal_type, fieldname):\n                migrate_refs(portal_type, relation, fieldname)\n\n        # remove at refs\n        for remove in refs_to_remove:\n            del_at_refs(remove)\n\n        # reindex objects\n        for obj in objs_to_reindex:\n            obj.reindexObject()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef touidref(src, dst, src_relation, src_portal_type, fieldname):\n    field = dst.getField(fieldname)\n    refs = src.getRefs(relationship=src_relation)\n\n    if len(refs) == 1:\n        value = get_uid(refs[0])\n    elif len(refs) > 1:\n        value = filter(lambda x: x, [get_uid(ref) for ref in refs])\n    else:\n        value = field.get(src)\n    if not value:\n        value = ''\n    if not field:\n        raise RuntimeError('Cannot find field %s/%s' % (fieldname, src))\n    if field.required and not value:\n        logger.exception('Required %s field %s/%s has no value' %\n                         (src.portal_type, src, fieldname))\n    field.set(src, value)", "response": "Convert an archetypes reference in src_relation to a UIDReference\n    in dst / fieldname."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking a brain or object and returns a valid UID.", "response": "def get_uid(value):\n    \"\"\"Takes a brain or object and returns a valid UID.\n    In this case, the object may come from portal_archivist, so we will\n    need to do a catalog query to get the UID of the current version\n    \"\"\"\n    if not value:\n        return ''\n    # Is value a brain?\n    if ICatalogBrain.providedBy(value):\n        value = value.getObject()\n    # validate UID\n    uid = value.UID()\n    uc = get_tool('uid_catalog')\n    if uc(UID=uid):\n        # The object is valid\n        return uid\n    # Otherwise the object is an old version\n    brains = uc(portal_type=value.portal_type, Title=value.Title())\n    if not brains:\n        # Cannot find UID\n        raise RuntimeError('The UID for %s/%s cannot be found!' %\n                           (value.portal_type, value.Title()))\n    if len(brains) > 1:\n        # Found multiple objects, this is a failure\n        raise RuntimeError('Searching for %s/%s returned multiple objects.' %\n                           (value.portal_type, value.Title()))\n    return brains[0].UID"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef migrateFileFields(portal):\n    portal_types = [\n        \"Attachment\",\n        \"ARImport\",\n        \"Instrument\",\n        \"InstrumentCertification\",\n        \"Method\",\n        \"Multifile\",\n        \"Report\",\n        \"ARReport\",\n        \"SamplePoint\"]\n\n    for portal_type in portal_types:\n        # Do the migration\n        migrate_to_blob(\n            portal,\n            portal_type=portal_type,\n            remove_old_value=True)", "response": "This function walks over all attachment types and migrates their FileField\n    fields."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_samples_from_request(self):\n        uids = self.request.form.get(\"uids\", \"\")\n        if isinstance(uids, basestring):\n            uids = uids.split(\",\")\n\n        uids = list(set(uids))\n        if not uids:\n            return []\n\n        # Filter those analysis requests \"reject\" transition is allowed\n        query = dict(portal_type=\"AnalysisRequest\", UID=uids)\n        brains = api.search(query, CATALOG_ANALYSIS_REQUEST_LISTING)\n        samples = map(api.get_object, brains)\n        return filter(lambda ob: wf.isTransitionAllowed(ob, \"reject\"), samples)", "response": "Returns a list of objects coming from the uids request parameter\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of Samples data", "response": "def get_samples_data(self):\n        \"\"\"Returns a list of Samples data (dictionary)\n        \"\"\"\n        for obj in self.get_samples_from_request():\n            yield {\n                \"obj\": obj,\n                \"id\": api.get_id(obj),\n                \"uid\": api.get_uid(obj),\n                \"title\": api.get_title(obj),\n                \"path\": api.get_path(obj),\n                \"url\": api.get_url(obj),\n                \"sample_type\": obj.getSampleTypeTitle(),\n                \"client_title\": obj.getClientTitle(),\n                \"date\": ulocalized_time(obj.created(), long_format=True),\n            }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef returns_json(func):\n    def decorator(*args, **kwargs):\n        instance = args[0]\n        request = getattr(instance, 'request', None)\n        request.response.setHeader(\"Content-Type\", \"application/json\")\n        result = func(*args, **kwargs)\n        return json.dumps(result)\n    return decorator", "response": "Decorator for functions which return JSON\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef returns_super_model(func):\n    def to_super_model(obj):\n        # avoid circular imports\n        from senaite.core.supermodel import SuperModel\n\n        # Object is already a SuperModel\n        if isinstance(obj, SuperModel):\n            return obj\n\n        # Only portal objects are supported\n        if not api.is_object(obj):\n            raise TypeError(\"Expected a portal object, got '{}'\"\n                            .format(type(obj)))\n\n        # Wrap the object into a specific Publication Object Adapter\n        uid = api.get_uid(obj)\n        portal_type = api.get_portal_type(obj)\n\n        adapter = queryAdapter(uid, ISuperModel, name=portal_type)\n        if adapter is None:\n            return SuperModel(uid)\n        return adapter\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        obj = func(*args, **kwargs)\n        if isinstance(obj, (list, tuple)):\n            return map(to_super_model, obj)\n        return to_super_model(obj)\n\n    return wrapper", "response": "Decorator to return standard content objects as SuperModels\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef timeit(threshold=0):\n\n    def inner(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            start = time.time()\n            return_value = func(*args, **kwargs)\n            end = time.time()\n            duration = float(end-start)\n            if duration > threshold:\n                logger.info(\"Execution of '{}{}' took {:2f}s\".format(\n                    func.__name__, args, duration))\n            return return_value\n        return wrapper\n    return inner", "response": "Decorator to log the execution time of a function\nInviteable"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef action_update(self):\n\n        order = []\n        form = self.request.form\n        attachments = form.get(\"attachments\", [])\n\n        for attachment in attachments:\n            # attachment is a form mapping, not a dictionary -> convert\n            values = dict(attachment)\n\n            uid = values.pop(\"UID\")\n            obj = api.get_object_by_uid(uid)\n\n            # delete the attachment if the delete flag is true\n            if values.pop(\"delete\", False):\n                self.delete_attachment(obj)\n                continue\n\n            # remember the order\n            order.append(uid)\n\n            # update the attachment with the given data\n            obj.update(**values)\n            obj.reindexObject()\n\n        # set the attachments order to the annotation storage\n        self.set_attachments_order(order)\n\n        # redirect back to the default view\n        return self.request.response.redirect(self.context.absolute_url())", "response": "Form action enpoint to update the attachments"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef action_add_to_ws(self):\n\n        ws = self.context\n        form = self.request.form\n        attachment_file = form.get('AttachmentFile_file', None)\n        analysis_uid = self.request.get('analysis_uid', None)\n        service_uid = self.request.get('Service', None)\n        AttachmentType = form.get('AttachmentType', '')\n        AttachmentKeys = form.get('AttachmentKeys', '')\n        ReportOption = form.get('ReportOption', 'r')\n\n        # nothing to do if the attachment file is missing\n        if attachment_file is None:\n            logger.warn(\"AttachmentView.action_add_attachment: Attachment file is missing\")\n            return\n\n        if analysis_uid:\n            rc = api.get_tool(\"reference_catalog\")\n            analysis = rc.lookupObject(analysis_uid)\n\n            # create attachment\n            attachment = self.create_attachment(\n                ws,\n                attachment_file,\n                AttachmentType=AttachmentType,\n                AttachmentKeys=AttachmentKeys,\n                ReportOption=ReportOption)\n\n            others = analysis.getAttachment()\n            attachments = []\n            for other in others:\n                attachments.append(other.UID())\n            attachments.append(attachment.UID())\n            analysis.setAttachment(attachments)\n\n            # The metadata for getAttachmentUIDs need to get updated,\n            # otherwise the attachments are not displayed\n            # https://github.com/senaite/bika.lims/issues/521\n            analysis.reindexObject()\n\n        if service_uid:\n            workflow = api.get_tool('portal_workflow')\n\n            # XXX: refactor out dependency to this view.\n            view = api.get_view(\"manage_results\", context=self.context, request=self.request)\n            analyses = self.context.getAnalyses()\n            allowed_states = [\"assigned\", \"unassigned\", \"to_be_verified\"]\n            for analysis in analyses:\n                if analysis.portal_type not in ('Analysis', 'DuplicateAnalysis'):\n                    continue\n                if not analysis.getServiceUID() == service_uid:\n                    continue\n                review_state = workflow.getInfoFor(analysis, 'review_state', '')\n                if review_state not in allowed_states:\n                    continue\n\n                # create attachment\n                attachment = self.create_attachment(\n                    ws,\n                    attachment_file,\n                    AttachmentType=AttachmentType,\n                    AttachmentKeys=AttachmentKeys,\n                    ReportOption=ReportOption)\n\n                others = analysis.getAttachment()\n                attachments = []\n                for other in others:\n                    attachments.append(other.UID())\n                attachments.append(attachment.UID())\n                analysis.setAttachment(attachments)\n\n                # The metadata for getAttachmentUIDs need to get updated,\n                # otherwise the attachments are not displayed\n                # https://github.com/senaite/bika.lims/issues/521\n                analysis.reindexObject()\n\n        if self.request['HTTP_REFERER'].endswith('manage_results'):\n            self.request.response.redirect('{}/manage_results'.format(\n                self.context.absolute_url()))\n        else:\n            self.request.response.redirect(self.context.absolute_url())", "response": "This action creates a new attachment in a worksheet and returns the new ID"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef action_add(self):\n\n        form = self.request.form\n        parent = api.get_parent(self.context)\n        attachment_file = form.get('AttachmentFile_file', None)\n        AttachmentType = form.get('AttachmentType', '')\n        AttachmentKeys = form.get('AttachmentKeys', '')\n        ReportOption = form.get('ReportOption', 'r')\n\n        # nothing to do if the attachment file is missing\n        if attachment_file is None:\n            logger.warn(\"AttachmentView.action_add_attachment: Attachment file is missing\")\n            return\n\n        # create attachment\n        attachment = self.create_attachment(\n            parent,\n            attachment_file,\n            AttachmentType=AttachmentType,\n            AttachmentKeys=AttachmentKeys,\n            ReportOption=ReportOption)\n\n        # append the new UID to the end of the current order\n        self.set_attachments_order(api.get_uid(attachment))\n\n        # handle analysis attachment\n        analysis_uid = form.get(\"Analysis\", None)\n        if analysis_uid:\n            analysis = api.get_object_by_uid(analysis_uid)\n            others = analysis.getAttachment()\n            attachments = []\n            for other in others:\n                attachments.append(other.UID())\n            attachments.append(attachment.UID())\n            analysis.setAttachment(attachments)\n            # The metadata for getAttachmentUIDs need to get updated,\n            # otherwise the attachments are not displayed\n            # https://github.com/senaite/bika.lims/issues/521\n            analysis.reindexObject()\n\n        else:\n            others = self.context.getAttachment()\n            attachments = []\n            for other in others:\n                attachments.append(other.UID())\n            attachments.append(attachment.UID())\n\n            self.context.setAttachment(attachments)\n\n        if self.request['HTTP_REFERER'].endswith('manage_results'):\n            self.request.response.redirect('{}/manage_results'.format(\n                self.context.absolute_url()))\n        else:\n            self.request.response.redirect(self.context.absolute_url())", "response": "This action creates a new attachment and adds it to the current order and returns the new ID"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_attachment(self, container, attachment_file, **kw):\n        filename = getattr(attachment_file, \"filename\", \"Attachment\")\n        attachment = api.create(container, \"Attachment\", title=filename)\n        attachment.edit(AttachmentFile=attachment_file, **kw)\n        attachment.processForm()\n        attachment.reindexObject()\n        logger.info(\"Created new Attachment {} in {}\".format(\n            repr(attachment), repr(container)))\n        return attachment", "response": "Create an Attachment object in the given container"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete an attachment from the AR or Analysis.", "response": "def delete_attachment(self, attachment):\n        \"\"\"Delete attachment from the AR or Analysis\n\n        The attachment will be only deleted if it is not further referenced by\n        another AR/Analysis.\n        \"\"\"\n\n        # Get the holding parent of this attachment\n        parent = None\n        if attachment.getLinkedRequests():\n            # Holding parent is an AR\n            parent = attachment.getRequest()\n        elif attachment.getLinkedAnalyses():\n            # Holding parent is an Analysis\n            parent = attachment.getAnalysis()\n\n        if parent is None:\n            logger.warn(\n                \"Attachment {} is nowhere assigned. This should never happen!\"\n                .format(repr(attachment)))\n            return False\n\n        # Get the other attachments of the holding parent\n        attachments = parent.getAttachment()\n\n        # New attachments to set\n        if attachment in attachments:\n            attachments.remove(attachment)\n\n        # Set the attachments w/o the current attachments\n        parent.setAttachment(attachments)\n\n        retain = False\n\n        # Attachment is referenced by another Analysis\n        if attachment.getLinkedAnalyses():\n            holder = attachment.getAnalysis()\n            logger.info(\"Attachment {} referenced by {} -> RETAIN\"\n                        .format(repr(attachment), repr(holder)))\n            retain = True\n\n        # Attachment is referenced by another AR\n        if attachment.getLinkedRequests():\n            holder = attachment.getRequest()\n            logger.info(\"Attachment {} referenced by {} -> RETAIN\"\n                        .format(repr(attachment), repr(holder)))\n            retain = True\n\n        # Delete attachment finally\n        if retain is False:\n            client = api.get_parent(attachment)\n            client.manage_delObjects([attachment.getId(), ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_attachment_size(self, attachment):\n        fsize = 0\n        file = attachment.getAttachmentFile()\n        if file:\n            fsize = file.get_size()\n        if fsize < 1024:\n            fsize = '%s b' % fsize\n        else:\n            fsize = '%s Kb' % (fsize / 1024)\n        return fsize", "response": "Get the human readable size of the attachment"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary of attachment information", "response": "def get_attachment_info(self, attachment):\n        \"\"\"Returns a dictionary of attachment information\n        \"\"\"\n\n        attachment_uid = api.get_uid(attachment)\n        attachment_file = attachment.getAttachmentFile()\n        attachment_type = attachment.getAttachmentType()\n        attachment_icon = attachment_file.icon\n\n        if callable(attachment_icon):\n            attachment_icon = attachment_icon()\n\n        return {\n            'keywords': attachment.getAttachmentKeys(),\n            'size': self.get_attachment_size(attachment),\n            'name': attachment_file.filename,\n            'Icon': attachment_icon,\n            'type': api.get_uid(attachment_type) if attachment_type else '',\n            'absolute_url': attachment.absolute_url(),\n            'UID': attachment_uid,\n            'report_option': attachment.getReportOption(),\n            'analysis': '',\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of attachments info dictionaries", "response": "def get_attachments(self):\n        \"\"\"Returns a list of attachments info dictionaries\n\n        Original code taken from bika.lims.analysisrequest.view\n        \"\"\"\n\n        attachments = []\n\n        # process AR attachments\n        for attachment in self.context.getAttachment():\n            attachment_info = self.get_attachment_info(attachment)\n            attachments.append(attachment_info)\n\n        # process analyses attachments\n        for analysis in self.context.getAnalyses(full_objects=True):\n            for attachment in analysis.getAttachment():\n                attachment_info = self.get_attachment_info(attachment)\n                attachment_info[\"analysis\"] = analysis.Title()\n                attachment_info[\"analysis_uid\"] = api.get_uid(analysis)\n                attachments.append(attachment_info)\n\n        return attachments"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_sorted_attachments(self):\n        inf = float(\"inf\")\n        order = self.get_attachments_order()\n        attachments = self.get_attachments()\n\n        def att_cmp(att1, att2):\n            _n1 = att1.get('UID')\n            _n2 = att2.get('UID')\n            _i1 = _n1 in order and order.index(_n1) + 1 or inf\n            _i2 = _n2 in order and order.index(_n2) + 1 or inf\n            return cmp(_i1, _i2)\n\n        sorted_attachments = sorted(attachments, cmp=att_cmp)\n        return sorted_attachments", "response": "Returns a sorted list of analysis info dictionaries that are present in the database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of available attachment types", "response": "def get_attachment_types(self):\n        \"\"\"Returns a list of available attachment types\n        \"\"\"\n        bika_setup_catalog = api.get_tool(\"bika_setup_catalog\")\n        attachment_types = bika_setup_catalog(portal_type='AttachmentType',\n                                              is_active=True,\n                                              sort_on=\"sortable_title\",\n                                              sort_order=\"ascending\")\n        return attachment_types"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of analyses from the AR", "response": "def get_analyses(self):\n        \"\"\"Returns a list of analyses from the AR\n        \"\"\"\n        analyses = self.context.getAnalyses(full_objects=True)\n        return filter(self.is_analysis_attachment_allowed, analyses)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the analysis is allowed to be used in the attachment", "response": "def is_analysis_attachment_allowed(self, analysis):\n        \"\"\"Checks if the analysis\n        \"\"\"\n        if analysis.getAttachmentOption() not in [\"p\", \"r\"]:\n            return False\n        if api.get_workflow_status_of(analysis) in [\"retracted\"]:\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef user_can_add_attachments(self):\n        if not self.global_attachments_allowed():\n            return False\n        context = self.context\n        pm = api.get_tool(\"portal_membership\")\n        return pm.checkPermission(AddAttachment, context)", "response": "Checks if the current logged in user is allowed to add attachments"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef user_can_update_attachments(self):\n        context = self.context\n        pm = api.get_tool(\"portal_membership\")\n        return pm.checkPermission(EditResults, context) or \\\n            pm.checkPermission(EditFieldResults, context)", "response": "Checks if the current logged in user is allowed to update attachments"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef user_can_delete_attachments(self):\n        context = self.context\n        user = api.get_current_user()\n        if not self.is_ar_editable():\n            return False\n        return (self.user_can_add_attachments() and\n                not user.allowed(context, [\"Client\"])) or \\\n            self.user_can_update_attachments()", "response": "Checks if the current logged in user is allowed to delete attachments"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving the whole storage from the cache.", "response": "def flush(self):\n        \"\"\"Remove the whole storage\n        \"\"\"\n        annotation = self.get_annotation()\n        if annotation.get(ATTACHMENTS_STORAGE) is not None:\n            del annotation[ATTACHMENTS_STORAGE]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremember the attachments order", "response": "def set_attachments_order(self, order):\n        \"\"\"Remember the attachments order\n        \"\"\"\n        # append single uids to the order\n        if isinstance(order, basestring):\n            new_order = self.storage.get(\"order\", [])\n            new_order.append(order)\n            order = new_order\n        self.storage.update({\"order\": order})"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dictionary with the settings for the analysis service that matches with the uid provided.", "response": "def getAnalysisServiceSettings(self, uid):\n        \"\"\"Returns a dictionary with the settings for the analysis service that\n           match with the uid provided.\n\n        If there are no settings for the analysis service and template, returns\n        a dictionary with the key 'uid'\n        \"\"\"\n        settings = self.getAnalysisServicesSettings()\n        sets = [s for s in settings if s.get(\"uid\", \"\") == uid]\n        return sets[0] if sets else {\"uid\": uid}"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove the service passed in from the AR Template. Returns True if the service has been removed successfully.", "response": "def remove_service(self, service):\n        \"\"\"Removes the service passed in from the services offered by the\n        current Template. If the Analysis Service passed in is not assigned to\n        this Analysis Template, returns False.\n        :param service: the service to be removed from this AR Template\n        :type service: AnalysisService\n        :return: True if the AnalysisService has been removed successfully\n        \"\"\"\n        uid = api.get_uid(service)\n\n        # Remove the service from the referenced services\n        services = self.getAnalyses()\n        num_services = len(services)\n        services = [item for item in services\n                    if item.get(\"service_uid\", \"\") != uid]\n        removed = len(services) < num_services\n        self.setAnalyses(services)\n\n        # Remove the service from the settings map\n        settings = self.getAnalysisServicesSettings()\n        settings = [item for item in settings if item.get('uid', '') != uid]\n        self.setAnalysisServicesSettings(settings)\n\n        return removed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nresolve a UID to an object.", "response": "def _get_object(context, value):\n    \"\"\"Resolve a UID to an object.\n\n    :param context: context is the object containing the field's schema.\n    :type context: BaseContent\n    :param value: A UID.\n    :type value: string\n    :return: Returns a Content object or None.\n    :rtype: BaseContent\n    \"\"\"\n    if not value:\n        return None\n    if api.is_brain(value):\n        return api.get_object(value)\n    if api.is_object(value):\n        return value\n    if api.is_uid(value):\n        uc = api.get_tool('uid_catalog', context=context)\n        brains = uc(UID=value)\n        if len(brains) == 0:\n            # Broken Reference!\n            logger.warn(\"Reference on {} with UID {} is broken!\"\n                        .format(repr(context), value))\n            return None\n        return brains[0].getObject()\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_backreferences(context, relationship=None, as_brains=None):\n\n    instance = context.aq_base\n    raw_backrefs = get_storage(instance)\n\n    if not relationship:\n        assert not as_brains, \"You cannot use as_brains with no relationship\"\n        return raw_backrefs\n\n    backrefs = list(raw_backrefs.get(relationship, []))\n    if not backrefs:\n        return []\n\n    if not as_brains:\n        return backrefs\n\n    cat = _get_catalog_for_uid(backrefs[0])\n    return cat(UID=backrefs)", "response": "Returns all objects which use a UIDReferenceField to reference the given context."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_relationship_key(self, context):\n        if not self.relationship:\n            return context.portal_type + self.getName()\n        return self.relationship", "response": "Return the configured relationship key or generate a new one"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef link_reference(self, source, target):\n        target_uid = api.get_uid(target)\n        # get the annotation storage key\n        key = self.get_relationship_key(target)\n        # get all backreferences from the source\n        # N.B. only like this we get the persistent mapping!\n        backrefs = get_backreferences(source, relationship=None)\n        if key not in backrefs:\n            backrefs[key] = PersistentList()\n        if target_uid not in backrefs[key]:\n            backrefs[key].append(target_uid)\n        return True", "response": "Link the target to the source"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unlink_reference(self, source, target):\n        target_uid = api.get_uid(target)\n        # get the storage key\n        key = self.get_relationship_key(target)\n        # get all backreferences from the source\n        # N.B. only like this we get the persistent mapping!\n        backrefs = get_backreferences(source, relationship=None)\n        if key not in backrefs:\n            logger.warn(\n                \"Referenced object {} has no backreferences for the key {}\"\n                .format(repr(source), key))\n            return False\n        if target_uid not in backrefs[key]:\n            logger.warn(\"Target {} was not linked by {}\"\n                        .format(repr(target), repr(source)))\n            return False\n        backrefs[key].remove(target_uid)\n        return True", "response": "Unlink the target from the source"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_object(self, context, value):\n        if not value:\n            return None\n        obj = _get_object(context, value)\n        if obj is None:\n            logger.warning(\n                \"{}.{}: Resolving UIDReference failed for {}.  No object will \"\n                \"be returned.\".format(context, self.getName(), value))\n        return obj", "response": "Resolve a UID to an object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntake a brain or object or UID and returns a UID.", "response": "def get_uid(self, context, value):\n        \"\"\"Takes a brain or object (or UID), and returns a UID.\n\n        :param context: context is the object who's schema contains this field.\n        :type context: BaseContent\n        :param value: Brain, object, or UID.\n        :type value: Any\n        :return: resolved UID.\n        :rtype: string\n        \"\"\"\n        # Empty string or list with single empty string, are commonly\n        # passed to us from form submissions\n        if not value or value == ['']:\n            ret = ''\n        elif api.is_brain(value):\n            ret = value.UID\n        elif api.is_at_content(value) or api.is_dexterity_content(value):\n            ret = value.UID()\n        elif api.is_uid(value):\n            ret = value\n        else:\n            raise ReferenceException(\"{}.{}: Cannot resolve UID for {}\".format(\n                context, self.getName(), value))\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngrabbing the stored value and resolve object ( s ) from UID catalog.", "response": "def get(self, context, **kwargs):\n        \"\"\"Grab the stored value, and resolve object(s) from UID catalog.\n\n        :param context: context is the object who's schema contains this field.\n        :type context: BaseContent\n        :param kwargs: kwargs are passed directly to the underlying get.\n        :type kwargs: dict\n        :return: object or list of objects for multiValued fields.\n        :rtype: BaseContent | list[BaseContent]\n        \"\"\"\n        value = StringField.get(self, context, **kwargs)\n        if not value:\n            return [] if self.multiValued else None\n        if self.multiValued:\n            # Only return objects which actually exist; this is necessary here\n            # because there are no HoldingReferences. This opens the\n            # possibility that deletions leave hanging references.\n            ret = filter(\n                lambda x: x, [self.get_object(context, uid) for uid in value])\n        else:\n            ret = self.get_object(context, value)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getRaw(self, context, aslist=False, **kwargs):\n        value = StringField.get(self, context, **kwargs)\n        if not value:\n            return [] if self.multiValued else None\n        if self.multiValued:\n            ret = value\n        else:\n            ret = self.get_uid(context, value)\n            if aslist:\n                ret = [ret]\n        return ret", "response": "Grab the stored value and return it directly as UIDs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_backreferences(self, context, items, **kwargs):\n\n        # Don't set any references during initialization.\n        # This might cause a recursion error when calling `getRaw` to fetch the\n        # current set UIDs!\n        initializing = kwargs.get('_initializing_', False)\n        if initializing:\n            return\n\n        # UID of the current object\n        uid = api.get_uid(context)\n        # current set UIDs\n        raw = self.getRaw(context) or []\n        # handle single reference fields\n        if isinstance(raw, basestring):\n            raw = [raw, ]\n        cur = set(raw)\n        # UIDs to be set\n        new = set(map(api.get_uid, items))\n        # removed UIDs\n        removed = cur.difference(new)\n\n        # Unlink removed UIDs from the source\n        for uid in removed:\n            source = api.get_object_by_uid(uid, None)\n            if source is None:\n                logger.warn(\"UID {} does not exist anymore\".format(uid))\n                continue\n            self.unlink_reference(source, context)\n\n        # Link backrefs\n        for item in items:\n            self.link_reference(item, context)", "response": "Set the back references on the linked items which point to the current context."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the value of the object in the specified context to the given value.", "response": "def set(self, context, value, **kwargs):\n        \"\"\"Accepts a UID, brain, or an object (or a list of any of these),\n        and stores a UID or list of UIDS.\n\n        :param context: context is the object who's schema contains this field.\n        :type context: BaseContent\n        :param value: A UID, brain or object (or a sequence of these).\n        :type value: Any\n        :param kwargs: kwargs are passed directly to the underlying get.\n        :type kwargs: dict\n        :return: None\n        \"\"\"\n        if self.multiValued:\n            if not value:\n                value = []\n            if type(value) not in (list, tuple):\n                value = [value, ]\n            ret = [self.get_object(context, val) for val in value if val]\n            self._set_backreferences(context, ret, **kwargs)\n            uids = [self.get_uid(context, r) for r in ret if r]\n            StringField.set(self, context, uids, **kwargs)\n        else:\n            # Sometimes we get given a list here with an empty string.\n            # This is generated by html forms with empty values.\n            # This is a single-valued field though, so:\n            if isinstance(value, list) and value:\n                if len(value) > 1:\n                    logger.warning(\n                        \"Found values '\\'{}\\'' for singleValued field <{}>.{} \"\n                        \"- using only the first value in the list.\".format(\n                            '\\',\\''.join(value), context.UID(), self.getName()))\n                value = value[0]\n            ret = self.get_object(context, value)\n            if ret:\n                self._set_backreferences(context, [ret, ], **kwargs)\n                uid = self.get_uid(context, ret)\n                StringField.set(self, context, uid, **kwargs)\n            else:\n                StringField.set(self, context, '', **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn whether the field is visible in a given mode", "response": "def isVisible(self, field, mode=\"view\", default=\"visible\"):\n        \"\"\"Returns whether the field is visible in a given mode\n        \"\"\"\n        if mode != \"edit\":\n            return default\n\n        # If this is a Secondary Analysis Request, this field is not editable\n        if IAnalysisRequestSecondary.providedBy(self.context):\n            return \"invisible\"\n\n        return self.context.isOpen() and \"visible\" or \"invisible\""}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nformatting a keyword in the order they appear in the Analysis Services.", "response": "def format_keyword(keyword):\n    \"\"\"\n    Removing special character from a keyword. Analysis Services must have\n    this kind of keywords. E.g. if assay name from the Instrument is\n    'HIV-1 2.0', an AS must be created on Bika with the keyword 'HIV120'\n    \"\"\"\n    import re\n    result = ''\n    if keyword:\n        result = re.sub(r\"\\W\", \"\", keyword)\n        result = re.sub(\"_\", \"\", result)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef guard_activate(analysis_service):\n    calculation = analysis_service.getCalculation()\n    if not calculation:\n        return True\n\n    # If the calculation is inactive, we cannot activate the service\n    if not api.is_active(calculation):\n        return False\n\n    # All services that we depend on to calculate our result are active or we\n    # don't depend on other services.\n    dependencies = calculation.getDependentServices()\n    for dependency in dependencies:\n        if not api.is_active(dependency):\n            return False\n\n    return True", "response": "Returns whether the transition activate can be performed for the analysis service passed in\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn whether the transition deactivate can be performed for the passed in analysis service", "response": "def guard_deactivate(analysis_service):\n    \"\"\"Returns whether the transition deactivate can be performed for the\n    analysis service passed in\n    \"\"\"\n    for dependant in analysis_service.getServiceDependants():\n        status = api.get_workflow_status_of(dependant)\n        if status == \"active\":\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self):\n        super(AnalysisRequestsView, self).update()\n\n        self.workflow = api.get_tool(\"portal_workflow\")\n        self.member = self.mtool.getAuthenticatedMember()\n        self.roles = self.member.getRoles()\n\n        setup = api.get_bika_setup()\n\n        # remove `to_be_sampled` filter\n        if not setup.getSamplingWorkflowEnabled():\n            self.review_states = filter(\n                lambda x: x.get(\"id\") != \"to_be_sampled\", self.review_states)\n\n        # remove `scheduled_sampling` filter\n        if not setup.getScheduleSamplingEnabled():\n            self.review_states = filter(\n                lambda x: x.get(\"id\") != \"scheduled_sampling\",\n                self.review_states)\n\n        # remove `to_be_preserved` filter\n        if not setup.getSamplePreservationEnabled():\n            self.review_states = filter(\n                lambda x: x.get(\"id\") != \"to_be_preserved\", self.review_states)\n\n        # remove `rejected` filter\n        if not setup.getRejectionReasons():\n            self.review_states = filter(\n                lambda x: x.get(\"id\") != \"rejected\", self.review_states)\n\n        self.hideclientlink = \"RegulatoryInspector\" in self.roles \\\n            and \"Manager\" not in self.roles \\\n            and \"LabManager\" not in self.roles \\\n            and \"LabClerk\" not in self.roles\n\n        if self.context.portal_type == \"AnalysisRequestsFolder\" and \\\n                (self.mtool.checkPermission(AddAnalysisRequest, self.context)):\n            self.context_actions[_(\"Add\")] = \\\n                {\"url\": \"ar_add?ar_count=1\",\n                 'permission': 'Add portal content',\n                 \"icon\": \"++resource++bika.lims.images/add.png\"}\n\n        self.editresults = -1\n        self.clients = {}\n        # self.user_is_preserver = \"Preserver\" in self.roles\n        # Printing workflow enabled?\n        # If not, remove the Column\n        self.printwfenabled = \\\n            self.context.bika_setup.getPrintingWorkflowEnabled()\n        printed_colname = \"Printed\"\n        if not self.printwfenabled and printed_colname in self.columns:\n            # Remove \"Printed\" columns\n            del self.columns[printed_colname]\n            tmprvs = []\n            for rs in self.review_states:\n                tmprs = rs\n                tmprs[\"columns\"] = [c for c in rs.get(\"columns\", []) if\n                                    c != printed_colname]\n                tmprvs.append(tmprs)\n            self.review_states = tmprvs\n        elif self.printwfenabled:\n            # Print button to choose multiple ARs and print them.\n            review_states = []\n            for review_state in self.review_states:\n                review_state.get(\"custom_transitions\", []).extend(\n                    [{\"id\": \"print_sample\",\n                      \"title\": _(\"Print\"),\n                      \"url\": \"workflow_action?action=print_sample\"}, ])\n                review_states.append(review_state)\n            self.review_states = review_states\n\n        # Only \"senaite.core: ManageAnalysisRequests\" may see the copy to new button.\n        # elsewhere it is hacked in where required.\n        if self.copy_to_new_allowed:\n            review_states = []\n            for review_state in self.review_states:\n                review_state.get(\"custom_transitions\", []).extend(\n                    [{\"id\": \"copy_to_new\",\n                      \"title\": _(\"Copy to new\"),\n                      \"url\": \"workflow_action?action=copy_to_new\"}, ])\n                review_states.append(review_state)\n            self.review_states = review_states", "response": "Called before the listing renders\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the percentage of completeness of the Analysis Request", "response": "def get_progress_percentage(self, ar_brain):\n        \"\"\"Returns the percentage of completeness of the Analysis Request\n        \"\"\"\n        review_state = ar_brain.review_state\n        if review_state == \"published\":\n            return 100\n\n        numbers = ar_brain.getAnalysesNum\n\n        num_analyses = numbers[1] or 0\n        if not num_analyses:\n            return 0\n\n        # [verified, total, not_submitted, to_be_verified]\n        num_to_be_verified = numbers[3] or 0\n        num_verified = numbers[0] or 0\n\n        # 2 steps per analysis (submit, verify) plus one step for publish\n        max_num_steps = (num_analyses * 2) + 1\n        num_steps = num_to_be_verified + (num_verified * 2)\n        if not num_steps:\n            return 0\n        if num_steps > max_num_steps:\n            return 100\n        return (num_steps * 100) / max_num_steps"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef folderitem(self, obj, item, index):\n\n        title = obj.Title()\n        description = obj.Description()\n        url = obj.absolute_url()\n\n        item[\"Description\"] = description\n\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n\n        retention_period = obj.getRetentionPeriod()\n        if retention_period:\n            hours = retention_period[\"hours\"]\n            minutes = retention_period[\"minutes\"]\n            days = retention_period[\"days\"]\n            item[\"RetentionPeriod\"] = _(\"hours: {} minutes: {} days: {}\"\n                                        .format(hours, minutes, days))\n        else:\n            item[\"RetentionPeriod\"] = \"\"\n\n        sample_matrix = obj.getSampleMatrix()\n        if sample_matrix:\n            title = sample_matrix.Title()\n            url = sample_matrix.absolute_url()\n            item[\"SampleMatrix\"] = title\n            item[\"replace\"][\"SampleMatrix\"] = get_link(url, value=title)\n        else:\n            item[\"SampleMatrix\"] = \"\"\n\n        container_type = obj.getContainerType()\n        if container_type:\n            title = container_type.Title()\n            url = container_type.absolute_url()\n            item[\"ContainerType\"] = title\n            item[\"replace\"][\"ContainerType\"] = get_link(url, value=title)\n        else:\n            item[\"ContainerType\"] = \"\"\n\n        sample_points = obj.getSamplePoints()\n        if sample_points:\n            links = map(\n                lambda sp: get_link(sp.absolute_url(),\n                                    value=sp.Title(),\n                                    css_class=\"link\"),\n                sample_points)\n            item[\"replace\"][\"getSamplePoints\"] = \", \".join(links)\n        else:\n            item[\"getSamplePoints\"] = \"\"\n\n        return item", "response": "This method is called when the user adds a new item to the template. It creates a new item based on the properties of the object and adds the links to the item."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\napplying new properties to the item that is currently being rendered as a row in the list .", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Applies new properties to the item (Client) that is currently being\n        rendered as a row in the list\n\n        :param obj: client to be rendered as a row in the list\n        :param item: dict representation of the client, suitable for the list\n        :param index: current position of the item within the list\n        :type obj: ATContentType/DexterityContentType\n        :type item: dict\n        :type index: int\n        :return: the dict representation of the item\n        :rtype: dict\n        \"\"\"\n\n        url = obj.absolute_url()\n        title = obj.Title()\n\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n\n        item[\"getEffectiveDate\"] = self.ulocalized_time(\n            obj.getEffectiveDate())\n\n        item[\"getExpirationDate\"] = self.ulocalized_time(\n            obj.getExpirationDate())\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating Reflex Rules inactive_state otherwise they don t have it by default.", "response": "def update_reflexrules_workflow_state(portal):\n    \"\"\"\n    Updates Reflex Rules' inactive_state, otherwise they don't have it by\n    default.\n    :param portal: Portal object\n    :return: None\n    \"\"\"\n    wf_tool = getToolByName(portal, 'portal_workflow')\n    logger.info(\"Updating Reflex Rules' 'inactive_state's...\")\n    wf = wf_tool.getWorkflowById(\"bika_inactive_workflow\")\n    uc = api.get_tool('portal_catalog')\n    r_rules = uc(portal_type='ReflexRule')\n    for rr in r_rules:\n        obj = rr.getObject()\n        wf.updateRoleMappingsFor(obj)\n        obj.reindexObject()\n    logger.info(\"Reflex Rules' 'inactive_state's were updated.\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_form(self, instance, field, form, empty_marker=None,\n                     emptyReturnsMarker=False):\n        \"\"\"\n        Basic impl for form processing in a widget plus allowing empty\n        values to be saved\n        \"\"\"\n\n        # a poor workaround for Plone repeating itself.\n        # XXX this is important XXX\n        key = field.getName() + '_value'\n        if key in instance.REQUEST:\n            return instance.REQUEST[key], {}\n        value = form.get(field.getName(), empty_marker)\n        # When a recordswidget's visibility is defined as hidden\n        # '...visible={'view': 'hidden', 'edit': 'hidden'},...' the template\n        # displays it as an input field with the attribute 'value' as a string\n        # 'value=\"[{:},{:}]\"'. This makes the system save the content of the\n        # widget as the string instead of a dictionary inside a list, so we\n        # need to check if the variable contains a python object as a string.\n        if value and value is not empty_marker and isinstance(value, str):\n            import ast\n            try:\n                value = ast.literal_eval(form.get(field.getName()))\n            except:\n                # cannot resolve string as a list!\n                return empty_marker\n\n        if not value:\n            return value, {}\n        if value is empty_marker:\n            return empty_marker\n        if emptyReturnsMarker and value == '':\n            return empty_marker\n\n        # we make sure that empty \"value\" inputs are saved as \"\" empty string.\n        for i in range(len(value)):\n            value[i] = dict(value[i])\n            if 'value' not in value[i]:\n                value[i]['value'] = ''\n\n        instance.REQUEST[key] = value\n        return value, {}", "response": "Basic implementation for form processing in a record widget plus allowing empty_marker values to be saved as empty string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the value of the same named field on the proxy object", "response": "def get(self, instance, **kwargs):\n        \"\"\"retrieves the value of the same named field on the proxy object\n        \"\"\"\n        # The default value\n        default = self.getDefault(instance)\n\n        # Retrieve the proxy object\n        proxy_object = self.get_proxy(instance)\n\n        # Return None if we could not find a proxied object, e.g. through\n        # the proxy expression 'context.getSample()' on an AR\n        if proxy_object is None:\n            logger.debug(\"Expression '{}' did not return a valid Proxy Object on {}\"\n                         .format(self.proxy, instance))\n            return default\n\n        # Lookup the proxied field by name\n        field_name = self.getName()\n        field = proxy_object.getField(field_name)\n\n        # Bail out if the proxy object has no identical named field\n        if field is None:\n            raise KeyError(\"Object '{}' with id '{}' has no field named '{}'\".format(\n                proxy_object.portal_type, proxy_object.getId(), field_name))\n\n        # return the value of the proxy field\n        return field.get(proxy_object)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set(self, instance, value, **kwargs):\n        # Retrieve the proxy object\n        proxy_object = self.get_proxy(instance)\n\n        # Return None if we could not find a proxied object, e.g. through\n        # the proxy expression 'context.getSample()' on an AR\n        if not proxy_object:\n            logger.debug(\"Expression '{}' did not return a valid Proxy Object on {}\"\n                         .format(self.proxy, instance))\n            return None\n\n        # Lookup the proxied field by name\n        field_name = self.getName()\n        field = proxy_object.getField(field_name)\n\n        # Bail out if the proxy object has no identical named field.\n        if field is None:\n            raise KeyError(\"Object '{}' with id '{}' has no field named '{}'\".format(\n                proxy_object.portal_type, proxy_object.getId(), field_name))\n\n        # set the value on the proxy object\n        field.set(proxy_object, value, **kwargs)\n\n        # get the current time\n        now = DateTime.DateTime()\n\n        # update the modification date of the proxied object\n        proxy_object.setModificationDate(now)\n\n        # update the modification date of the holding object\n        instance.setModificationDate(now)", "response": "writes the value to the same named field on the proxied object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getForm(instrument_name, request):\n    d = {'infile': request.form[instrument_name + '_file'],\n         'fileformat': request.form[instrument_name + '_format'],\n         'artoapply': request.form[instrument_name + '_artoapply'],\n         'override': request.form[instrument_name + '_override'],\n         'sample': request.form.get(instrument_name + '_sample',\n                                    'requestid'),\n         'instrument': request.form.get(instrument_name + '_instrument', None)}\n    return d", "response": "Returns the dictionary with the requests results."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimport the Sysmex XS - 500i analysis results from the specified file.", "response": "def Import(context, request, instrumentname='sysmex_xs_500i'):\n    \"\"\" Sysmex XS - 500i analysis results\n    \"\"\"\n    # I don't really know how this file works, for this reason I added an 'Analysis Service selector'.\n    # If non Analysis Service is selected, each 'data' column will be interpreted as a different Analysis Service. In\n    # the case that an Analysis Service was selected, all the data columns would be interpreted as different data from\n    # an unique Analysis Service.\n    formitems = getForm(instrumentname, request)\n    infile = formitems['infile']\n    fileformat = formitems['fileformat']\n    artoapply = formitems['artoapply']\n    override = formitems['override']\n    instrument = formitems['instrument']\n    errors = []\n    logs = []\n    warns = []\n\n    # Load the most suitable parser according to file extension/options/etc...\n    parser = None\n    if not hasattr(infile, 'filename'):\n        errors.append(_(\"No file selected\"))\n    if fileformat == 'csv':\n        # Get the Analysis Service selected, if there is one.\n        analysis = request.form.get('analysis_service', None)\n        if analysis:\n            # Get default result key\n            defaultresult = request.form.get('default_result', None)\n            # Rise an error if default result is missing.\n            parser = SysmexXS500iCSVParser(infile, analysis, defaultresult) if defaultresult \\\n                     else errors.append(t(_(\"You should introduce a default result key.\",\n                                             mapping={\"fileformat\": fileformat})))\n        else:\n            parser = SysmexXS500iCSVParser(infile)\n    else:\n        errors.append(t(_(\"Unrecognized file format ${fileformat}\",\n                          mapping={\"fileformat\": fileformat})))\n\n    if parser:\n        # Load the importer\n        status = ['sample_received', 'attachment_due', 'to_be_verified']\n        if artoapply == 'received':\n            status = ['sample_received']\n        elif artoapply == 'received_tobeverified':\n            status = ['sample_received', 'attachment_due', 'to_be_verified']\n\n        over = [False, False]\n        if override == 'nooverride':\n            over = [False, False]\n        elif override == 'override':\n            over = [True, False]\n        elif override == 'overrideempty':\n            over = [True, True]\n\n        importer = SysmexXS500iImporter(parser=parser,\n                                        context=context,\n                                        allowed_ar_states=status,\n                                        allowed_analysis_states=None,\n                                        override=over,\n                                        instrument_uid=instrument)\n        tbex = ''\n        try:\n            importer.process()\n        except:\n            tbex = traceback.format_exc()\n        errors = importer.errors\n        logs = importer.logs\n        warns = importer.warns\n        if tbex:\n            errors.append(tbex)\n\n    results = {'errors': errors, 'log': logs, 'warns': warns}\n\n    return json.dumps(results)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_analysisrequest(client, request, values, analyses=None,\n                           partitions=None, specifications=None, prices=None):\n    \"\"\"This is meant for general use and should do everything necessary to\n    create and initialise an AR and any other required auxilliary objects\n    (Sample, SamplePartition, Analysis...)\n    :param client:\n        The container (Client) in which the ARs will be created.\n    :param request:\n        The current Request object.\n    :param values:\n        a dict, where keys are AR|Sample schema field names.\n    :param analyses:\n        Analysis services list.  If specified, augments the values in\n        values['Analyses']. May consist of service objects, UIDs, or Keywords.\n    :param partitions:\n        A list of dictionaries, if specific partitions are required.  If not\n        specified, AR's sample is created with a single partition.\n    :param specifications:\n        These values augment those found in values['Specifications']\n    :param prices:\n        Allow different prices to be set for analyses.  If not set, prices\n        are read from the associated analysis service.\n    \"\"\"\n    # Don't pollute the dict param passed in\n    values = dict(values.items())\n\n    # Create the Analysis Request\n    ar = _createObjectByType('AnalysisRequest', client, tmpID())\n    ar.processForm(REQUEST=request, values=values)\n\n    # Resolve the services uids and set the analyses for this Analysis Request\n    service_uids = get_services_uids(context=client, values=values,\n                                     analyses_serv=analyses)\n    ar.setAnalyses(service_uids, prices=prices, specs=specifications)\n\n    # Handle secondary Analysis Request\n    primary = ar.getPrimaryAnalysisRequest()\n    if primary:\n        # Mark the secondary with the `IAnalysisRequestSecondary` interface\n        alsoProvides(ar, IAnalysisRequestSecondary)\n\n        # Rename the secondary according to the ID server setup\n        renameAfterCreation(ar)\n\n        # Set dates to match with those from the primary\n        ar.setDateSampled(primary.getDateSampled())\n        ar.setSamplingDate(primary.getSamplingDate())\n        ar.setDateReceived(primary.getDateReceived())\n\n        # Force the transition of the secondary to received and set the\n        # description/comment in the transition accordingly.\n        if primary.getDateReceived():\n            primary_id = primary.getId()\n            comment = \"Auto-received. Secondary Sample of {}\".format(primary_id)\n            changeWorkflowState(ar, AR_WORKFLOW_ID, \"sample_received\",\n                                action=\"receive\", comments=comment)\n\n            # Mark the secondary as received\n            alsoProvides(ar, IReceived)\n\n            # Initialize analyses\n            do_action_to_analyses(ar, \"initialize\")\n\n            # Reindex the AR\n            ar.reindexObject()\n\n            # In \"received\" state already\n            return ar\n\n    # Try first with no sampling transition, cause it is the most common config\n    success, message = doActionFor(ar, \"no_sampling_workflow\")\n    if not success:\n        doActionFor(ar, \"to_be_sampled\")\n    return ar", "response": "This function creates an Analysis Request object and sets it as the current Analysis Request object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _resolve_items_to_service_uids(items):\n    def resolve_to_uid(item):\n        if api.is_uid(item):\n            return item\n        elif IAnalysisService.providedBy(item):\n            return item.UID()\n        elif IRoutineAnalysis.providedBy(item):\n            return item.getServiceUID()\n\n        bsc = api.get_tool(\"bika_setup_catalog\")\n        brains = bsc(portal_type='AnalysisService', getKeyword=item)\n        if brains:\n            return brains[0].UID\n        brains = bsc(portal_type='AnalysisService', title=item)\n        if brains:\n            return brains[0].UID\n        raise RuntimeError(\n            str(item) + \" should be the UID, title, keyword \"\n                        \" or title of an AnalysisService.\")\n\n    # Maybe only a single item was passed\n    if type(items) not in (list, tuple):\n        items = [items, ]\n    service_uids = map(resolve_to_uid, list(set(items)))\n    return list(set(service_uids))", "response": "Returns a list of service uids without duplicates based on the items passed in."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef notify_rejection(analysisrequest):\n\n    # We do this imports here to avoid circular dependencies until we deal\n    # better with this notify_rejection thing.\n    from bika.lims.browser.analysisrequest.reject import \\\n        AnalysisRequestRejectPdfView, AnalysisRequestRejectEmailView\n\n    arid = analysisrequest.getId()\n\n    # This is the template to render for the pdf that will be either attached\n    # to the email and attached the the Analysis Request for further access\n    tpl = AnalysisRequestRejectPdfView(analysisrequest, analysisrequest.REQUEST)\n    html = tpl.template()\n    html = safe_unicode(html).encode('utf-8')\n    filename = '%s-rejected' % arid\n    pdf_fn = tempfile.mktemp(suffix=\".pdf\")\n    pdf = createPdf(htmlreport=html, outfile=pdf_fn)\n    if pdf:\n        # Attach the pdf to the Analysis Request\n        attid = analysisrequest.aq_parent.generateUniqueId('Attachment')\n        att = _createObjectByType(\n            \"Attachment\", analysisrequest.aq_parent, attid)\n        att.setAttachmentFile(open(pdf_fn))\n        # Awkward workaround to rename the file\n        attf = att.getAttachmentFile()\n        attf.filename = '%s.pdf' % filename\n        att.setAttachmentFile(attf)\n        att.unmarkCreationFlag()\n        renameAfterCreation(att)\n        analysisrequest.addAttachment(att)\n        os.remove(pdf_fn)\n\n    # This is the message for the email's body\n    tpl = AnalysisRequestRejectEmailView(\n        analysisrequest, analysisrequest.REQUEST)\n    html = tpl.template()\n    html = safe_unicode(html).encode('utf-8')\n\n    # compose and send email.\n    mailto = []\n    lab = analysisrequest.bika_setup.laboratory\n    mailfrom = formataddr((encode_header(lab.getName()), lab.getEmailAddress()))\n    mailsubject = _('%s has been rejected') % arid\n    contacts = [analysisrequest.getContact()] + analysisrequest.getCCContact()\n    for contact in contacts:\n        name = to_utf8(contact.getFullname())\n        email = to_utf8(contact.getEmailAddress())\n        if email:\n            mailto.append(formataddr((encode_header(name), email)))\n    if not mailto:\n        return False\n    mime_msg = MIMEMultipart('related')\n    mime_msg['Subject'] = mailsubject\n    mime_msg['From'] = mailfrom\n    mime_msg['To'] = ','.join(mailto)\n    mime_msg.preamble = 'This is a multi-part MIME message.'\n    msg_txt = MIMEText(html, _subtype='html')\n    mime_msg.attach(msg_txt)\n    if pdf:\n        attachPdf(mime_msg, pdf, filename)\n\n    try:\n        host = getToolByName(analysisrequest, 'MailHost')\n        host.send(mime_msg.as_string(), immediate=True)\n    except:\n        logger.warning(\n            \"Email with subject %s was not sent (SMTP connection error)\" % mailsubject)\n\n    return True", "response": "Notifies via email that a given Analysis Request has been rejected."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_partition(analysis_request, request, analyses, sample_type=None,\n                     container=None, preservation=None, skip_fields=None,\n                     remove_primary_analyses=True):\n    \"\"\"\n    Creates a partition for the analysis_request (primary) passed in\n    :param analysis_request: uid/brain/object of IAnalysisRequest type\n    :param request: the current request object\n    :param analyses: uids/brains/objects of IAnalysis type\n    :param sampletype: uid/brain/object of SampleType\n    :param container: uid/brain/object of Container\n    :param preservation: uid/brain/object of Preservation\n    :param skip_fields: names of fields to be skipped on copy from primary\n    :param remove_primary_analyses: removes the analyses from the parent\n    :return: the new partition\n    \"\"\"\n    partition_skip_fields = [\n        \"Analyses\",\n        \"Attachment\",\n        \"Client\",\n        \"Profile\",\n        \"Profiles\",\n        \"RejectionReasons\",\n        \"Remarks\",\n        \"ResultsInterpretation\",\n        \"ResultsInterpretationDepts\",\n        \"Sample\",\n        \"Template\",\n        \"creation_date\",\n        \"id\",\n        \"modification_date\",\n        \"ParentAnalysisRequest\",\n        \"PrimaryAnalysisRequest\",\n    ]\n    if skip_fields:\n        partition_skip_fields.extend(skip_fields)\n        partition_skip_fields = list(set(partition_skip_fields))\n\n    # Copy field values from the primary analysis request\n    ar = api.get_object(analysis_request)\n    record = fields_to_dict(ar, partition_skip_fields)\n\n    # Update with values that are partition-specific\n    record.update({\n        \"InternalUse\": True,\n        \"ParentAnalysisRequest\": api.get_uid(ar),\n    })\n    if sample_type is not None:\n        record[\"SampleType\"] = sample_type and api.get_uid(sample_type) or \"\"\n    if container is not None:\n        record[\"Container\"] = container and api.get_uid(container) or \"\"\n    if preservation is not None:\n        record[\"Preservation\"] = preservation and api.get_uid(preservation) or \"\"\n\n    # Create the Partition\n    client = ar.getClient()\n    analyses = list(set(map(api.get_object, analyses)))\n    services = map(lambda an: an.getAnalysisService(), analyses)\n    specs = ar.getSpecification()\n    specs = specs and specs.getResultsRange() or []\n    partition = create_analysisrequest(client, request=request, values=record,\n                                       analyses=services, specifications=specs)\n\n    # Remove analyses from the primary\n    if remove_primary_analyses:\n        analyses_ids = map(api.get_id, analyses)\n        ar.manage_delObjects(analyses_ids)\n\n    # Reindex Parent Analysis Request\n    ar.reindexObject(idxs=[\"isRootAncestor\"])\n\n    # Manually set the Date Received to match with its parent. This is\n    # necessary because crar calls to processForm, so DateReceived is not\n    # set because the partition has not been received yet\n    partition.setDateReceived(ar.getDateReceived())\n    partition.reindexObject(idxs=\"getDateReceived\")\n\n    # Force partition to same status as the primary\n    status = api.get_workflow_status_of(ar)\n    changeWorkflowState(partition, \"bika_ar_workflow\", status)\n    if IReceived.providedBy(ar):\n        alsoProvides(partition, IReceived)\n\n    # And initialize the analyses the partition contains. This is required\n    # here because the transition \"initialize\" of analyses rely on a guard,\n    # so the initialization can only be performed when the sample has been\n    # received (DateReceived is set)\n    ActionHandlerPool.get_instance().queue_pool()\n    for analysis in partition.getAnalyses(full_objects=True):\n        doActionFor(analysis, \"initialize\")\n    ActionHandlerPool.get_instance().resume()\n    return partition", "response": "Creates a new partition for the analysis request passed in."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary with the field values of the object passed in where where keys are the field names. Skips computed fields", "response": "def fields_to_dict(obj, skip_fields=None):\n    \"\"\"\n    Generates a dictionary with the field values of the object passed in, where\n    keys are the field names. Skips computed fields\n    \"\"\"\n    data = {}\n    obj = api.get_object(obj)\n    for field_name, field in api.get_fields(obj).items():\n        if skip_fields and field_name in skip_fields:\n            continue\n        if field.type == \"computed\":\n            continue\n        data[field_name] = field.get(obj)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nservice triggered each time an item is iterated in folderitems. The use of this service prevents the extra-loops in child objects. :obj: the instance of the class to be foldered :item: dict containing the properties of the object to be used by the template :index: current index of the item", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Service triggered each time an item is iterated in folderitems.\n\n        The use of this service prevents the extra-loops in child objects.\n\n        :obj: the instance of the class to be foldered\n        :item: dict containing the properties of the object to be used by\n            the template\n        :index: current index of the item\n        \"\"\"\n\n        name = obj.getName()\n        email = obj.getEmailAddress()\n        phone = obj.getPhone()\n        fax = obj.getFax()\n        url = obj.absolute_url()\n\n        item[\"Name\"] = name\n        item[\"Email\"] = email\n        item[\"Phone\"] = phone\n        item[\"Fax\"] = fax\n        item[\"replace\"][\"Name\"] = get_link(url, value=name)\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the permissions for rejected analysis requests so lab clerks clients owners and RegulatoryInspector can see rejected analysis requests on lists.", "response": "def update_permissions_rejected_analysis_requests():\n    \"\"\"\n    Maps and updates the permissions for rejected analysis requests so lab clerks, clients, owners and\n    RegulatoryInspector can see rejected analysis requests on lists.\n\n    :return: None\n    \"\"\"\n    workflow_tool = api.get_tool(\"portal_workflow\")\n    workflow = workflow_tool.getWorkflowById('bika_ar_workflow')\n    catalog = api.get_tool(CATALOG_ANALYSIS_REQUEST_LISTING)\n    brains = catalog(review_state='rejected')\n    counter = 0\n    total = len(brains)\n    logger.info(\n        \"Changing permissions for rejected analysis requests. \" +\n        \"Number of rejected analysis requests: {0}\".format(total))\n    for brain in brains:\n        if 'LabClerk' not in brain.allowedRolesAndUsers:\n            if counter % 100 == 0:\n                logger.info(\n                    \"Changing permissions for rejected analysis requests: \" +\n                    \"{0}/{1}\".format(counter, total))\n            obj = api.get_object(brain)\n            workflow.updateRoleMappingsFor(obj)\n            obj.reindexObject()\n        counter += 1\n    logger.info(\n        \"Changed permissions for rejected analysis requests: \" +\n        \"{0}/{1}\".format(counter, total))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmigrating FileFields fields to blob ones for a given portal_type", "response": "def migrate_to_blob(context, portal_type, query={}, remove_old_value=True):\n    \"\"\"Migrates FileFields fields to blob ones for a given portal_type.\n    The wueries are done against 'portal_catalog', 'uid_catalog' and\n    'reference_catalog'\n\n    :param context: portal root object as context\n    :param query: an expression to filter the catalog by other filters than\n    the portal_type.\n    :param portal_type: The portal type name the migration is migrating *from*\n    \"\"\"\n    migrator = makeMigrator(\n        context, portal_type, remove_old_value=remove_old_value)\n    walker = BikaCustomQueryWalker(context, migrator, query=query)\n    savepoint(optimistic=True)\n    walker.go()\n    return walker.getOutput()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef makeMigrator(context, portal_type, remove_old_value=True):\n    meta_type = portal_type\n\n    class BlobMigrator(BaseInlineMigrator):\n        \"\"\"in-place migrator for archetypes based content that copies\n        file/image data from old non-blob fields to new fields with the same\n        name  provided by archetypes.schemaextender.\n\n        see `plone3 to 4 migration guide`__\n\n        .. __: https://plone.org/documentation/manual/upgrade-guide/version\n        /upgrading-plone-3-x-to-4.0/updating-add-on-products-for-plone-4.0\n        /use-plone.app.blob-based-blob-storage\n        \"\"\"\n\n        src_portal_type = portal_type\n        src_meta_type = meta_type\n        dst_portal_type = portal_type\n        dst_meta_type = meta_type\n        fields = []\n\n        def getFields(self, obj):\n            if not self.fields:\n                # get the blob fields to migrate from the first object\n                for field in ISchema(obj).fields():\n                    if IBlobField.providedBy(field):\n                        self.fields.append(field.getName())\n            return self.fields\n\n        @property\n        def fields_map(self):\n            fields = self.getFields(None)\n            return dict([(name, None) for name in fields])\n\n        def migrate_data(self):\n            fields = self.getFields(self.obj)\n            for name in fields:\n                # access old field by not using schemaextender\n                oldfield = self.obj.schema[name]\n                is_imagefield = False\n                if hasattr(oldfield, 'removeScales'):\n                    # clean up old image scales\n                    is_imagefield = True\n                    oldfield.removeScales(self.obj)\n                value = oldfield.get(self.obj)\n\n                if not value:\n                    # no image/file data: don't copy it over to blob field\n                    # this way it's save to run migration multiple times w/o\n                    # overwriting existing data\n                    continue\n\n                if isinstance(aq_base(value), BlobWrapper):\n                    # already a blob field, no need to migrate it\n                    continue\n\n                # access new field via schemaextender\n                field = self.obj.getField(name)\n                field.getMutator(self.obj)(value)\n\n                if remove_old_value:\n                    # Remove data from old field to not end up with data\n                    # stored twice - in ZODB and blobstorage\n                    if is_imagefield:\n                        oldfield.set(self.obj, 'DELETE_IMAGE')\n                    else:\n                        oldfield.set(self.obj, 'DELETE_FILE')\n\n        def last_migrate_reindex(self):\n            # The original method checks the modification date in order to\n            # keep the old one, but we don't care about it.\n            self.obj.reindexObject()\n\n    return BlobMigrator", "response": "generate a migrator for the given at - based portal type"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwalk around and returns all objects which need migration", "response": "def walk(self):\n        \"\"\"\n        Walks around and returns all objects which needs migration\n        It does exactly the same as the original method, but add some\n        progress loggers.\n\n        :return: objects (with acquisition wrapper) that needs migration\n        :rtype: generator\n        \"\"\"\n        catalog = self.catalog\n        query = self.additionalQuery.copy()\n        query['portal_type'] = self.src_portal_type\n        query['meta_type'] = self.src_meta_type\n\n        if HAS_LINGUA_PLONE and 'Language' in catalog.indexes():\n            query['Language'] = 'all'\n\n        brains = catalog(query)\n        limit = getattr(self, 'limit', False)\n        if limit:\n            brains = brains[:limit]\n        obj_num_total = len(brains)\n        logger.info('{} {} objects will be migrated walking through {}'\n                    .format(obj_num_total, self.src_portal_type, catalog.id))\n        counter = 0\n        for brain in brains:\n            if counter % 100 == 0:\n                logger.info('Progress: {} objects have been migrated out of {}'\n                            .format(counter, obj_num_total))\n            try:\n                obj = brain.getObject()\n            except AttributeError:\n                LOG.error(\"Couldn't access %s\" % brain.getPath())\n                continue\n\n            if self.callBefore is not None and callable(self.callBefore):\n                if not self.callBefore(obj, **self.kwargs):\n                    continue\n\n            try:\n                state = obj._p_changed\n            except Exception:\n                state = 0\n            if obj is not None:\n                yield obj\n                # safe my butt\n                if state is None:\n                    obj._p_deactivate()\n            counter += 1\n            if obj_num_total == counter:\n                logger.info(\n                    'Progress: {} objects have been migrated out of {}'\n                    .format(counter, obj_num_total))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef recursiveUpdateRoleMappings(self, ob, wfs=None, commit_window=1000):\n        wf_tool = api.get_tool(\"portal_workflow\")\n\n        if wfs is None:\n            wfs = {}\n            for id in wf_tool.objectIds():\n                wf = wf_tool.getWorkflowById(id)\n                if hasattr(aq_base(wf), 'updateRoleMappingsFor'):\n                    wfs[id] = wf\n\n        # Returns a count of updated objects.\n        count = 0\n        wf_ids = wf_tool.getChainFor(ob)\n        if wf_ids:\n            changed = 0\n            for wf_id in wf_ids:\n                wf = wfs.get(wf_id, None)\n                if wf is not None:\n                    did = wf.updateRoleMappingsFor(ob)\n                    if did:\n                        changed = 1\n            if changed:\n                count = count + 1\n                if hasattr(aq_base(ob), 'reindexObject'):\n                    # Reindex security-related indexes\n                    try:\n                        ob.reindexObject(idxs=['allowedRolesAndUsers'])\n                    except TypeError:\n                        # Catch attempts to reindex portal_catalog.\n                        pass\n        if hasattr(aq_base(ob), 'objectItems'):\n            obs = ob.objectItems()\n            if obs:\n                committed = 0\n                logged = 0\n                for k, v in obs:\n                    if count - logged >= 100:\n                        logger.info(\n                            \"Updating role mappings for {}: {}\".format(\n                                repr(ob), count))\n                        logged += count\n\n                    changed = getattr(v, '_p_changed', 0)\n                    processed = self.recursiveUpdateRoleMappings(v, wfs,\n                                                                 commit_window)\n                    count += processed\n                    if changed is None:\n                        # Re-ghostify.\n                        v._p_deactivate()\n\n                    if count - committed >= commit_window:\n                        commit_transaction()\n                        committed += count\n        return count", "response": "Recursive updates the role mappings for an object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set(self, instance, value, **kwargs):\n        val = get_date(instance, value)\n        super(DateTimeField, self).set(instance, val, **kwargs)", "response": "Set the value of the object to the given value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setLayout(self, value):\n        new_layout = sorted(value, key=lambda k: k['position'])\n        self.getField('Layout').set(self, new_layout)", "response": "Sets the worksheet layout keeping it sorted by position"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef addAnalyses(self, analyses):\n        actions_pool = ActionHandlerPool.get_instance()\n        actions_pool.queue_pool()\n        for analysis in analyses:\n            self.addAnalysis(api.get_object(analysis))\n        actions_pool.resume()", "response": "Adds a collection of analyses to the Worksheet at once"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd the analysis to the worksheet and returns the unique ID of the new analysis.", "response": "def addAnalysis(self, analysis, position=None):\n        \"\"\"- add the analysis to self.Analyses().\n           - position is overruled if a slot for this analysis' parent exists\n           - if position is None, next available pos is used.\n        \"\"\"\n        # Cannot add an analysis if not open, unless a retest\n        if api.get_review_status(self) not in [\"open\", \"to_be_verified\"]:\n            retracted = analysis.getRetestOf()\n            if retracted not in self.getAnalyses():\n                return\n\n        # Cannot add an analysis that is assigned already\n        if analysis.getWorksheet():\n            return\n\n        # Just in case\n        analyses = self.getAnalyses()\n        if analysis in analyses:\n            analyses = filter(lambda an: an != analysis, analyses)\n            self.setAnalyses(analyses)\n            self.updateLayout()\n\n        # Cannot add an analysis if the assign transition is not possible\n        # We need to bypass the guard's check for current context!\n        api.get_request().set(\"ws_uid\", api.get_uid(self))\n        if not isTransitionAllowed(analysis, \"assign\"):\n            return\n\n        # Assign the instrument from the worksheet to the analysis, if possible\n        instrument = self.getInstrument()\n        if instrument and analysis.isInstrumentAllowed(instrument):\n            # TODO Analysis Instrument + Method assignment\n            methods = instrument.getMethods()\n            if methods:\n                # Set the first method assigned to the selected instrument\n                analysis.setMethod(methods[0])\n            analysis.setInstrument(instrument)\n        elif not instrument:\n            # If the ws doesn't have an instrument try to set the method\n            method = self.getMethod()\n            if method and analysis.isMethodAllowed(method):\n                analysis.setMethod(method)\n\n        # Transition analysis to \"assigned\"\n        actions_pool = ActionHandlerPool.get_instance()\n        actions_pool.queue_pool()\n        doActionFor(analysis, \"assign\")\n        self.setAnalyses(analyses + [analysis])\n        self.addToLayout(analysis, position)\n\n        # Try to rollback the worksheet to prevent inconsistencies\n        doActionFor(self, \"rollback_to_open\")\n\n        # Reindex Worksheet\n        idxs = [\"getAnalysesUIDs\"]\n        push_reindex_to_actions_pool(self, idxs=idxs)\n\n        # Reindex Analysis Request, if any\n        if IRequestAnalysis.providedBy(analysis):\n            idxs = ['assigned_state', 'getDueDate']\n            push_reindex_to_actions_pool(analysis.getRequest(), idxs=idxs)\n\n        # Resume the actions pool\n        actions_pool.resume()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves the analysis passed in from the worksheet.", "response": "def removeAnalysis(self, analysis):\n        \"\"\" Unassigns the analysis passed in from the worksheet.\n        Delegates to 'unassign' transition for the analysis passed in\n        \"\"\"\n        # We need to bypass the guard's check for current context!\n        api.get_request().set(\"ws_uid\", api.get_uid(self))\n        if analysis.getWorksheet() == self:\n            doActionFor(analysis, \"unassign\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the analysis passed in to the worksheet s layout.", "response": "def addToLayout(self, analysis, position=None):\n        \"\"\" Adds the analysis passed in to the worksheet's layout\n        \"\"\"\n        # TODO Redux\n        layout = self.getLayout()\n        container_uid = self.get_container_for(analysis)\n        if IRequestAnalysis.providedBy(analysis) and \\\n                not IDuplicateAnalysis.providedBy(analysis):\n            container_uids = map(lambda slot: slot['container_uid'], layout)\n            if container_uid in container_uids:\n                position = [int(slot['position']) for slot in layout if\n                            slot['container_uid'] == container_uid][0]\n            elif not position:\n                used_positions = [0, ] + [int(slot['position']) for slot in\n                                          layout]\n                position = [pos for pos in range(1, max(used_positions) + 2)\n                            if pos not in used_positions][0]\n\n        an_type = self.get_analysis_type(analysis)\n        self.setLayout(layout + [{'position': position,\n                                  'type': an_type,\n                                  'container_uid': container_uid,\n                                  'analysis_uid': api.get_uid(analysis)}, ])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npurge the layout of not assigned analyses", "response": "def purgeLayout(self):\n        \"\"\" Purges the layout of not assigned analyses\n        \"\"\"\n        uids = map(api.get_uid, self.getAnalyses())\n        layout = filter(lambda slot: slot.get(\"analysis_uid\", None) in uids,\n                        self.getLayout())\n        self.setLayout(layout)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding reference analyses to the specified slot.", "response": "def addReferenceAnalyses(self, reference, services, slot=None):\n        \"\"\" Creates and add reference analyses to the slot by using the\n        reference sample and service uids passed in.\n        If no destination slot is defined, the most suitable slot will be used,\n        typically a new slot at the end of the worksheet will be added.\n        :param reference: reference sample to which ref analyses belong\n        :param service_uids: he uid of the services to create analyses from\n        :param slot: slot where reference analyses must be stored\n        :return: the list of reference analyses added\n        \"\"\"\n        service_uids = list()\n        for service in services:\n            if api.is_uid(service):\n                service_uids.append(service)\n            else:\n                service_uids.append(api.get_uid(service))\n        service_uids = list(set(service_uids))\n\n        # Cannot add a reference analysis if not open\n        if api.get_workflow_status_of(self) != \"open\":\n            return []\n\n        slot_to = to_int(slot)\n        if slot_to < 0:\n            return []\n\n        if not slot_to:\n            # Find the suitable slot to add these references\n            slot_to = self.get_suitable_slot_for_reference(reference)\n            return self.addReferenceAnalyses(reference, service_uids, slot_to)\n\n        processed = list()\n        for analysis in self.get_analyses_at(slot_to):\n            if api.get_review_status(analysis) != \"retracted\":\n                service = analysis.getAnalysisService()\n                processed.append(api.get_uid(service))\n        query = dict(portal_type=\"AnalysisService\", UID=service_uids,\n                     sort_on=\"sortable_title\")\n        services = filter(lambda service: api.get_uid(service) not in processed,\n                          api.search(query, \"bika_setup_catalog\"))\n\n        # Ref analyses from the same slot must have the same group id\n        ref_gid = self.nextRefAnalysesGroupID(reference)\n        ref_analyses = list()\n        for service in services:\n            service_obj = api.get_object(service)\n            ref_analysis = self.add_reference_analysis(reference, service_obj,\n                                                        slot_to, ref_gid)\n            if not ref_analysis:\n                continue\n            ref_analyses.append(ref_analysis)\n        return ref_analyses"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a reference analysis to the current worksheet and adds it to the layout.", "response": "def add_reference_analysis(self, reference, service, slot, ref_gid=None):\n        \"\"\"\n        Creates a reference analysis in the destination slot (dest_slot) passed\n        in, by using the reference and service_uid. If the analysis\n        passed in is not an IReferenceSample or has dependent services, returns\n        None. If no reference analyses group id (refgid) is set, the value will\n        be generated automatically.\n        :param reference: reference sample to create an analysis from\n        :param service: the service object to create an analysis from\n        :param slot: slot where the reference analysis must be stored\n        :param refgid: the reference analyses group id to be set\n        :return: the reference analysis or None\n        \"\"\"\n        if not reference or not service:\n            return None\n\n        if not IReferenceSample.providedBy(reference):\n            logger.warning('Cannot create reference analysis from a non '\n                           'reference sample: {}'.format(reference.getId()))\n            return None\n\n        calc = service.getCalculation()\n        if calc and calc.getDependentServices():\n            logger.warning('Cannot create reference analyses with dependent'\n                           'services: {}'.format(service.getId()))\n            return None\n\n        # Create the reference analysis\n        ref_analysis = reference.addReferenceAnalysis(service)\n        if not ref_analysis:\n            logger.warning(\"Unable to create a reference analysis for \"\n                           \"reference '{0}' and service '{1}'\"\n                           .format(reference.getId(), service.getKeyword()))\n            return None\n\n        # Set ReferenceAnalysesGroupID (same id for the analyses from\n        # the same Reference Sample and same Worksheet)\n        gid = ref_gid and ref_gid or self.nextRefAnalysesGroupID(reference)\n        ref_analysis.setReferenceAnalysesGroupID(gid)\n\n        # Add the reference analysis into the worksheet\n        self.setAnalyses(self.getAnalyses() + [ref_analysis, ])\n        self.addToLayout(ref_analysis, slot)\n\n        # Reindex\n        ref_analysis.reindexObject(idxs=[\"getAnalyst\", \"getWorksheetUID\",\n                                         \"getReferenceAnalysesGroupID\"])\n        self.reindexObject(idxs=[\"getAnalysesUIDs\"])\n        return ref_analysis"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nextRefAnalysesGroupID(self, reference):\n        prefix = reference.id + \"-\"\n        if not IReferenceSample.providedBy(reference):\n            # Not a ReferenceSample, so this is a duplicate\n            prefix = reference.id + \"-D\"\n        bac = getToolByName(reference, 'bika_analysis_catalog')\n        ids = bac.Indexes['getReferenceAnalysesGroupID'].uniqueValues()\n        rr = re.compile(\"^\" + prefix + \"[\\d+]+$\")\n        ids = [int(i.split(prefix)[1]) for i in ids if i and rr.match(i)]\n        ids.sort()\n        _id = ids[-1] if ids else 0\n        suffix = str(_id + 1).zfill(int(3))\n        if not IReferenceSample.providedBy(reference):\n            # Not a ReferenceSample, so this is a duplicate\n            suffix = str(_id + 1).zfill(2)\n        return '%s%s' % (prefix, suffix)", "response": "Returns the next ReferenceAnalysesGroupID for the given reference"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef addDuplicateAnalyses(self, src_slot, dest_slot=None):\n        # Duplicate analyses can only be added if the state of the ws is open\n        # unless we are adding a retest\n        if api.get_workflow_status_of(self) != \"open\":\n            return []\n\n        slot_from = to_int(src_slot, 0)\n        if slot_from < 1:\n            return []\n\n        slot_to = to_int(dest_slot, 0)\n        if slot_to < 0:\n            return []\n\n        if not slot_to:\n            # Find the suitable slot to add these duplicates\n            slot_to = self.get_suitable_slot_for_duplicate(slot_from)\n            return self.addDuplicateAnalyses(src_slot, slot_to)\n\n        processed = map(lambda an: api.get_uid(an.getAnalysis()),\n                        self.get_analyses_at(slot_to))\n        src_analyses = list()\n        for analysis in self.get_analyses_at(slot_from):\n            if api.get_uid(analysis) in processed:\n                if api.get_workflow_status_of(analysis) != \"retracted\":\n                    continue\n            src_analyses.append(analysis)\n        ref_gid = None\n        duplicates = list()\n        for analysis in src_analyses:\n            duplicate = self.add_duplicate_analysis(analysis, slot_to, ref_gid)\n            if not duplicate:\n                continue\n            # All duplicates from the same slot must have the same group id\n            ref_gid = ref_gid or duplicate.getReferenceAnalysesGroupID()\n            duplicates.append(duplicate)\n        return duplicates", "response": "Creates and adds duplicate analyses from the source slot to the destination slot."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a duplicate from the source analysis to the destination analysis.", "response": "def add_duplicate_analysis(self, src_analysis, destination_slot,\n                               ref_gid=None):\n        \"\"\"\n        Creates a duplicate of the src_analysis passed in. If the analysis\n        passed in is not an IRoutineAnalysis, is retracted or has dependent\n        services, returns None.If no reference analyses group id (ref_gid) is\n        set, the value will be generated automatically.\n        :param src_analysis: analysis to create a duplicate from\n        :param destination_slot: slot where duplicate analysis must be stored\n        :param ref_gid: the reference analysis group id to be set\n        :return: the duplicate analysis or None\n        \"\"\"\n        if not src_analysis:\n            return None\n\n        if not IRoutineAnalysis.providedBy(src_analysis):\n            logger.warning('Cannot create duplicate analysis from a non '\n                           'routine analysis: {}'.format(src_analysis.getId()))\n            return None\n\n        if api.get_review_status(src_analysis) == 'retracted':\n            logger.warning('Cannot create duplicate analysis from a retracted'\n                           'analysis: {}'.format(src_analysis.getId()))\n            return None\n\n        # TODO Workflow - Duplicate Analyses - Consider duplicates with deps\n        # Removing this check from here and ensuring that duplicate.getSiblings\n        # returns the analyses sorted by priority (duplicates from same\n        # AR > routine analyses from same AR > duplicates from same WS >\n        # routine analyses from same WS) should be almost enough\n        calc = src_analysis.getCalculation()\n        if calc and calc.getDependentServices():\n            logger.warning('Cannot create duplicate analysis from an'\n                           'analysis with dependent services: {}'\n                           .format(src_analysis.getId()))\n            return None\n\n        # Create the duplicate\n        duplicate = _createObjectByType(\"DuplicateAnalysis\", self, tmpID())\n        duplicate.setAnalysis(src_analysis)\n\n        # Set ReferenceAnalysesGroupID (same id for the analyses from\n        # the same Reference Sample and same Worksheet)\n        if not ref_gid:\n            ref_gid = self.nextRefAnalysesGroupID(duplicate.getRequest())\n        duplicate.setReferenceAnalysesGroupID(ref_gid)\n\n        # Add the duplicate into the worksheet\n        self.addToLayout(duplicate, destination_slot)\n        self.setAnalyses(self.getAnalyses() + [duplicate, ])\n\n        # Reindex\n        duplicate.reindexObject(idxs=[\"getAnalyst\", \"getWorksheetUID\",\n                                      \"getReferenceAnalysesGroupID\"])\n        self.reindexObject(idxs=[\"getAnalysesUIDs\"])\n        return duplicate"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_suitable_slot_for_duplicate(self, src_slot):\n        slot_from = to_int(src_slot, 0)\n        if slot_from < 1:\n            return -1\n\n        # Are the analyses from src_slot suitable for duplicates creation?\n        container = self.get_container_at(slot_from)\n        if not container or not IAnalysisRequest.providedBy(container):\n            # We cannot create duplicates from analyses other than routine ones,\n            # those that belong to an Analysis Request.\n            return -1\n\n        occupied = self.get_slot_positions(type='all')\n        wst = self.getWorksheetTemplate()\n        if not wst:\n            # No worksheet template assigned, add a new slot at the end of\n            # the worksheet with the duplicate there\n            slot_to = max(occupied) + 1\n            return slot_to\n\n        # If there is a match with the layout defined in the Worksheet\n        # Template, use that slot instead of adding a new one at the end of\n        # the worksheet\n        layout = wst.getLayout()\n        for pos in layout:\n            if pos['type'] != 'd' or to_int(pos['dup']) != slot_from:\n                continue\n            slot_to = int(pos['pos'])\n            if slot_to in occupied:\n                # Not an empty slot\n                continue\n\n            # This slot is empty, use it instead of adding a new\n            # slot at the end of the worksheet\n            return slot_to\n\n        # Add a new slot at the end of the worksheet, but take into account\n        # that a worksheet template is assigned, so we need to take care to\n        # not override slots defined by its layout\n        occupied.append(len(layout))\n        slot_to = max(occupied) + 1\n        return slot_to", "response": "Returns the suitable slot for a duplicate of src_slot."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the suitable slot for the given reference sample.", "response": "def get_suitable_slot_for_reference(self, reference):\n        \"\"\"Returns the suitable position for reference analyses, taking into\n        account if there is a WorksheetTemplate assigned to this worksheet.\n\n        By default, returns a new slot at the end of the worksheet unless there\n        is a slot defined for a reference of the same type (blank or control)\n        in the worksheet template's layout that hasn't been used yet.\n\n        :param reference: ReferenceSample the analyses will be created from\n        :return: suitable slot position for reference analyses\n        \"\"\"\n        if not IReferenceSample.providedBy(reference):\n            return -1\n\n        occupied = self.get_slot_positions(type='all') or [0]\n        wst = self.getWorksheetTemplate()\n        if not wst:\n            # No worksheet template assigned, add a new slot at the end of the\n            # worksheet with the reference analyses there\n            slot_to = max(occupied) + 1\n            return slot_to\n\n        # If there is a match with the layout defined in the Worksheet Template,\n        # use that slot instead of adding a new one at the end of the worksheet\n        slot_type = reference.getBlank() and 'b' or 'c'\n        layout = wst.getLayout()\n\n        for pos in layout:\n            if pos['type'] != slot_type:\n                continue\n            slot_to = int(pos['pos'])\n            if slot_to in occupied:\n                # Not an empty slot\n                continue\n\n            # This slot is empty, use it instead of adding a new slot at the end\n            # of the worksheet\n            return slot_to\n\n        # Add a new slot at the end of the worksheet, but take into account\n        # that a worksheet template is assigned, so we need to take care to\n        # not override slots defined by its layout\n        occupied.append(len(layout))\n        slot_to = max(occupied) + 1\n        return slot_to"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the duplicates generated by the analysis passed in", "response": "def get_duplicates_for(self, analysis):\n        \"\"\"Returns the duplicates from the current worksheet that were created\n        by using the analysis passed in as the source\n\n        :param analysis: routine analyses used as the source for the duplicates\n        :return: a list of duplicates generated from the analysis passed in\n        \"\"\"\n        if not analysis:\n            return list()\n        uid = api.get_uid(analysis)\n        return filter(lambda dup: api.get_uid(dup.getAnalysis()) == uid,\n                      self.getDuplicateAnalyses())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the list of analyses assigned to the slot passed in sorted by the analyses they have within the slot.", "response": "def get_analyses_at(self, slot):\n        \"\"\"Returns the list of analyses assigned to the slot passed in, sorted by\n        the positions they have within the slot.\n\n        :param slot: the slot where the analyses are located\n        :type slot: int\n        :return: a list of analyses\n        \"\"\"\n\n        # ensure we have an integer\n        slot = to_int(slot)\n\n        if slot < 1:\n            return list()\n\n        analyses = list()\n        layout = self.getLayout()\n\n        for pos in layout:\n            layout_slot = to_int(pos['position'])\n            uid = pos['analysis_uid']\n            if layout_slot != slot or not uid:\n                continue\n            analyses.append(api.get_object_by_uid(uid))\n\n        return analyses"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_container_at(self, slot):\n\n        # ensure we have an integer\n        slot = to_int(slot)\n\n        if slot < 1:\n            return None\n\n        layout = self.getLayout()\n\n        for pos in layout:\n            layout_slot = to_int(pos['position'])\n            uid = pos['container_uid']\n            if layout_slot != slot or not uid:\n                continue\n            return api.get_object_by_uid(uid)\n\n        return None", "response": "Returns the container object assigned to the slot passed in\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list with the slots occupied for the type passed in.", "response": "def get_slot_positions(self, type='a'):\n        \"\"\"Returns a list with the slots occupied for the type passed in.\n\n        Allowed type of analyses are:\n\n            'a'   (routine analysis)\n            'b'   (blank analysis)\n            'c'   (control)\n            'd'   (duplicate)\n            'all' (all analyses)\n\n        :param type: type of the analysis\n        :return: list of slot positions\n        \"\"\"\n        if type not in ALLOWED_ANALYSES_TYPES and type != ALL_ANALYSES_TYPES:\n            return list()\n\n        layout = self.getLayout()\n        slots = list()\n\n        for pos in layout:\n            if type != ALL_ANALYSES_TYPES and pos['type'] != type:\n                continue\n            slots.append(to_int(pos['position']))\n\n        # return a unique list of sorted slot positions\n        return sorted(set(slots))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_slot_position(self, container, type='a'):\n        if not container or type not in ALLOWED_ANALYSES_TYPES:\n            return None\n        uid = api.get_uid(container)\n        layout = self.getLayout()\n\n        for pos in layout:\n            if pos['type'] != type or pos['container_uid'] != uid:\n                continue\n            return to_int(pos['position'])\n        return None", "response": "Returns the position of the first analysis in the worksheet that is located within the container."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_analysis_type(self, instance):\n        if IDuplicateAnalysis.providedBy(instance):\n            return 'd'\n        elif IReferenceAnalysis.providedBy(instance):\n            return instance.getReferenceType()\n        elif IRoutineAnalysis.providedBy(instance):\n            return 'a'\n        return None", "response": "Returns the string used in slots to differentiate amongst analysis\n        types"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the container id used in slots to group analyses", "response": "def get_container_for(self, instance):\n        \"\"\"Returns the container id used in slots to group analyses\n        \"\"\"\n        if IReferenceAnalysis.providedBy(instance):\n            return api.get_uid(instance.getSample())\n        return instance.getRequestUID()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_slot_position_for(self, instance):\n        uid = api.get_uid(instance)\n        slot = filter(lambda s: s['analysis_uid'] == uid, self.getLayout())\n        if not slot:\n            return None\n        return to_int(slot[0]['position'])", "response": "Returns the slot where the instance passed in is located. If not found returns None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resolve_available_slots(self, worksheet_template, type='a'):\n        if not worksheet_template or type not in ALLOWED_ANALYSES_TYPES:\n            return list()\n\n        ws_slots = self.get_slot_positions(type)\n        layout = worksheet_template.getLayout()\n        slots = list()\n\n        for row in layout:\n            # skip rows that do not match with the given type\n            if row['type'] != type:\n                continue\n\n            slot = to_int(row['pos'])\n\n            if slot in ws_slots:\n                # We only want those that are empty\n                continue\n\n            slots.append(slot)\n        return slots", "response": "Returns the available slots from the current worksheet that fits with the layout defined in the worksheet_template and type of analysis\n            passed in."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply the layout passed in to the worksheet template passed in w/o overwriting slots that are already filled.", "response": "def _apply_worksheet_template_routine_analyses(self, wst):\n        \"\"\"Add routine analyses to worksheet according to the worksheet template\n        layout passed in w/o overwriting slots that are already filled.\n\n        If the template passed in has an instrument assigned, only those\n        routine analyses that allows the instrument will be added.\n\n        If the template passed in has a method assigned, only those routine\n        analyses that allows the method will be added\n\n        :param wst: worksheet template used as the layout\n        :returns: None\n        \"\"\"\n        bac = api.get_tool(\"bika_analysis_catalog\")\n        services = wst.getService()\n        wst_service_uids = [s.UID() for s in services]\n        query = {\n            \"portal_type\": \"Analysis\",\n            \"getServiceUID\": wst_service_uids,\n            \"review_state\": \"unassigned\",\n            \"isSampleReceived\": True,\n            \"is_active\": True,\n            \"sort_on\": \"getPrioritySortkey\"\n        }\n        # Filter analyses their Analysis Requests have been received\n        analyses = bac(query)\n\n        # No analyses, nothing to do\n        if not analyses:\n            return\n\n        # Available slots for routine analyses. Sort reverse, cause we need a\n        # stack for sequential assignment of slots\n        available_slots = self.resolve_available_slots(wst, 'a')\n        available_slots.sort(reverse=True)\n\n        # If there is an instrument assigned to this Worksheet Template, take\n        # only the analyses that allow this instrument into consideration.\n        instrument = wst.getInstrument()\n\n        # If there is method assigned to the Worksheet Template, take only the\n        # analyses that allow this method into consideration.\n        method = wst.getRestrictToMethod()\n\n        # This worksheet is empty?\n        num_routine_analyses = len(self.getRegularAnalyses())\n\n        # Group Analyses by Analysis Requests\n        ar_analyses = dict()\n        ar_slots = dict()\n        ar_fixed_slots = dict()\n\n        for brain in analyses:\n            obj = api.get_object(brain)\n            arid = obj.getRequestID()\n\n            if instrument and not obj.isInstrumentAllowed(instrument):\n                # Exclude those analyses for which the worksheet's template\n                # instrument is not allowed\n                continue\n\n            if method and not obj.isMethodAllowed(method):\n                # Exclude those analyses for which the worksheet's template\n                # method is not allowed\n                continue\n\n            slot = ar_slots.get(arid, None)\n            if not slot:\n                # We haven't processed other analyses that belong to the same\n                # Analysis Request as the current one.\n                if len(available_slots) == 0 and num_routine_analyses == 0:\n                    # No more slots available for this worksheet/template, so\n                    # we cannot add more analyses to this WS. Also, there is no\n                    # chance to process a new analysis with an available slot.\n                    break\n\n                if num_routine_analyses == 0:\n                    # This worksheet is empty, but there are slots still\n                    # available, assign the next available slot to this analysis\n                    slot = available_slots.pop()\n                else:\n                    # This worksheet is not empty and there are slots still\n                    # available.\n                    slot = self.get_slot_position(obj.getRequest())\n                    if slot:\n                        # Prefixed slot position\n                        ar_fixed_slots[arid] = slot\n                        if arid not in ar_analyses:\n                            ar_analyses[arid] = list()\n                        ar_analyses[arid].append(obj)\n                        continue\n\n                    # This worksheet does not contain any other analysis\n                    # belonging to the same Analysis Request as the current\n                    if len(available_slots) == 0:\n                        # There is the chance to process a new analysis that\n                        # belongs to an Analysis Request that is already\n                        # in this worksheet.\n                        continue\n\n                    # Assign the next available slot\n                    slot = available_slots.pop()\n\n            ar_slots[arid] = slot\n            if arid not in ar_analyses:\n                ar_analyses[arid] = list()\n            ar_analyses[arid].append(obj)\n\n        # Sort the analysis requests by sortable_title, so the ARs will appear\n        # sorted in natural order. Since we will add the analysis with the\n        # exact slot where they have to be displayed, we need to sort the slots\n        # too and assign them to each group of analyses in natural order\n        sorted_ar_ids = sorted(ar_analyses.keys())\n        slots = sorted(ar_slots.values(), reverse=True)\n\n        # Add regular analyses\n        for ar_id in sorted_ar_ids:\n            slot = ar_fixed_slots.get(ar_id, None)\n            if not slot:\n                slot = slots.pop()\n            ar_ans = ar_analyses[ar_id]\n            for ar_an in ar_ans:\n                self.addAnalysis(ar_an, slot)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _apply_worksheet_template_duplicate_analyses(self, wst):\n        wst_layout = wst.getLayout()\n\n        for row in wst_layout:\n            if row['type'] != 'd':\n                continue\n\n            src_pos = to_int(row['dup'])\n            dest_pos = to_int(row['pos'])\n\n            self.addDuplicateAnalyses(src_pos, dest_pos)", "response": "Add duplicate analyses to the worksheet according to the worksheet template passed in w/o overwrite slots that are already filled."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresolving the reference sample from a list of reference samples that fits better with the service uid requirements.", "response": "def _resolve_reference_sample(self, reference_samples=None,\n                                  service_uids=None):\n        \"\"\"Returns the reference sample from reference_samples passed in that fits\n        better with the service uid requirements. This is, the reference sample\n        that covers most (or all) of the service uids passed in and has less\n        number of remaining service_uids.\n\n        If no reference_samples are set, returns None\n\n        If no service_uids are set, returns the first reference_sample\n\n        :param reference_samples: list of reference samples\n        :param service_uids: list of service uids\n        :return: the reference sample that fits better with the service uids\n        \"\"\"\n        if not reference_samples:\n            return None, list()\n\n        if not service_uids:\n            # Since no service filtering has been defined, there is no need to\n            # look for the best choice. Return the first one\n            sample = reference_samples[0]\n            spec_uids = sample.getSupportedServices(only_uids=True)\n            return sample, spec_uids\n\n        best_score = [0, 0]\n        best_sample = None\n        best_supported = None\n        for sample in reference_samples:\n            specs_uids = sample.getSupportedServices(only_uids=True)\n            supported = [uid for uid in specs_uids if uid in service_uids]\n            matches = len(supported)\n            overlays = len(service_uids) - matches\n            overlays = 0 if overlays < 0 else overlays\n\n            if overlays == 0 and matches == len(service_uids):\n                # Perfect match.. no need to go further\n                return sample, supported\n\n            if not best_sample \\\n                    or matches > best_score[0] \\\n                    or (matches == best_score[0] and overlays < best_score[1]):\n                best_sample = sample\n                best_score = [matches, overlays]\n                best_supported = supported\n\n        return best_sample, best_supported"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _resolve_reference_samples(self, wst, type):\n        if not type or type not in ['b', 'c']:\n            return []\n\n        bc = api.get_tool(\"bika_catalog\")\n        wst_type = type == 'b' and 'blank_ref' or 'control_ref'\n\n        slots_sample = list()\n        available_slots = self.resolve_available_slots(wst, type)\n        wst_layout = wst.getLayout()\n        for row in wst_layout:\n            slot = int(row['pos'])\n            if slot not in available_slots:\n                continue\n\n            ref_definition_uid = row.get(wst_type, None)\n            if not ref_definition_uid:\n                # Only reference analyses with reference definition can be used\n                # in worksheet templates\n                continue\n\n            samples = bc(portal_type='ReferenceSample',\n                         review_state='current',\n                         is_active=True,\n                         getReferenceDefinitionUID=ref_definition_uid)\n\n            # We only want the reference samples that fit better with the type\n            # and with the analyses defined in the Template\n            services = wst.getService()\n            services = [s.UID() for s in services]\n            candidates = list()\n            for sample in samples:\n                obj = api.get_object(sample)\n                if (type == 'b' and obj.getBlank()) or \\\n                        (type == 'c' and not obj.getBlank()):\n                    candidates.append(obj)\n\n            sample, uids = self._resolve_reference_sample(candidates, services)\n            if not sample:\n                continue\n\n            slots_sample.append({'slot': slot,\n                                 'sample': sample,\n                                 'supported_services': uids})\n\n        return slots_sample", "response": "Resolves the slots and reference samples in accordance with the type passed in and returns a list of dictionaries where the slot and sample are found in the order they appear."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _apply_worksheet_template_reference_analyses(self, wst, type='all'):\n        if type == 'all':\n            self._apply_worksheet_template_reference_analyses(wst, 'b')\n            self._apply_worksheet_template_reference_analyses(wst, 'c')\n            return\n\n        if type not in ['b', 'c']:\n            return\n\n        references = self._resolve_reference_samples(wst, type)\n        for reference in references:\n            slot = reference['slot']\n            sample = reference['sample']\n            services = reference['supported_services']\n            self.addReferenceAnalyses(sample, services, slot)", "response": "Adds reference analyses to the worksheet according to the worksheet template passed in."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply the given worksheet template to the current object.", "response": "def applyWorksheetTemplate(self, wst):\n        \"\"\" Add analyses to worksheet according to wst's layout.\n            Will not overwrite slots which are filled already.\n            If the selected template has an instrument assigned, it will\n            only be applied to those analyses for which the instrument\n            is allowed, the same happens with methods.\n        \"\"\"\n        # Store the Worksheet Template field\n        self.getField('WorksheetTemplate').set(self, wst)\n\n        if not wst:\n            return\n\n        # Apply the template for routine analyses\n        self._apply_worksheet_template_routine_analyses(wst)\n\n        # Apply the template for duplicate analyses\n        self._apply_worksheet_template_duplicate_analyses(wst)\n\n        # Apply the template for reference analyses (blanks and controls)\n        self._apply_worksheet_template_reference_analyses(wst)\n\n        # Assign the instrument\n        instrument = wst.getInstrument()\n        if instrument:\n            self.setInstrument(instrument, True)\n\n        # Assign the method\n        method = wst.getRestrictToMethod()\n        if method:\n            self.setMethod(method, True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getWorksheetServices(self):\n        services = []\n        for analysis in self.getAnalyses():\n            service = analysis.getAnalysisService()\n            if service and service not in services:\n                services.append(service)\n        return services", "response": "get list of analysis services present on this worksheet"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the QC analyses in the current locale", "response": "def getQCAnalyses(self):\n        \"\"\"\n        Return the Quality Control analyses.\n        :returns: a list of QC analyses\n        :rtype: List of ReferenceAnalysis/DuplicateAnalysis\n        \"\"\"\n        qc_types = ['ReferenceAnalysis', 'DuplicateAnalysis']\n        analyses = self.getAnalyses()\n        return [a for a in analyses if a.portal_type in qc_types]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the duplicate analyses assigned to the current worksheet", "response": "def getDuplicateAnalyses(self):\n        \"\"\"Return the duplicate analyses assigned to the current worksheet\n        :return: List of DuplicateAnalysis\n        :rtype: List of IDuplicateAnalysis objects\"\"\"\n        ans = self.getAnalyses()\n        duplicates = [an for an in ans if IDuplicateAnalysis.providedBy(an)]\n        return duplicates"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getReferenceAnalyses(self):\n        ans = self.getAnalyses()\n        references = [an for an in ans if IReferenceAnalysis.providedBy(an)]\n        return references", "response": "Return the reference analyses assigned to the current\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getRegularAnalyses(self):\n        qc_types = ['ReferenceAnalysis', 'DuplicateAnalysis']\n        analyses = self.getAnalyses()\n        return [a for a in analyses if a.portal_type not in qc_types]", "response": "Returns the list of regular analyses assigned to the current worksheet that are not QC analyses"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the number of QC samples in the current QC.", "response": "def getNumberOfQCSamples(self):\n        \"\"\"\n        Returns the number of Quality Control samples.\n        :returns: number of QC samples\n        :rtype: integer\n        \"\"\"\n        qc_analyses = self.getQCAnalyses()\n        qc_samples = [a.getSample().UID() for a in qc_analyses]\n        # discarding any duplicate values\n        return len(set(qc_samples))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the number of regular samples.", "response": "def getNumberOfRegularSamples(self):\n        \"\"\"\n        Returns the number of regular samples.\n        :returns: number of regular samples\n        :rtype: integer\n        \"\"\"\n        analyses = self.getRegularAnalyses()\n        samples = [a.getRequestUID() for a in analyses]\n        # discarding any duplicate values\n        return len(set(samples))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the specified instrument to the Analysis from the Analysis Service and Method.", "response": "def setInstrument(self, instrument, override_analyses=False):\n        \"\"\" Sets the specified instrument to the Analysis from the\n            Worksheet. Only sets the instrument if the Analysis\n            allows it, according to its Analysis Service and Method.\n            If an analysis has already assigned an instrument, it won't\n            be overriden.\n            The Analyses that don't allow the instrument specified will\n            not be modified.\n            Returns the number of analyses affected\n        \"\"\"\n        analyses = [an for an in self.getAnalyses()\n                    if (not an.getInstrument() or override_analyses) and\n                    an.isInstrumentAllowed(instrument)]\n        total = 0\n        for an in analyses:\n            # An analysis can be done using differents Methods.\n            # Un method can be supported by more than one Instrument,\n            # but not all instruments support one method.\n            # We must force to set the instrument's method too. Otherwise,\n            # the WS manage results view will display the an's default\n            # method and its instruments displaying, only the instruments\n            # for the default method in the picklist.\n            instr_methods = instrument.getMethods()\n            meth = instr_methods[0] if instr_methods else None\n            if meth and an.isMethodAllowed(meth):\n                if an.getMethod() not in instr_methods:\n                    an.setMethod(meth)\n\n            an.setInstrument(instrument)\n            total += 1\n\n        self.getField('Instrument').set(self, instrument)\n        return total"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the specified method to the Analyses from the Worksheet. Only sets the method if the Analysis allows to keep the integrity.", "response": "def setMethod(self, method, override_analyses=False):\n        \"\"\" Sets the specified method to the Analyses from the\n            Worksheet. Only sets the method if the Analysis\n            allows to keep the integrity.\n            If an analysis has already been assigned to a method, it won't\n            be overriden.\n            Returns the number of analyses affected.\n        \"\"\"\n        analyses = [an for an in self.getAnalyses()\n                    if (not an.getMethod() or\n                        not an.getInstrument() or\n                        override_analyses) and an.isMethodAllowed(method)]\n        total = 0\n        for an in analyses:\n            success = False\n            if an.isMethodAllowed(method):\n                success = an.setMethod(method)\n            if success is True:\n                total += 1\n\n        self.getField('Method').set(self, method)\n        return total"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getAnalystName(self):\n        mtool = getToolByName(self, 'portal_membership')\n        analyst = self.getAnalyst().strip()\n        analyst_member = mtool.getMemberById(analyst)\n        if analyst_member is not None:\n            return analyst_member.getProperty('fullname')\n        return analyst", "response": "Returns the name of the currently assigned analyst"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef workflow_script_reject(self):\n        if skip(self, \"reject\"):\n            return\n        workflow = self.portal_workflow\n\n        def copy_src_fields_to_dst(src, dst):\n            # These will be ignored when copying field values between analyses\n            ignore_fields = [\n                'UID',\n                'id',\n                'title',\n                'allowDiscussion',\n                'subject',\n                'description',\n                'location',\n                'contributors',\n                'creators',\n                'effectiveDate',\n                'expirationDate',\n                'language',\n                'rights',\n                'creation_date',\n                'modification_date',\n                'Layout',    # ws\n                'Analyses',  # ws\n            ]\n            fields = src.Schema().fields()\n            for field in fields:\n                fieldname = field.getName()\n                if fieldname in ignore_fields:\n                    continue\n                getter = getattr(src, 'get' + fieldname,\n                                 src.Schema().getField(fieldname).getAccessor(src))\n                setter = getattr(dst, 'set' + fieldname,\n                                 dst.Schema().getField(fieldname).getMutator(dst))\n                if getter is None or setter is None:\n                    # ComputedField\n                    continue\n                setter(getter())\n\n        analysis_positions = {}\n        for item in self.getLayout():\n            analysis_positions[item['analysis_uid']] = item['position']\n        old_layout = []\n        new_layout = []\n\n        # New worksheet\n        worksheets = self.aq_parent\n        new_ws = _createObjectByType('Worksheet', worksheets, tmpID())\n        new_ws.unmarkCreationFlag()\n        new_ws_id = renameAfterCreation(new_ws)\n        copy_src_fields_to_dst(self, new_ws)\n        new_ws.edit(\n            Number=new_ws_id,\n            Remarks=self.getRemarks()\n        )\n\n        # Objects are being created inside other contexts, but we want their\n        # workflow handlers to be aware of which worksheet this is occurring in.\n        # We save the worksheet in request['context_uid'].\n        # We reset it again below....  be very sure that this is set to the\n        # UID of the containing worksheet before invoking any transitions on\n        # analyses.\n        self.REQUEST['context_uid'] = new_ws.UID()\n\n        # loop all analyses\n        analyses = self.getAnalyses()\n        new_ws_analyses = []\n        old_ws_analyses = []\n        for analysis in analyses:\n            # Skip published or verified analyses\n            review_state = workflow.getInfoFor(analysis, 'review_state', '')\n            if review_state in ['published', 'verified', 'retracted']:\n                old_ws_analyses.append(analysis.UID())\n\n                # XXX where does position come from?\n                old_layout.append({'position': position,\n                                   'type': 'a',\n                                   'analysis_uid': analysis.UID(),\n                                   'container_uid': analysis.aq_parent.UID()})\n                continue\n            # Normal analyses:\n            # - Create matching RejectAnalysis inside old WS\n            # - Link analysis to new WS in same position\n            # - Copy all field values\n            # - Clear analysis result, and set Retested flag\n            if analysis.portal_type == 'Analysis':\n                reject = _createObjectByType('RejectAnalysis', self, tmpID())\n                reject.unmarkCreationFlag()\n                copy_src_fields_to_dst(analysis, reject)\n                reject.setAnalysis(analysis)\n                reject.reindexObject()\n                analysis.edit(\n                    Result=None,\n                    Retested=True,\n                )\n                analysis.reindexObject()\n                position = analysis_positions[analysis.UID()]\n                old_ws_analyses.append(reject.UID())\n                old_layout.append({'position': position,\n                                   'type': 'r',\n                                   'analysis_uid': reject.UID(),\n                                   'container_uid': self.UID()})\n                new_ws_analyses.append(analysis.UID())\n                new_layout.append({'position': position,\n                                   'type': 'a',\n                                   'analysis_uid': analysis.UID(),\n                                   'container_uid': analysis.aq_parent.UID()})\n            # Reference analyses\n            # - Create a new reference analysis in the new worksheet\n            # - Transition the original analysis to 'rejected' state\n            if analysis.portal_type == 'ReferenceAnalysis':\n                service_uid = analysis.getServiceUID()\n                reference = analysis.aq_parent\n                new_reference = reference.addReferenceAnalysis(service_uid)\n                reference_type = new_reference.getReferenceType()\n                new_analysis_uid = api.get_uid(new_reference)\n                position = analysis_positions[analysis.UID()]\n                old_ws_analyses.append(analysis.UID())\n                old_layout.append({'position': position,\n                                   'type': reference_type,\n                                   'analysis_uid': analysis.UID(),\n                                   'container_uid': reference.UID()})\n                new_ws_analyses.append(new_analysis_uid)\n                new_layout.append({'position': position,\n                                   'type': reference_type,\n                                   'analysis_uid': new_analysis_uid,\n                                   'container_uid': reference.UID()})\n                workflow.doActionFor(analysis, 'reject')\n                analysis.reindexObject()\n            # Duplicate analyses\n            # - Create a new duplicate inside the new worksheet\n            # - Transition the original analysis to 'rejected' state\n            if analysis.portal_type == 'DuplicateAnalysis':\n                duplicate_id = new_ws.generateUniqueId('DuplicateAnalysis')\n                new_duplicate = _createObjectByType('DuplicateAnalysis',\n                                                    new_ws, duplicate_id)\n                new_duplicate.unmarkCreationFlag()\n                copy_src_fields_to_dst(analysis, new_duplicate)\n                new_duplicate.reindexObject()\n                position = analysis_positions[analysis.UID()]\n                old_ws_analyses.append(analysis.UID())\n                old_layout.append({'position': position,\n                                   'type': 'd',\n                                   'analysis_uid': analysis.UID(),\n                                   'container_uid': self.UID()})\n                new_ws_analyses.append(new_duplicate.UID())\n                new_layout.append({'position': position,\n                                   'type': 'd',\n                                   'analysis_uid': new_duplicate.UID(),\n                                   'container_uid': new_ws.UID()})\n                workflow.doActionFor(analysis, 'reject')\n                analysis.reindexObject()\n\n        new_ws.setAnalyses(new_ws_analyses)\n        new_ws.setLayout(new_layout)\n        new_ws.replaces_rejected_worksheet = self.UID()\n        for analysis in new_ws.getAnalyses():\n            review_state = workflow.getInfoFor(analysis, 'review_state', '')\n            if review_state == 'to_be_verified':\n                # TODO Workflow - Analysis Retest transition within a Worksheet\n                changeWorkflowState(analysis, \"bika_analysis_workflow\", \"assigned\")\n        self.REQUEST['context_uid'] = self.UID()\n        self.setLayout(old_layout)\n        self.setAnalyses(old_ws_analyses)\n        self.replaced_by = new_ws.UID()", "response": "Copy real analyses to RejectAnalysis create a new worksheet with the original analyses and duplicates and references to match the rejected\n           worksheet."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getAnalysesUIDs(self):\n        analyses = self.getAnalyses()\n        if isinstance(analyses, list):\n            return [an.UID() for an in analyses]\n        return []", "response": "Returns the UIDs of the analyses assigned to this worksheet"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the percentage of this worksheet s content.", "response": "def getProgressPercentage(self):\n        \"\"\"Returns the progress percentage of this worksheet\n        \"\"\"\n        state = api.get_workflow_status_of(self)\n        if state == \"verified\":\n            return 100\n\n        steps = 0\n        query = dict(getWorksheetUID=api.get_uid(self))\n        analyses = api.search(query, CATALOG_ANALYSIS_LISTING)\n        max_steps = len(analyses) * 2\n        for analysis in analyses:\n            an_state = analysis.review_state\n            if an_state in [\"rejected\", \"retracted\", \"cancelled\"]:\n                steps += 2\n            elif an_state in [\"verified\", \"published\"]:\n                steps += 2\n            elif an_state == \"to_be_verified\":\n                steps += 1\n        if steps == 0:\n            return 0\n        if steps > max_steps:\n            return 100\n        return (steps * 100)/max_steps"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_data_line(self, sline):\n        # if there are less values founded than headers, it's an error\n        if len(sline) != len(self._columns):\n            self.err(\"One data line has the wrong number of items\")\n            return -1\n\n        rawdict = {}\n        for idx, result in enumerate(sline):\n            rawdict[self._columns[idx]] = result\n        # Getting resid\n        resid = rawdict['Sample name']\n        del rawdict['Sample name']\n        # Getting date\n        rawdict['DateTime'] = self.csvDate2BikaDate(rawdict['Date'], rawdict['Time'])\n        del rawdict['Date']\n        del rawdict['Time']\n        # Getting remarks\n        rawdict['Remarks'] = rawdict['Remark']\n        del rawdict['Remark']\n        # Getting errors\n        rawdict['Error'] = rawdict['Error/Warning']\n        if rawdict['Error/Warning']:\n            self.warn('Analysis warn', numline=self._numline)\n        del rawdict['Error/Warning']\n\n        rawdict['DefaultResult'] = 'Concentration'\n        self._addRawResult(resid,\n                          {rawdict['Parameter'].replace(' ', ''): rawdict},\n                           False)\n        return 0", "response": "Parses the data line and builds the dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef folderitem(self, obj, item, index):\n\n        url = obj.absolute_url()\n        title = obj.Title()\n\n        item['Description'] = obj.Description()\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n\n        # Icons\n        after_icons = \"\"\n        if obj.getBlank():\n            after_icons += get_image(\n                \"blank.png\", title=t(_(\"Blank\")))\n        if obj.getHazardous():\n            after_icons += get_image(\n                \"hazardous.png\", title=t(_(\"Hazardous\")))\n        if after_icons:\n            item[\"after\"][\"Title\"] = after_icons\n\n        return item", "response": "Applies new properties to the item that is currently being\n        rendered as a row in the list\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the best possible title or id of an object regardless of whether it s a catalog brain or an object.", "response": "def _pretty_title_or_id(context, obj, empty_value=_marker):\n    \"\"\"Return the best possible title or id of an item, regardless\n       of whether obj is a catalog brain or an object, but returning an\n       empty title marker if the id is not set (i.e. it's auto-generated).\n    \"\"\"\n    # if safe_hasattr(obj, 'aq_explicit'):\n    #    obj = obj.aq_explicit\n    #title = getattr(obj, 'Title', None)\n    title = None\n    if base_hasattr(obj, 'Title'):\n        title = getattr(obj, 'Title', None)\n    if safe_callable(title):\n        title = title()\n    if title:\n        return title\n    item_id = getattr(obj, 'getId', None)\n    if safe_callable(item_id):\n        item_id = item_id()\n    if item_id and not isIDAutoGenerated(context, item_id):\n        return item_id\n    if empty_value is _marker:\n        empty_value = getEmptyTitle(context)\n    return empty_value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an array with the Templates available in the Bika LIMS add - on and the add - ons directory specified and the templates from the resources directory specified and the add - on s directory specified.", "response": "def getTemplates(bikalims_path, restype, filter_by_type=False):\n    \"\"\" Returns an array with the Templates available in the Bika LIMS path\n        specified plus the templates from the resources directory specified and\n        available on each additional product (restype).\n\n        Each array item is a dictionary with the following structure:\n            {'id': <template_id>,\n             'title': <template_title>}\n\n        If the template lives outside the bika.lims add-on, both the template_id\n        and template_title include a prefix that matches with the add-on\n        identifier. template_title is the same name as the id, but with\n        whitespaces and without extension.\n\n        As an example, for a template from the my.product add-on located in\n        <restype> resource dir, and with a filename \"My_cool_report.pt\", the\n        dictionary will look like:\n            {'id': 'my.product:My_cool_report.pt',\n             'title': 'my.product: My cool report'}\n\n        :param bikalims_path: the path inside bika lims to find the stickers.\n        :type bikalims_path: an string as a path\n        :param restype: the resource directory type to search for inside\n            an addon.\n        :type restype: string\n        :param filter_by_type: the folder name to look for inside the\n        templates path\n        :type filter_by_type: string/boolean\n    \"\"\"\n    # Retrieve the templates from bika.lims add-on\n    templates_dir = resource_filename(\"bika.lims\", bikalims_path)\n    tempath = os.path.join(templates_dir, '*.pt')\n    templates = [os.path.split(x)[-1] for x in glob.glob(tempath)]\n\n    # Retrieve the templates from other add-ons\n    for templates_resource in iterDirectoriesOfType(restype):\n        prefix = templates_resource.__name__\n        if prefix == 'bika.lims':\n            continue\n        directory = templates_resource.directory\n        # Only use the directory asked in 'filter_by_type'\n        if filter_by_type:\n            directory = directory + '/' + filter_by_type\n        if os.path.isdir(directory):\n            dirlist = os.listdir(directory)\n            exts = ['{0}:{1}'.format(prefix, tpl) for tpl in dirlist if\n                    tpl.endswith('.pt')]\n            templates.extend(exts)\n\n    out = []\n    templates.sort()\n    for template in templates:\n        title = template[:-3]\n        title = title.replace('_', ' ')\n        title = title.replace(':', ': ')\n        out.append({'id': template,\n                    'title': title})\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getStickerTemplates(filter_by_type=False):\n    # Retrieve the templates from bika.lims add-on\n    # resdirname\n    resdirname = 'stickers'\n    if filter_by_type:\n        bikalims_path = os.path.join(\n            \"browser\", \"templates\", resdirname, filter_by_type)\n    else:\n        bikalims_path = os.path.join(\"browser\", \"templates\", resdirname)\n    # getTemplates needs two parameters, the first one is the bikalims path\n    # where the stickers will be found. The second one is the resource\n    # directory type. This allows us to filter stickers by the type we want.\n    return getTemplates(bikalims_path, resdirname, filter_by_type)", "response": "Returns an array with the sticker templates available in the templates folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef guard_create_partitions(analysis_request):\n    if not analysis_request.bika_setup.getShowPartitions():\n        # If partitions are disabled in Setup, return False\n        return False\n\n    if analysis_request.isPartition():\n        # Do not allow the creation of partitions from partitions\n        return False\n\n    # Allow only the creation of partitions if all analyses from the Analysis\n    # Request are in unassigned state. Otherwise, we could end up with\n    # inconsistencies, because original analyses are deleted when the partition\n    # is created. Note here we exclude analyses from children (partitions).\n    analyses = analysis_request.objectValues(\"Analysis\")\n    for analysis in analyses:\n        if api.get_workflow_status_of(analysis) != \"unassigned\":\n            return False\n    return analyses and True or False", "response": "Returns True if partitions can be created using the analysis request\n    passed in as the source."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn whether the transition submit can be performed or not.", "response": "def guard_submit(analysis_request):\n    \"\"\"Return whether the transition \"submit\" can be performed or not.\n    Returns True if there is at least one analysis in a non-detached state and\n    all analyses in a non-detached analyses have been submitted.\n    \"\"\"\n    analyses_ready = False\n    for analysis in analysis_request.getAnalyses():\n        analysis = api.get_object(analysis)\n        analysis_status = api.get_workflow_status_of(analysis)\n        if analysis_status in ANALYSIS_DETACHED_STATES:\n            continue\n        if analysis_status in ['assigned', 'unassigned', 'registered']:\n            return False\n        analyses_ready = True\n    return analyses_ready"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn whether rollback_to_receive transition can be performed or not.", "response": "def guard_rollback_to_receive(analysis_request):\n    \"\"\"Return whether 'rollback_to_receive' transition can be performed or not.\n    Returns True if the analysis request has at least one analysis in 'assigned'\n    or 'unassigned' status. Otherwise, returns False\n    \"\"\"\n    skipped = 0\n    valid_states = ['unassigned', 'assigned']\n    skip_states = ['retracted', 'rejected']\n    analyses = analysis_request.getAnalyses()\n    for analysis in analyses:\n        analysis = api.get_object(analysis)\n        status = api.get_workflow_status_of(analysis)\n        if status in valid_states:\n            return True\n        elif status in skip_states:\n            skipped += 1\n    return len(analyses) == skipped"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn whether cancel transition can be performed or not. Returns whether cancel transition can be performed or not. Returns whether cancel transition can be performed or not. Returns whether cancel transition can be performed or not. Returns True only if all analyses are in unassigned status", "response": "def guard_cancel(analysis_request):\n    \"\"\"Returns whether 'cancel' transition can be performed or not. Returns\n    True only if all analyses are in \"unassigned\" status\n    \"\"\"\n    # Ask to partitions\n    for partition in analysis_request.getDescendants(all_descendants=False):\n        if not isTransitionAllowed(partition, \"cancel\"):\n            return False\n\n    # Look through analyses. We've checked the partitions already, so there is\n    # no need to look through analyses from partitions again, but through the\n    # analyses directly bound to the current Analysis Request.\n    cancellable_states = [\"unassigned\", \"registered\"]\n    for analysis in analysis_request.objectValues(\"Analysis\"):\n        if api.get_workflow_status_of(analysis) not in cancellable_states:\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns whether sample transition can be performed or not. Returns True only if the user has the Sampler set or the DateSampled and Sampler set. Returns False only if the user belongs to the Samplers group", "response": "def guard_sample(analysis_request):\n    \"\"\"Returns whether 'sample' transition can be performed or not. Returns\n    True only if the analysis request has the DateSampled and Sampler set or if\n    the user belongs to the Samplers group\n    \"\"\"\n    if analysis_request.getDateSampled() and analysis_request.getSampler():\n        return True\n\n    current_user = api.get_current_user()\n    return \"Sampler\" in current_user.getRolesInContext(analysis_request)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn assigned or unassigned depending on the state of the Analysis analyses the analysisrequest contains. Returns unassigned if the Analysis analyses are not assigned. Returns assigned if the Analysis analyses are not assigned. Returns unassigned if the Analysis analyses are not assigned.", "response": "def assigned_state(instance):\n    \"\"\"Returns `assigned` or `unassigned` depending on the state of the\n    analyses the analysisrequest contains. Return `unassigned` if the Analysis\n    Request has at least one analysis in `unassigned` state.\n    Otherwise, returns `assigned`\n    \"\"\"\n    analyses = instance.getAnalyses()\n    if not analyses:\n        return \"unassigned\"\n    for analysis in analyses:\n        analysis_object = api.get_object(analysis)\n        if not analysis_object.getWorksheet():\n            return \"unassigned\"\n    return \"assigned\""}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef listing_searchable_text(instance):\n    entries = set()\n    catalog = api.get_tool(CATALOG_ANALYSIS_REQUEST_LISTING)\n    columns = catalog.schema()\n    brains = catalog({\"UID\": api.get_uid(instance)})\n    brain = brains[0] if brains else None\n    for column in columns:\n        brain_value = api.safe_getattr(brain, column, None)\n        instance_value = api.safe_getattr(instance, column, None)\n        parsed = api.to_searchable_text_metadata(brain_value or instance_value)\n        entries.add(parsed)\n\n    # add metadata of all descendants\n    for descendant in instance.getDescendants():\n        entries.add(listing_searchable_text(descendant)())\n\n    # Concatenate all strings to one text blob\n    return \" \".join(entries)", "response": "Returns all the values of metadata columns in the catalog for\n    wildcard searches\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_errors(f):\n    import traceback\n    from plone.jsonapi.core.helpers import error\n\n    def decorator(*args, **kwargs):\n        try:\n            return f(*args, **kwargs)\n        except Exception:\n            var = traceback.format_exc()\n            return error(var)\n\n    return decorator", "response": "Simple error handler for the\nridge API."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_include_fields(request):\n    include_fields = []\n    rif = request.get(\"include_fields\", \"\")\n    if \"include_fields\" in request:\n        include_fields = [x.strip()\n                          for x in rif.split(\",\")\n                          if x.strip()]\n    if \"include_fields[]\" in request:\n        include_fields = request['include_fields[]']\n    return include_fields", "response": "Retrieve include_fields values from the request"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_brain_metadata(proxy, include_fields):\n    ret = {}\n    for index in proxy.indexes():\n        if index not in proxy:\n            continue\n        if include_fields and index not in include_fields:\n            continue\n        val = getattr(proxy, index)\n        if val != Missing.Value:\n            try:\n                json.dumps(val)\n            except:\n                continue\n            ret[index] = val\n    return ret", "response": "Load values from the catalog metadata into a list of dictionaries"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_field_values(instance, include_fields):\n    ret = {}\n    schema = instance.Schema()\n    val = None\n    for field in schema.fields():\n        fieldname = field.getName()\n        if include_fields and fieldname not in include_fields:\n            continue\n        try:\n            val = field.get(instance)\n        except AttributeError:\n            # If this error is raised, make a look to the add-on content\n            # expressions used to obtain their data.\n            print \"AttributeError:\", sys.exc_info()[1]\n            print \"Unreachable object. Maybe the object comes from an Add-on\"\n            print traceback.format_exc()\n\n        if val:\n            field_type = field.type\n            # If it a proxy field, we should know to the type of the proxied\n            # field\n            if field_type == 'proxy':\n                actual_field = field.get_proxy(instance)\n                field_type = actual_field.type\n            if field_type == \"blob\" or field_type == 'file':\n                continue\n            # I put the UID of all references here in *_uid.\n            if field_type == 'reference':\n                if type(val) in (list, tuple):\n                    ret[fieldname + \"_uid\"] = [v.UID() for v in val]\n                    val = [to_utf8(v.Title()) for v in val]\n                else:\n                    ret[fieldname + \"_uid\"] = val.UID()\n                    val = to_utf8(val.Title())\n            elif field_type == 'boolean':\n                val = True if val else False\n            elif field_type == 'text':\n                val = to_utf8(val)\n\n        try:\n            json.dumps(val)\n        except:\n            val = str(val)\n        ret[fieldname] = val\n    return ret", "response": "Load values from an AT object schema fields into a list of dictionaries"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_include_methods(request):\n    methods = request.get(\"include_methods\", \"\")\n    include_methods = [\n        x.strip() for x in methods.split(\",\") if x.strip()]\n    return include_methods", "response": "Retrieve include_methods values from the request"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_fields_from_request(obj, request):\n    schema = obj.Schema()\n    # fields contains all schema-valid field values from the request.\n    fields = {}\n    for fieldname, value in request.items():\n        if fieldname not in schema:\n            continue\n        field = schema[fieldname]\n        widget = field.getWidgetName()\n        if widget in [\"ReferenceWidget\"]:\n            brains = []\n            if value:\n                brains = resolve_request_lookup(obj, request, fieldname)\n                if not brains:\n                    logger.warning(\n                        \"JSONAPI: Can't resolve reference: {} {}\"\n                        .format(fieldname, value))\n                    return []\n            if schema[fieldname].multiValued:\n                value = [b.UID for b in brains] if brains else []\n            else:\n                value = brains[0].UID if brains else None\n        fields[fieldname] = value\n    # Write fields.\n    for fieldname, value in fields.items():\n        field = schema[fieldname]\n        fieldtype = field.getType()\n        if fieldtype == 'Products.Archetypes.Field.BooleanField':\n            if value.lower() in ('0', 'false', 'no') or not value:\n                value = False\n            else:\n                value = True\n        elif fieldtype in ['Products.ATExtensions.field.records.RecordsField',\n                           'Products.ATExtensions.field.records.RecordField']:\n            try:\n                value = eval(value)\n            except:\n                logger.warning(\n                    \"JSONAPI: \" + fieldname + \": Invalid \"\n                    \"JSON/Python variable\")\n                return []\n        mutator = field.getMutator(obj)\n        if mutator:\n            mutator(value)\n        else:\n            field.set(obj, value)\n    obj.reindexObject()\n    return fields.keys()", "response": "Set the fields of the object to the values in the request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove(self, context, request):\n\n        savepoint = transaction.savepoint()\n        uc = getToolByName(context, 'uid_catalog')\n\n        _uid = request.get('UID', '')\n        if not _uid:\n            raise BadRequest(\"No UID specified in request\")\n\n        ret = {\n            \"url\": router.url_for(\"remove\", force_external=True),\n            \"success\": True,\n            \"error\": False,\n        }\n        \n        data = uc(UID=_uid)\n        if not data:\n            raise BadRequest(\"No objects found\")\n        \n        for proxy in data:\n            try:\n                parent = proxy.getObject().aq_parent\n                parent.manage_delObjects([proxy.id])\n            except Exception as e:\n                savepoint.rollback()\n                msg = \"Cannot delete '{0}' because ({1})\".format(_uid, e.message)\n                raise BadRequest(msg)\n        return ret", "response": "This function is called by the remove action of the object. It deletes the object from the object store and returns the object store."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef isVisible(self, instance, mode='view', default=None, field=None):\n        vis = super(RejectionWidget, self).isVisible(\n            instance=instance, mode=mode, default=default, field=field)\n        if instance.bika_setup.isRejectionWorkflowEnabled():\n            return vis\n        else:\n            return 'invisible'", "response": "This function returns the visibility of the widget depending on whether the rejection workflow is enabled or not."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a sorted list with the options defined in bikasetup", "response": "def rejectionOptionsList(self):\n        \"Return a sorted list with the options defined in bikasetup\"\n        plone = getSite()\n        settings = plone.bika_setup\n        # RejectionReasons will return something like:\n        # [{'checkbox': u'on', 'textfield-2': u'b', 'textfield-1': u'c', 'textfield-0': u'a'}]\n        if len(settings.RejectionReasons) > 0:\n            reject_reasons = settings.RejectionReasons[0]\n        else:\n            return []\n        sorted_keys = sorted(reject_reasons.keys())\n        if 'checkbox' in sorted_keys:\n            sorted_keys.remove('checkbox')\n        # Building the list with the values only because the keys are not needed any more\n        items = []\n        for key in sorted_keys:\n            items.append(reject_reasons[key].strip())\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a string with the options both from selected and input items", "response": "def getRejectionReasons(self,dd):\n        \"\"\"\n        'd' is a dictionary with the stored data in the widget like:\n        {u'selected': [u'a', u'b'], u'checkbox': True, u'other': 'dsadas', u'checkbox_other': True}\n        Returns a string with the options both from selected and input items\n        \"\"\"\n        keys = dd.keys()\n        reasons = []\n        if not('checkbox' in keys ) or not(dd['checkbox']):\n            return 0\n        if 'selected' in keys:\n            reasons += dd['selected']\n        if 'other' in keys and dd['checkbox_other']:\n            reasons.append(dd['other'])\n        if len(reasons) < 1:\n            return \"Yes, unknow\"\n        return ', '.join(reasons)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread Dimensional - CSV analysis results", "response": "def Import(context, request):\n    \"\"\" Read Dimensional-CSV analysis results\n    \"\"\"\n    form = request.form\n    # TODO form['file'] sometimes returns a list\n    infile = form['instrument_results_file'][0] if \\\n        isinstance(form['instrument_results_file'], list) else \\\n        form['instrument_results_file']\n    artoapply = form['artoapply']\n    override = form['results_override']\n\n    instrument = form.get('instrument', None)\n    errors = []\n    logs = []\n\n    # Load the most suitable parser according to file extension/options/etc...\n    parser = None\n    if not hasattr(infile, 'filename'):\n        errors.append(_(\"No file selected\"))\n    parser = TwoDimensionCSVParser(infile)\n    status = get_instrument_import_ar_allowed_states(artoapply)\n    over = get_instrument_import_override(override)\n    importer = TwoDimensionImporter(parser=parser,\n                                    context=context,\n                                    allowed_ar_states=status,\n                                    allowed_analysis_states=None,\n                                    override=over,\n                                    instrument_uid=instrument,\n                                    form=form)\n    tbex = ''\n    try:\n        importer.process()\n    except:\n        tbex = traceback.format_exc()\n    errors = importer.errors\n    logs = importer.logs\n    warns = importer.warns\n    if tbex:\n        errors.append(tbex)\n\n    results = {'errors': errors, 'log': logs, 'warns': warns}\n\n    return json.dumps(results)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_kw(ar_or_sample, kw):\n    for analysis in find_analyses(ar_or_sample):\n        if kw in get_interims_keywords(analysis):\n            return analysis.getKeyword()\n    return None", "response": "This function returns the keyword that is used by other Analysis objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a header line and sets self. _end_header to True.", "response": "def parse_headerline(self, line):\n        \"\"\" Parses header lines\n\n            Keywords example:\n            Keyword1, Keyword2, Keyword3, ..., end\n        \"\"\"\n        if self._end_header is True:\n            # Header already processed\n            return 0\n\n        splitted = [token.strip() for token in line.split(',')]\n        if splitted[-1] == 'end':\n            self._keywords = splitted[1:-1]  # exclude the word end\n            self._end_header = True\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_resultsline(self, line):\n        splitted = [token.strip() for token in line.split(',')]\n\n        if splitted[0] == 'end':\n            return 0\n\n        blank_line = [i for i in splitted if i != '']\n        if len(blank_line) == 0:\n            return 0\n\n        quantitation = {}\n        list_of_interim_results = []\n        # list_of_interim_results is a list that will have interim fields on\n        # the current line so that we don't have to call self._addRawResult\n        # for the same interim fields, ultimately we want a dict that looks\n        # like quantitation = {'AR': 'AP-0001-R01', 'interim1': 83.12, 'interim2': 22.3}\n        # self._addRawResult(quantitation['AR'],\n        #                    values={kw: quantitation},\n        #                    override=False)\n        # We use will one of the interims to find the analysis in this case new_kw which becomes kw\n        # kw is the analysis keyword which sometimes we have to find using the interim field\n        # because we have the result of the interim field and not of the analysis\n\n        found = False  # This is just a flag used to check values in list_of_interim_results\n        clean_splitted = splitted[1:-1]  # First value on the line is AR\n        for i in range(len(clean_splitted)):\n            token = clean_splitted[i]\n            if i < len(self._keywords):\n                quantitation['AR'] = splitted[0]\n                # quantitation['AN'] = self._keywords[i]\n                quantitation['DefaultResult'] = 'resultValue'\n                quantitation['resultValue'] = token\n            elif token:\n                self.err(\"Orphan value in column ${index} (${token})\",\n                         mapping={\"index\": str(i + 1),\n                                  \"token\": token},\n                         numline=self._numline, line=line)\n\n            result = quantitation[quantitation['DefaultResult']]\n            column_name = quantitation['DefaultResult']\n            result = self.get_result(column_name, result, line)\n            quantitation[quantitation['DefaultResult']] = result\n\n            kw = re.sub(r\"\\W\", \"\", self._keywords[i])\n            if not is_keyword(kw):\n                new_kw = find_kw(quantitation['AR'], kw)\n                if new_kw:\n                    quantitation[kw] = quantitation['resultValue']\n                    del quantitation['resultValue']\n                    for interim_res in list_of_interim_results:\n                        if kw in interim_res:\n                            # Interim field already in quantitation dict\n                            found = True\n                            break\n                    if found:\n                        continue\n                    interims = find_analysis_interims(quantitation['AR'])\n                    # pairing headers(keywords) and their values(results) per line\n                    keyword_value_dict = dict(zip(self._keywords, clean_splitted))\n                    for interim in interims:\n                        if interim in keyword_value_dict:\n                            quantitation[interim] = keyword_value_dict[interim]\n                            list_of_interim_results.append(quantitation)\n                    kw = new_kw\n                    kw = re.sub(r\"\\W\", \"\", kw)\n\n            self._addRawResult(quantitation['AR'],\n                               values={kw: quantitation},\n                               override=False)\n            quantitation = {}\n            found = False", "response": "Parses the results line and returns a list of dictionaries that contains the keys of the result and the values of the interims that are found in the result."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef guard_unassign(duplicate_analysis):\n    analysis = duplicate_analysis.getAnalysis()\n    if wf.isTransitionAllowed(analysis, \"unassign\"):\n        return True\n    skip = [\"retracted\", \"rejected\", \"unassigned\"]\n    if api.get_review_status(analysis) in skip:\n        return True\n    return analysis_guards.guard_unassign(duplicate_analysis)", "response": "Returns whether the transition unassign can be performed or not"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef folderitem(self, obj, item, index):\n        item[\"Description\"] = obj.Description()\n        item[\"replace\"][\"Title\"] = get_link(item[\"url\"], item[\"Title\"])\n\n        instrument = obj.getInstrument()\n        if instrument:\n            instrument_url = api.get_url(instrument)\n            instrument_title = api.get_title(instrument)\n            item[\"Instrument\"] = instrument_title\n            item[\"replace\"][\"Instrument\"] = get_link(\n                instrument_url, value=instrument_title)\n\n        # Method\n        method_uid = obj.getMethodUID()\n        if method_uid:\n            method = api.get_object_by_uid(method_uid)\n            method_url = api.get_url(method)\n            method_title = api.get_title(method)\n            item[\"Method\"] = method_title\n            item[\"replace\"][\"Method\"] = get_link(\n                method_url, value=method_title)\n\n        return item", "response": "This method is called by the folderitem service triggered each time an item is iterated in folderitems. It returns the item dict containing the properties of the object to be used by the template."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the values from the ReflexRule section and returns them in the correct way to be saved. So the function will return a list of dictionaries: [...,{ 'conditions':[{ 'range1': 'X', 'range0': 'X', 'cond_row_idx':'X' 'and_or': 'and', 'analysisservice': '<as_uid>', }, {...}], 'trigger': 'xxx', 'actions':[{'action':'<action_name>', 'act_row_idx':'X', 'otherWS':'to_another', 'analyst': '<analyst_id>', 'an_result_id':'rep-1',...}, ] }, { 'conditions':[{ 'range1': 'X', 'range0': 'X', 'trigger': 'xxx', 'cond_row_idx':'X' 'and_or': 'and', 'analysisservice': '<as_uid>', }, { 'discreteresult': 'X', 'trigger': 'xxx', 'cond_row_idx':'X' 'and_or': 'and', 'analysisservice': '<as_uid>', }, {...}], 'trigger': 'xxx', 'actions':[{'action':'<action_name>', 'act_row_idx':'X', 'otherWS':to_another, 'analyst': '<analyst_id>', 'an_result_id':'rep-1',...}, {'action':'<action_name>', 'act_row_idx':'X', 'otherWS':to_another, 'analyst': '<analyst_id>', 'an_result_id':'rep-2'...}, ] }, ...]", "response": "def process_form(self, instance, field, form, empty_marker=None,\n                     emptyReturnsMarker=False):\n        \"\"\"\n        Gets the values from the ReflexRule section and returns them in the\n        correct way to be saved.\n        So the function will return a list of dictionaries:\n\n        [...,{\n            'conditions':[{\n                'range1': 'X', 'range0': 'X',\n                'cond_row_idx':'X'\n                'and_or': 'and',\n                'analysisservice': '<as_uid>',\n                }, {...}],\n            'trigger': 'xxx',\n            'actions':[{'action':'<action_name>', 'act_row_idx':'X',\n                            'otherWS':'to_another', 'analyst': '<analyst_id>',\n                            'an_result_id':'rep-1',...},\n                    ]\n        },\n        {\n            'conditions':[{\n                'range1': 'X', 'range0': 'X',\n                'trigger': 'xxx',\n                'cond_row_idx':'X'\n                'and_or': 'and',\n                'analysisservice': '<as_uid>',\n                },\n                {\n                'discreteresult': 'X',\n                'trigger': 'xxx',\n                'cond_row_idx':'X'\n                'and_or': 'and',\n                'analysisservice': '<as_uid>',\n                }, {...}],\n            'trigger': 'xxx',\n            'actions':[{'action':'<action_name>', 'act_row_idx':'X',\n                        'otherWS':to_another, 'analyst': '<analyst_id>',\n                        'an_result_id':'rep-1',...},\n                      {'action':'<action_name>', 'act_row_idx':'X',\n                        'otherWS':to_another, 'analyst': '<analyst_id>',\n                        'an_result_id':'rep-2'...},\n                ]\n        }, ...]\n        \"\"\"\n        if field.getName() != 'ReflexRules':\n            return RecordsWidget.process_form(\n                self, instance, field, form, empty_marker, emptyReturnsMarker)\n        raw_data = RecordsWidget.process_form(\n            self, instance, field, form, empty_marker, emptyReturnsMarker)\n        # 'value' is a list which will be saved\n        value = []\n        rulenum = 0\n        # Building the actions and conditions list\n        for raw_set in raw_data[0]:\n            d = self._format_conditions_and_actions(raw_set)\n            # Adding the rule number\n            d['rulenumber'] = str(rulenum)\n            # Filling the dict with the mother service UID\n            d['mother_service_uid'] = raw_data[0][0].get(\n                'analysisservice-0', '')\n            value.append(d)\n            rulenum += 1\n\n        return value, {}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_sorted_conditions_list(self, raw_set):\n        keys_list = raw_set.keys()\n        # cond_dicts_l is the final list which will contain the the\n        # dictionaries with the conditions.\n        # The dictionaries will be sorted by the index obtained in the\n        # template.\n        cond_dicts_l = []\n        # c_count is a counter which is incremented every time a new cond is\n        # added to the list, so we can give it a index.\n        c_count = 0\n        # cond_list will contain the keys starting with 'analysisservice-'\n        # but sorted by their index\n        cond_list = self._get_sorted_cond_keys(keys_list)\n        for key in cond_list:\n            range0_key = 'range0-'+str(c_count)\n            range0 = raw_set.get(range0_key, '')\n            range1_key = 'range1-'+str(c_count)\n            range1 = raw_set.get(range1_key, '')\n            discreteresult_key = 'discreteresult-'+str(c_count)\n            discreteresult = raw_set.get(discreteresult_key, '')\n            and_or_key = 'and_or-'+str(c_count)\n            and_or = raw_set.get(and_or_key, '')\n            # Building the conditions dict\n            cond_dict = {\n                'analysisservice': raw_set[key],\n                'cond_row_idx': c_count,\n                'range0': range0,\n                'range1': range1,\n                'discreteresult': discreteresult,\n                'and_or': and_or,\n                }\n            # Saves the conditions as a new dict inside the actions list\n            cond_dicts_l.append(cond_dict)\n            c_count += 1\n        return cond_dicts_l", "response": "This returns a list of dictionaries with the conditions got in the base class. The list is sorted by the index obtained by the analysisservice. The list is sorted by the conditions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_sorted_cond_keys(self, keys_list):\n        # The names can be found in reflexrulewidget.pt inside the\n        # conditionscontainer div.\n        cond_list = []\n        for key in keys_list:\n            if key.startswith('analysisservice-'):\n                cond_list.append(key)\n        cond_list.sort()\n        return cond_list", "response": "This function returns only the elements starting with the analysisservice - in keys_list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_sorted_action_keys(self, keys_list):\n        # The names can be found in reflexrulewidget.pt inside the\n        # Reflex action rules list section.\n        action_list = []\n        for key in keys_list:\n            if key.startswith('action-'):\n                action_list.append(key)\n        action_list.sort()\n        return action_list", "response": "This function returns only the elements starting with action - in\n        keys_list. The returned list is sorted by the index appended to the end of each element."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a json dict with all the setup data needed to build the reflex rule.", "response": "def getReflexRuleSetup(self):\n        \"\"\"\n        Return a json dict with all the setup data necessary to build the\n        relations:\n        - Relations between methods and analysis services options.\n        - The current saved data\n        the functions returns:\n        {'<method_uid>': {\n            'analysisservices': {\n                '<as_uid>': {'as_id': '<as_id>',\n                            'as_title':'<as_title>',\n                            'resultoptions': [,,]}\n                '<as_uid>': {'as_id': '<as_id>',\n                            'as_title':'<as_title>',\n                            'resultoptions': [{\n                                'ResultText': 'Failed',\n                                'ResultValue': '1', 'value': ''},\n                                ...\n                            ]}\n            },\n          'as_keys': ['<as_uid>', '<as_uid>'],\n          'method_id': '<method_id>',\n          'method_tile': '<method_tile>'\n          },\n        '<method_uid>': {\n            'analysisservices': {\n                '<as_uid>': {'as_id': '<as_id>',\n                            'as_title':'<as_title>',\n                            'resultoptions': [,,]}\n               '<as_uid>': {'as_id': '<as_id>',\n                            'as_title':'<as_title>',\n                            'resultoptions': [,,]}\n            },\n          'as_keys': ['<as_uid>', '<as_uid>'],\n          'method_id': '<method_id>',\n          'method_tile': '<method_tile>'\n          },\n         'saved_actions': {'rules': [\n                    {'actions': [{'act_row_idx': 0,\n                                   'action': 'repeat',\n                                   'an_result_id': '',\n                                   'analyst': '',\n                                   'otherWS': current,\n                                   'setresultdiscrete': '',\n                                   'setresulton': 'original',\n                                   'setresultvalue': '',\n                                   'worksheettemplate': '70d48adfb34c4231a145f76a858e94cf',}],\n                      'conditions': [{'analysisservice': 'd802cdbf1f4742c094d45997b1038f9c',\n                                      'and_or': 'no',\n                                      'cond_row_idx': 0,\n                                      'discreteresult': '',\n                                      'range0': '12',\n                                      'range1': '12'}],\n                      'rulenumber': '1',\n                      'trigger': 'submit'},...],\n                   'method_id': '<method_uid>',\n                   'method_tile': '<method_tile>',\n                   'method_uid': '<method_uid>'\n                   }\n        }\n        \"\"\"\n        relations = {}\n        # Getting all the methods from the system\n        pc = getToolByName(self, 'portal_catalog')\n        methods = [obj.getObject() for obj in pc(\n                    portal_type='Method',\n                    is_active=True)]\n        bsc = getToolByName(self, 'bika_setup_catalog')\n        for method in methods:\n            # Get the analysis services related to each method\n            an_servs_brains = bsc(\n                portal_type='AnalysisService',\n                getMethodUIDs={\n                    \"query\": method.UID(),\n                    \"operator\": \"or\"\n                })\n            analysiservices = {}\n            for analysiservice in an_servs_brains:\n                analysiservice = analysiservice.getObject()\n                # Getting the worksheet templates that could be used with the\n                # analysis, those worksheet templates are the ones without\n                # method and the ones with a method shared with the\n                # analysis service.\n                service_methods_uid = analysiservice.getAvailableMethodUIDs()\n                query_dict = {\n                    'portal_type': 'WorksheetTemplate',\n                    'is_active': True,\n                    'sort_on': 'sortable_title',\n                    'getMethodUID': {\n                        \"query\": service_methods_uid + [''],\n                        \"operator\": \"or\"\n                    }\n                }\n                wst_brains = bsc(query_dict)\n                analysiservices[analysiservice.UID()] = {\n                    'as_id': analysiservice.getId(),\n                    'as_title': analysiservice.Title(),\n                    'resultoptions':\n                        analysiservice.getResultOptions()\n                        if analysiservice.getResultOptions()\n                        else [],\n                    'wstoptions': [\n                        (brain.UID, brain.Title) for brain in wst_brains]\n                }\n            # Make the json dict\n            relations[method.UID()] = {\n                'method_id': method.getId(),\n                'method_tile': method.Title(),\n                'analysisservices': analysiservices,\n                'as_keys': analysiservices.keys(),\n            }\n        # Get the data saved in the object\n        reflex_rule = self.aq_parent.aq_inner\n        saved_method = reflex_rule.getMethod()\n        relations['saved_actions'] = {\n            'method_uid': saved_method.UID() if\n            saved_method else '',\n            'method_id': saved_method.getId() if\n            saved_method else '',\n            'method_tile': saved_method.Title() if\n            saved_method else '',\n            'rules': reflex_rule.getReflexRules(),\n            }\n        return json.dumps(relations)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getReflexRuleElement(self, idx=0, element=''):\n        rules_list = self.aq_parent.aq_inner.getReflexRules()\n        if len(rules_list) > idx:\n            value = rules_list[idx].get(element, '')\n            if element == 'actions' and value == '':\n                return [{'action': '', 'act_row_idx': '0',\n                        'otherWS': 'current', 'analyst': '',\n                        'setresulton': '', 'setresultdiscrete': '',\n                        'worksheettemplate': '',\n                        'setresultvalue': '', 'an_result_id': ''}, ]\n            elif element == 'conditions' and value == '':\n                return [{'analysisservice': '', 'cond_row_idx': '0',\n                        'range0': '', 'range1': '',\n                        'discreteresult': '', 'and_or': 'no'}, ]\n            else:\n                return value\n        if element == 'actions':\n            return [{'action': '', 'act_row_idx': '0',\n                    'otherWS': 'current', 'analyst': '',\n                    'worksheettemplate': '',\n                    'setresulton': '', 'setresultdiscrete': '',\n                    'setresultvalue': '', 'an_result_id': ''}, ]\n        elif element == 'conditions':\n            return [{'analysisservice': '', 'cond_row_idx': '0',\n                    'range0': '', 'range1': '',\n                    'discreteresult': '', 'and_or': 'no'}, ]\n        else:\n            return ''", "response": "Returns the expected value saved in the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getReflexRuleActionElement(self, set_idx=0, row_idx=0, element=''):\n        if isinstance(set_idx, str):\n            set_idx = int(set_idx)\n        if isinstance(row_idx, str):\n            row_idx = int(row_idx)\n        actions = self.getReflexRuleElement(idx=set_idx, element='actions')\n        return actions[row_idx].get(element, '')", "response": "Returns the expected value saved in the reflex rules set at the specified index."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the expected value saved in the action list object.", "response": "def getReflexRuleConditionElement(self, set_idx=0, row_idx=0, element=''):\n        \"\"\"\n        Returns the expected value saved in the action list object.\n        :set_idx: it is an integer with the position of the reflex rules set\n        in the widget's list.\n        :row_idx: is an integer with the numer of the row from the set\n        :element: a string with the name of the element of the action to\n            obtain: 'analysisservice', 'cond_row_idx', 'range0', 'range1',\n                    'discreteresult', and_or\n        \"\"\"\n        if isinstance(set_idx, str):\n            set_idx = int(set_idx)\n        if isinstance(row_idx, str):\n            row_idx = int(row_idx)\n        cond = self.getReflexRuleElement(idx=set_idx, element='conditions')\n        return cond[row_idx].get(element, '')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates hook for analysis profile analyses view", "response": "def update(self):\n        \"\"\"Update hook\n        \"\"\"\n        super(AnalysisProfileAnalysesView, self).update()\n        self.allow_edit = self.is_edit_allowed()\n        self.configuration = self.get_configuration()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_configuration(self):\n        mapping = {}\n        settings = self.get_settings()\n        for service in self.context.getService():\n            uid = api.get_uid(service)\n            setting = settings.get(uid, {})\n            config = {\n                \"hidden\": setting.get(\"hidden\", False),\n            }\n            mapping[uid] = config\n        return mapping", "response": "Returns a mapping of UID - > configuration"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of items in the folder.", "response": "def folderitems(self):\n        \"\"\"TODO: Refactor to non-classic mode\n        \"\"\"\n        items = super(AnalysisProfileAnalysesView, self).folderitems()\n        self.categories.sort()\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_form(self, instance, field, form, empty_marker=None,\n                     emptyReturnsMarker=False):\n        \"\"\"Return UIDs of the selected services for the AnalysisProfile reference field\n        \"\"\"\n\n        # selected services\n        service_uids = form.get(\"uids\", [])\n        # hidden services\n        hidden_services = form.get(\"Hidden\", {})\n\n        # get the service objects\n        services = map(api.get_object_by_uid, service_uids)\n        # get dependencies\n        dependencies = map(lambda s: s.getServiceDependencies(), services)\n        dependencies = list(itertools.chain.from_iterable(dependencies))\n        # Merge dependencies and services\n        services = set(services + dependencies)\n\n        as_settings = []\n        for service in services:\n            service_uid = api.get_uid(service)\n            hidden = hidden_services.get(service_uid, \"\") == \"on\"\n            as_settings.append({\"uid\": service_uid, \"hidden\": hidden})\n\n        # set the analysis services settings\n        instance.setAnalysisServicesSettings(as_settings)\n\n        return map(api.get_uid, services), {}", "response": "Process the form and set the analysis services settings for the AnalysisProfile reference field\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert the given date to a localized time string", "response": "def to_localized_time(self, date, **kw):\n        \"\"\"Converts the given date to a localized time string\n        \"\"\"\n        if date is None:\n            return \"\"\n        # default options\n        options = {\n            \"long_format\": True,\n            \"time_only\": False,\n            \"context\": api.get_portal(),\n            \"request\": api.get_request(),\n            \"domain\": \"senaite.core\",\n        }\n        options.update(kw)\n        return ulocalized_time(date, **options)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nformatting the price with the set decimal mark and currencySymbol", "response": "def format_price(self, price):\n        \"\"\"Formats the price with the set decimal mark and currency\n        \"\"\"\n        # ensure we have a float\n        price = api.to_float(price, default=0.0)\n        dm = self.get_decimal_mark()\n        cur = self.get_currency_symbol()\n        price = \"%s %.2f\" % (cur, price)\n        return price.replace(\".\", dm)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_billable_items(self):\n        items = []\n        for obj in self.context.getBillableItems():\n            if self.is_profile(obj):\n                items.append({\n                    \"obj\": obj,\n                    \"title\": obj.Title(),\n                    \"vat\": obj.getAnalysisProfileVAT(),\n                    \"price\": self.format_price(obj.getAnalysisProfilePrice()),\n                })\n            if self.is_analysis(obj):\n                items.append({\n                    \"obj\": obj,\n                    \"title\": obj.Title(),\n                    \"vat\": obj.getVAT(),\n                    \"price\": self.format_price(obj.getPrice()),\n                })\n        return items", "response": "Returns a list of billable items in the context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getInterval(self):\n        items = (\n            (\"\", _(u\"Not set\")),\n            (\"1\", _(u\"daily\")),\n            (\"7\", _(u\"weekly\")),\n            (\"30\", _(u\"monthly\")),\n            (\"90\", _(u\"quarterly\")),\n            (\"180\", _(u\"biannually\")),\n            (\"365\", _(u\"yearly\")),\n        )\n        return DisplayList(items)", "response": "Vocabulary of date intervals to calculate the To field date based\n            from the From field date."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef isValid(self):\n\n        today = DateTime()\n        valid_from = self.getValidFrom()\n        valid_to = self.getValidTo()\n\n        return valid_from <= today <= valid_to", "response": "Returns True if the current certificate is in a valid date range"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the days until this certificate expires", "response": "def getDaysToExpire(self):\n        \"\"\"Returns the days until this certificate expires\n\n        :returns: Days until the certificate expires\n        :rtype: int\n        \"\"\"\n\n        delta = 0\n        today = DateTime()\n        valid_from = self.getValidFrom() or today\n        valid_to = self.getValidTo()\n\n        # one of the fields is not set, return 0 days\n        if not valid_from or not valid_to:\n            return 0\n        # valid_from comes after valid_to?\n        if valid_from > valid_to:\n            return 0\n        # calculate the time between today and valid_to, even if valid_from\n        # is in the future.\n        else:\n            delta = valid_to - today\n\n        return int(math.ceil(delta))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_workflows():\n    wftool = api.get_tool(\"portal_workflow\")\n    wfs = {}\n    for wfid in wftool.objectIds():\n        wf = wftool.getWorkflowById(wfid)\n        if hasattr(aq_base(wf), \"updateRoleMappingsFor\"):\n            wfs[wfid] = wf\n    return wfs", "response": "Returns a mapping of id - > workflow"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the role mappings of the given object", "response": "def update_role_mappings(obj, wfs=None, reindex=True):\n    \"\"\"Update the role mappings of the given object\n    \"\"\"\n    wftool = api.get_tool(\"portal_workflow\")\n    if wfs is None:\n        wfs = get_workflows()\n    chain = wftool.getChainFor(obj)\n    for wfid in chain:\n        wf = wfs[wfid]\n        wf.updateRoleMappingsFor(obj)\n    if reindex is True:\n        obj.reindexObject(idxs=[\"allowedRolesAndUsers\"])\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fix_ar_sample_workflow(brain_or_object):\n\n    def log_change_state(ar_id, obj_id, src, dst):\n        msg = \"While fixing {ar_id}: \" \\\n              \"state changed for {obj_id}: \" \\\n              \"{src} -> {dst}\".format(**locals())\n\n    ar = get_object(brain_or_object)\n    if not IAnalysisRequest.providedBy(ar):\n        return\n\n    wf = api.get_tool('portal_workflow')\n    arwf = wf['bika_ar_workflow']\n    anwf = wf['bika_analysis_workflow']\n    swf = wf['bika_sample_workflow']\n    ignored = ['retracted', 'rejected']\n\n    tmp = filter(lambda x: x[0] not in ignored, arwf.states.items())\n    arstates = OrderedDict(tmp)\n    tmp = filter(lambda x: x[0] not in ignored, swf.states.items())\n    samplestates = OrderedDict(tmp)\n    tmp = filter(lambda x: x[0] in arstates, anwf.states.items())\n    anstates = OrderedDict(tmp)\n\n    # find least-early analysis state\n    # !!! Assumes states in definitions are roughly ordered earliest to latest\n    ar_dest_state = arstates.items()[0][0]\n    for anstate in anstates:\n        if ar.getAnalyses(review_state=anstate):\n            ar_dest_state = anstate\n\n    # Force state of AR\n    ar_state = get_review_status(ar)\n    if ar_state != ar_dest_state:\n        changeWorkflowState(ar, arwf.id, ar_dest_state)\n        log_change_state(ar.id, ar.id, ar_state, ar_dest_state)\n\n    # Force state of Sample\n    sample = ar.getSample()\n    sample_state = get_review_status(sample)\n    if ar_dest_state in samplestates:\n        changeWorkflowState(sample, swf.id, ar_dest_state)\n        log_change_state(ar.id, sample.id, sample_state, ar_dest_state)\n\n        # Force states of Partitions\n        for part in sample.objectValues():\n            part_state = get_review_status(part)\n            if get_review_status(part) != ar_dest_state:\n                changeWorkflowState(sample, swf.id, ar_dest_state)\n                log_change_state(ar.id, part.id, part_state, ar_dest_state)", "response": "Re - set the state of an AR Sample and SamplePartition to match the\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fix_items_stuck_in_sample_prep_states(portal, ut):\n    wftool = api.get_tool('portal_workflow')\n    catalog_ids = ['bika_catalog',\n                   'bika_analysis_catalog',\n                   'bika_catalog_analysisrequest_listing']\n    for catalog_id in catalog_ids:\n        catalog = api.get_tool(catalog_id)\n        brains = catalog(review_state='sample_prep')\n        for brain in brains:\n            instance = brain.getObject()\n            wfid = get_workflows_for(instance)[0]\n            wf = wftool[wfid]\n            # get event properties for last event that is not sample_prep\n            rh = wftool.getInfoFor(instance, 'review_history')\n            event = [x for x in rh if 'prep' not in x['review_state']\n                     and not x['comments']][-1]\n            state_id, action_id = event['review_state'], event['action']\n            # set state\n            changeWorkflowState(instance, wfid, state_id)\n            # fire transition handler for the action that originally was fired.\n            old_sdef = new_sdef = wf.states[state_id]\n            if action_id is not None:\n                tdef = wf.transitions[action_id]\n                notify(AfterTransitionEvent(\n                    instance, wf, old_sdef, new_sdef, tdef, event, {}))\n            # check AR state matches the analyses\n            if IAnalysisRequest.providedBy(instance):\n                fix_ar_sample_workflow(instance)\n        logger.info(\"Removed sample_prep state from {} items in {}.\"\n                    .format(len(brains), catalog_id))", "response": "Removes items that are stuck in sample_prep states"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef before_sample(analysis_request):\n    if not analysis_request.getDateSampled():\n        analysis_request.setDateSampled(DateTime())\n    if not analysis_request.getSampler():\n        analysis_request.setSampler(api.get_current_user().id)", "response": "Method triggered before sample transition for the Analysis Request passed in is performed"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef after_receive(analysis_request):\n    # Mark this analysis request as IReceived\n    alsoProvides(analysis_request, IReceived)\n\n    analysis_request.setDateReceived(DateTime())\n    do_action_to_analyses(analysis_request, \"initialize\")", "response": "Method triggered after the transition for the Analysis Request\n    passed in is performed"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getClientTitle(self):\n        request = self.getRequest()\n        if request:\n            client = request.getClient()\n            if client:\n                return client.Title()", "response": "Used to populate catalog values.\n        Returns the Title of the client for this analysis AR."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getClientUID(self):\n        request = self.getRequest()\n        if request:\n            client = request.getClient()\n            if client:\n                return client.UID()", "response": "Used to populate catalog values.\n        Returns the UID of the client for this analysis AR."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getClientURL(self):\n        request = self.getRequest()\n        if request:\n            client = request.getClient()\n            if client:\n                return client.absolute_url_path()", "response": "This method is used to populate catalog values\n        Returns the URL of the client for this analysis AR."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getDateReceived(self):\n        request = self.getRequest()\n        if request:\n            ar_date = request.getDateReceived()\n            if ar_date and self.created() > ar_date:\n                return self.created()\n            return ar_date\n        return None", "response": "Used to populate catalog values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getDueDate(self):\n        tat = self.getMaxTimeAllowed()\n        if not tat:\n            return None\n        start = self.getStartProcessDate()\n        if not start:\n            return None\n\n        # delta time when the first analysis is considered as late\n        delta = timedelta(minutes=api.to_minutes(**tat))\n\n        # calculated due date\n        end = dt2DT(DT2dt(start) + delta)\n\n        # delta is within one day, return immediately\n        if delta.days == 0:\n            return end\n\n        # get the laboratory workdays\n        setup = api.get_setup()\n        workdays = setup.getWorkdays()\n\n        # every day is a workday, no need for calculation\n        if workdays == tuple(map(str, range(7))):\n            return end\n\n        # reset the due date to the received date, and add only for configured\n        # workdays another day\n        due_date = end - delta.days\n\n        days = 0\n        while days < delta.days:\n            # add one day to the new due date\n            due_date += 1\n            # skip if the weekday is a non working day\n            if str(due_date.asdatetime().weekday()) not in workdays:\n                continue\n            days += 1\n\n        return due_date", "response": "This method calculates the difference between the time the analysis started and the maximum turnaround time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getResultsRange(self):\n        specs = ResultsRangeDict()\n        analysis_request = self.getRequest()\n        if not analysis_request:\n            return specs\n\n        keyword = self.getKeyword()\n        ar_ranges = analysis_request.getResultsRange()\n        # Get the result range that corresponds to this specific analysis\n        an_range = [rr for rr in ar_ranges if rr.get('keyword', '') == keyword]\n        return an_range and an_range[0] or specs", "response": "Returns the valid result range for this analysis based on the results ranges defined in the Analysis Request this routine is assigned to."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of Analyses who depend on us to calculate their result.", "response": "def getDependents(self, retracted=False):\n        \"\"\"\n        Returns a list of siblings who depend on us to calculate their result.\n        :param retracted: If false, retracted/rejected dependents are dismissed\n        :type retracted: bool\n        :return: Analyses the current analysis depends on\n        :rtype: list of IAnalysis\n        \"\"\"\n        def is_dependent(analysis):\n            calculation = analysis.getCalculation()\n            if not calculation:\n                return False\n\n            services = calculation.getRawDependentServices()\n            if not services:\n                return False\n\n            query = dict(UID=services, getKeyword=self.getKeyword())\n            services = api.search(query, \"bika_setup_catalog\")\n            return len(services) > 0\n\n        siblings = self.getSiblings(retracted=retracted)\n        return filter(lambda sib: is_dependent(sib), siblings)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of siblings who depend on this analysis.", "response": "def getDependencies(self, retracted=False):\n        \"\"\"\n        Return a list of siblings who we depend on to calculate our result.\n        :param retracted: If false retracted/rejected dependencies are dismissed\n        :type retracted: bool\n        :return: Analyses the current analysis depends on\n        :rtype: list of IAnalysis\n        \"\"\"\n        calc = self.getCalculation()\n        if not calc:\n            return []\n\n        dependencies = []\n        for sibling in self.getSiblings(retracted=retracted):\n            # We get all analyses that depend on me, also if retracted (maybe\n            # I am one of those that are retracted!)\n            deps = map(api.get_uid, sibling.getDependents(retracted=True))\n            if self.UID() in deps:\n                dependencies.append(sibling)\n        return dependencies"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getPrioritySortkey(self):\n        analysis_request = self.getRequest()\n        if analysis_request is None:\n            return None\n        ar_sort_key = analysis_request.getPrioritySortkey()\n        ar_id = analysis_request.getId().lower()\n        title = sortable_title(self)\n        if callable(title):\n            title = title()\n        return '{}.{}.{}'.format(ar_sort_key, ar_id, title)", "response": "Returns the key used for sorting the current Analysis from most prioritary to less prioritary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getHidden(self):\n        if self.getHiddenManually():\n            return self.getField('Hidden').get(self)\n        request = self.getRequest()\n        if request:\n            service_uid = self.getServiceUID()\n            ar_settings = request.getAnalysisServiceSettings(service_uid)\n            return ar_settings.get('hidden', False)\n        return False", "response": "Returns whether the analysis must be displayed in results\n        reports or not"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setHidden(self, hidden):\n        self.setHiddenManually(True)\n        self.getField('Hidden').set(self, hidden)", "response": "Sets the value of Hidden in this report and\n        in manage analyses view."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the reflex analysis of this object.", "response": "def setReflexAnalysisOf(self, analysis):\n        \"\"\"Sets the analysis that has been reflexed in order to create this\n        one, but if the analysis is the same as self, do nothing.\n        :param analysis: an analysis object or UID\n        \"\"\"\n        if not analysis or analysis.UID() == self.UID():\n            pass\n        else:\n            self.getField('ReflexAnalysisOf').set(self, analysis)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes VATAmount of the current object", "response": "def getVATAmount(self):\n        \"\"\" Compute VATAmount\n        \"\"\"\n        try:\n            vatamount = self.getTotalPrice() - Decimal(self.getPrice())\n        except:\n            vatamount = Decimal('0.00')\n        return vatamount.quantize(Decimal('0.00'))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getContactUIDForUser(self):\n        user = self.REQUEST.AUTHENTICATED_USER\n        user_id = user.getUserName()\n        r = self.portal_catalog(\n            portal_type='Contact',\n            getUsername=user_id\n        )\n        if len(r) == 1:\n            return r[0].UID", "response": "get the UID of the contact associated with the authenticated\n            user"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getSubtotal(self):\n        if self.supplyorder_lineitems:\n            return sum(\n                [(Decimal(obj['Quantity']) * Decimal(obj['Price'])) for obj in self.supplyorder_lineitems])\n        return 0", "response": "Compute Subtotal of the invoice"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the uids of the products referenced by order items", "response": "def getProductUIDs(self):\n        \"\"\" return the uids of the products referenced by order items\n        \"\"\"\n        uids = []\n        for orderitem in self.objectValues('XupplyOrderItem'):\n            product = orderitem.getProduct()\n            if product is not None:\n                uids.append(orderitem.getProduct().UID())\n        return uids"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_form(self, instance, field, form, empty_marker=None,\n                     emptyReturnsMarker=False):\n        \"\"\"Return a UID so that ReferenceField understands.\n        \"\"\"\n        fieldName = field.getName()\n        if fieldName + \"_uid\" in form:\n            uid = form.get(fieldName + \"_uid\", '')\n            if field.multiValued and\\\n                    (isinstance(uid, str) or isinstance(uid, unicode)):\n                uid = uid.split(\",\")\n        elif fieldName in form:\n            uid = form.get(fieldName, '')\n            if field.multiValued and\\\n                    (isinstance(uid, str) or isinstance(uid, unicode)):\n                uid = uid.split(\",\")\n        else:\n            uid = None\n        return uid, {}", "response": "Return a UID so that ReferenceField understands."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the field names to get values for the user", "response": "def get_field_names(self):\n        \"\"\"Return the field names to get values for\n        \"\"\"\n        col_model = self.request.get(\"colModel\", None)\n        if not col_model:\n            return [\"UID\",]\n\n        names = []\n        col_model = json.loads(_u(col_model))\n        if isinstance(col_model, (list, tuple)):\n            names = map(lambda c: c.get(\"columnName\", \"\").strip(), col_model)\n\n        # UID is used by reference widget to know the object that the user\n        # selected from the popup list\n        if \"UID\" not in names:\n            names.append(\"UID\")\n\n        return filter(None, names)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_data_record(self, brain, field_names):\n        record = {}\n        model = None\n\n        for field_name in field_names:\n            # First try to get the value directly from the brain\n            value = getattr(brain, field_name, None)\n\n            # No metadata for this column name\n            if value is None:\n                logger.warn(\"Not a metadata field: {}\".format(field_name))\n                model = model or SuperModel(brain)\n                value = model.get(field_name, None)\n                if callable(value):\n                    value = value()\n\n            # '&nbsp;' instead of '' because empty div fields don't render\n            # correctly in combo results table\n            record[field_name] = value or \"&nbsp;\"\n\n        return record", "response": "Returns a dict with the column values for the given brain and field names"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef search(self):\n        brains = []\n        # TODO Legacy\n        for name, adapter in getAdapters((self.context, self.request),\n                                         IReferenceWidgetVocabulary):\n            brains.extend(adapter())\n        return brains", "response": "Returns the list of brains that match with the request criteria\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_data_rows(self, brains):\n        fields = self.get_field_names()\n        return map(lambda brain: self.get_data_record(brain, fields), brains)", "response": "Returns a list of dictionaries representing the values of each brain"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the json payload for the given data rows", "response": "def to_json_payload(self, data_rows):\n        \"\"\"Returns the json payload\n        \"\"\"\n        num_rows = len(data_rows)\n        num_page = self.num_page\n        num_rows_page = self.num_rows_page\n\n        pages = num_rows / num_rows_page\n        pages += divmod(num_rows, num_rows_page)[1] and 1 or 0\n        start = (num_page - 1) * num_rows_page\n        end = num_page * num_rows_page\n        payload = {\"page\": num_page,\n                   \"total\": pages,\n                   \"records\": num_rows,\n                   \"rows\": data_rows[start:end]}\n        return json.dumps(payload)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_orphaned_attachments(portal):\n    attachments = api.search({\"portal_type\": \"Attachment\"})\n    total = len(attachments)\n    logger.info(\"Integrity checking %d attachments\" % total)\n    for num, attachment in enumerate(attachments):\n        obj = api.get_object(attachment)\n        # The method `getRequest` from the attachment tries to get the AR\n        # either directly or from one of the linked Analyses. If it returns\n        # `None`, we can be sure that the attachment is neither assigned\n        # directly to an AR nor to an Analysis.\n        ar = obj.getRequest()\n        if ar is None:\n            obj_id = api.get_id(obj)\n            api.get_parent(obj).manage_delObjects(obj_id)\n            logger.info(\"Deleted orphaned Attachment {}\".format(obj_id))", "response": "Delete orphaned attachments where the Analysis was removed\n      "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reindex_client_local_owner_permissions(portal):\n    start = time.time()\n    bsc = portal.bika_setup_catalog\n    uids = [c.UID() for c in portal.clients.objectValues()]\n    brains = bsc(getClientUID=uids)\n    total = len(brains)\n    for num, brain in enumerate(brains):\n        ob = brain.getObject()\n        logger.info(\"Reindexing permission for {}/{} ({})\"\n                    .format(num, total, ob.absolute_url()))\n        ob.reindexObjectSecurity()\n    end = time.time()\n    logger.info(\"Fixing local owner role on client objects took {:.2f}s\"\n                .format(end-start))\n    transaction.commit()", "response": "Reindex bika_setup\n    objects located in clients to give proper permissions to client contacts."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef migrate_attachment_report_options(portal):\n    attachments = api.search({\"portal_type\": \"Attachment\"})\n    total = len(attachments)\n    logger.info(\"Migrating 'Attach to Report' -> 'Ingore in Report' \"\n                \"for %d attachments\" % total)\n    for num, attachment in enumerate(attachments):\n        obj = api.get_object(attachment)\n\n        if obj.getReportOption() in [\"a\", \"\"]:\n            obj.setReportOption(\"i\")\n            obj.reindexObject()\n            logger.info(\"Migrated Attachment %s\" % obj.getTextTitle())", "response": "Migrate Attachments with the report option a to i"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rename_retract_ar_transition(portal):\n    logger.info(\"Renaming 'retract_ar' transition to 'invalidate'\")\n    wf_tool = api.get_tool(\"portal_workflow\")\n    workflow = wf_tool.getWorkflowById(\"bika_ar_workflow\")\n\n    if \"invalidate\" not in workflow.transitions:\n        workflow.transitions.addTransition(\"invalidate\")\n    transition = workflow.transitions.invalidate\n    transition.setProperties(\n        title=\"Invalidate\",\n        new_state_id=\"invalid\",\n        after_script_name=\"\",\n        actbox_name=\"Invalidate\",\n    )\n    guard = transition.guard or Guard()\n    guard_props = {\"guard_permissions\": \"BIKA: Retract\",\n                   \"guard_roles\": \"\",\n                   \"guard_expr\": \"python:here.guard_cancelled_object()\"}\n    guard.changeFromProperties(guard_props)\n    transition.guard = guard\n\n    for state in workflow.states.values():\n        if 'retract_ar' in state.transitions:\n            trans = filter(lambda id: id != 'retract_ar', state.transitions)\n            trans += ('invalidate', )\n            state.transitions = trans\n\n    if \"retract_ar\" in workflow.transitions:\n        workflow.transitions.deleteTransitions([\"retract_ar\"])", "response": "Renames retract_ar transition to invalidate"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef recatalog_analyses_due_date(portal):\n    logger.info(\"Updating Analyses getDueDate\")\n    # No need to update those analyses that are verified or published. Only\n    # those that are under work\n    catalog = api.get_tool(CATALOG_ANALYSIS_LISTING)\n    review_states = [\"retracted\", \"sample_due\", \"attachment_due\",\n                     \"sample_received\", \"to_be_verified\"]\n    query = dict(portal_type=\"Analysis\", review_state=review_states)\n    analyses = api.search(query, CATALOG_ANALYSIS_LISTING)\n    total = len(analyses)\n    num = 0\n    for num, analysis in enumerate(analyses, start=1):\n        analysis = api.get_object(analysis)\n        catalog.catalog_object(analysis, idxs=['getDueDate'])\n        if num % 100 == 0:\n            logger.info(\"Updating Analysis getDueDate: {0}/{1}\"\n                        .format(num, total))\n\n    logger.info(\"{} Analyses updated\".format(num))", "response": "Recatalog the index and metadata field getDueDate"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding the permission Reject Analysis Request and update the permission mappings accordingly", "response": "def update_rejection_permissions(portal):\n    \"\"\"Adds the permission 'Reject Analysis Request' and update the permission\n     mappings accordingly \"\"\"\n    updated = update_rejection_permissions_for(portal, \"bika_ar_workflow\",\n                                               \"Reject Analysis Request\")\n    if updated:\n        brains = api.search(dict(portal_type=\"AnalysisRequest\"),\n                            CATALOG_ANALYSIS_REQUEST_LISTING)\n        update_rolemappings_for(brains, \"bika_ar_workflow\")\n\n\n    updated = update_rejection_permissions_for(portal, \"bika_sample_workflow\",\n                                               \"Reject Sample\")\n    if updated:\n        brains = api.search(dict(portal_type=\"Sample\"), \"bika_catalog\")\n        update_rolemappings_for(brains, \"bika_sample_workflow\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_analaysisrequests_due_date(portal):\n    logger.info(\"Updating getLate -> getDueDate metadata columns ...\")\n    catalog_objects = False\n    catalog = api.get_tool(CATALOG_ANALYSIS_REQUEST_LISTING)\n    if \"getLate\" in catalog.schema():\n        catalog.delColumn(\"getLate\")\n\n    if \"getDueDate\" in catalog.schema():\n        logger.info(\"getDueDate column already in catalog [SKIP]\")\n    else:\n        logger.info(\"Adding column 'getDueDate' to catalog '{}' ...\"\n                    .format(catalog.id))\n        catalog.addColumn(\"getDueDate\")\n        catalog_objects = True\n\n    if \"getDueDate\" in catalog.indexes():\n        logger.info(\"getDueDate index already in catalog [SKIP]\")\n    else:\n        logger.info(\"Adding index 'getDueDate' to catalog '{}'\"\n                    .format(catalog.id))\n        catalog.addIndex(\"getDueDate\", \"DateIndex\")\n        if not catalog_objects:\n            catalog.manage_reindexIndex(\"getDueDate\")\n\n    if catalog_objects:\n        # Only recatalog the objects if the column getDueDate was not there\n        num = 0\n        query = dict(portal_type=\"AnalysisRequest\")\n        ar_brains = api.search(query, CATALOG_ANALYSIS_REQUEST_LISTING)\n        total = len(ar_brains)\n        for num, analysis_request in enumerate(ar_brains):\n            analysis_request = api.get_object(analysis_request)\n            analysis_request.reindexObject(idxs=['getDueDate'])\n            if num % 100 == 0:\n                logger.info(\"Updating Analysis Request getDueDate: {0}/{1}\"\n                            .format(num, total))\n        logger.info(\"{} Analysis Requests updated\".format(num))\n\n    logger.info(\"Updating getLate -> getDueDate metadata columns [DONE]\")", "response": "Updates the metadata getLate and getDueDate columns of the AnalaysisRequests catalog and the Analysis Requests objects in ar - catalog."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nservicing triggered each time an item is iterated in folderitems. The use of this service prevents the extra-loops in child objects. :obj: the instance of the class to be foldered :item: dict containing the properties of the object to be used by the template :index: current index of the item", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Service triggered each time an item is iterated in folderitems.\n        The use of this service prevents the extra-loops in child objects.\n        :obj: the instance of the class to be foldered\n        :item: dict containing the properties of the object to be used by\n            the template\n        :index: current index of the item\n        \"\"\"\n\n        # We are using the existing logic from the auditview\n        logview = api.get_view(\"auditlog\", context=obj, request=self.request)\n\n        # get the last snapshot\n        snapshot = get_last_snapshot(obj)\n        # get the metadata of the last snapshot\n        metadata = get_snapshot_metadata(snapshot)\n\n        title = obj.Title()\n        url = obj.absolute_url()\n        auditlog_url = \"{}/@@auditlog\".format(url)\n\n        # Title\n        item[\"title\"] = title\n        # Link the title to the auditlog of the object\n        item[\"replace\"][\"title\"] = get_link(auditlog_url, value=title)\n\n        # Version\n        version = get_snapshot_version(obj, snapshot)\n        item[\"version\"] = version\n\n        # Modification Date\n        m_date = metadata.get(\"modified\")\n        item[\"modified\"] = logview.to_localized_time(m_date)\n\n        # Actor\n        actor = metadata.get(\"actor\")\n        item[\"actor\"] = actor\n\n        # Fullname\n        properties = api.get_user_properties(actor)\n        item[\"fullname\"] = properties.get(\"fullname\", actor)\n\n        # Roles\n        roles = metadata.get(\"roles\", [])\n        item[\"roles\"] = \", \".join(roles)\n\n        # Remote Address\n        remote_address = metadata.get(\"remote_address\")\n        item[\"remote_address\"] = remote_address\n\n        # Action\n        action = metadata.get(\"action\")\n        item[\"action\"] = logview.translate_state(action)\n\n        # Review State\n        review_state = metadata.get(\"review_state\")\n        item[\"review_state\"] = logview.translate_state(review_state)\n\n        # get the previous snapshot\n        prev_snapshot = get_snapshot_by_version(obj, version-1)\n        if prev_snapshot:\n            prev_metadata = get_snapshot_metadata(prev_snapshot)\n            prev_review_state = prev_metadata.get(\"review_state\")\n            if prev_review_state != review_state:\n                item[\"replace\"][\"review_state\"] = \"{} &rarr; {}\".format(\n                    logview.translate_state(prev_review_state),\n                    logview.translate_state(review_state))\n\n            # Rendered Diff\n            diff = compare_snapshots(snapshot, prev_snapshot)\n            item[\"diff\"] = logview.render_diff(diff)\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstore the analyses to be removed after partitions creation", "response": "def push_primary_analyses_for_removal(self, analysis_request, analyses):\n        \"\"\"Stores the analyses to be removed after partitions creation\n        \"\"\"\n        to_remove = self.analyses_to_remove.get(analysis_request, [])\n        to_remove.extend(analyses)\n        self.analyses_to_remove[analysis_request] = list(set(to_remove))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_primary_analyses(self):\n        for ar, analyses in self.analyses_to_remove.items():\n            analyses_ids = list(set(map(api.get_id, analyses)))\n            ar.manage_delObjects(analyses_ids)\n        self.analyses_to_remove = dict()", "response": "Remove analyses relocated to partitions\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_ar_data(self):\n        for obj in self.get_objects():\n            info = self.get_base_info(obj)\n            info.update({\n                \"analyses\": self.get_analysis_data_for(obj),\n                \"sampletype\": self.get_base_info(obj.getSampleType()),\n                \"number_of_partitions\": self.get_number_of_partitions_for(obj),\n                \"template\": self.get_template_data_for(obj),\n            })\n            yield info", "response": "Returns a list of AR data for all objects in the system"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_sampletype_data(self):\n        for obj in self.get_sampletypes():\n            info = self.get_base_info(obj)\n            yield info", "response": "Returns a list of SampleType data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_container_data(self):\n        for obj in self.get_containers():\n            info = self.get_base_info(obj)\n            yield info", "response": "Returns a list of Container data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_preservation_data(self):\n        for obj in self.get_preservations():\n            info = self.get_base_info(obj)\n            yield info", "response": "Returns a list of Preservation data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_objects(self):\n        # Create a mapping of source ARs for copy\n        uids = self.request.form.get(\"uids\", \"\")\n        if not uids:\n            # check for the `items` parammeter\n            uids = self.request.form.get(\"items\", \"\")\n        if isinstance(uids, basestring):\n            uids = uids.split(\",\")\n        unique_uids = OrderedDict().fromkeys(uids).keys()\n        return filter(None, map(self.get_object_by_uid, unique_uids))", "response": "Returns a list of objects coming from the uids request parameter\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the available SampleTypes of the system", "response": "def get_sampletypes(self):\n        \"\"\"Returns the available SampleTypes of the system\n        \"\"\"\n        query = {\n            \"portal_type\": \"SampleType\",\n            \"sort_on\": \"sortable_title\",\n            \"sort_order\": \"ascending\",\n            \"is_active\": True,\n        }\n        results = api.search(query, \"bika_setup_catalog\")\n        return map(api.get_object, results)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_containers(self):\n        query = dict(portal_type=\"Container\",\n                     sort_on=\"sortable_title\",\n                     sort_order=\"ascending\",\n                     is_active=True)\n        results = api.search(query, \"bika_setup_catalog\")\n        return map(api.get_object, results)", "response": "Returns the available Containers of the system"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_analysis_data_for(self, ar):\n        # Exclude analyses from children (partitions)\n        analyses = ar.objectValues(\"Analysis\")\n        out = []\n        for an in analyses:\n            info = self.get_base_info(an)\n            info.update({\n                \"service_uid\": an.getServiceUID(),\n            })\n            out.append(info)\n        return out", "response": "Return the Analysis data for this AR"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_template_data_for(self, ar):\n        info = None\n        template = ar.getTemplate()\n        ar_sampletype_uid = api.get_uid(ar.getSampleType())\n        ar_container_uid = \"\"\n        if ar.getContainer():\n            ar_container_uid = api.get_uid(ar.getContainer())\n        ar_preservation_uid = \"\"\n        if ar.getPreservation():\n            ar_preservation_uid = api.get_uid(ar.getPreservation())\n\n        if template:\n            info = self.get_base_info(template)\n\n            analyses = template.getAnalyses()\n            partition_analyses = map(\n                lambda x: (x.get(\"partition\"), x.get(\"service_uid\")), analyses)\n\n            analyses_by_partition = defaultdict(list)\n            for partition, service_uid in partition_analyses:\n                analyses_by_partition[partition].append(service_uid)\n\n            sampletypes_by_partition = defaultdict(list)\n            containers_by_partition = defaultdict(list)\n            preservations_by_partition = defaultdict(list)\n            for part in template.getPartitions():\n                part_id = part.get(\"part_id\")\n                sampletype_uid = part.get('sampletype_uid', ar_sampletype_uid)\n                sampletypes_by_partition[part_id] = sampletype_uid\n                container_uid = part.get(\"container_uid\", ar_container_uid)\n                containers_by_partition[part_id] = container_uid\n                preserv_uid = part.get(\"preservation_uid\", ar_preservation_uid)\n                preservations_by_partition[part_id] = preserv_uid\n\n            partitions = map(lambda p: p.get(\"part_id\"),\n                             template.getPartitions())\n            info.update({\n                \"analyses\": analyses_by_partition,\n                \"partitions\": partitions,\n                \"sample_types\": sampletypes_by_partition,\n                \"containers\": containers_by_partition,\n                \"preservations\": preservations_by_partition,\n            })\n        else:\n            info = {\n                \"analyses\": {},\n                \"partitions\": [],\n                \"sample_types\": {},\n                \"containers\": {},\n                \"preservations\": {},\n            }\n        return info", "response": "Returns the Template data for this AR"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the number of partitions for the given AR", "response": "def get_number_of_partitions_for(self, ar):\n        \"\"\"Return the number of selected partitions\n        \"\"\"\n        # fetch the number of partitions from the request\n        uid = api.get_uid(ar)\n        num = self.request.get(\"primary\", {}).get(uid)\n\n        if num is None:\n            # get the number of partitions from the template\n            template = ar.getTemplate()\n            if template:\n                num = len(template.getPartitions())\n            else:\n                num = DEFAULT_NUMBER_OF_PARTITIONS\n        try:\n            num = int(num)\n        except (TypeError, ValueError):\n            num = DEFAULT_NUMBER_OF_PARTITIONS\n        return num"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_base_info(self, obj):\n        obj = api.get_object(obj)\n        review_state = api.get_workflow_status_of(obj)\n        state_title = review_state.capitalize().replace(\"_\", \" \")\n        return {\n            \"obj\": obj,\n            \"id\": api.get_id(obj),\n            \"uid\": api.get_uid(obj),\n            \"title\": api.get_title(obj),\n            \"path\": api.get_path(obj),\n            \"url\": api.get_url(obj),\n            \"review_state\": review_state,\n            \"state_title\": state_title,\n        }", "response": "Extract the base info from the given object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of all categories with accredited services", "response": "def selected_cats(self, items):\n        \"\"\"return a list of all categories with accredited services\n        \"\"\"\n        cats = []\n        for item in items:\n            if 'category' in item and item['category'] not in cats:\n                cats.append(item['category'])\n        return cats"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef before_render(self):\n        # Render the Add button if the user has the AddClient permission\n        if check_permission(AddMethod, self.context):\n            self.context_actions[_(\"Add\")] = {\n                \"url\": \"createObject?type_name=Method\",\n                \"icon\": \"++resource++bika.lims.images/add.png\"\n            }\n        # Don't allow any context actions on the Methods folder\n        self.request.set(\"disable_border\", 1)", "response": "Render the Add button if the user has the AddClient permission"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply new properties to the item that is currently being rendered as a row in the list .", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Applies new properties to the item (Client) that is currently being\n        rendered as a row in the list\n\n        :param obj: client to be rendered as a row in the list\n        :param item: dict representation of the client, suitable for the list\n        :param index: current position of the item within the list\n        :type obj: ATContentType/DexterityContentType\n        :type item: dict\n        :type index: int\n        :return: the dict representation of the item\n        :rtype: dict\n        \"\"\"\n\n        url = obj.absolute_url()\n        title = obj.Title()\n\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n\n        instruments = obj.getInstruments()\n        if instruments:\n            links = map(\n                lambda i: get_link(i.absolute_url(), i.Title()), instruments)\n            item[\"replace\"][\"Instrument\"] = \", \".join(links)\n        else:\n            item[\"Instrument\"] = \"\"\n\n        calculation = obj.getCalculation()\n        if calculation:\n            title = calculation.Title()\n            url = calculation.absolute_url()\n            item[\"Calculation\"] = title\n            item[\"replace\"][\"Calculation\"] = get_link(url, value=title)\n        else:\n            item[\"Calculation\"] = \"\"\n\n        manual_entry_of_results_allowed = obj.isManualEntryOfResults()\n        item[\"ManualEntry\"] = manual_entry_of_results_allowed\n        item[\"replace\"][\"ManualEntry\"] = \" \"\n        if manual_entry_of_results_allowed:\n            item[\"replace\"][\"ManualEntry\"] = get_image(\"ok.png\")\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef contentsMethod(self, contentFilter):\n        allowedroles = ['Manager', 'LabManager', 'Client', 'LabClerk']\n        pm = getToolByName(self.context, \"portal_membership\")\n        member = pm.getAuthenticatedMember()\n        roles = member.getRoles()\n        allowed = [a for a in allowedroles if a in roles]\n        return self.context.objectValues('ARReport') if allowed else []", "response": "Return the ARReport objects associated to the current Analysis request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply the filter by client to the search query", "response": "def add_filter_by_client(self, query, out_params):\n        \"\"\"Applies the filter by client to the search query\n        \"\"\"\n        current_client = logged_in_client(self.context)\n        if current_client:\n            query['getClientUID'] = api.get_uid(current_client)\n        elif self.request.form.get(\"ClientUID\", \"\"):\n            query['getClientUID'] = self.request.form['ClientUID']\n            client = api.get_object_by_uid(query['getClientUID'])\n            out_params.append({'title': _('Client'),\n                               'value': client.Title(),\n                               'type': 'text'})"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_filter_by_date(self, query, out_params):\n        date_query = formatDateQuery(self.context, 'Requested')\n        if date_query:\n            query['created'] = date_query\n            requested = formatDateParms(self.context, 'Requested')\n            out_params.append({'title': _('Requested'),\n                               'value': requested,\n                               'type': 'text'})", "response": "Applies the filter by Date to the search query"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies the filter by review_state to the search query", "response": "def add_filter_by_review_state(self, query, out_params):\n        \"\"\"Applies the filter by review_state to the search query\n        \"\"\"\n        self.add_filter_by_wf_state(query=query, out_params=out_params,\n                                    wf_id=\"bika_analysis_workflow\",\n                                    index=\"review_state\",\n                                    title=_(\"Status\"))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nimport the Beckman Coulter Access 2 analysis results from the request.", "response": "def Import(context, request):\n    \"\"\" Beckman Coulter Access 2 analysis results\n    \"\"\"\n    infile = request.form['rochecobas_taqman_model48_file']\n    fileformat = request.form['rochecobas_taqman_model48_format']\n    artoapply = request.form['rochecobas_taqman_model48_artoapply']\n    override = request.form['rochecobas_taqman_model48_override']\n    instrument = request.form.get('instrument', None)\n    errors = []\n    logs = []\n    warns = []\n\n    # Load the most suitable parser according to file extension/options/etc...\n    parser = None\n    if not hasattr(infile, 'filename'):\n        errors.append(_(\"No file selected\"))\n    if fileformat == 'rsf':\n        parser = RocheCobasTaqmanParser(infile)\n    if fileformat == 'csv':\n        parser = RocheCobasTaqmanParser(infile, \"csv\")\n    else:\n        errors.append(t(_(\"Unrecognized file format ${fileformat}\",\n                          mapping={\"fileformat\": fileformat})))\n\n    if parser:\n        # Load the importer\n        status = ['sample_received', 'attachment_due', 'to_be_verified']\n        if artoapply == 'received':\n            status = ['sample_received']\n        elif artoapply == 'received_tobeverified':\n            status = ['sample_received', 'attachment_due', 'to_be_verified']\n\n        over = [False, False]\n        if override == 'nooverride':\n            over = [False, False]\n        elif override == 'override':\n            over = [True, False]\n        elif override == 'overrideempty':\n            over = [True, True]\n\n        importer = RocheCobasTaqman48Importer(parser=parser,\n                                              context=context,\n                                              allowed_ar_states=status,\n                                              allowed_analysis_states=None,\n                                              override=over,\n                                              instrument_uid=instrument)\n        tbex = ''\n        try:\n            importer.process()\n        except:\n            tbex = traceback.format_exc()\n        errors = importer.errors\n        logs = importer.logs\n        warns = importer.warns\n        if tbex:\n            errors.append(tbex)\n\n    results = {'errors': errors, 'log': logs, 'warns': warns}\n\n    return json.dumps(results)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the data line and builds the dictionary.", "response": "def parse_data_line(self, sline):\n        \"\"\"\n        Parses the data line and builds the dictionary.\n        :param sline: a split data line to parse\n        :returns: the number of rows to jump and parse the next data line or return the code error -1\n        \"\"\"\n        # if there are less values founded than headers, it's an error\n        if len(sline) != len(self._columns):\n            self.err(\"One data line has the wrong number of items\")\n            return -1\n        rawdict = {}\n        for idx, result in enumerate(sline):\n            rawdict[self._columns[idx]] = result\n        # Getting key values\n        resid = rawdict['Sample ID']\n        if resid == '' and rawdict['Order Number'] != '':\n            # If Sample ID is void, we will use the OrderNumber as a reference.\n            resid = rawdict['Order Number']\n        elif resid == '' and rawdict['Order Number'] == '':\n            # If there isn't Sample ID or Order Number\n            self.err(\"Result identification not found.\", numline=self._numline)\n            return -1\n        testname = rawdict['Test']\n        if testname == '':\n            # None test name found\n            self.err(\"Result test name not found.\", numline=self._numline)\n            return -1\n        del rawdict['Test']\n\n        # Building the new dict\n        rawdict['DefaultResult'] = 'Result'\n        rawdict['Remarks'] = ''.join([rawdict['Result'], \" on Order Number.\", resid]) \\\n            if rawdict['Result'] == \"Target Not Detected\" else ''\n\n        if self._fileformat == \"csv\":\n            rawdict['DateTime'] = rawdict['Accepted Date/Time']\n        else:\n            rawdict['DateTime'] = self.csvDate2BikaDate(\n                rawdict['Accepted Date/Time'])\n\n        self._addRawResult(resid, {testname: rawdict}, False)\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getClientUID(self):\n        client = api.get_parent(self)\n        if not client:\n            return \"\"\n        return api.get_uid(client)", "response": "Return the UID of the client"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getLinkedRequests(self):\n        rc = api.get_tool(\"reference_catalog\")\n        refs = rc.getBackReferences(self, \"AnalysisRequestAttachment\")\n        # fetch the objects by UID and handle nonexisting UIDs gracefully\n        ars = map(lambda ref: api.get_object_by_uid(ref.sourceUID, None), refs)\n        # filter out None values (nonexisting UIDs)\n        ars = filter(None, ars)\n        # sort by physical path, so that attachments coming from an AR with a\n        # higher \"-Rn\" suffix get sorted correctly.\n        # N.B. the created date is the same, hence we can not use it\n        return sorted(ars, key=api.get_path, reverse=True)", "response": "Lookup linked Analysis Requests\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getLinkedAnalyses(self):\n        # Fetch the linked Analyses UIDs\n        refs = get_backreferences(self, \"AnalysisAttachment\")\n        # fetch the objects by UID and handle nonexisting UIDs gracefully\n        ans = map(lambda uid: api.get_object_by_uid(uid, None), refs)\n        # filter out None values (nonexisting UIDs)\n        ans = filter(None, ans)\n        # sort by physical path, so that attachments coming from an AR with a\n        # higher \"-Rn\" suffix get sorted correctly.\n        # N.B. the created date is the same, hence we can not use it\n        return sorted(ans, key=api.get_path, reverse=True)", "response": "Lookup linked Analyses UIDs\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getTextTitle(self):\n        request_id = self.getRequestID()\n        if not request_id:\n            return \"\"\n\n        analysis = self.getAnalysis()\n        if not analysis:\n            return request_id\n\n        return \"%s - %s\" % (request_id, analysis.Title())", "response": "Return a title for texts and listings\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getRequest(self):\n        ars = self.getLinkedRequests()\n\n        if len(ars) > 1:\n            # Attachment is assigned to more than one Analysis Request.\n            # This might happen when the AR was invalidated\n            ar_ids = \", \".join(map(api.get_id, ars))\n            logger.info(\"Attachment assigned to more than one AR: [{}]. \"\n                        \"The first AR will be returned\".format(ar_ids))\n\n        # return the first AR\n        if len(ars) >= 1:\n            return ars[0]\n\n        # Check if the attachment is linked to an analysis and try to get the\n        # AR from the linked analysis\n        analysis = self.getAnalysis()\n        if IRequestAnalysis.providedBy(analysis):\n            return analysis.getRequest()\n\n        return None", "response": "Return the primary AR this attachment is linked\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the primary analysis this attachment is linked", "response": "def getAnalysis(self):\n        \"\"\"Return the primary analysis this attachment is linked\n        \"\"\"\n        analysis = None\n        ans = self.getLinkedAnalyses()\n\n        if len(ans) > 1:\n            # Attachment is assigned to more than one Analysis. This might\n            # happen when the AR was invalidated\n            an_ids = \", \".join(map(api.get_id, ans))\n            logger.info(\"Attachment assigned to more than one Analysis: [{}]. \"\n                        \"The first Analysis will be returned\".format(an_ids))\n\n        if len(ans) >= 1:\n            analysis = ans[0]\n\n        return analysis"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a DisplayList of all the Instruments in this object.", "response": "def getInstrumentsDisplayList(self):\n        \"\"\"Instruments capable to perform this method\n        \"\"\"\n        items = [(i.UID(), i.Title()) for i in self.getInstruments()]\n        return DisplayList(list(items))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of roles for the current user", "response": "def get_user_roles(self):\n        \"\"\"Returns a list of roles for the current user\n        \"\"\"\n        if self.is_anonymous_user():\n            return []\n        current_user = ploneapi.user.get_current()\n        return ploneapi.user.get_roles(user=current_user)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_versions(self):\n        self.versions = {}\n        self.upgrades = {}\n        qi = getToolByName(self.context, \"portal_quickinstaller\")\n        for key in qi.keys():\n            self.versions[key] = qi.getProductVersion(key)\n            info = qi.upgradeInfo(key)\n            if info and 'installedVersion' in info:\n                self.upgrades[key] = info['installedVersion']", "response": "Configure a list of product versions from portal. quickinstaller\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns all the Analysis Request brains linked to the Sampling Round", "response": "def getAnalysisRequests(self):\n        \"\"\" Return all the Analysis Request brains linked to the Sampling Round\n        \"\"\"\n        # I have to get the catalog in this way because I can't do it with 'self'...\n        pc = getToolByName(api.portal.get(), 'portal_catalog')\n        contentFilter = {'portal_type': 'AnalysisRequest',\n                         'is_active': True,\n                         'SamplingRoundUID': self.UID()}\n        return pc(contentFilter)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getAnalysisRequestTemplates(self):\n        l = []\n        art_uids = self.ar_templates\n        # I have to get the catalog in this way because I can't do it with 'self'...\n        pc = getToolByName(api.portal.get(), 'uid_catalog')\n        for art_uid in art_uids:\n            art_obj = pc(UID=art_uid)\n            if len(art_obj) != 0:\n                l.append((art_obj[0].Title, art_uid))\n        return l", "response": "This function builds a list of tuples with the object AnalysisRequestTemplates uids and names."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dict with the department infomration", "response": "def getDepartmentInfo(self):\n        \"\"\"\n        Returns a dict with the department infomration\n        {'uid':'xxxx','id':'xxxx','title':'xxx','url':'xxx'}\n        \"\"\"\n        pc = getToolByName(api.portal.get(), 'portal_catalog')\n        contentFilter = {'portal_type': 'Department',\n                         'UID': self.department}\n        departmentlist = pc(contentFilter)\n        departmentdict = {'uid': '', 'id': '', 'title': '', 'url': ''}\n        if len(departmentlist) == 1:\n            department = departmentlist[0].getObject()\n            departmentdict = {\n                'uid': department.id,\n                'id': department.UID(),\n                'title': department.title,\n                'url': department.absolute_url(),\n            }\n        else:\n            from bika.lims import logger\n            error = \"Error when looking for department with uid '%s'. \"\n            logger.exception(error, self.department)\n        return departmentdict"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getSRTemplateInfo(self):\n        pc = getToolByName(api.portal.get(), 'portal_catalog')\n        contentFilter = {'portal_type': 'SRTemplate',\n                         'UID': self.sr_template}\n        srt = pc(contentFilter)\n        srtdict = {'uid': '', 'id': '', 'title': '', 'url': ''}\n        if len(srt) == 1:\n            template = srt[0].getObject()\n            srtdict = {\n                'uid': template.id,\n                'id': template.UID(),\n                'title': template.title,\n                'url': template.absolute_url(),\n            }\n        else:\n            from bika.lims import logger\n            error = \"Error when looking for sr template with uid '%s'. \"\n            logger.exception(error, self.sr_template)\n        return srtdict", "response": "Returns a dict with the SRTemplate infomration\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getClientContact(self):\n        pc = getToolByName(api.portal.get(), 'portal_catalog')\n        contentFilter = {'portal_type': 'Contact',\n                         'id': self.client_contact}\n        cnt = pc(contentFilter)\n        cntdict = {'uid': '', 'id': '', 'fullname': '', 'url': ''}\n        if len(cnt) == 1:\n            cnt = cnt[0].getObject()\n            cntdict = {\n                'uid': cnt.id,\n                'id': cnt.UID(),\n                'fullname': cnt.getFullname(),\n                'url': cnt.absolute_url(),\n            }\n        else:\n            from bika.lims import logger\n            error = \"Error when looking for contact with id '%s'. \"\n            logger.exception(error, self.client_contact)\n        return cntdict", "response": "Returns info from the Client contact who coordinates with the lab\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hasUserAddEditPermission(self):\n        mtool = getToolByName(self, 'portal_membership')\n        checkPermission = mtool.checkPermission\n        # In bika_samplinground_workflow.csv there are defined the ModifyPortalContent statements. There is said that\n        # client has ModifyPortalContent permission enabled, so here we have to check if the client satisfy the\n        # condition wrote in the function's description\n        if (checkPermission(ModifyPortalContent, self) or checkPermission(AddPortalContent, self)) \\\n                and 'Client' in api.user.get_current().getRoles():\n            # Checking if the current user is a current client's contact\n            userID = api.user.get_current().id\n            contact_objs = self.getContacts()\n            contact_ids = [obj.getUsername() for obj in contact_objs]\n            if userID in contact_ids:\n                return True\n            else:\n                return False\n        return checkPermission(ModifyPortalContent, self) or checkPermission(AddPortalContent, self)", "response": "Checks if the current user has privileges to add the editing view."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncancels all the ARs and Samples in this round.", "response": "def workflow_script_cancel(self):\n        \"\"\"\n        When the round is cancelled, all its associated Samples and ARs are cancelled by the system.\n        \"\"\"\n        if skip(self, \"cancel\"):\n            return\n        self.reindexObject(idxs=[\"is_active\", ])\n        # deactivate all analysis requests in this sampling round.\n        analysis_requests = self.getAnalysisRequests()\n        for ar in analysis_requests:\n            ar_obj = ar.getObject()\n            workflow = getToolByName(self, 'portal_workflow')\n            if workflow.getInfoFor(ar_obj, 'review_state') != 'cancelled':\n                doActionFor(ar.getObject(), 'cancel')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_view_url(self):\n        request = self.request\n        url = request.getURL()\n        qs = request.getHeader(\"query_string\")\n        if not qs:\n            return url\n        return \"{}?{}\".format(url, qs)", "response": "Return the current view url including request parameters"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_currency(self):\n        bika_setup = api.get_bika_setup()\n        currency = bika_setup.getCurrency()\n        currencies = locales.getLocale('en').numbers.currencies\n        return currencies[currency]", "response": "Returns the configured currency"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the ar_count request paramteter", "response": "def get_ar_count(self):\n        \"\"\"Return the ar_count request paramteter\n        \"\"\"\n        ar_count = 1\n        try:\n            ar_count = int(self.request.form.get(\"ar_count\", 1))\n        except (TypeError, ValueError):\n            ar_count = 1\n        return ar_count"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_ar(self):\n        if not self.tmp_ar:\n            logger.info(\"*** CREATING TEMPORARY AR ***\")\n            self.tmp_ar = self.context.restrictedTraverse(\n                \"portal_factory/AnalysisRequest/Request new analyses\")\n        return self.tmp_ar", "response": "Create a temporary AR to fetch the fields from the current AR"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a new fieldname with a - <arnum > suffix", "response": "def get_fieldname(self, field, arnum):\n        \"\"\"Generate a new fieldname with a '-<arnum>' suffix\n        \"\"\"\n        name = field.getName()\n        # ensure we have only *one* suffix\n        base_name = name.split(\"-\")[0]\n        suffix = \"-{}\".format(arnum)\n        return \"{}{}\".format(base_name, suffix)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_specifications(self, count=1):\n\n        out = {}\n\n        # mapping of UID index to AR objects {1: <AR1>, 2: <AR2> ...}\n        copy_from = self.get_copy_from()\n\n        for arnum in range(count):\n            # get the source object\n            source = copy_from.get(arnum)\n\n            if source is None:\n                out[arnum] = {}\n                continue\n\n            # get the results range from the source object\n            results_range = source.getResultsRange()\n\n            # mapping of keyword -> rr specification\n            specification = {}\n            for rr in results_range:\n                specification[rr.get(\"keyword\")] = rr\n            out[arnum] = specification\n\n        return out", "response": "Returns a mapping of count - > specification\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the field widget of the AR in column <arnum >", "response": "def get_input_widget(self, fieldname, arnum=0, **kw):\n        \"\"\"Get the field widget of the AR in column <arnum>\n\n        :param fieldname: The base fieldname\n        :type fieldname: string\n        \"\"\"\n\n        # temporary AR Context\n        context = self.get_ar()\n        # request = self.request\n        schema = context.Schema()\n\n        # get original field in the schema from the base_fieldname\n        base_fieldname = fieldname.split(\"-\")[0]\n        field = context.getField(base_fieldname)\n\n        # fieldname with -<arnum> suffix\n        new_fieldname = self.get_fieldname(field, arnum)\n        new_field = field.copy(name=new_fieldname)\n\n        # get the default value for this field\n        fieldvalues = self.fieldvalues\n        field_value = fieldvalues.get(new_fieldname)\n        # request_value = request.form.get(new_fieldname)\n        # value = request_value or field_value\n        value = field_value\n\n        def getAccessor(instance):\n            def accessor(**kw):\n                return value\n            return accessor\n\n        # inject the new context for the widget renderer\n        # see: Products.Archetypes.Renderer.render\n        kw[\"here\"] = context\n        kw[\"context\"] = context\n        kw[\"fieldName\"] = new_fieldname\n\n        # make the field available with this name\n        # XXX: This is a hack to make the widget available in the template\n        schema._fields[new_fieldname] = new_field\n        new_field.getAccessor = getAccessor\n\n        # set the default value\n        form = dict()\n        form[new_fieldname] = value\n        self.request.form.update(form)\n        logger.info(\"get_input_widget: fieldname={} arnum={} \"\n                    \"-> new_fieldname={} value={}\".format(\n                        fieldname, arnum, new_fieldname, value))\n        widget = context.widget(new_fieldname, **kw)\n        return widget"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_copy_from(self):\n        # Create a mapping of source ARs for copy\n        copy_from = self.request.form.get(\"copy_from\", \"\").split(\",\")\n        # clean out empty strings\n        copy_from_uids = filter(lambda x: x, copy_from)\n        out = dict().fromkeys(range(len(copy_from_uids)))\n        for n, uid in enumerate(copy_from_uids):\n            ar = self.get_object_by_uid(uid)\n            if ar is None:\n                continue\n            out[n] = ar\n        logger.info(\"get_copy_from: uids={}\".format(copy_from_uids))\n        return out", "response": "Returns a mapping of UID index -> AR object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_default_value(self, field, context, arnum):\n        name = field.getName()\n        default = field.getDefault(context)\n        if name == \"Batch\":\n            batch = self.get_batch()\n            if batch is not None:\n                default = batch\n        if name == \"Client\":\n            client = self.get_client()\n            if client is not None:\n                default = client\n        # only set default contact for first column\n        if name == \"Contact\" and arnum == 0:\n            contact = self.get_default_contact()\n            if contact is not None:\n                default = contact\n        if name == \"Sample\":\n            sample = self.get_sample()\n            if sample is not None:\n                default = sample\n        # Querying for adapters to get default values from add-ons':\n        # We don't know which fields the form will render since\n        # some of them may come from add-ons. In order to obtain the default\n        # value for those fields we take advantage of adapters. Adapters\n        # registration should have the following format:\n        # < adapter\n        #   factory = ...\n        #   for = \"*\"\n        #   provides = \"bika.lims.interfaces.IGetDefaultFieldValueARAddHook\"\n        #   name = \"<fieldName>_default_value_hook\"\n        # / >\n        hook_name = name + '_default_value_hook'\n        adapter = queryAdapter(\n            self.request,\n            name=hook_name,\n            interface=IGetDefaultFieldValueARAddHook)\n        if adapter is not None:\n            default = adapter(self.context)\n        logger.info(\"get_default_value: context={} field={} value={} arnum={}\"\n                    .format(context, name, default, arnum))\n        return default", "response": "Get the default value of the field."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the stored value of the field", "response": "def get_field_value(self, field, context):\n        \"\"\"Get the stored value of the field\n        \"\"\"\n        name = field.getName()\n        value = context.getField(name).get(context)\n        logger.info(\"get_field_value: context={} field={} value={}\".format(\n            context, name, value))\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_parent_ar(self, ar):\n        parent = ar.getParentAnalysisRequest()\n\n        # Return immediately if we have no parent\n        if parent is None:\n            return None\n\n        # Walk back the chain until we reach the source AR\n        while True:\n            pparent = parent.getParentAnalysisRequest()\n            if pparent is None:\n                break\n            # remember the new parent\n            parent = pparent\n\n        return parent", "response": "Returns the parent AR\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_fieldvalues(self, count=1):\n        ar_context = self.get_ar()\n\n        # mapping of UID index to AR objects {1: <AR1>, 2: <AR2> ...}\n        copy_from = self.get_copy_from()\n\n        out = {}\n        # the original schema fields of an AR (including extended fields)\n        fields = self.get_ar_fields()\n\n        # generate fields for all requested ARs\n        for arnum in range(count):\n            source = copy_from.get(arnum)\n            parent = None\n            if source is not None:\n                parent = self.get_parent_ar(source)\n            for field in fields:\n                value = None\n                fieldname = field.getName()\n                if source and fieldname not in SKIP_FIELD_ON_COPY:\n                    # get the field value stored on the source\n                    context = parent or source\n                    value = self.get_field_value(field, context)\n                else:\n                    # get the default value of this field\n                    value = self.get_default_value(\n                        field, ar_context, arnum=arnum)\n                # store the value on the new fieldname\n                new_fieldname = self.get_fieldname(field, arnum)\n                out[new_fieldname] = value\n\n        return out", "response": "Returns a mapping of fieldname - count to the default value of the field or the field value of the source AR."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the default contact for the AR", "response": "def get_default_contact(self, client=None):\n        \"\"\"Logic refactored from JavaScript:\n\n        * If client only has one contact, and the analysis request comes from\n        * a client, then Auto-complete first Contact field.\n        * If client only has one contect, and the analysis request comes from\n        * a batch, then Auto-complete all Contact field.\n\n        :returns: The default contact for the AR\n        :rtype: Client object or None\n        \"\"\"\n        catalog = api.get_tool(\"portal_catalog\")\n        client = client or self.get_client()\n        path = api.get_path(self.context)\n        if client:\n            path = api.get_path(client)\n        query = {\n            \"portal_type\": \"Contact\",\n            \"path\": {\n                \"query\": path,\n                \"depth\": 1\n            },\n            \"incactive_state\": \"active\",\n        }\n        contacts = catalog(query)\n        if len(contacts) == 1:\n            return api.get_object(contacts[0])\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_field_visible(self, field):\n        context = self.context\n        fieldname = field.getName()\n\n        # hide the Client field on client and batch contexts\n        if fieldname == \"Client\" and context.portal_type in (\"Client\", ):\n            return False\n\n        # hide the Batch field on batch contexts\n        if fieldname == \"Batch\" and context.portal_type in (\"Batch\", ):\n            return False\n\n        return True", "response": "Check if the field is visible on the current context"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the AR fields with the current visibility", "response": "def get_fields_with_visibility(self, visibility, mode=\"add\"):\n        \"\"\"Return the AR fields with the current visibility\n        \"\"\"\n        ar = self.get_ar()\n        mv = api.get_view(\"ar_add_manage\", context=ar)\n        mv.get_field_order()\n\n        out = []\n        for field in mv.get_fields_with_visibility(visibility, mode):\n            # check custom field condition\n            visible = self.is_field_visible(field)\n            if visible is False and visibility != \"hidden\":\n                continue\n            out.append(field)\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn all service categories in the right order", "response": "def get_service_categories(self, restricted=True):\n        \"\"\"Return all service categories in the right order\n\n        :param restricted: Client settings restrict categories\n        :type restricted: bool\n        :returns: Category catalog results\n        :rtype: brains\n        \"\"\"\n        bsc = api.get_tool(\"bika_setup_catalog\")\n        query = {\n            \"portal_type\": \"AnalysisCategory\",\n            \"is_active\": True,\n            \"sort_on\": \"sortable_title\",\n        }\n        categories = bsc(query)\n        client = self.get_client()\n        if client and restricted:\n            restricted_categories = client.getRestrictedCategories()\n            restricted_category_ids = map(\n                lambda c: c.getId(), restricted_categories)\n            # keep correct order of categories\n            if restricted_category_ids:\n                categories = filter(\n                    lambda c: c.getId in restricted_category_ids, categories)\n        return categories"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns all Services in a category", "response": "def get_services(self, poc=\"lab\"):\n        \"\"\"Return all Services\n\n        :param poc: Point of capture (lab/field)\n        :type poc: string\n        :returns: Mapping of category -> list of services\n        :rtype: dict\n        \"\"\"\n        bsc = api.get_tool(\"bika_setup_catalog\")\n        query = {\n            \"portal_type\": \"AnalysisService\",\n            \"getPointOfCapture\": poc,\n            \"is_active\": True,\n            \"sort_on\": \"sortable_title\",\n        }\n        services = bsc(query)\n        categories = self.get_service_categories(restricted=False)\n        analyses = {key: [] for key in map(lambda c: c.Title, categories)}\n\n        # append the empty category as well\n        analyses[\"\"] = []\n\n        for brain in services:\n            category = brain.getCategoryTitle\n            if category in analyses:\n                analyses[category].append(brain)\n        return analyses"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the service uid from the analysis", "response": "def get_service_uid_from(self, analysis):\n        \"\"\"Return the service from the analysis\n        \"\"\"\n        analysis = api.get_object(analysis)\n        return api.get_uid(analysis.getAnalysisService())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_service_selected(self, service):\n        service_uid = api.get_uid(service)\n        for arnum in range(self.ar_count):\n            analyses = self.fieldvalues.get(\"Analyses-{}\".format(arnum))\n            if not analyses:\n                continue\n            service_uids = map(self.get_service_uid_from, analyses)\n            if service_uid in service_uids:\n                return True\n        return False", "response": "Checks if the given service is selected by one of the ARs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_sorted_fields(self):\n        inf = float(\"inf\")\n        order = self.get_field_order()\n\n        def field_cmp(field1, field2):\n            _n1 = field1.getName()\n            _n2 = field2.getName()\n            _i1 = _n1 in order and order.index(_n1) + 1 or inf\n            _i2 = _n2 in order and order.index(_n2) + 1 or inf\n            return cmp(_i1, _i2)\n\n        return sorted(self.get_fields(), cmp=field_cmp)", "response": "Return the sorted fields of the log entry"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_fields_with_visibility(self, visibility=\"edit\", mode=\"add\"):\n        fields = self.get_sorted_fields()\n\n        out = []\n\n        for field in fields:\n            v = field.widget.isVisible(\n                self.context, mode, default='invisible', field=field)\n\n            if self.is_field_visible(field) is False:\n                v = \"hidden\"\n\n            visibility_guard = True\n            # visibility_guard is a widget field defined in the schema in order\n            # to know the visibility of the widget when the field is related to\n            # a dynamically changing content such as workflows. For instance\n            # those fields related to the workflow will be displayed only if\n            # the workflow is enabled, otherwise they should not be shown.\n            if 'visibility_guard' in dir(field.widget):\n                visibility_guard = eval(field.widget.visibility_guard)\n            if v == visibility and visibility_guard:\n                out.append(field)\n\n        return out", "response": "Return the fields with visibility"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_iso_date(self, dt):\n        if dt is None:\n            return \"\"\n        if isinstance(dt, DateTime):\n            return dt.ISO8601()\n        if isinstance(dt, datetime):\n            return dt.isoformat()\n        raise TypeError(\"{} is neiter an instance of DateTime nor datetime\"\n                        .format(repr(dt)))", "response": "Return the ISO representation of a date object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of AR records that are used in the AR Add form.", "response": "def get_records(self):\n        \"\"\"Returns a list of AR records\n\n        Fields coming from `request.form` have a number prefix, e.g. Contact-0.\n        Fields with the same suffix number are grouped together in a record.\n        Each record represents the data for one column in the AR Add form and\n        contains a mapping of the fieldName (w/o prefix) -> value.\n\n        Example:\n        [{\"Contact\": \"Rita Mohale\", ...}, {Contact: \"Neil Standard\"} ...]\n        \"\"\"\n        form = self.request.form\n        ar_count = self.get_ar_count()\n\n        records = []\n        # Group belonging AR fields together\n        for arnum in range(ar_count):\n            record = {}\n            s1 = \"-{}\".format(arnum)\n            keys = filter(lambda key: s1 in key, form.keys())\n            for key in keys:\n                new_key = key.replace(s1, \"\")\n                value = form.get(key)\n                record[new_key] = value\n            records.append(record)\n        return records"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of parsed UIDs from a single form field identified by the given key.", "response": "def get_uids_from_record(self, record, key):\n        \"\"\"Returns a list of parsed UIDs from a single form field identified by\n        the given key.\n\n        A form field ending with `_uid` can contain an empty value, a\n        single UID or multiple UIDs separated by a comma.\n\n        This method parses the UID value and returns a list of non-empty UIDs.\n        \"\"\"\n        value = record.get(key, None)\n        if value is None:\n            return []\n        if isinstance(value, basestring):\n            value = value.split(\",\")\n        return filter(lambda uid: uid, value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_objs_from_record(self, record, key):\n        uids = self.get_uids_from_record(record, key)\n        objs = map(self.get_object_by_uid, uids)\n        return dict(zip(uids, objs))", "response": "Returns a mapping of UID - > object"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the base info of an object", "response": "def get_base_info(self, obj):\n        \"\"\"Returns the base info of an object\n        \"\"\"\n        if obj is None:\n            return {}\n\n        info = {\n            \"id\": obj.getId(),\n            \"uid\": obj.UID(),\n            \"title\": obj.Title(),\n            \"description\": obj.Description(),\n            \"url\": obj.absolute_url(),\n        }\n\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the client info of an object", "response": "def get_client_info(self, obj):\n        \"\"\"Returns the client info of an object\n        \"\"\"\n        info = self.get_base_info(obj)\n\n        default_contact_info = {}\n        default_contact = self.get_default_contact(client=obj)\n        if default_contact:\n            default_contact_info = self.get_contact_info(default_contact)\n\n        info.update({\n            \"default_contact\": default_contact_info\n        })\n\n        # UID of the client\n        uid = api.get_uid(obj)\n\n        # Bika Setup folder\n        bika_setup = api.get_bika_setup()\n\n        # bika samplepoints\n        bika_samplepoints = bika_setup.bika_samplepoints\n        bika_samplepoints_uid = api.get_uid(bika_samplepoints)\n\n        # bika artemplates\n        bika_artemplates = bika_setup.bika_artemplates\n        bika_artemplates_uid = api.get_uid(bika_artemplates)\n\n        # bika analysisprofiles\n        bika_analysisprofiles = bika_setup.bika_analysisprofiles\n        bika_analysisprofiles_uid = api.get_uid(bika_analysisprofiles)\n\n        # bika analysisspecs\n        bika_analysisspecs = bika_setup.bika_analysisspecs\n        bika_analysisspecs_uid = api.get_uid(bika_analysisspecs)\n\n        # catalog queries for UI field filtering\n        filter_queries = {\n            \"contact\": {\n                \"getParentUID\": [uid]\n            },\n            \"cc_contact\": {\n                \"getParentUID\": [uid]\n            },\n            \"invoice_contact\": {\n                \"getParentUID\": [uid]\n            },\n            \"samplepoint\": {\n                \"getClientUID\": [uid, bika_samplepoints_uid],\n            },\n            \"artemplates\": {\n                \"getClientUID\": [uid, bika_artemplates_uid],\n            },\n            \"analysisprofiles\": {\n                \"getClientUID\": [uid, bika_analysisprofiles_uid],\n            },\n            \"analysisspecs\": {\n                \"getClientUID\": [uid, bika_analysisspecs_uid],\n            },\n            \"samplinground\": {\n                \"getParentUID\": [uid],\n            },\n            \"sample\": {\n                \"getClientUID\": [uid],\n            },\n        }\n        info[\"filter_queries\"] = filter_queries\n\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the client info of an object", "response": "def get_contact_info(self, obj):\n        \"\"\"Returns the client info of an object\n        \"\"\"\n\n        info = self.get_base_info(obj)\n        fullname = obj.getFullname()\n        email = obj.getEmailAddress()\n\n        # Note: It might get a circular dependency when calling:\n        #       map(self.get_contact_info, obj.getCCContact())\n        cccontacts = {}\n        for contact in obj.getCCContact():\n            uid = api.get_uid(contact)\n            fullname = contact.getFullname()\n            email = contact.getEmailAddress()\n            cccontacts[uid] = {\n                \"fullname\": fullname,\n                \"email\": email\n            }\n\n        info.update({\n            \"fullname\": fullname,\n            \"email\": email,\n            \"cccontacts\": cccontacts,\n        })\n\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the info for a Service object", "response": "def get_service_info(self, obj):\n        \"\"\"Returns the info for a Service\n        \"\"\"\n        info = self.get_base_info(obj)\n\n        info.update({\n            \"short_title\": obj.getShortTitle(),\n            \"scientific_name\": obj.getScientificName(),\n            \"unit\": obj.getUnit(),\n            \"keyword\": obj.getKeyword(),\n            \"methods\": map(self.get_method_info, obj.getMethods()),\n            \"calculation\": self.get_calculation_info(obj.getCalculation()),\n            \"price\": obj.getPrice(),\n            \"currency_symbol\": self.get_currency().symbol,\n            \"accredited\": obj.getAccredited(),\n            \"category\": obj.getCategoryTitle(),\n            \"poc\": obj.getPointOfCapture(),\n\n        })\n\n        dependencies = get_calculation_dependencies_for(obj).values()\n        info[\"dependencies\"] = map(self.get_base_info, dependencies)\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the info for a Template object", "response": "def get_template_info(self, obj):\n        \"\"\"Returns the info for a Template\n        \"\"\"\n        client = self.get_client()\n        client_uid = api.get_uid(client) if client else \"\"\n\n        profile = obj.getAnalysisProfile()\n        profile_uid = api.get_uid(profile) if profile else \"\"\n        profile_title = profile.Title() if profile else \"\"\n\n        sample_type = obj.getSampleType()\n        sample_type_uid = api.get_uid(sample_type) if sample_type else \"\"\n        sample_type_title = sample_type.Title() if sample_type else \"\"\n\n        sample_point = obj.getSamplePoint()\n        sample_point_uid = api.get_uid(sample_point) if sample_point else \"\"\n        sample_point_title = sample_point.Title() if sample_point else \"\"\n\n        service_uids = []\n        analyses_partitions = {}\n        analyses = obj.getAnalyses()\n\n        for record in analyses:\n            service_uid = record.get(\"service_uid\")\n            service_uids.append(service_uid)\n            analyses_partitions[service_uid] = record.get(\"partition\")\n\n        info = self.get_base_info(obj)\n        info.update({\n            \"analyses_partitions\": analyses_partitions,\n            \"analysis_profile_title\": profile_title,\n            \"analysis_profile_uid\": profile_uid,\n            \"client_uid\": client_uid,\n            \"composite\": obj.getComposite(),\n            \"partitions\": obj.getPartitions(),\n            \"remarks\": obj.getRemarks(),\n            \"sample_point_title\": sample_point_title,\n            \"sample_point_uid\": sample_point_uid,\n            \"sample_type_title\": sample_type_title,\n            \"sample_type_uid\": sample_type_uid,\n            \"service_uids\": service_uids,\n        })\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the info for a Profile", "response": "def get_profile_info(self, obj):\n        \"\"\"Returns the info for a Profile\n        \"\"\"\n        info = self.get_base_info(obj)\n        info.update({})\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the info for a Method", "response": "def get_method_info(self, obj):\n        \"\"\"Returns the info for a Method\n        \"\"\"\n        info = self.get_base_info(obj)\n        info.update({})\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_calculation_info(self, obj):\n        info = self.get_base_info(obj)\n        info.update({})\n        return info", "response": "Returns the info for a Calculation object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_sampletype_info(self, obj):\n        info = self.get_base_info(obj)\n\n        # Bika Setup folder\n        bika_setup = api.get_bika_setup()\n\n        # bika samplepoints\n        bika_samplepoints = bika_setup.bika_samplepoints\n        bika_samplepoints_uid = api.get_uid(bika_samplepoints)\n\n        # bika analysisspecs\n        bika_analysisspecs = bika_setup.bika_analysisspecs\n        bika_analysisspecs_uid = api.get_uid(bika_analysisspecs)\n\n        # client\n        client = self.get_client()\n        client_uid = client and api.get_uid(client) or \"\"\n\n        # sample matrix\n        sample_matrix = obj.getSampleMatrix()\n        sample_matrix_uid = sample_matrix and sample_matrix.UID() or \"\"\n        sample_matrix_title = sample_matrix and sample_matrix.Title() or \"\"\n\n        # container type\n        container_type = obj.getContainerType()\n        container_type_uid = container_type and container_type.UID() or \"\"\n        container_type_title = container_type and container_type.Title() or \"\"\n\n        # sample points\n        sample_points = obj.getSamplePoints()\n        sample_point_uids = map(lambda sp: sp.UID(), sample_points)\n        sample_point_titles = map(lambda sp: sp.Title(), sample_points)\n\n        info.update({\n            \"prefix\": obj.getPrefix(),\n            \"minimum_volume\": obj.getMinimumVolume(),\n            \"hazardous\": obj.getHazardous(),\n            \"retention_period\": obj.getRetentionPeriod(),\n            \"sample_matrix_uid\": sample_matrix_uid,\n            \"sample_matrix_title\": sample_matrix_title,\n            \"container_type_uid\": container_type_uid,\n            \"container_type_title\": container_type_title,\n            \"sample_point_uids\": sample_point_uids,\n            \"sample_point_titles\": sample_point_titles,\n        })\n\n        # catalog queries for UI field filtering\n        filter_queries = {\n            \"samplepoint\": {\n                \"getSampleTypeTitles\": [obj.Title(), ''],\n                \"getClientUID\": [client_uid, bika_samplepoints_uid],\n                \"sort_order\": \"descending\",\n            },\n            \"specification\": {\n                \"getSampleTypeTitle\": obj.Title(),\n                \"getClientUID\": [client_uid, bika_analysisspecs_uid],\n                \"sort_order\": \"descending\",\n            }\n        }\n        info[\"filter_queries\"] = filter_queries\n\n        return info", "response": "Returns the info for a Sample Type"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the info for a Sample object.", "response": "def get_sample_info(self, obj):\n        \"\"\"Returns the info for a Sample\n        \"\"\"\n        info = self.get_base_info(obj)\n\n        # sample type\n        sample_type = obj.getSampleType()\n        sample_type_uid = sample_type and sample_type.UID() or \"\"\n        sample_type_title = sample_type and sample_type.Title() or \"\"\n\n        # sample condition\n        sample_condition = obj.getSampleCondition()\n        sample_condition_uid = sample_condition \\\n            and sample_condition.UID() or \"\"\n        sample_condition_title = sample_condition \\\n            and sample_condition.Title() or \"\"\n\n        # storage location\n        storage_location = obj.getStorageLocation()\n        storage_location_uid = storage_location \\\n            and storage_location.UID() or \"\"\n        storage_location_title = storage_location \\\n            and storage_location.Title() or \"\"\n\n        # sample point\n        sample_point = obj.getSamplePoint()\n        sample_point_uid = sample_point and sample_point.UID() or \"\"\n        sample_point_title = sample_point and sample_point.Title() or \"\"\n\n        # container type\n        container_type = sample_type and sample_type.getContainerType() or None\n        container_type_uid = container_type and container_type.UID() or \"\"\n        container_type_title = container_type and container_type.Title() or \"\"\n\n        # Sampling deviation\n        deviation = obj.getSamplingDeviation()\n        deviation_uid = deviation and deviation.UID() or \"\"\n        deviation_title = deviation and deviation.Title() or \"\"\n\n        info.update({\n            \"sample_id\": obj.getId(),\n            \"batch_uid\": obj.getBatchUID() or None,\n            \"date_sampled\": self.to_iso_date(obj.getDateSampled()),\n            \"sampling_date\": self.to_iso_date(obj.getSamplingDate()),\n            \"sample_type_uid\": sample_type_uid,\n            \"sample_type_title\": sample_type_title,\n            \"container_type_uid\": container_type_uid,\n            \"container_type_title\": container_type_title,\n            \"sample_condition_uid\": sample_condition_uid,\n            \"sample_condition_title\": sample_condition_title,\n            \"storage_location_uid\": storage_location_uid,\n            \"storage_location_title\": storage_location_title,\n            \"sample_point_uid\": sample_point_uid,\n            \"sample_point_title\": sample_point_title,\n            \"environmental_conditions\": obj.getEnvironmentalConditions(),\n            \"composite\": obj.getComposite(),\n            \"client_uid\": obj.getClientUID(),\n            \"client_title\": obj.getClientTitle(),\n            \"contact\": self.get_contact_info(obj.getContact()),\n            \"client_order_number\": obj.getClientOrderNumber(),\n            \"client_sample_id\": obj.getClientSampleID(),\n            \"client_reference\": obj.getClientReference(),\n            \"sampling_deviation_uid\": deviation_uid,\n            \"sampling_deviation_title\": deviation_title,\n            \"sampling_workflow_enabled\": obj.getSamplingWorkflowEnabled(),\n            \"remarks\": obj.getRemarks(),\n        })\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the info for a Specification", "response": "def get_specification_info(self, obj):\n        \"\"\"Returns the info for a Specification\n        \"\"\"\n        info = self.get_base_info(obj)\n\n        results_range = obj.getResultsRange()\n        info.update({\n            \"results_range\": results_range,\n            \"sample_type_uid\": obj.getSampleTypeUID(),\n            \"sample_type_title\": obj.getSampleTypeTitle(),\n            \"client_uid\": obj.getClientUID(),\n        })\n\n        bsc = api.get_tool(\"bika_setup_catalog\")\n\n        def get_service_by_keyword(keyword):\n            if keyword is None:\n                return []\n            return map(api.get_object, bsc({\n                \"portal_type\": \"AnalysisService\",\n                \"getKeyword\": keyword\n            }))\n\n        # append a mapping of service_uid -> specification\n        specifications = {}\n        for spec in results_range:\n            service_uid = spec.get(\"uid\")\n            if service_uid is None:\n                # service spec is not attached to a specific service, but to a\n                # keyword\n                for service in get_service_by_keyword(spec.get(\"keyword\")):\n                    service_uid = api.get_uid(service)\n                    specifications[service_uid] = spec\n                continue\n            specifications[service_uid] = spec\n        info[\"specifications\"] = specifications\n        # spec'd service UIDs\n        info[\"service_uids\"] = specifications.keys()\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the info for a Container", "response": "def get_container_info(self, obj):\n        \"\"\"Returns the info for a Container\n        \"\"\"\n        info = self.get_base_info(obj)\n        info.update({})\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the info for a Preservation", "response": "def get_preservation_info(self, obj):\n        \"\"\"Returns the info for a Preservation\n        \"\"\"\n        info = self.get_base_info(obj)\n        info.update({})\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ajax_get_service(self):\n        uid = self.request.form.get(\"uid\", None)\n\n        if uid is None:\n            return self.error(\"Invalid UID\", status=400)\n\n        service = self.get_object_by_uid(uid)\n        if not service:\n            return self.error(\"Service not found\", status=404)\n\n        info = self.get_service_info(service)\n        return info", "response": "Returns the services information"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ajax_recalculate_records(self):\n        out = {}\n\n        # The sorted records from the request\n        records = self.get_records()\n\n        for n, record in enumerate(records):\n\n            # Mapping of client UID -> client object info\n            client_metadata = {}\n            # Mapping of contact UID -> contact object info\n            contact_metadata = {}\n            # Mapping of sample UID -> sample object info\n            sample_metadata = {}\n            # Mapping of sampletype UID -> sampletype object info\n            sampletype_metadata = {}\n            # Mapping of specification UID -> specification object info\n            specification_metadata = {}\n            # Mapping of specification UID -> list of service UIDs\n            specification_to_services = {}\n            # Mapping of service UID -> list of specification UIDs\n            service_to_specifications = {}\n            # Mapping of template UID -> template object info\n            template_metadata = {}\n            # Mapping of template UID -> list of service UIDs\n            template_to_services = {}\n            # Mapping of service UID -> list of template UIDs\n            service_to_templates = {}\n            # Mapping of profile UID -> list of service UIDs\n            profile_to_services = {}\n            # Mapping of service UID -> list of profile UIDs\n            service_to_profiles = {}\n            # Profile metadata for UI purposes\n            profile_metadata = {}\n            # Mapping of service UID -> service object info\n            service_metadata = {}\n            # mapping of service UID -> unmet service dependency UIDs\n            unmet_dependencies = {}\n\n            # Mappings of UID -> object of selected items in this record\n            _clients = self.get_objs_from_record(record, \"Client_uid\")\n            _contacts = self.get_objs_from_record(record, \"Contact_uid\")\n            _specifications = self.get_objs_from_record(\n                record, \"Specification_uid\")\n            _templates = self.get_objs_from_record(record, \"Template_uid\")\n            _samples = self.get_objs_from_record(record, \"PrimaryAnalysisRequest_uid\")\n            _profiles = self.get_objs_from_record(record, \"Profiles_uid\")\n            _services = self.get_objs_from_record(record, \"Analyses\")\n            _sampletypes = self.get_objs_from_record(record, \"SampleType_uid\")\n\n            # CLIENTS\n            for uid, obj in _clients.iteritems():\n                # get the client metadata\n                metadata = self.get_client_info(obj)\n                # remember the sampletype metadata\n                client_metadata[uid] = metadata\n\n            # CONTACTS\n            for uid, obj in _contacts.iteritems():\n                # get the client metadata\n                metadata = self.get_contact_info(obj)\n                # remember the sampletype metadata\n                contact_metadata[uid] = metadata\n\n            # SPECIFICATIONS\n            for uid, obj in _specifications.iteritems():\n                # get the specification metadata\n                metadata = self.get_specification_info(obj)\n                # remember the metadata of this specification\n                specification_metadata[uid] = metadata\n                # get the spec'd service UIDs\n                service_uids = metadata[\"service_uids\"]\n                # remember a mapping of specification uid -> spec'd services\n                specification_to_services[uid] = service_uids\n                # remember a mapping of service uid -> specifications\n                for service_uid in service_uids:\n                    if service_uid in service_to_specifications:\n                        service_to_specifications[service_uid].append(uid)\n                    else:\n                        service_to_specifications[service_uid] = [uid]\n\n            # AR TEMPLATES\n            for uid, obj in _templates.iteritems():\n                # get the template metadata\n                metadata = self.get_template_info(obj)\n                # remember the template metadata\n                template_metadata[uid] = metadata\n\n                # profile from the template\n                profile = obj.getAnalysisProfile()\n                # add the profile to the other profiles\n                if profile is not None:\n                    profile_uid = api.get_uid(profile)\n                    _profiles[profile_uid] = profile\n\n                # get the template analyses\n                # [{'partition': 'part-1', 'service_uid': '...'},\n                # {'partition': 'part-1', 'service_uid': '...'}]\n                analyses = obj.getAnalyses() or []\n                # get all UIDs of the template records\n                service_uids = map(\n                    lambda rec: rec.get(\"service_uid\"), analyses)\n                # remember a mapping of template uid -> service\n                template_to_services[uid] = service_uids\n                # remember a mapping of service uid -> templates\n                for service_uid in service_uids:\n                    # append service to services mapping\n                    service = self.get_object_by_uid(service_uid)\n                    # remember the template of all services\n                    if service_uid in service_to_templates:\n                        service_to_templates[service_uid].append(uid)\n                    else:\n                        service_to_templates[service_uid] = [uid]\n\n                    # remember the service metadata\n                    if service_uid not in service_metadata:\n                        metadata = self.get_service_info(service)\n                        service_metadata[service_uid] = metadata\n\n            # PROFILES\n            for uid, obj in _profiles.iteritems():\n                # get the profile metadata\n                metadata = self.get_profile_info(obj)\n                # remember the profile metadata\n                profile_metadata[uid] = metadata\n                # get all services of this profile\n                services = obj.getService()\n                # get all UIDs of the profile services\n                service_uids = map(api.get_uid, services)\n                # remember all services of this profile\n                profile_to_services[uid] = service_uids\n                # remember a mapping of service uid -> profiles\n                for service in services:\n                    # get the UID of this service\n                    service_uid = api.get_uid(service)\n                    # add the service to the other services\n                    _services[service_uid] = service\n                    # remember the profiles of this service\n                    if service_uid in service_to_profiles:\n                        service_to_profiles[service_uid].append(uid)\n                    else:\n                        service_to_profiles[service_uid] = [uid]\n\n            # PRIMARY ANALYSIS REQUESTS\n            for uid, obj in _samples.iteritems():\n                # get the sample metadata\n                metadata = self.get_sample_info(obj)\n                # remember the sample metadata\n                sample_metadata[uid] = metadata\n\n            # SAMPLETYPES\n            for uid, obj in _sampletypes.iteritems():\n                # get the sampletype metadata\n                metadata = self.get_sampletype_info(obj)\n                # remember the sampletype metadata\n                sampletype_metadata[uid] = metadata\n\n            # SERVICES\n            for uid, obj in _services.iteritems():\n                # get the service metadata\n                metadata = self.get_service_info(obj)\n\n                # remember the services' metadata\n                service_metadata[uid] = metadata\n\n            #  DEPENDENCIES\n            for uid, obj in _services.iteritems():\n                # get the dependencies of this service\n                deps = get_service_dependencies_for(obj)\n\n                # check for unmet dependencies\n                for dep in deps[\"dependencies\"]:\n                    # we use the UID to test for equality\n                    dep_uid = api.get_uid(dep)\n                    if dep_uid not in _services.keys():\n                        if uid in unmet_dependencies:\n                            unmet_dependencies[uid].append(\n                                self.get_base_info(dep))\n                        else:\n                            unmet_dependencies[uid] = [self.get_base_info(dep)]\n                # remember the dependencies in the service metadata\n                service_metadata[uid].update({\n                    \"dependencies\": map(\n                        self.get_base_info, deps[\"dependencies\"]),\n                })\n\n            # Each key `n` (1,2,3...) contains the form data for one AR Add\n            # column in the UI.\n            # All relevant form data will be set accoriding to this data.\n            out[n] = {\n                \"client_metadata\": client_metadata,\n                \"contact_metadata\": contact_metadata,\n                \"sample_metadata\": sample_metadata,\n                \"sampletype_metadata\": sampletype_metadata,\n                \"specification_metadata\": specification_metadata,\n                \"specification_to_services\": specification_to_services,\n                \"service_to_specifications\": service_to_specifications,\n                \"template_metadata\": template_metadata,\n                \"template_to_services\": template_to_services,\n                \"service_to_templates\": service_to_templates,\n                \"profile_metadata\": profile_metadata,\n                \"profile_to_services\": profile_to_services,\n                \"service_to_profiles\": service_to_profiles,\n                \"service_metadata\": service_metadata,\n                \"unmet_dependencies\": unmet_dependencies,\n            }\n\n        return out", "response": "Recalculate all AR records and dependencies of all AR records"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsubmit the AR Add form and create the ARs.", "response": "def ajax_submit(self):\n        \"\"\"Submit & create the ARs\n        \"\"\"\n\n        # Get AR required fields (including extended fields)\n        fields = self.get_ar_fields()\n\n        # extract records from request\n        records = self.get_records()\n\n        fielderrors = {}\n        errors = {\"message\": \"\", \"fielderrors\": {}}\n\n        attachments = {}\n        valid_records = []\n\n        # Validate required fields\n        for n, record in enumerate(records):\n\n            # Process UID fields first and set their values to the linked field\n            uid_fields = filter(lambda f: f.endswith(\"_uid\"), record)\n            for field in uid_fields:\n                name = field.replace(\"_uid\", \"\")\n                value = record.get(field)\n                if \",\" in value:\n                    value = value.split(\",\")\n                record[name] = value\n\n            # Extract file uploads (fields ending with _file)\n            # These files will be added later as attachments\n            file_fields = filter(lambda f: f.endswith(\"_file\"), record)\n            attachments[n] = map(lambda f: record.pop(f), file_fields)\n\n            # Process Specifications field (dictionary like records instance).\n            # -> Convert to a standard Python dictionary.\n            specifications = map(\n                lambda x: dict(x), record.pop(\"Specifications\", []))\n            record[\"Specifications\"] = specifications\n\n            # Required fields and their values\n            required_keys = [field.getName() for field in fields\n                             if field.required]\n            required_values = [record.get(key) for key in required_keys]\n            required_fields = dict(zip(required_keys, required_values))\n\n            # Client field is required but hidden in the AR Add form. We remove\n            # it therefore from the list of required fields to let empty\n            # columns pass the required check below.\n            if record.get(\"Client\", False):\n                required_fields.pop('Client', None)\n\n            # Contacts get pre-filled out if only one contact exists.\n            # We won't force those columns with only the Contact filled out to\n            # be required.\n            contact = required_fields.pop(\"Contact\", None)\n\n            # None of the required fields are filled, skip this record\n            if not any(required_fields.values()):\n                continue\n\n            # Re-add the Contact\n            required_fields[\"Contact\"] = contact\n\n            # Missing required fields\n            missing = [f for f in required_fields if not record.get(f, None)]\n\n            # If there are required fields missing, flag an error\n            for field in missing:\n                fieldname = \"{}-{}\".format(field, n)\n                msg = _(\"Field '{}' is required\".format(field))\n                fielderrors[fieldname] = msg\n\n            # Process valid record\n            valid_record = dict()\n            for fieldname, fieldvalue in record.iteritems():\n                # clean empty\n                if fieldvalue in ['', None]:\n                    continue\n                valid_record[fieldname] = fieldvalue\n\n            # append the valid record to the list of valid records\n            valid_records.append(valid_record)\n\n        # return immediately with an error response if some field checks failed\n        if fielderrors:\n            errors[\"fielderrors\"] = fielderrors\n            return {'errors': errors}\n\n        # Process Form\n        actions = ActionHandlerPool.get_instance()\n        actions.queue_pool()\n        ARs = OrderedDict()\n        for n, record in enumerate(valid_records):\n            client_uid = record.get(\"Client\")\n            client = self.get_object_by_uid(client_uid)\n\n            if not client:\n                actions.resume()\n                raise RuntimeError(\"No client found\")\n\n            # get the specifications and pass them to the AR create function.\n            specifications = record.pop(\"Specifications\", {})\n\n            # Create the Analysis Request\n            try:\n                ar = crar(\n                    client,\n                    self.request,\n                    record,\n                    specifications=specifications\n                )\n            except (KeyError, RuntimeError) as e:\n                actions.resume()\n                errors[\"message\"] = e.message\n                return {\"errors\": errors}\n            # We keep the title to check if AR is newly created\n            # and UID to print stickers\n            ARs[ar.Title()] = ar.UID()\n            for attachment in attachments.get(n, []):\n                if not attachment.filename:\n                    continue\n                att = _createObjectByType(\"Attachment\", client, tmpID())\n                att.setAttachmentFile(attachment)\n                att.processForm()\n                ar.addAttachment(att)\n        actions.resume()\n\n        level = \"info\"\n        if len(ARs) == 0:\n            message = _('No Samples could be created.')\n            level = \"error\"\n        elif len(ARs) > 1:\n            message = _('Samples ${ARs} were successfully created.',\n                        mapping={'ARs': safe_unicode(', '.join(ARs.keys()))})\n        else:\n            message = _('Sample ${AR} was successfully created.',\n                        mapping={'AR': safe_unicode(ARs.keys()[0])})\n\n        # Display a portal message\n        self.context.plone_utils.addPortalMessage(message, level)\n\n        # Automatic label printing\n        bika_setup = api.get_bika_setup()\n        auto_print = bika_setup.getAutoPrintStickers()\n        if 'register' in auto_print and ARs:\n            return {\n                'success': message,\n                'stickers': ARs.values(),\n                'stickertemplate': bika_setup.getAutoStickerTemplate()\n            }\n        else:\n            return {'success': message}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if a line is a Section or Subsection header.", "response": "def is_header(line):\n    \"\"\"\n    If a line has only one column, then it is a Section or Subsection\n    header.\n    :param line: Line to check\n    :return: boolean -If line is header\n    \"\"\"\n    if len(line) == 1:\n        return True\n    for idx, val in enumerate(line):\n        if idx > 0 and val:\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parseline(self, line):\n        sline = line.split(SEPARATOR)\n\n        if is_header(sline):\n            return self._handle_header(sline)\n        # If it is not an header it contains some data. but we need data only\n        # from the RESULT TABLE section.\n        elif not self._cur_section == SECTION_RESULT_TABLE:\n            return 0\n        # If line starts with 'Sample ID', then it is res_id of current record.\n        elif sline[0].lower() == 'Sample ID'.lower():\n            self._cur_res_id = sline[1].strip()\n            return 0\n        # If line starts with 'Assay' column,it is keyword of Analysis Service.\n        elif sline[0].lower() == 'Assay'.lower():\n            self._keyword = self._format_keyword(sline[1])\n            return 0\n        # Results are in Analyte Result subsection. We can skip other lines.\n        elif not self._cur_sub_section == SUBSECTION_ANALYTE_RESULT:\n            return 0\n        # If this is header line of Analyte Result table, set the columns\n        elif self._is_header_line:\n            self._is_header_line = False\n            self._columns = sline\n        # Must be result line of Analyte Result Table.\n        else:\n            return self._handle_result_line(sline)", "response": "Parse a single line of result CSV Line Parser."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _handle_header(self, sline):\n        # All characters UPPER CASE means it is a Section.\n        if sline[0].isupper():\n            self._cur_section = sline[0]\n            self._cur_sub_section = ''\n            return 0\n        else:\n            self._cur_sub_section = sline[0]\n            if sline[0] == SUBSECTION_ANALYTE_RESULT:\n                # This is Analyte Result Line, next line will be headers of\n                # results table\n                self._is_header_line = True\n                return 0\n            elif sline[0] in ('Detail', 'Errors', 'Messages'):\n                # It is end of Analyte Result sub-section. Add the last\n                # record as a raw result and reset everything.\n                self._submit_result()\n                return 0", "response": "Handle the first line of a header line."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _handle_result_line(self, sline):\n        # If there are less values founded than headers, it's an error\n        if len(sline) != len(self._columns):\n            self.err(\"One data line has the wrong number of items\")\n            return -1\n        result = ''\n        name = ''\n        for idx, val in enumerate(sline):\n            if self._columns[idx] == 'Analyte Name':\n                name = val\n            elif self._columns[idx] == 'Analyte Result':\n                result = self._convert_result(val)\n\n        if not self._cur_values.get(self._keyword, None):\n            self._cur_values[self._keyword] = {}\n\n        self._cur_values[self._keyword][name] = result\n\n        # Each line goes to result values as well in case AS'es for each\n        # Analyte is created in Bika. For those kind of AS'es, keywords must be\n        # the same as Analyte Name field from CSV.\n        self._cur_values[name] = {\n                'DefaultResult': 'Analyte Result',\n                'Remarks': '',\n                'Analyte Result': result,\n                }\n\n        # Adding this result to the previous results\n        # in the self._cur_values dictionary, as an interim field.\n        for key in self._cur_values:\n            if key == name:\n                continue\n            self._cur_values[key][name] = result\n\n        return 0", "response": "Parses the data line and adds the result to the dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsubmits the current result to the system.", "response": "def _submit_result(self):\n        \"\"\"\n        Adding current values as a Raw Result and Resetting everything.\n        Notice that we are not calculating final result of assay. We just set\n        NP and GP values and in Bika, AS will have a Calculation to generate\n        final result based on NP and GP values.\n        \"\"\"\n        if self._cur_res_id and self._cur_values:\n            # Setting DefaultResult just because it is obligatory. However,\n            # it won't be used because AS must have a Calculation based on\n            # GP and NP results.\n            self._cur_values[self._keyword]['DefaultResult'] = 'DefResult'\n            self._cur_values[self._keyword]['DefResult'] = ''\n            # If we add results as a raw result, AnalysisResultsImporter will\n            # automatically import them to the system. The only important thing\n            # here is to respect the dictionary format.\n            self._addRawResult(self._cur_res_id, self._cur_values)\n            self._reset()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _format_keyword(self, keyword):\n        import re\n        result = ''\n        if keyword:\n            result = re.sub(r\"\\W\", \"\", keyword)\n            # Remove underscores ('_') too.\n            result = re.sub(r\"_\", \"\", result)\n        return result", "response": "Formats a keyword for the analysis service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the current instrument title", "response": "def get_instrument_title(self):\n        \"\"\"Return the current instrument title\n        \"\"\"\n        instrument = self.context.getInstrument()\n        if not instrument:\n            return \"\"\n        return api.get_title(instrument)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_assignment_allowed(self):\n        if not self.is_manage_allowed():\n            return False\n        review_state = api.get_workflow_status_of(self.context)\n        edit_states = [\"open\", \"attachment_due\", \"to_be_verified\"]\n        return review_state in edit_states", "response": "Check if an analyst assignment is allowed"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_wide_interims(self):\n        outdict = {}\n        allowed_states = ['assigned', 'unassigned']\n        for analysis in self.context.getAnalyses():\n            # TODO Workflow - Analysis Use a query instead of this\n            if api.get_workflow_status_of(analysis) not in allowed_states:\n                continue\n\n            if analysis.getKeyword() in outdict.keys():\n                continue\n\n            calculation = analysis.getCalculation()\n            if not calculation:\n                continue\n\n            andict = {\n                \"analysis\": analysis.Title(),\n                \"keyword\": analysis.getKeyword(),\n                \"interims\": {}\n            }\n\n            # Analysis Service interim defaults\n            for field in analysis.getInterimFields():\n                if field.get(\"wide\", False):\n                    andict[\"interims\"][field[\"keyword\"]] = field\n\n            # Interims from calculation\n            for field in calculation.getInterimFields():\n                if field[\"keyword\"] not in andict[\"interims\"].keys() \\\n                   and field.get(\"wide\", False):\n                    andict[\"interims\"][field[\"keyword\"]] = field\n\n            if andict[\"interims\"]:\n                outdict[analysis.getKeyword()] = andict\n        return outdict", "response": "Returns a dictionary with the analyses services from the current worksheet which have at least one interim with Wide attribute set to\n        true and that have not yet submitted\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking the validity of the instruments used in the Analyses", "response": "def checkInstrumentsValidity(self):\n        \"\"\"Checks the validity of the instruments used in the Analyses If an\n        analysis with an invalid instrument (out-of-date or with calibration\n        tests failed) is found, a warn message will be displayed.\n        \"\"\"\n        invalid = []\n        ans = self.context.getAnalyses()\n        for an in ans:\n            valid = an.isInstrumentValid()\n            if not valid:\n                instrument = an.getInstrument()\n                inv = \"%s (%s)\" % (\n                    safe_unicode(an.Title()), safe_unicode(instrument.Title()))\n                if inv not in invalid:\n                    invalid.append(inv)\n        if len(invalid) > 0:\n            message = _(\"Some analyses use out-of-date or uncalibrated \"\n                        \"instruments. Results edition not allowed\")\n            message = \"%s: %s\" % (message, (\", \".join(invalid)))\n            self.context.plone_utils.addPortalMessage(message, \"warning\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_headerline(self, line):\n        if self._end_header is True:\n            # Header already processed\n            return 0\n\n        splitted = [token.strip() for token in line.split(self.COMMAS)]\n\n        # Analysis Time\t7/13/2017 9:55\n        if splitted[0] == self.HEADERKEY_ANALYSISTIME:\n            if splitted[1]:\n                try:\n                    d = datetime.strptime(splitted[1], \"%m/%d/%Y %H:%M\")\n                    self._header[self.HEADERKEY_ANALYSISTIME] = d\n                except ValueError:\n                    self.err(\"Invalid Output Time format\",\n                             numline=self._numline, line=line)\n            else:\n                self.warn(\"Output Time not found or empty\",\n                          numline=self._numline, line=line)\n                d = datetime.strptime(splitted[1], \"%I:%M %p\")\n\n        # Analyst Name\tMassHunter01\\Agilent\n        elif splitted[0] == self.HEADERKEY_ANALYSTNAME:\n            if self.HEADERKEY_ANALYSTNAME in self._header:\n                self.warn(\"Header File Data Name already found. Discarding\",\n                          numline=self._numline, line=line)\n                return 0\n\n            if splitted[1]:\n                self._header[self.HEADERKEY_ANALYSTNAME] = splitted[1]\n            else:\n                self.warn(\"File Data Name not found or empty\",\n                          numline=self._numline, line=line)\n\n        # Batch Data Path\n        # D:\\MassHunter\\GCMS\\Terpenes\\2017\\July\\20170711\\QuantResults\\20170711\n        elif splitted[0] == self.HEADERKEY_BATCHDATAPATH:\n            if self.HEADERKEY_BATCHDATAPATH in self._header:\n                self.warn(\"Header File Data Name already found. Discarding\",\n                          numline=self._numline, line=line)\n                return 0\n\n            if splitted[1]:\n                self._header[self.HEADERKEY_BATCHDATAPATH] = splitted[1]\n            else:\n                self.warn(\"File Data Name not found or empty\",\n                          numline=self._numline, line=line)\n\n        # Batch Name\t20170711 Sample Workup\n        elif splitted[0] == self.HEADERKEY_BATCHNAME:\n            if self.HEADERKEY_BATCHNAME in self._header:\n                self.warn(\"Header File Data Name already found. Discarding\",\n                          numline=self._numline, line=line)\n                return 0\n\n            if splitted[1]:\n                self._header[self.HEADERKEY_BATCHNAME] = splitted[1]\n            else:\n                self.warn(\"File Data Name not found or empty\",\n                          numline=self._numline, line=line)\n\n        # Batch State\tProcessed\n        elif splitted[0] == self.HEADERKEY_BATCHSTATE:\n            if self.HEADERKEY_BATCHSTATE in self._header:\n                self.warn(\"Header File Data Name already found. Discarding\",\n                          numline=self._numline, line=line)\n                return 0\n\n            if splitted[1]:\n                self._header[self.HEADERKEY_BATCHNAME] = splitted[1]\n            else:\n                self.warn(\"File Data Name not found or empty\",\n                          numline=self._numline, line=line)\n\n        # Calibration Last Updated Time\t6/29/2017 15:57\n        elif splitted[0] == self.HEADERKEY_LASTCALIBRATION:\n            if self.HEADERKEY_LASTCALIBRATION in self._header:\n                self.warn(\"Header File Data Name already found. Discarding\",\n                          numline=self._numline, line=line)\n                return 0\n\n            if splitted[1]:\n                self._header[self.HEADERKEY_LASTCALIBRATION] = splitted[1]\n            else:\n                self.warn(\"File Data Name not found or empty\",\n                          numline=self._numline, line=line)\n\n        # Report Generation Time\t1/1/0001 12:00:00 AM\n        elif splitted[0] == self.HEADERKEY_REPORTGENERATIONTIME:\n            if self.HEADERKEY_REPORTGENERATIONTIME in self._header:\n                self.warn(\"Header File Data Name already found. Discarding\",\n                          numline=self._numline, line=line)\n                return 0\n\n            if splitted[1]:\n                self._header[self.HEADERKEY_REPORTGENERATIONTIME] = splitted[1]\n            else:\n                self.warn(\"File Data Name not found or empty\",\n                          numline=self._numline, line=line)\n\n        # Report Generator Name\tNone\n        elif splitted[0] == self.HEADERKEY_REPORTGENERATORNAME:\n            if self.HEADERKEY_REPORTGENERATORNAME in self._header:\n                self.warn(\"Header File Data Name already found. Discarding\",\n                          numline=self._numline, line=line)\n                return 0\n\n            if splitted[1]:\n                self._header[self.HEADERKEY_REPORTGENERATORNAME] = splitted[1]\n            else:\n                self.warn(\"File Data Name not found or empty\",\n                          numline=self._numline, line=line)\n\n        # Report Results Data Path\tNone\n        elif splitted[0] == self.HEADERKEY_REPORTRESULTSDATAPATH:\n            if self.HEADERKEY_REPORTRESULTSDATAPATH in self._header:\n                self.warn(\"Header File Data Name already found. Discarding\",\n                          numline=self._numline, line=line)\n                return 0\n\n            if splitted[1]:\n                self._header[self.HEADERKEY_REPORTRESULTSDATAPATH] = \\\n                    splitted[1]\n            else:\n                self.warn(\"File Data Name not found or empty\",\n                          numline=self._numline, line=line)\n\n        # SchemaVersion\t65586\n        elif splitted[0] == self.HEADERKEY_SCHEMAVERSION:\n            if self.HEADERKEY_SCHEMAVERSION in self._header:\n                self.warn(\"Header File Data Name already found. Discarding\",\n                          numline=self._numline, line=line)\n                return 0\n\n            if splitted[1]:\n                self._header[self.HEADERKEY_SCHEMAVERSION] = splitted[1]\n            else:\n                self.warn(\"File Data Name not found or empty\",\n                          numline=self._numline, line=line)\n\n        # Quant Batch Version\tB.08.00\n        elif splitted[0] == self.HEADERKEY_QUANTBATCHVERSION:\n            if self.HEADERKEY_QUANTBATCHVERSION in self._header:\n                self.warn(\"Header File Data Name already found. Discarding\",\n                          numline=self._numline, line=line)\n                return 0\n\n            if splitted[1]:\n                self._header[self.HEADERKEY_QUANTBATCHVERSION] = splitted[1]\n            else:\n                self.warn(\"File Data Name not found or empty\",\n                          numline=self._numline, line=line)\n\n        # Quant Report Version\tB.08.00\n        elif splitted[0] == self.HEADERKEY_QUANTREPORTVERSION:\n            if self.HEADERKEY_QUANTREPORTVERSION in self._header:\n                self.warn(\"Header File Data Name already found. Discarding\",\n                          numline=self._numline, line=line)\n                return 0\n\n            if splitted[1]:\n                self._header[self.HEADERKEY_QUANTREPORTVERSION] = splitted[1]\n            else:\n                self.warn(\"File Data Name not found or empty\",\n                          numline=self._numline, line=line)\n\n        # Blank lines\n        if splitted[0] == '':\n            self._end_header = True\n            if len(self._header) == 0:\n                self.err(\"No header found\", numline=self._numline)\n                return -1\n            return 0\n\n        return 0", "response": "Parses the header line and sets the internal _header dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_quantitationesultsline(self, line):\n\n        if line == ',,,,,,,,,,,,,,,,,,':\n            return 0\n\n        if line.startswith('SampleID'):\n            self._end_header = True\n            self._quantitationresultsheader = [token.strip() for token\n                                               in line.split(self.COMMAS)\n                                               if token.strip()]\n            return 0\n\n        splitted = [token.strip() for token in line.split(self.COMMAS)]\n        quantitation = {'DefaultResult': 'FinalConcentration'}\n        for colname in self._quantitationresultsheader:\n            quantitation[colname] = ''\n\n        for i in range(len(splitted)):\n            token = splitted[i]\n            if i < len(self._quantitationresultsheader):\n                colname = self._quantitationresultsheader[i]\n                if colname in self.QUANTITATIONRESULTS_NUMERICHEADERS:\n                    try:\n                        quantitation[colname] = float(token)\n                    except ValueError:\n                        self.warn(\n                            \"No valid number ${token} in column \"\n                            \"${index} (${column_name})\",\n                            mapping={\"token\": token,\n                                     \"index\": str(i + 1),\n                                     \"column_name\": colname},\n                            numline=self._numline, line=line)\n                        quantitation[colname] = token\n                else:\n                    quantitation[colname] = token\n\n            elif token:\n                self.err(\"Orphan value in column ${index} (${token})\",\n                         mapping={\"index\": str(i+1),\n                                  \"token\": token},\n                         numline=self._numline, line=line)\n\n        result = quantitation[quantitation['DefaultResult']]\n        column_name = quantitation['DefaultResult']\n        result = self.zeroValueDefaultInstrumentResults(column_name,\n                                                        result, line)\n        quantitation[quantitation['DefaultResult']] = result\n\n        d = datetime.strptime(quantitation['AcqDateTime'], \"%m/%d/%Y %H:%M\")\n        quantitation['AcqDateTime'] = d\n        val = re.sub(r\"\\W\", \"\", quantitation['Compound'])\n        self._addRawResult(quantitation['DataFileName'],\n                           values={val: quantitation},\n                           override=False)", "response": "Parses quantitation results lines from the output file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of editable fields for the given instance", "response": "def editableFields(self, instance, visible_only=False):\n    \"\"\"Returns a list of editable fields for the given instance\n    \"\"\"\n    ret = []\n    portal = getToolByName(instance, 'portal_url').getPortalObject()\n    for field in self.fields():\n        if field.writeable(instance, debug=False) and    \\\n               (not visible_only or\n                field.widget.isVisible(\n                    instance, mode='edit', field=field) != 'invisible') and \\\n                field.widget.testCondition(instance.aq_parent, portal, instance):\n            ret.append(field)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef isVisible(self, instance, mode='view', default=\"visible\", field=None):\n    # Emulate Products.Archetypes.Widget.TypesWidget#isVisible first\n    vis_dic = getattr(aq_base(self), 'visible', _marker)\n    state = default\n    if vis_dic is _marker:\n        return state\n    if type(vis_dic) is DictType:\n        state = vis_dic.get(mode, state)\n    elif not vis_dic:\n        state = 'invisible'\n    elif vis_dic < 0:\n        state = 'hidden'\n\n    # Our custom code starts here\n    if not field:\n        return state\n\n    # Look for visibility from the adapters provided by IATWidgetVisibility\n    adapters = sorted(getAdapters([instance], IATWidgetVisibility),\n                      key=lambda adapter: getattr(adapter[1], \"sort\", 1000),\n                      reverse=True)\n    for adapter_name, adapter in adapters:\n        if field.getName() not in getattr(adapter, \"field_names\", []):\n            # Omit those adapters that are not suitable for this field\n            continue\n        adapter_state = adapter(instance, mode, field, state)\n        adapter_name = adapter.__class__.__name__\n        logger.info(\"IATWidgetVisibility rule {} for {}.{} ({}): {} -> {}\"\n            .format(adapter_name, instance.id, field.getName(), mode, state,\n                    adapter_state))\n        if adapter_state == state:\n            continue\n        return adapter_state\n\n    return state", "response": "decide if a field is visible in a given mode -> state."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef skip(instance, action, peek=False, unskip=False):\n\n    uid = callable(instance.UID) and instance.UID() or instance.UID\n    skipkey = \"%s_%s\" % (uid, action)\n    if 'workflow_skiplist' not in instance.REQUEST:\n        if not peek and not unskip:\n            instance.REQUEST['workflow_skiplist'] = [skipkey, ]\n    else:\n        if skipkey in instance.REQUEST['workflow_skiplist']:\n            if unskip:\n                instance.REQUEST['workflow_skiplist'].remove(skipkey)\n            else:\n                return True\n        else:\n            if not peek and not unskip:\n                instance.REQUEST[\"workflow_skiplist\"].append(skipkey)", "response": "Returns True if the transition is to be SKIPPED\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntries to perform the transition to the instance.", "response": "def doActionFor(instance, action_id, idxs=None):\n    \"\"\"Tries to perform the transition to the instance.\n    Object is reindexed after the transition takes place, but only if succeeds.\n    If idxs is set, only these indexes will be reindexed. Otherwise, will try\n    to use the indexes defined in ACTIONS_TO_INDEX mapping if any.\n    :param instance: Object to be transitioned\n    :param action_id: transition id\n    :param idxs: indexes to be reindexed after the transition\n    :returns: True if the transition has been performed, together with message\n    :rtype: tuple (bool,str)\n    \"\"\"\n    if not instance:\n        return False, \"\"\n\n    if isinstance(instance, list):\n        # TODO Workflow . Check if this is strictly necessary\n        # This check is here because sometimes Plone creates a list\n        # from submitted form elements.\n        logger.warn(\"Got a list of obj in doActionFor!\")\n        if len(instance) > 1:\n            logger.warn(\n                \"doActionFor is getting an instance parameter which is a list \"\n                \"with more than one item. Instance: '{}', action_id: '{}'\"\n                .format(instance, action_id)\n            )\n\n        return doActionFor(instance=instance[0], action_id=action_id, idxs=idxs)\n\n    # Since a given transition can cascade or promote to other objects, we want\n    # to reindex all objects for which the transition succeed at once, at the\n    # end of process. Otherwise, same object will be reindexed multiple times\n    # unnecessarily. Also, ActionsHandlerPool ensures the same transition is not\n    # applied twice to the same object due to cascade/promote recursions.\n    pool = ActionHandlerPool.get_instance()\n    if pool.succeed(instance, action_id):\n        return False, \"Transition {} for {} already done\"\\\n             .format(action_id, instance.getId())\n\n    # Return False if transition is not permitted\n    if not isTransitionAllowed(instance, action_id):\n        return False, \"Transition {} for {} is not allowed\"\\\n            .format(action_id, instance.getId())\n\n    # Add this batch process to the queue\n    pool.queue_pool()\n    succeed = False\n    message = \"\"\n    workflow = getToolByName(instance, \"portal_workflow\")\n    try:\n        workflow.doActionFor(instance, action_id)\n        succeed = True\n    except WorkflowException as e:\n        message = str(e)\n        curr_state = getCurrentState(instance)\n        clazz_name = instance.__class__.__name__\n        logger.warning(\n            \"Transition '{0}' not allowed: {1} '{2}' ({3})\"\\\n            .format(action_id, clazz_name, instance.getId(), curr_state))\n        logger.error(message)\n\n    # If no indexes to reindex have been defined, try to use those defined in\n    # the ACTIONS_TO_INDEXES mapping. Reindexing only those indexes that might\n    # be affected by the transition boosts the overall performance!.\n    if idxs is None:\n        portal_type = instance.portal_type\n        idxs = ACTIONS_TO_INDEXES.get(portal_type, {}).get(action_id, [])\n\n    # Add the current object to the pool and resume\n    pool.push(instance, action_id, succeed, idxs=idxs)\n    pool.resume()\n\n    return succeed, message"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall the instance s workflow event", "response": "def call_workflow_event(instance, event, after=True):\n    \"\"\"Calls the instance's workflow event\n    \"\"\"\n    if not event.transition:\n        return False\n\n    portal_type = instance.portal_type\n    wf_module = _load_wf_module('{}.events'.format(portal_type.lower()))\n    if not wf_module:\n        return False\n\n    # Inspect if event_<transition_id> function exists in the module\n    prefix = after and \"after\" or \"before\"\n    func_name = \"{}_{}\".format(prefix, event.transition.id)\n    func = getattr(wf_module, func_name, False)\n    if not func:\n        return False\n\n    logger.info('WF event: {0}.events.{1}'\n                .format(portal_type.lower(), func_name))\n    func(instance)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncompiling a list of possible workflow actions for this object", "response": "def get_workflow_actions(obj):\n    \"\"\" Compile a list of possible workflow transitions for this object\n    \"\"\"\n\n    def translate(id):\n        return t(PMF(id + \"_transition_title\"))\n\n    transids = getAllowedTransitions(obj)\n    actions = [{'id': it, 'title': translate(it)} for it in transids]\n    return actions"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef isTransitionAllowed(instance, transition_id):\n    wf_tool = getToolByName(instance, \"portal_workflow\")\n    for wf_id in wf_tool.getChainFor(instance):\n        wf = wf_tool.getWorkflowById(wf_id)\n        if wf and wf.isActionSupported(instance, transition_id):\n            return True\n\n    return False", "response": "Checks if the object can perform the transition passed in."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list with the transition ids that can be performed against the content object passed in.", "response": "def getAllowedTransitions(instance):\n    \"\"\"Returns a list with the transition ids that can be performed against\n    the instance passed in.\n    :param instance: A content object\n    :type instance: ATContentType\n    :returns: A list of transition/action ids\n    :rtype: list\n    \"\"\"\n    wftool = getToolByName(instance, \"portal_workflow\")\n    transitions = wftool.getTransitionsFor(instance)\n    return [trans['id'] for trans in transitions]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list with the statuses of the instance from the review_history", "response": "def get_review_history_statuses(instance, reverse=False):\n    \"\"\"Returns a list with the statuses of the instance from the review_history\n    \"\"\"\n    review_history = getReviewHistory(instance, reverse=reverse)\n    return map(lambda event: event[\"review_state\"], review_history)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_prev_status_from_history(instance, status=None):\n    target = status or api.get_workflow_status_of(instance)\n    history = getReviewHistory(instance, reverse=True)\n    history = map(lambda event: event[\"review_state\"], history)\n    if target not in history or history.index(target) == len(history)-1:\n        return None\n    return history[history.index(target)+1]", "response": "Returns the previous status of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if the object passed matches with the states passed in", "response": "def in_state(obj, states, stateflowid='review_state'):\n    \"\"\" Returns if the object passed matches with the states passed in\n    \"\"\"\n    if not states:\n        return False\n    obj_state = getCurrentState(obj, stateflowid=stateflowid)\n    return obj_state in states"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the actor that performed a given transition.", "response": "def getTransitionActor(obj, action_id):\n    \"\"\"Returns the actor that performed a given transition. If transition has\n    not been perormed, or current user has no privileges, returns None\n    :return: the username of the user that performed the transition passed-in\n    :type: string\n    \"\"\"\n    review_history = getReviewHistory(obj)\n    for event in review_history:\n        if event.get('action') == action_id:\n            return event.get('actor')\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the date of the transition for the object.", "response": "def getTransitionDate(obj, action_id, return_as_datetime=False):\n    \"\"\"\n    Returns date of action for object. Sometimes we need this date in Datetime\n    format and that's why added return_as_datetime param.\n    \"\"\"\n    review_history = getReviewHistory(obj)\n    for event in review_history:\n        if event.get('action') == action_id:\n            evtime = event.get('time')\n            if return_as_datetime:\n                return evtime\n            if evtime:\n                value = ulocalized_time(evtime, long_format=True,\n                                        time_only=False, context=obj)\n                return value\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getTransitionUsers(obj, action_id, last_user=False):\n    workflow = getToolByName(obj, 'portal_workflow')\n    users = []\n    try:\n        # https://jira.bikalabs.com/browse/LIMS-2242:\n        # Sometimes the workflow history is inexplicably missing!\n        review_history = list(workflow.getInfoFor(obj, 'review_history'))\n    except WorkflowException:\n        logger.error(\n            \"workflow history is inexplicably missing.\"\n            \" https://jira.bikalabs.com/browse/LIMS-2242\")\n        return users\n    # invert the list, so we always see the most recent matching event\n    review_history.reverse()\n    for event in review_history:\n        if event.get('action', '') == action_id:\n            value = event.get('actor', '')\n            users.append(value)\n            if last_user:\n                return users\n    return users", "response": "This function returns a list with the users who have done the transition."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef guard_handler(instance, transition_id):\n    if not instance:\n        return True\n    clazz_name = instance.portal_type\n    # Inspect if bika.lims.workflow.<clazzname>.<guards> module exists\n    wf_module = _load_wf_module('{0}.guards'.format(clazz_name.lower()))\n    if not wf_module:\n        return True\n\n    # Inspect if guard_<transition_id> function exists in the above module\n    key = 'guard_{0}'.format(transition_id)\n    guard = getattr(wf_module, key, False)\n    if not guard:\n        return True\n\n    #logger.info('{0}.guards.{1}'.format(clazz_name.lower(), key))\n    return guard(instance)", "response": "Generic workflow guard handler that returns true if the transition_id passed in can be performed to the instance passed in."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _load_wf_module(module_relative_name):\n    if not module_relative_name:\n        return None\n    if not isinstance(module_relative_name, basestring):\n        return None\n\n    rootmodname = __name__\n    modulekey = '{0}.{1}'.format(rootmodname, module_relative_name)\n    if modulekey in sys.modules:\n        return sys.modules.get(modulekey, None)\n\n    # Try to load the module recursively\n    modname = None\n    tokens = module_relative_name.split('.')\n    for part in tokens:\n        modname = '.'.join([modname, part]) if modname else part\n        import importlib\n        try:\n            _module = importlib.import_module('.'+modname, package=rootmodname)\n            if not _module:\n                return None\n        except Exception:\n            return None\n    return sys.modules.get(modulekey, None)", "response": "Loads a python module based on the module relative name passed in."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef push_reindex_to_actions_pool(obj, idxs=None):\n    indexes = idxs and idxs or []\n    pool = ActionHandlerPool.get_instance()\n    pool.push(obj, \"reindex\", success=True, idxs=indexes)", "response": "Push a reindex job to the actions handler pool\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds an instance into the pool", "response": "def push(self, instance, action, success, idxs=_marker):\n        \"\"\"Adds an instance into the pool, to be reindexed on resume\n        \"\"\"\n        uid = api.get_uid(instance)\n        info = self.objects.get(uid, {})\n        idx = [] if idxs is _marker else idxs\n        info[action] = {'success': success, 'idxs': idx}\n        self.objects[uid] = info"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef succeed(self, instance, action):\n        uid = api.get_uid(instance)\n        return self.objects.get(uid, {}).get(action, {}).get('success', False)", "response": "Returns if the task for the instance took place successfully\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresumes the pool and reindex all objects processed", "response": "def resume(self):\n        \"\"\"Resumes the pool and reindex all objects processed\n        \"\"\"\n        self.num_calls -= 1\n        if self.num_calls > 0:\n            return\n        logger.info(\"Resume actions for {} objects\".format(len(self)))\n\n        # Fetch the objects from the pool\n        processed = list()\n        for brain in api.search(dict(UID=self.objects.keys()), UID_CATALOG):\n            uid = api.get_uid(brain)\n            if uid in processed:\n                # This object has been processed already, do nothing\n                continue\n\n            # Reindex the object\n            obj = api.get_object(brain)\n            idxs = self.get_indexes(uid)\n            idxs_str = idxs and ', '.join(idxs) or \"-- All indexes --\"\n            logger.info(\"Reindexing {}: {}\".format(obj.getId(), idxs_str))\n            obj.reindexObject(idxs=idxs)\n            processed.append(uid)\n\n        # Cleanup the pool\n        logger.info(\"Objects processed: {}\".format(len(processed)))\n        self.objects = collections.OrderedDict()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the names of the indexes to be reindexed for the object with the uid passed in.", "response": "def get_indexes(self, uid):\n        \"\"\"Returns the names of the indexes to be reindexed for the object with\n        the uid passed in. If no indexes for this object have been specified\n        within the action pool job, returns an empty list (reindex all).\n        Otherwise, return all the indexes that have been specified for the\n        object within the action pool job.\n        \"\"\"\n        idxs = []\n        info = self.objects.get(uid, {})\n        for action_id, value in info.items():\n            obj_idxs = value.get('idxs', None)\n            if obj_idxs is None:\n                # Don't reindex!\n                continue\n            elif len(obj_idxs) == 0:\n                # Reindex all indexes!\n                return []\n            idxs.extend(obj_idxs)\n        # Always reindex review_state and is_active\n        idxs.extend([\"review_state\", \"is_active\"])\n        return list(set(idxs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef folderitem(self, obj, item, index):\n        title = obj.Title()\n        url = obj.absolute_url()\n\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n\n        sampletype = obj.getSampleType()\n        if sampletype:\n            title = sampletype.Title()\n            url = sampletype.absolute_url()\n            item[\"replace\"][\"SampleType\"] = get_link(url, value=title)\n\n        return item", "response": "This method is called every time an item is iterated in folderitems. It is called every time an item is iterated in folderitems. It is called every time an item is iterated in folderitems."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getMenuItems(self, context, request):\n        actions = super(WorkflowMenu, self).getMenuItems(context, request)\n        # Remove status history menu item ('Advanced...')\n        actions = [action for action in actions\n            if not action['action'].endswith('/content_status_history')]\n        return actions", "response": "Overrides the workflow actions menu displayed top right in the object s view. Returns the list of actions that can be performed."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns whether the passed in object needs to be partitioned by the current template", "response": "def is_auto_partition_required(self, brain_or_object):\n        \"\"\"Returns whether the passed in object needs to be partitioned\n        \"\"\"\n        obj = api.get_object(brain_or_object)\n        if not IAnalysisRequest.providedBy(obj):\n            return False\n        template = obj.getTemplate()\n        return template and template.getAutoPartition()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef notify_ar_retract(self, sample):\n        retest = sample.getRetest()\n        if not retest:\n            logger.warn(\"No retest found for {}. And it should!\"\n                        .format(api.get_id(sample)))\n            return\n\n        # Email fields\n        sample_id = api.get_id(sample)\n        subject = t(_(\"Erroneous result publication from {}\").format(sample_id))\n        emails_lab = self.get_lab_managers_formatted_emails()\n        emails_sample = self.get_sample_contacts_formatted_emails(sample)\n        recipients = list(set(emails_lab + emails_sample))\n\n        msg = MIMEMultipart(\"related\")\n        msg[\"Subject\"] = subject\n        msg[\"From\"] = self.get_laboratory_formatted_email()\n        msg[\"To\"] = \", \".join(recipients)\n        body = self.get_email_body(sample)\n        msg_txt = MIMEText(safe_unicode(body).encode('utf-8'), _subtype='html')\n        msg.preamble = 'This is a multi-part MIME message.'\n        msg.attach(msg_txt)\n\n        # Send the email\n        try:\n            host = api.get_tool(\"MailHost\")\n            host.send(msg.as_string(), immediate=True)\n        except Exception as err_msg:\n            message = _(\"Unable to send an email to alert lab \"\n                        \"client contacts that the Sample has been \"\n                        \"retracted: ${error}\",\n                        mapping={'error': safe_unicode(err_msg)})\n            self.context.plone_utils.addPortalMessage(message, 'warning')", "response": "Sends an email to the client contacts that the sample has been retracted."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_email_body(self, sample):\n        retest = sample.getRetest()\n        lab_address = api.get_bika_setup().laboratory.getPrintAddress()\n        setup = api.get_setup()\n        body = Template(setup.getEmailBodySampleInvalidation())\\\n            .safe_substitute(\n            dict(sample_link=self.get_html_link(sample),\n                 retest_link=self.get_html_link(retest),\n                 sample_id=api.get_id(sample),\n                 retest_id=api.get_id(retest),\n                 lab_address=\"<br/>\".join(lab_address)))\n        return body", "response": "Returns the email body text for the given sample"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the laboratory formatted email", "response": "def get_laboratory_formatted_email(self):\n        \"\"\"Returns the laboratory email formatted\n        \"\"\"\n        lab = api.get_bika_setup().laboratory\n        return self.get_formatted_email((lab.getName(), lab.getEmailAddress()))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_lab_managers_formatted_emails(self):\n        users = api.get_users_by_roles(\"LabManager\")\n        users = map(lambda user: (user.getProperty(\"fullname\"),\n                                  user.getProperty(\"email\")), users)\n        return map(self.get_formatted_email, users)", "response": "Returns a list with lab managers formatted emails"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a string with the formatted email for the given contact", "response": "def get_contact_formatted_email(self, contact):\n        \"\"\"Returns a string with the formatted email for the given contact\n        \"\"\"\n        contact_name = contact.Title()\n        contact_email = contact.getEmailAddress()\n        return self.get_formatted_email((contact_name, contact_email))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_sample_contacts_formatted_emails(self, sample):\n        contacts = list(set([sample.getContact()] + sample.getCCContact()))\n        return map(self.get_contact_formatted_email, contacts)", "response": "Returns a list with the formatted emails from sample contacts\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_html_link(self, obj):\n        return \"<a href='{}'>{}</a>\".format(api.get_url(obj), api.get_id(obj))", "response": "Returns an html formatted link for the given object\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the printed time of the last results report from the sample", "response": "def set_printed_time(self, sample):\n        \"\"\"Updates the printed time of the last results report from the sample\n        \"\"\"\n        if api.get_workflow_status_of(sample) != \"published\":\n            return False\n        reports = sample.objectValues(\"ARReport\")\n        reports = sorted(reports, key=lambda report: report.getDatePublished())\n        last_report = reports[-1]\n        if not last_report.getDatePrinted():\n            last_report.setDatePrinted(DateTime())\n            sample.reindexObject(idxs=[\"getPrinted\"])\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns whether the request Hidden param for the given obj is True", "response": "def is_hidden(self, service):\n        \"\"\"Returns whether the request Hidden param for the given obj is True\n        \"\"\"\n        uid = api.get_uid(service)\n        hidden_ans = self.request.form.get(\"Hidden\", {})\n        return hidden_ans.get(uid, \"\") == \"on\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_specs(self, service):\n        uid = api.get_uid(service)\n        keyword = service.getKeyword()\n        specs = ResultsRangeDict(keyword=keyword, uid=uid).copy()\n        for key in specs.keys():\n            specs_value = self.request.form.get(key, [{}])[0].get(uid, None)\n            specs[key] = specs_value or specs.get(key)\n        return specs", "response": "Returns the analysis specs available in the request for the given uid\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the data line and return the number of rows to jump and parse the next data line.", "response": "def parse_data_line(self, sline):\n        \"\"\"\n        Parse the data line. If an AS was selected it can distinguish between data rows and information rows.\n        :param sline: a split data line to parse\n        :returns: the number of rows to jump and parse the next data line or return the code error -1\n        \"\"\"\n        # if there are less values founded than headers, it's an error\n        if len(sline) != len(self._columns):\n            self.err(\"One data line has the wrong number of items\")\n            return 0\n        if self.analysiskey:\n            # If an AS is selected it should saves all the columns from the same row under the selected AS.\n            rawdict = {}\n            for idx, result in enumerate(sline):\n                # All data is interpreted as different fields from the same AS\n                rawdict[self._columns[idx]] = result\n            rid = rawdict.get('Sample ID No')\n            if not rid:\n                self.err(\"No Sample ID defined\",\n                         numline=self._numline)\n                return -1\n            rawdict['DefaultResult'] = self.defaultresult \\\n                                     if self.defaultresult in self._columns \\\n                                     else self.err(\"Default Result Key \" + self.defaultresult + \" not found\")\n            rawdict['DateTime'] = csvDate2BikaDate(rawdict['Analysis Date'], rawdict['Analysis Time'])\n            self._addRawResult(rid, {self.analysiskey: rawdict}, False)\n\n        else:\n            # If non AS is selected it should saves all data under the same analysed sample (Sample ID No), and ignore\n            # the less important rows from the line.\n            headerdict = {}\n            datadict = {}\n            for idx, result in enumerate(sline):\n                if self._columns[idx] not in self._excludedcolumns:\n                    datadict[self._columns[idx]] = {'Result': result, 'DefaultData': 'Result'}\n                else:\n                    headerdict[self._columns[idx]] = result\n            rid = headerdict['Sample ID No']\n            datadict['DateTime'] = csvDate2BikaDate(headerdict['Analysis Date'], headerdict['Analysis Time'])\n            self._addRawResult(rid, datadict, False)\n            self._header = headerdict\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nservicing triggered each time an item is iterated in folderitems. The use of this service prevents the extra-loops in child objects. :obj: the instance of the class to be foldered :item: dict containing the properties of the object to be used by the template :index: current index of the item", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Service triggered each time an item is iterated in folderitems.\n        The use of this service prevents the extra-loops in child objects.\n        :obj: the instance of the class to be foldered\n        :item: dict containing the properties of the object to be used by\n            the template\n        :index: current index of the item\n        \"\"\"\n        item[\"OrderNumber\"] = obj.getOrderNumber()\n        item[\"OrderDate\"] = self.ulocalized_time(obj.getOrderDate())\n        item[\"DateDispatched\"] = self.ulocalized_time(obj.getDateDispatched())\n        item[\"replace\"][\"OrderNumber\"] = get_link(\n            api.get_url(obj), obj.getOrderNumber())\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the object field names to search against the current object", "response": "def search_fields(self):\n        \"\"\"Returns the object field names to search against\n        \"\"\"\n        search_fields = self.request.get(\"search_fields\", None)\n        if not search_fields:\n            return []\n\n        search_fields = json.loads(_u(search_fields))\n        return search_fields"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the search term", "response": "def search_term(self):\n        \"\"\"Returns the search term\n        \"\"\"\n        search_term = _c(self.request.get(\"searchTerm\", \"\"))\n        return search_term.lower().strip()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_utf8(self, data):\n        # if this is a unicode string, return its string representation\n        if isinstance(data, unicode):\n            return data.encode('utf-8')\n\n        # if this is a list of values, return list of string values\n        if isinstance(data, list):\n            return [self.to_utf8(item) for item in data]\n\n        # if this is a dictionary, return dictionary of string keys and values\n        if isinstance(data, dict):\n            return {\n                self.to_utf8(key): self.to_utf8(value)\n                for key, value in data.iteritems()\n            }\n\n        # if it's anything else, return it in its original form\n        return data", "response": "Convert unicode values to strings even if they belong to lists or dicts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the query inferred from the request name", "response": "def get_query_from_request(self, name):\n        \"\"\"Returns the query inferred from the request\n        \"\"\"\n        query = self.request.get(name, \"{}\")\n        # json.loads does unicode conversion, which will fail in the catalog\n        # search for some cases. So we need to convert the strings to utf8\n        # https://github.com/senaite/senaite.core/issues/443\n        query = json.loads(query)\n        return self.to_utf8(query)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_raw_query(self):\n        query = self.base_query.copy()\n        search_query = self.search_query.copy()\n        query.update(search_query)\n\n        # Add sorting criteria\n        sorting = self.resolve_sorting(query)\n        query.update(sorting)\n\n        # Check if sort_on is an index and if is sortable. Otherwise, assume\n        # the sorting must be done manually\n        catalog = api.get_tool(self.catalog_name)\n        sort_on = query.get(\"sort_on\", None)\n        if sort_on and not self.is_sortable_index(sort_on, catalog):\n            del(query[\"sort_on\"])\n        return query", "response": "Returns the raw query to use for current search"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresolving the sorting criteria for the given query.", "response": "def resolve_sorting(self, query):\n        \"\"\"Resolves the sorting criteria for the given query\n        \"\"\"\n        sorting = {}\n\n        # Sort on\n        sort_on = query.get(\"sidx\", None)\n        sort_on = sort_on or query.get(\"sort_on\", None)\n        sort_on = sort_on == \"Title\" and \"sortable_title\" or sort_on\n        if sort_on:\n            sorting[\"sort_on\"] = sort_on\n\n            # Sort order\n            sort_order = query.get(\"sord\", None)\n            sort_order = sort_order or query.get(\"sort_order\", None)\n            if sort_order in [\"desc\", \"reverse\", \"rev\", \"descending\"]:\n                sorting[\"sort_order\"] = \"descending\"\n            else:\n                sorting[\"sort_order\"] = \"ascending\"\n\n            # Sort limit\n            sort_limit = api.to_int(query.get(\"limit\", 30), default=30)\n            if sort_limit:\n                sorting[\"sort_limit\"] = sort_limit\n\n        return sorting"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_sortable_index(self, index_name, catalog):\n        index = self.get_index(index_name, catalog)\n        if not index:\n            return False\n        return index.meta_type in [\"FieldIndex\", \"DateIndex\"]", "response": "Returns whether the index is sortable by the given catalog."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_index(self, field_name, catalog):\n        index = catalog.Indexes.get(field_name, None)\n        if not index and field_name == \"Title\":\n            # Legacy\n            return self.get_index(\"sortable_title\", catalog)\n        return index", "response": "Returns the index of the given field_name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform a search against the catalog and returns the brains", "response": "def search(self, query, search_term, search_field, catalog):\n        \"\"\"Performs a search against the catalog and returns the brains\n        \"\"\"\n        logger.info(\"Reference Widget Catalog: {}\".format(catalog.id))\n        if not search_term:\n            return catalog(query)\n\n        index = self.get_index(search_field, catalog)\n        if not index:\n            logger.warn(\"*** Index not found: '{}'\".format(search_field))\n            return []\n\n        meta = index.meta_type\n        if meta == \"TextIndexNG3\":\n            query[index.id] = \"{}*\".format(search_term)\n\n        elif meta == \"ZCTextIndex\":\n            logger.warn(\"*** Field '{}' ({}). Better use TextIndexNG3\"\n                        .format(meta, search_field))\n            query[index.id] = \"{}*\".format(search_term)\n\n        elif meta in [\"FieldIndex\", \"KeywordIndex\"]:\n            logger.warn(\"*** Field '{}' ({}). Better use TextIndexNG3\"\n                        .format(meta, search_field))\n            query[index.id] = search_term\n\n        else:\n            logger.warn(\"*** Index '{}' ({}) not supported\"\n                        .format(search_field, meta))\n            return []\n\n        logger.info(\"Reference Widget Query: {}\".format(repr(query)))\n        return catalog(query)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the mapped permissions for the given object", "response": "def get_mapped_permissions_for(brain_or_object):\n    \"\"\"Get the mapped permissions for the given object\n\n    A mapped permission is one that is used in the object.\n\n    Each permission string, e.g. \"senaite.core: Field: Edit Analysis Remarks\" is\n    translated by the function `AccessControl.Permission.pname` to a valid\n    attribute name:\n\n    >>> from bika.lims.permissions import FieldEditAnalysisResult\n    >>> AccessControl.Permission import pname\n    >>> pname(FieldEditAnalysisResult)\n    _Field__Edit_Result_Permission\n\n    This attribute is looked up in the object by `getPermissionMapping`:\n\n    >>> from AccessControl.PermissionMapping import getPermissionMapping\n    >>> getPermissionMapping(FieldEditAnalysisResult, wrapper)\n    (\"Manager\", \"Sampler\")\n\n    Therefore, only those permissions which have roles mapped on the object\n    or by objects within the acquisition chain are considered.\n\n    Code extracted from `IRoleManager.manage_getUserRolesAndPermissions`\n\n    :param brain_or_object: Catalog brain or object\n    :returns: List of permissions\n    \"\"\"\n    obj = api.get_object(brain_or_object)\n    mapping = obj.manage_getPermissionMapping()\n    return map(lambda item: item[\"permission_name\"], mapping)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_allowed_permissions_for(brain_or_object, user=None):\n    allowed = []\n    user = get_user(user)\n    obj = api.get_object(brain_or_object)\n    for permission in get_mapped_permissions_for(brain_or_object):\n        if user.has_permission(permission, obj):\n            allowed.append(permission)\n    return allowed", "response": "Get the allowed permissions for the given object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_disallowed_permissions_for(brain_or_object, user=None):\n    disallowed = []\n    user = get_user(user)\n    obj = api.get_object(brain_or_object)\n    for permission in get_mapped_permissions_for(brain_or_object):\n        if not user.has_permission(permission, obj):\n            disallowed.append(permission)\n    return disallowed", "response": "Get the disallowed permissions for the given object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_permission(permission, brain_or_object):\n    sm = get_security_manager()\n    obj = api.get_object(brain_or_object)\n    return sm.checkPermission(permission, obj) == 1", "response": "Checks whether the security context allows the given permission on the given brain or object"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the permissions of the role which are granted on the object", "response": "def get_permissions_for_role(role, brain_or_object):\n    \"\"\"Return the permissions of the role which are granted on the object\n\n    Code extracted from `IRoleManager.permissionsOfRole`\n\n    :param role: The role to check the permission\n    :param brain_or_object: Catalog brain or object\n    :returns: List of permissions of the role\n    \"\"\"\n    obj = api.get_object(brain_or_object)\n\n    # Raise an error if the role is invalid\n    valid_roles = get_valid_roles_for(obj)\n    if role not in valid_roles:\n        raise ValueError(\"The Role '{}' is invalid.\".format(role))\n\n    out = []\n    for item in obj.ac_inherited_permissions(1):\n        name, value = item[:2]\n        # Permission maps a named permission to a set of attribute names\n        permission = Permission(name, value, obj)\n        if role in permission.getRoles():\n            out.append(name)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the roles that are granted on the object Code extracted from IRoleManager. rolesOfPermission", "response": "def get_roles_for_permission(permission, brain_or_object):\n    \"\"\"Return the roles of the permission that is granted on the object\n\n    Code extracted from `IRoleManager.rolesOfPermission`\n\n    :param permission: The permission to get the roles\n    :param brain_or_object: Catalog brain or object\n    :returns: List of roles having the permission\n    \"\"\"\n    obj = api.get_object(brain_or_object)\n    valid_roles = get_valid_roles_for(obj)\n    for item in obj.ac_inherited_permissions(1):\n        name, value = item[:2]\n        # found the requested permission\n        if name == permission:\n            # Permission maps a named permission to a set of attribute names\n            permission = Permission(name, value, obj)\n            roles = permission.getRoles()\n            # return only valid roles that have the permission granted\n            return filter(lambda r: r in valid_roles, roles)\n    # Raise an error if the permission is invalid\n    raise ValueError(\"The permission {} is invalid.\".format(permission))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_local_roles_for(brain_or_object, user=None):\n    user_id = get_user_id(user)\n    obj = api.get_object(brain_or_object)\n    return sorted(obj.get_local_roles_for_userid(user_id))", "response": "Get the local defined roles on the context\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef grant_local_roles_for(brain_or_object, roles, user=None):\n    user_id = get_user_id(user)\n    obj = api.get_object(brain_or_object)\n\n    if isinstance(roles, basestring):\n        roles = [roles]\n\n    obj.manage_addLocalRoles(user_id, roles)\n    return get_local_roles_for(brain_or_object)", "response": "Grant local roles for the object"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrevokes local roles for the object", "response": "def revoke_local_roles_for(brain_or_object, roles, user=None):\n    \"\"\"Revoke local roles for the object\n\n    Code extracted from `IRoleManager.manage_setLocalRoles`\n\n    :param brain_or_object: Catalog brain or object\n    :param roles: The local roles to revoke for the current user\n    :param user: A user ID, user object or None (for the current user)\n    \"\"\"\n    user_id = get_user_id(user)\n    obj = api.get_object(brain_or_object)\n    valid_roles = get_valid_roles_for(obj)\n    to_grant = list(get_local_roles_for(obj))\n\n    if isinstance(roles, basestring):\n        roles = [roles]\n\n    for role in roles:\n        if role in to_grant:\n            if role not in valid_roles:\n                raise ValueError(\"The Role '{}' is invalid.\".format(role))\n            # Remove the role\n            to_grant.remove(role)\n\n    if len(to_grant) > 0:\n        obj.manage_setLocalRoles(user_id, to_grant)\n    else:\n        obj.manage_delLocalRoles([user_id])\n    return get_local_roles_for(brain_or_object)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef grant_permission_for(brain_or_object, permission, roles, acquire=0):\n    obj = api.get_object(brain_or_object)\n    valid_roles = get_valid_roles_for(obj)\n    to_grant = list(get_roles_for_permission(permission, obj))\n\n    if isinstance(roles, basestring):\n        roles = [roles]\n\n    for role in roles:\n        if role not in to_grant:\n            if role not in valid_roles:\n                raise ValueError(\"The Role '{}' is invalid.\".format(role))\n            # Append the role\n            to_grant.append(role)\n\n    manage_permission_for(obj, permission, to_grant, acquire=acquire)", "response": "Grant the permission for the object to the defined roles"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmanaging the permissions for the given permission.", "response": "def manage_permission_for(brain_or_object, permission, roles, acquire=0):\n    \"\"\"Change the settings for the given permission.\n\n    Code extracted from `IRoleManager.manage_permission`\n\n    :param brain_or_object: Catalog brain or object\n    :param permission: The permission to be granted\n    :param roles: The roles the permission to be granted to\n    :param acquire: Flag to acquire the permission\n    \"\"\"\n    obj = api.get_object(brain_or_object)\n\n    if isinstance(roles, basestring):\n        roles = [roles]\n\n    for item in obj.ac_inherited_permissions(1):\n        name, value = item[:2]\n        if name == permission:\n            permission = Permission(name, value, obj)\n            if acquire:\n                roles = list(roles)\n            else:\n                roles = tuple(roles)\n            permission.setRoles(roles)\n            return\n\n    # Raise an error if the permission is invalid\n    raise ValueError(\"The permission {} is invalid.\".format(permission))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a DisplayList with the available templates found in", "response": "def getSRTemplates(self):\n        \"\"\"\n        Returns a DisplayList with the available templates found in\n        browser/samplinground/templates/\n        \"\"\"\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n        templates_dir = os.path.join(this_dir, self._TEMPLATES_DIR)\n        tempath = '%s/%s' % (templates_dir, '*.pt')\n        templates = [t.split('/')[-1] for t in glob.glob(tempath)]\n        out = []\n        for template in templates:\n            out.append({'id': template, 'title': template[:-3]})\n        for templates_resource in iterDirectoriesOfType(self._TEMPLATES_ADDON_DIR):\n            prefix = templates_resource.__name__\n            templates = [\n                tpl for tpl in templates_resource.listDirectory()\n                if tpl.endswith('.pt')\n                ]\n            for template in templates:\n                out.append({\n                    'id': '{0}:{1}'.format(prefix, template),\n                    'title': '{0} ({1})'.format(template[:-3], prefix),\n                })\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the current samplinground rendered with the template specified in the request.", "response": "def getFormTemplate(self):\n        \"\"\"Returns the current samplinground rendered with the template\n            specified in the request (param 'template').\n            Moves the iterator to the next samplinground available.\n        \"\"\"\n        templates_dir = self._TEMPLATES_DIR\n        embedt = self.request.get('template', self._DEFAULT_TEMPLATE)\n        if embedt.find(':') >= 0:\n            prefix, embedt = embedt.split(':')\n            templates_dir = queryResourceDirectory(self._TEMPLATES_ADDON_DIR, prefix).directory\n        embed = ViewPageTemplateFile(os.path.join(templates_dir, embedt))\n        reptemplate = \"\"\n        try:\n            reptemplate = embed(self)\n        except:\n            tbex = traceback.format_exc()\n            wsid = self._samplingrounds[self._current_sr_index].id\n            reptemplate = \"<div class='error-print'>%s - %s '%s':<pre>%s</pre></div>\" % (wsid, _(\"Unable to load the template\"), embedt, tbex)\n        if self._current_sr_index < len(self._samplingrounds):\n            self._current_sr_index += 1\n        return reptemplate"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the css style to be used for the current template.", "response": "def getCSS(self):\n        \"\"\" Returns the css style to be used for the current template.\n            If the selected template is 'default.pt', this method will\n            return the content from 'default.css'. If no css file found\n            for the current template, returns empty string\n        \"\"\"\n        template = self.request.get('template', self._DEFAULT_TEMPLATE)\n        content = ''\n        if template.find(':') >= 0:\n            prefix, template = template.split(':')\n            resource = queryResourceDirectory(\n                self._TEMPLATES_ADDON_DIR, prefix)\n            css = '{0}.css'.format(template[:-3])\n            if css in resource.listDirectory():\n                content = resource.readFile(css)\n        else:\n            this_dir = os.path.dirname(os.path.abspath(__file__))\n            templates_dir = os.path.join(this_dir, self._TEMPLATES_DIR)\n            path = '%s/%s.css' % (templates_dir, template[:-3])\n            with open(path, 'r') as content_file:\n                content = content_file.read()\n        return content"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getAnalysisRequestTemplatesInfo(self):\n        arts_list = []\n        for art in self.context.ar_templates:\n            pc = getToolByName(self.context, 'portal_catalog')\n            contentFilter = {'portal_type': 'ARTemplate',\n                             'UID': art}\n            art_brain = pc(contentFilter)\n            if len(art_brain) == 1:\n                art_obj = art_brain[0].getObject()\n                arts_list.append({\n                    'uid': art_obj.UID(),\n                    'id': art_obj.id,\n                    'title': art_obj.title,\n                    'url': art_obj.absolute_url(),\n                })\n        return arts_list", "response": "Returns a lost of dicts with the analysis request templates infomration\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getAnalysisRequestBySample(self):\n        # rows will contain the data for each html row\n        rows = []\n        # columns will be used to sort and define the columns\n        columns = {\n            'column_order': [\n                'sample_id',\n                'sample_type',\n                'sampling_point',\n                'sampling_date',\n                'analyses',\n                ],\n            'titles': {\n                'sample_id': _('Sample ID'),\n                'sample_type': _('Sample Type'),\n                'sampling_point': _('Sampling Point'),\n                'sampling_date': _('Sampling Date'),\n                'analyses': _('Analysis'),\n            }\n        }\n        ars = self.context.getAnalysisRequests()\n        for ar in ars:\n            ar = ar.getObject()\n            arcell = False\n            analyses = ar.getAnalyses()\n            numans = len(analyses)\n            for analysis in analyses:\n                row = {\n                    'sample_id': {\n                        'hidden': True if arcell else False,\n                        'rowspan': numans,\n                        'value': ar.getId(),\n                        },\n                    'sample_type': {\n                        'hidden': True if arcell else False,\n                        'rowspan': numans,\n                        'value': ar.getSampleType().title,\n                        },\n                    'sampling_point': {\n                        'hidden': True if arcell else False,\n                        'rowspan': numans,\n                        'value': ar.getSamplePoint().title if ar.getSamplePoint() else '',\n                        },\n                    'sampling_date': {\n                        'hidden': True if arcell else False,\n                        'rowspan': numans,\n                        'value': self.context.sampling_date,\n                        },\n                    'analyses': {\n                        'title': analysis.title,\n                        'units': analysis.getUnit,\n                    },\n                }\n                rows.append(row)\n                arcell = True\n\n        # table will contain the data that from where the html\n        # will take the info\n        table = {\n            'columns': columns,\n            'rows': rows,\n        }\n        return table", "response": "Returns a list of dictionaries sorted by Sample Partition and Container\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nservicing triggered each time an item is iterated in folderitems. The use of this service prevents the extra-loops in child objects. :obj: the instance of the class to be foldered :item: dict containing the properties of the object to be used by the template :index: current index of the item", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Service triggered each time an item is iterated in folderitems.\n        The use of this service prevents the extra-loops in child objects.\n        :obj: the instance of the class to be foldered\n        :item: dict containing the properties of the object to be used by\n            the template\n        :index: current index of the item\n        \"\"\"\n        title = obj.Title()\n        description = obj.Description()\n        url = obj.absolute_url()\n\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n        item[\"Description\"] = description\n\n        department = obj.getDepartment()\n        if department:\n            title = department.Title()\n            url = department.absolute_url()\n            item[\"replace\"][\"Department\"] = get_link(url, value=title)\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getAnalysisServicesVocabulary(self):\n        bsc = getToolByName(self, 'bika_setup_catalog')\n        brains = bsc(portal_type='AnalysisService',\n                     is_active=True)\n        items = [(b.UID, b.Title) for b in brains]\n        items.insert(0, (\"\", \"\"))\n        items.sort(lambda x, y: cmp(x[1], y[1]))\n        return DisplayList(list(items))", "response": "Get all active Analysis Services from Bika Setup and return them as Display List."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getPrefixFor(self, portal_type):\n        prefix = [p for p in self.getIDFormatting() if p['portal_type'] == portal_type]\n        if prefix:\n            return prefix[0]['prefix']\n        else:\n            return portal_type", "response": "Returns the prefix for a portal_type. If not found uses the portal_type itself"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef isRejectionWorkflowEnabled(self):\n        widget = self.getRejectionReasons()\n        # widget will be something like:\n        # [{'checkbox': u'on', 'textfield-2': u'b', 'textfield-1': u'c', 'textfield-0': u'a'}]\n        if len(widget) > 0:\n            checkbox = widget[0].get('checkbox', False)\n            return True if checkbox == 'on' and len(widget[0]) > 1 else False\n        else:\n            return False", "response": "Return True if the rejection workflow is enabled."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the list of predefined rejection reasons", "response": "def getRejectionReasonsItems(self):\n        \"\"\"Return the list of predefined rejection reasons\n        \"\"\"\n        reasons = self.getRejectionReasons()\n        if not reasons:\n            return []\n        reasons = reasons[0]\n        keys = filter(lambda key: key != \"checkbox\", reasons.keys())\n        return map(lambda key: reasons[key], sorted(keys)) or []"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_analysis(self, analysis):\n        analysis_object = api.get_object(analysis)\n        result = analysis_object.getResult()\n        results_range = analysis_object.getResultsRange()\n        range_result = results_range.get('result', None)\n        range_min = results_range.get('min', None)\n        range_max = results_range.get('max', None)\n        # All them must be floatable\n        for value in [result, range_result, range_min, range_max]:\n            if not api.is_floatable(value):\n                return\n        cap_date = analysis_object.getResultCaptureDate()\n        cap_date = api.is_date(cap_date) and \\\n                   cap_date.strftime('%Y-%m-%d %I:%M %p') or ''\n        if not cap_date:\n            return\n\n        # Create json\n        ref_sample_id = analysis_object.getSample().getId()\n        as_keyword = analysis_object.getKeyword()\n        as_name = analysis_object.Title()\n        as_ref = '{} ({})'.format(as_name, as_keyword)\n        as_rows = self.analyses_dict.get(as_ref, {})\n        an_rows = as_rows.get(ref_sample_id, [])\n        an_rows.append({\n            'date': cap_date,\n            'target': api.to_float(range_result),\n            'upper': api.to_float(range_max),\n            'lower': api.to_float(range_min),\n            'result': api.to_float(result),\n            'unit': analysis_object.getUnit(),\n            'id': api.get_uid(analysis_object)\n        })\n        as_rows[ref_sample_id] = an_rows\n        self.analyses_dict[as_ref] = as_rows", "response": "Adds an analysis to the Analyses Chart machinery."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef allowed_transitions_for_many(self, context, request):\n        wftool = getToolByName(context, \"portal_workflow\")\n        uc = getToolByName(context, 'uid_catalog')\n        uids = json.loads(request.get('uid', '[]'))\n        if not uids:\n            raise BadRequest(\"No object UID specified in request\")\n\n        allowed_transitions = []\n        try:\n            brains = uc(UID=uids)\n            for brain in brains:\n                obj = brain.getObject()\n                trans = [{'id': t['id'], 'title': t['title']} for t in\n                         wftool.getTransitionsFor(obj)]\n                allowed_transitions.append(\n                    {'uid': obj.UID(), 'transitions': trans})\n        except Exception as e:\n            msg = \"Cannot get the allowed transitions ({})\".format(e.message)\n            raise BadRequest(msg)\n\n        ret = {\n            \"url\": router.url_for(\"allowedTransitionsFor_many\",\n                                  force_external=True),\n            \"success\": True,\n            \"error\": False,\n            \"transitions\": allowed_transitions\n        }\n        return ret", "response": "Returns a list of dictionaries that contains the allowed transitions for the object in the context"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of the rejection reasons as strings or an empty list if no rejection reasons are found.", "response": "def get_rejection_reasons(self, keyword=None):\n        \"\"\"\n        Returns a list with the rejection reasons as strings\n\n        :param keyword: set of rejection reasons to be retrieved.\n        Possible values are:\n            - 'selected': Get, amongst the set of predefined reasons, the ones selected\n            - 'other': Get the user free-typed reason for rejection\n            - None: Get all rejection reasons\n        :return: list of rejection reasons as strings or an empty list\n        \"\"\"\n        keys = ['selected', 'other']\n        if keyword is None:\n            return sum(map(self.get_rejection_reasons, keys), [])\n        if keyword not in keys:\n            return []\n        rejection_reasons = self.context.getRejectionReasons()\n        rejection_reasons = rejection_reasons and rejection_reasons[0] or {}\n        if keyword == 'other':\n            return rejection_reasons.get(keyword, '') and [rejection_reasons.get(keyword, '')] or []\n        return rejection_reasons.get(keyword, [])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clearFindAndRebuild(self):\n        def indexObject(obj, path):\n            self.reindexObject(obj)\n            self.counter += 1\n            if self.counter % 100 == 0:\n                logger.info('Progress: {} objects have been cataloged for {}.'\n                            .format(self.counter, self.id))\n\n        logger.info('Cleaning and rebuilding %s...' % self.id)\n        try:\n            at = getToolByName(self, 'archetype_tool')\n            types = [k for k, v in at.catalog_map.items()\n                     if self.id in v]\n            self.counter = 0\n            self.manage_catalogClear()\n            portal = getToolByName(self, 'portal_url').getPortalObject()\n            portal.ZopeFindAndApply(portal,\n                                    obj_metatypes=types,\n                                    search_sub=True,\n                                    apply_func=indexObject)\n        except:\n            logger.error(traceback.format_exc())\n            e = sys.exc_info()\n            logger.error(\n                \"Unable to clean and rebuild %s due to: %s\" % (self.id, e))\n        logger.info('%s cleaned and rebuilt' % self.id)", "response": "Empties catalog then finds all contentish objects and reindexes them."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nempty catalog, then finds all contentish objects quering over uid_catalog and reindexes them. This may take a long time and will not care about missing objects in uid_catalog.", "response": "def softClearFindAndRebuild(self):\n        \"\"\"\n            Empties catalog, then finds all contentish objects quering over\n            uid_catalog and reindexes them.\n            This may take a long time and will not care about missing\n            objects in uid_catalog.\n        \"\"\"\n        logger.info('Soft cleaning and rebuilding %s...' % self.id)\n        try:\n            at = getToolByName(self, 'archetype_tool')\n            types = [k for k, v in at.catalog_map.items()\n                     if self.id in v]\n            self.counter = 0\n            self.manage_catalogClear()\n            # Getting UID catalog\n            portal = getToolByName(self, 'portal_url').getPortalObject()\n            uid_c = getToolByName(portal, 'uid_catalog')\n            brains = uid_c(portal_type=types)\n            self.total = len(brains)\n            for brain in brains:\n                obj = brain.getObject()\n                self.catalog_object(\n                    obj, idxs=self.indexes(),\n                    update_metadata=True)\n                self.counter += 1\n                if self.counter % 100 == 0:\n                    logger.info(\n                        'Progress: {}/{} objects have been cataloged for {}.'\n                            .format(self.counter, self.total, self.id))\n                    if self.counter % 1000 == 0:\n                        transaction.commit()\n                        logger.info(\n                            '{0} items processed.'\n                                .format(self.counter))\n            transaction.commit()\n            logger.info(\n                '{0} items processed.'\n                    .format(self.counter))\n        except:\n            logger.error(traceback.format_exc())\n            e = sys.exc_info()\n            logger.error(\n                \"Unable to clean and rebuild %s due to: %s\" % (self.id, e))\n        logger.info('%s cleaned and rebuilt' % self.id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_form_submit(self):\n        protect.CheckAuthenticator(self.request)\n        logger.info(\"Handle ResultsInterpration Submit\")\n        # Save the results interpretation\n        res = self.request.form.get(\"ResultsInterpretationDepts\", [])\n        self.context.setResultsInterpretationDepts(res)\n        self.add_status_message(_(\"Changes Saved\"), level=\"info\")\n        # reindex the object after save to update all catalog metadata\n        self.context.reindexObject()\n        # notify object edited event\n        event.notify(ObjectEditedEvent(self.context))", "response": "Handle the submission of a ResultsInterpration request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if edit is allowed", "response": "def is_edit_allowed(self):\n        \"\"\"Check if edit is allowed\n        \"\"\"\n        checkPermission = self.context.portal_membership.checkPermission\n        return checkPermission(FieldEditResultsInterpretation, self.context)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the text saved for the selected department", "response": "def get_text(self, department, mode=\"raw\"):\n        \"\"\"Returns the text saved for the selected department\n        \"\"\"\n        row = self.context.getResultsInterpretationByDepartment(department)\n        rt = RichTextValue(row.get(\"richtext\", \"\"), \"text/plain\", \"text/html\")\n        if mode == \"output\":\n            return rt.output\n        return rt.raw"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the UID of the user associated with the authenticated user", "response": "def getContactUIDForUser(self):\n        \"\"\"Get the UID of the user associated with the authenticated user\n        \"\"\"\n        membership_tool = api.get_tool(\"portal_membership\")\n        member = membership_tool.getAuthenticatedMember()\n        username = member.getUserName()\n        r = self.portal_catalog(\n            portal_type=\"Contact\",\n            getUsername=username\n        )\n        if len(r) == 1:\n            return r[0].UID"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getAnalysisCategories(self):\n        bsc = api.get_tool(\"bika_setup_catalog\")\n        cats = []\n        for st in bsc(portal_type=\"AnalysisCategory\",\n                      is_active=True,\n                      sort_on=\"sortable_title\"):\n            cats.append((st.UID, st.Title))\n        return DisplayList(cats)", "response": "Return all available analysis categories"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an array containing the contacts from this Client", "response": "def getContacts(self, only_active=True):\n        \"\"\"Return an array containing the contacts from this Client\n        \"\"\"\n        contacts = self.objectValues(\"Contact\")\n        if only_active:\n            contacts = filter(api.is_active, contacts)\n        return contacts"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getDecimalMark(self):\n        if self.getDefaultDecimalMark() is False:\n            return self.Schema()[\"DecimalMark\"].get(self)\n        return self.bika_setup.getDecimalMark()", "response": "Return the decimal mark for this client"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the Country from the Physical or Postal Address", "response": "def getCountry(self, default=None):\n        \"\"\"Return the Country from the Physical or Postal Address\n        \"\"\"\n        physical_address = self.getPhysicalAddress().get(\"country\", default)\n        postal_address = self.getPostalAddress().get(\"country\", default)\n        return physical_address or postal_address"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getProvince(self, default=None):\n        physical_address = self.getPhysicalAddress().get(\"state\", default)\n        postal_address = self.getPostalAddress().get(\"state\", default)\n        return physical_address or postal_address", "response": "Return the Province from the Physical or Postal Address\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the Province from the Physical or Postal Address", "response": "def getDistrict(self, default=None):\n        \"\"\"Return the Province from the Physical or Postal Address\n        \"\"\"\n        physical_address = self.getPhysicalAddress().get(\"district\", default)\n        postal_address = self.getPostalAddress().get(\"district\", default)\n        return physical_address or postal_address"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef manage_delObjects(self, ids=None, REQUEST=None):\n        if ids is None:\n            ids = []\n        if isinstance(ids, basestring):\n            ids = [ids]\n\n        for id in ids:\n            item = self._getOb(id)\n            if isinstance(item, Attachment):\n                # Ignore DeleteObjects permission check\n                continue\n            if not _checkPermission(permissions.DeleteObjects, item):\n                raise Unauthorized, (\n                    \"Do not have permissions to remove this object\")\n        return PortalFolder.manage_delObjects(self, ids, REQUEST=REQUEST)", "response": "Overrides parent function to ignore the DeleteObjects permission."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the results line and returns the current object.", "response": "def parse_resultsline(self, line):\n        \"\"\" Parses result lines\n        \"\"\"\n\n        split_row = [token.strip() for token in line.split('\\t')]\n        if len(split_row) == 1:\n            self._currentanalysiskw = split_row[0]\n            return 0\n\n        # 'Average\\t\\t-----\\t-----\\t-----\\t-----\\t-----\\t\\t-----\\t\\t\\t\\t-----'\n        elif split_row[0] == 'Average':\n            return 0\n        # '%RSD\\t\\t-----\\t-----\\t-----\\t-----\\t-----\\t\\t-----\\t\\t\\t\\t-----'\n        elif split_row[0] == '%RSD':\n            return 0\n        # 'Maximum\\t\\t-----\\t-----\\t-----\\t-----\\t-----\\t\\t-----\\t\\t\\t\\t-----'\n        elif split_row[0] == 'Maximum':\n            return 0\n        # 'Minimum\\t\\t-----\\t-----\\t-----\\t-----\\t-----\\t\\t-----\\t\\t\\t\\t-----'\n        elif split_row[0] == 'Minimum':\n            return 0\n        # 'Std. Dev.\\t\\t-----\\t-----\\t-----\\t-----\\t-----\\t\\t-----\\t\\t\\t\\t----'\n        elif split_row[0] == 'Std. Dev.':\n            return 0\n        else:\n            self._currentresultsheader = ['Title%s' % i for i\n                                          in range(len(split_row))]\n            _results = {'DefaultResult': 'Title3'}\n            _results.update(dict(zip(self._currentresultsheader, split_row)))\n\n            result = _results[_results['DefaultResult']]\n            column_name = _results['DefaultResult']\n            result = self.zeroValueDefaultInstrumentResults(\n                                                    column_name, result, line)\n            _results[_results['DefaultResult']] = result\n\n            val = re.sub(r\"\\W\", \"\", self._currentanalysiskw)\n            self._addRawResult(_results['Title1'],\n                               values={val: _results},\n                               override=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef folderitem(self, obj, item, index):\n        url = item.get(\"url\")\n        title = item.get(\"Title\")\n\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n        item[\"getType\"] = _(obj.getType()[0])\n        item[\"getDownFrom\"] = self.localize_date(obj.getDownFrom())\n        item[\"getDownTo\"] = self.localize_date(obj.getDownTo())\n        item[\"getMaintainer\"] = obj.getMaintainer()\n\n        status = obj.getCurrentState()\n        statustext = obj.getCurrentStateI18n()\n        statusimg = \"\"\n\n        if status == mstatus.CLOSED:\n            statusimg = \"instrumentmaintenance_closed.png\"\n            item[\"state_class\"] = \"state-inactive\"\n        elif status == mstatus.CANCELLED:\n            statusimg = \"instrumentmaintenance_cancelled.png\"\n            item[\"state_class\"] = \"state-cancelled\"\n        elif status == mstatus.INQUEUE:\n            statusimg = \"instrumentmaintenance_inqueue.png\"\n            item[\"state_class\"] = \"state-open\"\n        elif status == mstatus.OVERDUE:\n            statusimg = \"instrumentmaintenance_overdue.png\"\n            item[\"state_class\"] = \"state-open\"\n        elif status == mstatus.PENDING:\n            statusimg = \"instrumentmaintenance_pending.png\"\n            item[\"state_class\"] = \"state-pending\"\n\n        item[\"replace\"][\"getCurrentState\"] = get_image(\n            statusimg, title=statustext)\n        return item", "response": "Augment folder listing item with information from the status of the item."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef folderitem(self, obj, item, index):\n        url = item.get(\"url\")\n        title = item.get(\"Title\")\n        calibrator = obj.getCalibrator()\n\n        item[\"getDownFrom\"] = self.localize_date(obj.getDownFrom())\n        item[\"getDownTo\"] = self.localize_date(obj.getDownTo())\n        item[\"getCalibrator\"] = \"\"\n        if calibrator:\n            props = api.get_user_properties(calibrator)\n            name = props.get(\"fullname\", calibrator)\n            item[\"getCalibrator\"] = name\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n\n        # calibration with the most remaining days\n        if obj == self.latest_calibration:\n            item[\"state_class\"] = \"state-published\"\n        # running calibrations\n        elif obj in self.active_calibrations:\n            item[\"state_class\"] = \"state-active\"\n        # inactive calibrations\n        else:\n            item[\"state_class\"] = \"state-inactive\"\n\n        return item", "response": "Augment folder listing item with additional information."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef folderitem(self, obj, item, index):\n        url = item.get(\"url\")\n        title = item.get(\"Title\")\n\n        item[\"getDownFrom\"] = self.localize_date(obj.getDownFrom())\n        item[\"getDownTo\"] = self.localize_date(obj.getDownTo())\n        item[\"getValidator\"] = obj.getValidator()\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n\n        # validation with the most remaining days\n        if obj == self.latest_validation:\n            item[\"state_class\"] = \"state-published\"\n        # running validations\n        elif obj in self.active_validations:\n            item[\"state_class\"] = \"state-active\"\n        # inactive validations\n        else:\n            item[\"state_class\"] = \"state-inactive\"\n\n        return item", "response": "Augment folder listing item with the contents of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\naugmenting folder listing item", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Augment folder listing item\n        \"\"\"\n        url = item.get(\"url\")\n        title = item.get(\"Title\")\n        creator = obj.Creator()\n\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n        item[\"created\"] = self.localize_date(obj.created())\n        item[\"getType\"] = _(obj.getType()[0])\n        item[\"creator\"] = \"\"\n        if creator:\n            props = api.get_user_properties(creator)\n            name = props.get(\"fullname\", creator)\n            item[\"creator\"] = name\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\naugments folder listing item with additional data", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Augment folder listing item with additional data\n        \"\"\"\n        url = item.get(\"url\")\n        title = item.get(\"Title\")\n\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n        item[\"getDate\"] = self.localize_date(obj.getDate())\n        item[\"getValidFrom\"] = self.localize_date(obj.getValidFrom())\n        item[\"getValidTo\"] = self.localize_date(obj.getValidTo())\n\n        if obj.getInternal() is True:\n            item[\"replace\"][\"getAgency\"] = \"\"\n            item[\"state_class\"] = \"%s %s\" % \\\n                (item[\"state_class\"], \"internalcertificate\")\n\n        item[\"getDocument\"] = \"\"\n        item[\"replace\"][\"getDocument\"] = \"\"\n        doc = self.get_document(obj)\n        if doc and doc.get_size() > 0:\n            filename = doc.filename\n            download_url = \"{}/at_download/Document\".format(url)\n            anchor = get_link(download_url, filename)\n            item[\"getDocument\"] = filename\n            item[\"replace\"][\"getDocument\"] = anchor\n\n        # Latest valid certificate\n        if obj == self.latest_certificate:\n            item[\"state_class\"] = \"state-published\"\n        # Valid certificate\n        elif obj in self.valid_certificates:\n            item[\"state_class\"] = \"state-valid state-published\"\n        # Invalid certificates\n        else:\n            img = get_image(\"exclamation.png\", title=t(_(\"Out of date\")))\n            item[\"replace\"][\"getValidTo\"] = \"%s %s\" % (item[\"getValidTo\"], img)\n            item[\"state_class\"] = \"state-invalid\"\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef folderitem(self, obj, item, index):\n        url = item.get(\"url\")\n        title = item.get(\"DocumentID\")\n\n        item[\"replace\"][\"DocumentID\"] = get_link(url, title)\n\n        item[\"FileDownload\"] = \"\"\n        item[\"replace\"][\"FileDownload\"] = \"\"\n        file = self.get_file(obj)\n        if file and file.get_size() > 0:\n            filename = file.filename\n            download_url = \"{}/at_download/File\".format(url)\n            anchor = get_link(download_url, filename)\n            item[\"FileDownload\"] = filename\n            item[\"replace\"][\"FileDownload\"] = anchor\n\n        item[\"DocumentVersion\"] = obj.getDocumentVersion()\n        item[\"DocumentLocation\"] = obj.getDocumentLocation()\n        item[\"DocumentType\"] = obj.getDocumentType()\n\n        return item", "response": "Augment folder listing item with additional data\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_TSVline(self, line):\n\n        split_row = [token.strip() for token in line.split('\\t')]\n        _results = {'DefaultResult': 'Conc.'}\n\n        # ID# 1\n        if split_row[0] == 'ID#':\n            return 0\n        # Name\tCBDV - cannabidivarin\n        elif split_row[0] == 'Name':\n            if split_row[1]:\n                self._currentanalysiskw = split_row[1]\n                return 0\n            else:\n                self.warn(\"Analysis Keyword not found or empty\",\n                          numline=self._numline, line=line)\n        # Data Filename\tSample Name\tSample ID\tSample Type\tLevel#\n        elif 'Sample ID' in split_row:\n            split_row.insert(0, '#')\n            self._currentresultsheader = split_row\n            return 0\n        # 1\tQC PREP A_QC PREP A_009.lcd\tQC PREP\n        elif split_row[0].isdigit():\n            _results.update(dict(zip(self._currentresultsheader, split_row)))\n\n            # 10/17/2016 7:55:06 PM\n            try:\n                da = datetime.strptime(\n                    _results['Date Acquired'], \"%m/%d/%Y %I:%M:%S %p\")\n                self._header['Output Date'] = da\n                self._header['Output Time'] = da\n            except ValueError:\n                self.err(\"Invalid Output Time format\",\n                         numline=self._numline, line=line)\n\n            result = _results[_results['DefaultResult']]\n            column_name = _results['DefaultResult']\n            result = self.zeroValueDefaultInstrumentResults(\n                                                    column_name, result, line)\n            _results[_results['DefaultResult']] = result\n\n            self._addRawResult(_results['Sample ID'],\n                               values={self._currentanalysiskw: _results},\n                               override=False)", "response": "Parses a TSV line and returns the ID Name Filename Name Type\tCBDV Type\tLevel"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef addDocument(self, row_dict, folder):\n    if folder:\n        # This content type need a file\n        if row_dict.get('File', None):\n            path = resource_filename(\n                self.dataset_project,\n                \"setupdata/%s/%s\" % (self.dataset_name,\n                                     row_dict['File'])\n            )\n            try:\n                file_data = read_file(path)\n            except Exception as msg:\n                file_data = None\n                logger.warning(msg[0] + \" Error on sheet: \" + self.sheetname)\n\n            # Obtain all created instrument documents content type\n            catalog = getToolByName(self.context, 'bika_setup_catalog')\n            documents_brains = catalog.searchResults({'portal_type': 'Multifile'})\n            # If a the new document has the same DocumentID as a created document, this object won't be created.\n            idAlreadyInUse = False\n            for item in documents_brains:\n                if item.getObject().getDocumentID() == row_dict.get('DocumentID', ''):\n                    warning = \"The ID '%s' used for this document is already in use on instrument '%s', consequently \" \\\n                              \"the file hasn't been upload.\" % (row_dict.get('DocumentID', ''), row_dict.get('instrument', ''))\n                    self.context.plone_utils.addPortalMessage(warning)\n                    idAlreadyInUse = True\n            if not idAlreadyInUse:\n                obj = _createObjectByType(\"Multifile\", folder, tmpID())\n                obj.edit(\n                    DocumentID=row_dict.get('DocumentID', ''),\n                    DocumentVersion=row_dict.get('DocumentVersion', ''),\n                    DocumentLocation=row_dict.get('DocumentLocation', ''),\n                    DocumentType=row_dict.get('DocumentType', ''),\n                    File=file_data\n                )\n                obj.unmarkCreationFlag()\n                renameAfterCreation(obj)\n                notify(ObjectInitializedEvent(obj))", "response": "This function adds a multifile object to the instrument folder"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a generator for all rows in a sheet.", "response": "def get_rows(self, startrow=3, worksheet=None):\n        \"\"\"Returns a generator for all rows in a sheet.\n           Each row contains a dictionary where the key is the value of the\n           first row of the sheet for each column.\n           The data values are returned in utf-8 format.\n           Starts to consume data from startrow\n        \"\"\"\n\n        headers = []\n        row_nr = 0\n        worksheet = worksheet if worksheet else self.worksheet\n        for row in worksheet.rows:  # .iter_rows():\n            row_nr += 1\n            if row_nr == 1:\n                # headers = [cell.internal_value for cell in row]\n                headers = [cell.value for cell in row]\n                continue\n            if row_nr % 1000 == 0:\n                transaction.savepoint()\n            if row_nr <= startrow:\n                continue\n            # row = [_c(cell.internal_value).decode('utf-8') for cell in row]\n            new_row = []\n            for cell in row:\n                value = cell.value\n                if value is None:\n                    value = ''\n                if isinstance(value, unicode):\n                    value = value.encode('utf-8')\n                # Strip any space, \\t, \\n, or \\r characters from the left-hand\n                # side, right-hand side, or both sides of the string\n                if isinstance(value, str):\n                    value = value.strip(' \\t\\n\\r')\n                new_row.append(value)\n            row = dict(zip(headers, new_row))\n\n            # parse out addresses\n            for add_type in ['Physical', 'Postal', 'Billing']:\n                row[add_type] = {}\n                if add_type + \"_Address\" in row:\n                    for key in ['Address', 'City', 'State', 'District', 'Zip', 'Country']:\n                        row[add_type][key] = str(row.get(\"%s_%s\" % (add_type, key), ''))\n\n            yield row"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a sheet string value to a boolean value.", "response": "def to_bool(self, value):\n        \"\"\" Converts a sheet string value to a boolean value.\n            Needed because of utf-8 conversions\n        \"\"\"\n\n        try:\n            value = value.lower()\n        except:\n            pass\n        try:\n            value = value.encode('utf-8')\n        except:\n            pass\n        try:\n            value = int(value)\n        except:\n            pass\n        if value in ('true', 1):\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfilling the address fields of the specified object based on the values in the row.", "response": "def fill_addressfields(self, row, obj):\n        \"\"\" Fills the address fields for the specified object if allowed:\n            PhysicalAddress, PostalAddress, CountryState, BillingAddress\n        \"\"\"\n        addresses = {}\n        for add_type in ['Physical', 'Postal', 'Billing', 'CountryState']:\n            addresses[add_type] = {}\n            for key in ['Address', 'City', 'State', 'District', 'Zip', 'Country']:\n                addresses[add_type][key.lower()] = str(row.get(\"%s_%s\" % (add_type, key), ''))\n\n        if addresses['CountryState']['country'] == '' \\\n            and addresses['CountryState']['state'] == '':\n            addresses['CountryState']['country'] = addresses['Physical']['country']\n            addresses['CountryState']['state'] = addresses['Physical']['state']\n\n        if hasattr(obj, 'setPhysicalAddress'):\n            obj.setPhysicalAddress(addresses['Physical'])\n        if hasattr(obj, 'setPostalAddress'):\n            obj.setPostalAddress(addresses['Postal'])\n        if hasattr(obj, 'setCountryState'):\n            obj.setCountryState(addresses['CountryState'])\n        if hasattr(obj, 'setBillingAddress'):\n            obj.setBillingAddress(addresses['Billing'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfilling the contact fields for the specified object.", "response": "def fill_contactfields(self, row, obj):\n        \"\"\" Fills the contact fields for the specified object if allowed:\n            EmailAddress, Phone, Fax, BusinessPhone, BusinessFax, HomePhone,\n            MobilePhone\n        \"\"\"\n        fieldnames = ['EmailAddress',\n                      'Phone',\n                      'Fax',\n                      'BusinessPhone',\n                      'BusinessFax',\n                      'HomePhone',\n                      'MobilePhone',\n                      ]\n        schema = obj.Schema()\n        fields = dict([(field.getName(), field) for field in schema.fields()])\n        for fieldname in fieldnames:\n            try:\n                field = fields[fieldname]\n            except:\n                if fieldname in row:\n                    logger.info(\"Address field %s not found on %s\"%(fieldname,obj))\n                continue\n            value = row.get(fieldname, '')\n            field.set(obj, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_relations(self, service_title, default_obj, obj_type, catalog_name, sheet_name, column):\n        out_objects = [default_obj] if default_obj else []\n        cat = getToolByName(self.context, catalog_name)\n        worksheet = self.workbook.get_sheet_by_name(sheet_name)\n        if not worksheet:\n            return out_objects\n        for row in self.get_rows(3, worksheet=worksheet):\n            row_as_title = row.get('Service_title')\n            if not row_as_title:\n                return out_objects\n            elif row_as_title != service_title:\n                continue\n            obj = self.get_object(cat, obj_type, row.get(column))\n            if obj:\n                if default_obj and default_obj.UID() == obj.UID():\n                    continue\n                out_objects.append(obj)\n        return out_objects", "response": "Return an array of objects of the specified type in accordance to\n            the object titles defined in the sheet specified in the paramenter catalog_name and the paramenter column."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfixes the workflow transitions in the order they are added to the target states.", "response": "def fix_workflow_transitions(portal):\n    \"\"\"\n    Replace target states from some workflow statuses\n    \"\"\"\n    logger.info(\"Fixing workflow transitions...\")\n    tochange = [\n        {'wfid': 'bika_duplicateanalysis_workflow',\n         'trid': 'submit',\n         'changes': {\n             'new_state_id': 'to_be_verified',\n             'guard_expr': ''\n            },\n         'update': {\n             'catalog': CATALOG_ANALYSIS_LISTING,\n             'portal_type': 'DuplicateAnalysis',\n             'status_from': 'attachment_due',\n             'status_to': 'to_be_verified'\n            }\n         }\n    ]\n\n    wtool = api.get_tool('portal_workflow')\n    for item in tochange:\n        wfid = item['wfid']\n        trid = item['trid']\n        workflow = wtool.getWorkflowById(wfid)\n        transitions = workflow.transitions\n        transition = transitions[trid]\n        changes = item.get('changes', {})\n\n        if 'new_state_id' in changes:\n            new_state_id = changes['new_state_id']\n            oldstate = transition.new_state_id\n            logger.info(\n                \"Replacing target state '{0}' from '{1}.{2}' to {3}\"\n                    .format(oldstate, wfid, trid, new_state_id)\n            )\n            transition.new_state_id = new_state_id\n\n        if 'guard_expr' in changes:\n            new_guard = changes['guard_expr']\n            if not new_guard:\n                transition.guard = None\n                logger.info(\n                    \"Removing guard expression from '{0}.{1}'\"\n                        .format(wfid, trid))\n            else:\n                guard = transition.getGuard()\n                guard.expr = Expression(new_guard)\n                transition.guard = guard\n                logger.info(\n                    \"Replacing guard expression from '{0}.{1}' to {2}\"\n                        .format(wfid, trid, new_guard))\n\n        update = item.get('update', {})\n        if update:\n            catalog_id = update['catalog']\n            portal_type = update['portal_type']\n            catalog = api.get_tool(catalog_id)\n            brains = catalog(portal_type=portal_type)\n            for brain in brains:\n                obj = api.get_object(brain)\n                if 'status_from' in update and 'status_to' in update:\n                    status_from = update['status_from']\n                    status_to = update['status_to']\n                    if status_from == brain.review_state:\n                        logger.info(\n                            \"Changing status for {0} from '{1} to {2}\"\n                                .format(obj.getId(), status_from, status_to))\n                        changeWorkflowState(obj, wfid, status_to)\n\n                workflow.updateRoleMappingsFor(obj)\n                obj.reindexObject()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprepare a listing data item for the listing.", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Prepare a data item for the listing.\n\n        :param obj: The catalog brain or content object\n        :param item: Listing item (dictionary)\n        :param index: Index of the listing item\n        :returns: Augmented listing data item\n        \"\"\"\n\n        obj = obj.getObject()\n        # Group items by RefSample - Worksheet - Position\n        ws = obj.getWorksheet()\n        wsid = ws and ws.id or ''\n        wshref = ws.absolute_url() or None\n        if wshref:\n            item['replace']['Worksheet'] = \"<a href='%s'>%s</a>\" % (\n                wshref, wsid)\n\n        imgtype = \"\"\n        if obj.portal_type == 'ReferenceAnalysis':\n            antype = QCANALYSIS_TYPES.getValue(obj.getReferenceType())\n            if obj.getReferenceType() == 'c':\n                imgtype = \"<img title='%s' \" \\\n                          \"src='%s/++resource++bika.lims.images/control.png\" \\\n                          \"'/>&nbsp;\" % (\n                              antype, self.context.absolute_url())\n            if obj.getReferenceType() == 'b':\n                imgtype = \"<img title='%s' \" \\\n                          \"src='%s/++resource++bika.lims.images/blank.png\" \\\n                          \"'/>&nbsp;\" % (\n                              antype, self.context.absolute_url())\n            item['replace']['Partition'] = \"<a href='%s'>%s</a>\" % (\n                obj.aq_parent.absolute_url(), obj.aq_parent.id)\n        elif obj.portal_type == 'DuplicateAnalysis':\n            antype = QCANALYSIS_TYPES.getValue('d')\n            imgtype = \"<img title='%s' \" \\\n                      \"src='%s/++resource++bika.lims.images/duplicate.png\" \\\n                      \"'/>&nbsp;\" % (\n                          antype, self.context.absolute_url())\n            item['sortcode'] = '%s_%s' % (obj.getRequestID(), obj.getKeyword())\n\n        item['before']['Service'] = imgtype\n        item['sortcode'] = '%s_%s' % (obj.getReferenceAnalysesGroupID(),\n                                      obj.getKeyword())\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_headerline(self, line):\n        if line.startswith('Date'):\n            splitted = self.splitLine(line)\n            if len(splitted) > 1:\n                self._header['Date'] = splitted[1]\n                if len(splitted) > 2 and splitted[2] == 'User':\n                    self._header['Date'] = splitted[1]\n                    self._header['User'] = splitted[3] \\\n                        if len(splitted) > 3 else ''\n                else:\n                    self.warn(\"Unexpected header format\", numline=self._numline)\n            else:\n                self.warn(\"Unexpected header format\", numline=self._numline)\n\n            return 0\n\n        if line.startswith('Time'):\n            splitted = self.splitLine(line)\n            if len(splitted) > 1:\n                self._header['Time'] = splitted[1]\n            else:\n                self.warn(\"Unexpected header format\", numline=self._numline)\n\n            return 0\n\n        if line.startswith('Sample/ctrl'):\n            # Sample/ctrl ID    Pat/Ctr/cAl    Test name    Test type\n            if len(self._header) == 0:\n                self.warn(\"No header found\", numline=self._numline)\n                return -1\n\n            #Grab column names\n            self._end_header = True\n            self._columns = self.splitLine(line)\n            return 1", "response": "Parses a single line of a header line and returns the ID of the record."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parseline(self, line):\n        split_line = line.split(self._separator)\n        if self._is_header:\n            self._is_header = False\n            self._current_section = split_line[0]\n            return 1  # Skip the line of equal signs after a header\n\n        # If the line has only one column and it is made entirely of equal signs\n        # then it is the beginning or the end a section definition. Since equal sign\n        # lines corresponding to the end of section definitions are skipped this must\n        # be the beginning of a section definition.\n        elif len(split_line) == 1 and all(x == '=' for x in split_line[0]):\n            self._is_header = True\n            # if exiting result information section then reset column names and AR keyword\n            if 'result information' in self._current_section.lower():\n                self._reset()\n        # From the assay calibration section the assay name is retrieved\n        elif 'assay calibration' in self._current_section.lower():\n            # if line inside assay calibration section starts with assay name,\n            # then its value is the analysis service keyword\n            if 'assay name' in split_line[0].lower():\n                self._ar_keyword = self._format_keyword(split_line[1])\n        # If current section is results information then process the results\n        elif 'result information' in self._current_section.lower():\n            if self._columns is None:\n                self._columns = split_line\n            else:\n                result_id, values = self._handle_result_line(split_line)\n                if result_id and values:\n                    self._addRawResult(result_id, values)\n\n        return 0", "response": "The parse method in\n           ResultsFileParser calls this method for each line in the results log file that is to be parsed and returns the number of rows to jump and parse the next data line."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the data line and adds the results to the dictionary.", "response": "def _handle_result_line(self, split_line):\n        \"\"\"\n        Parses the data line and adds the results to the dictionary.\n        :param split_line: a split data line to parse\n        :returns: the current result id and the dictionary of values obtained from the results\n        \"\"\"\n        values = {}\n        result_id = ''\n        if self._ar_keyword:\n            # Create a new entry in the values dictionary and store the results\n            values[self._ar_keyword] = {}\n            for idx, val in enumerate(split_line):\n                if self._columns[idx].lower() == 'sampleid':\n                    result_id = val\n                else:\n                    # columns with date in its name store only the date and\n                    # columns with time in its name store date and time.\n                    if val and ('date' in self._columns[idx].lower()\n                                or 'time' in self._columns[idx].lower()):\n                        val = self._date_to_bika_date(val, 'date' in self._columns[idx].lower())\n                    values[self._ar_keyword][self._columns[idx]] = val\n                values[self._ar_keyword]['DefaultResult'] = 'FinalResult'\n\n        return result_id, values"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _date_to_bika_date(self, date_time, only_date):\n        # if only date the input date format is: 2017/01/26\n        # else: 2017/01/26 12:47:09 PM\n        if only_date:\n            return datetime.strptime(date_time, '%Y/%m/%d').strftime('%Y%m%d')\n        else:\n            return datetime.strptime(date_time, \"%Y/%m/%d %I:%M:%S %p\").strftime(\"%Y%m%d %H:%M:%S\")", "response": "Convert a string containing a date from results file to bika format\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the default sticker ID for that content type depending on the size.", "response": "def get_default_sticker_id(self):\n        \"\"\"\n        Gets the default sticker for that content type depending on the\n        requested size.\n\n        :return: An sticker ID as string\n        \"\"\"\n        size = self.request.get('size', '')\n        if size == 'small':\n            return self.sample_type.getDefaultSmallSticker()\n        return self.sample_type.getDefaultLargeSticker()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of unique Identifier strings", "response": "def IdentifiersIndexer(instance):\n    \"\"\"Return a list of unique Identifier strings\n    This populates the Identifiers Keyword index, but with some\n    replacements to prevent the word-splitter etc from taking effect.\n    \"\"\"\n    identifiers = instance.Schema()['Identifiers'].get(instance)\n    return [safe_unicode(i['Identifier']) for i in identifiers]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn modified order of fields schemata.", "response": "def getOrder(self, schematas):\n        \"\"\"Return modified order of field schemats.\n        \"\"\"\n        schemata = self.context.schema['description'].schemata\n        fields = schematas[schemata]\n        fields.insert(fields.index('description') + 1,\n                      'Identifiers')\n        return schematas"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getVerificators(self):\n        verifiers = list()\n        actions = [\"verify\", \"multi_verify\"]\n        for event in wf.getReviewHistory(self):\n            if event['action'] in actions:\n                verifiers.append(event['actor'])\n        sorted(verifiers, reverse=True)\n        return verifiers", "response": "Returns the user ids of the users that verified this analysis"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the uncertainty value if the result falls within the specified ranges for the service from which this analysis was derived.", "response": "def getDefaultUncertainty(self, result=None):\n        \"\"\"Return the uncertainty value, if the result falls within\n        specified ranges for the service from which this analysis was derived.\n        \"\"\"\n\n        if result is None:\n            result = self.getResult()\n\n        uncertainties = self.getUncertainties()\n        if uncertainties:\n            try:\n                res = float(result)\n            except (TypeError, ValueError):\n                # if analysis result is not a number, then we assume in range\n                return None\n\n            for d in uncertainties:\n                _min = float(d['intercept_min'])\n                _max = float(d['intercept_max'])\n                if _min <= res and res <= _max:\n                    if str(d['errorvalue']).strip().endswith('%'):\n                        try:\n                            percvalue = float(d['errorvalue'].replace('%', ''))\n                        except ValueError:\n                            return None\n                        uncertainty = res / 100 * percvalue\n                    else:\n                        uncertainty = float(d['errorvalue'])\n\n                    return uncertainty\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the value of the Uncertainty field for this analysis and result. Returns None if the analysis is below detection limits and the Service has an Uncertainty field for this analysis and result. Returns the value from Schema s Uncertainty field if the option AllowManualUncertainty is set to True.", "response": "def getUncertainty(self, result=None):\n        \"\"\"Returns the uncertainty for this analysis and result.\n        Returns the value from Schema's Uncertainty field if the Service has\n        the option 'Allow manual uncertainty'. Otherwise, do a callback to\n        getDefaultUncertainty(). Returns None if no result specified and the\n        current result for this analysis is below or above detections limits.\n        \"\"\"\n        uncertainty = self.getField('Uncertainty').get(self)\n        if result is None and (self.isAboveUpperDetectionLimit() or\n                               self.isBelowLowerDetectionLimit()):\n            return None\n\n        if uncertainty and self.getAllowManualUncertainty() is True:\n            try:\n                uncertainty = float(uncertainty)\n                return uncertainty\n            except (TypeError, ValueError):\n                # if uncertainty is not a number, return default value\n                pass\n        return self.getDefaultUncertainty(result)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the uncertainty for this analysis.", "response": "def setUncertainty(self, unc):\n        \"\"\"Sets the uncertainty for this analysis. If the result is a\n        Detection Limit or the value is below LDL or upper UDL, sets the\n        uncertainty value to 0\n        \"\"\"\n        # Uncertainty calculation on DL\n        # https://jira.bikalabs.com/browse/LIMS-1808\n        if self.isAboveUpperDetectionLimit() or \\\n                self.isBelowLowerDetectionLimit():\n            self.getField('Uncertainty').set(self, None)\n        else:\n            self.getField('Uncertainty').set(self, unc)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the detection limit operand for this analysis.", "response": "def setDetectionLimitOperand(self, value):\n        \"\"\"Set detection limit operand for this analysis\n        Allowed detection limit operands are `<` and `>`.\n        \"\"\"\n        manual_dl = self.getAllowManualDetectionLimit()\n        selector = self.getDetectionLimitSelector()\n        if not manual_dl and not selector:\n            # Don't allow the user to set the limit operand if manual assignment\n            # is not allowed and selector is not visible\n            return\n\n        # Changing the detection limit operand has a side effect on the result\n        result = self.getResult()\n        if value in [LDL, UDL]:\n            # flush uncertainty\n            self.setUncertainty(\"\")\n\n            # If no previous result or user is not allowed to manually set the\n            # the detection limit, override the result with default LDL/UDL\n            has_result = api.is_floatable(result)\n            if not has_result or not manual_dl:\n                # set the result according to the system default UDL/LDL values\n                if value == LDL:\n                    result = self.getLowerDetectionLimit()\n                else:\n                    result = self.getUpperDetectionLimit()\n\n        else:\n            value = \"\"\n            # Restore the DetectionLimitSelector, cause maybe its visibility\n            # was changed because allow manual detection limit was enabled and\n            # the user set a result with \"<\" or \">\"\n            if manual_dl:\n                service = self.getAnalysisService()\n                selector = service.getDetectionLimitSelector()\n                self.setDetectionLimitSelector(selector)\n\n        # Set the result\n        self.getField(\"Result\").set(self, result)\n\n        # Set the detection limit to the field\n        self.getField(\"DetectionLimitOperand\").set(self, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getLowerDetectionLimit(self):\n        if self.isLowerDetectionLimit():\n            result = self.getResult()\n            try:\n                # in this case, the result itself is the LDL.\n                return float(result)\n            except (TypeError, ValueError):\n                logger.warn(\"The result for the analysis %s is a lower \"\n                            \"detection limit, but not floatable: '%s'. \"\n                            \"Returnig AS's default LDL.\" %\n                            (self.id, result))\n        return AbstractBaseAnalysis.getLowerDetectionLimit(self)", "response": "Returns the Lower Detection Limit that applies to this analysis in particular."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getUpperDetectionLimit(self):\n        if self.isUpperDetectionLimit():\n            result = self.getResult()\n            try:\n                # in this case, the result itself is the LDL.\n                return float(result)\n            except (TypeError, ValueError):\n                logger.warn(\"The result for the analysis %s is a lower \"\n                            \"detection limit, but not floatable: '%s'. \"\n                            \"Returnig AS's default LDL.\" %\n                            (self.id, result))\n        return AbstractBaseAnalysis.getUpperDetectionLimit(self)", "response": "Returns the Upper Detection Limit that applies to this analysis in particular."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef isBelowLowerDetectionLimit(self):\n        if self.isLowerDetectionLimit():\n            return True\n\n        result = self.getResult()\n        if result and str(result).strip().startswith(LDL):\n            return True\n\n        if api.is_floatable(result):\n            return api.to_float(result) < self.getLowerDetectionLimit()\n\n        return False", "response": "Returns True if the result is below the Lower Detection Limit or if the Lower Detection Limit has been manually set to True."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef isAboveUpperDetectionLimit(self):\n        if self.isUpperDetectionLimit():\n            return True\n\n        result = self.getResult()\n        if result and str(result).strip().startswith(UDL):\n            return True\n\n        if api.is_floatable(result):\n            return api.to_float(result) > self.getUpperDetectionLimit()\n\n        return False", "response": "Returns True if the result is above the Upper Detection Limit or if the Upper Detection Limit has been manually set to True."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate and set a value into the Result field taking into account the Detection Limits.", "response": "def setResult(self, value):\n        \"\"\"Validate and set a value into the Result field, taking into\n        account the Detection Limits.\n        :param value: is expected to be a string.\n        \"\"\"\n        # Always update ResultCapture date when this field is modified\n        self.setResultCaptureDate(DateTime())\n\n        # Ensure result integrity regards to None, empty and 0 values\n        val = str(\"\" if not value and value != 0 else value).strip()\n\n        # UDL/LDL directly entered in the results field\n        if val and val[0] in [LDL, UDL]:\n            # Result prefixed with LDL/UDL\n            oper = val[0]\n            # Strip off LDL/UDL from the result\n            val = val.replace(oper, \"\", 1)\n            # Check if the value is indeterminate / non-floatable\n            try:\n                val = float(val)\n            except (ValueError, TypeError):\n                val = value\n\n            # We dismiss the operand and the selector visibility unless the user\n            # is allowed to manually set the detection limit or the DL selector\n            # is visible.\n            allow_manual = self.getAllowManualDetectionLimit()\n            selector = self.getDetectionLimitSelector()\n            if allow_manual or selector:\n                # Ensure visibility of the detection limit selector\n                self.setDetectionLimitSelector(True)\n\n                # Set the detection limit operand\n                self.setDetectionLimitOperand(oper)\n\n                if not allow_manual:\n                    # Override value by default DL\n                    if oper == LDL:\n                        val = self.getLowerDetectionLimit()\n                    else:\n                        val = self.getUpperDetectionLimit()\n\n        # Set the result field\n        self.getField(\"Result\").set(self, val)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calculateResult(self, override=False, cascade=False):\n        if self.getResult() and override is False:\n            return False\n\n        calc = self.getCalculation()\n        if not calc:\n            return False\n\n        mapping = {}\n\n        # Interims' priority order (from low to high):\n        # Calculation < Analysis\n        interims = calc.getInterimFields() + self.getInterimFields()\n\n        # Add interims to mapping\n        for i in interims:\n            if 'keyword' not in i:\n                continue\n            # skip unset values\n            if i['value'] == '':\n                continue\n            try:\n                ivalue = float(i['value'])\n                mapping[i['keyword']] = ivalue\n            except (TypeError, ValueError):\n                # Interim not float, abort\n                return False\n\n        # Add dependencies results to mapping\n        dependencies = self.getDependencies()\n        for dependency in dependencies:\n            result = dependency.getResult()\n            if not result:\n                # Dependency without results found\n                if cascade:\n                    # Try to calculate the dependency result\n                    dependency.calculateResult(override, cascade)\n                    result = dependency.getResult()\n                else:\n                    return False\n            if result:\n                try:\n                    result = float(str(result))\n                    key = dependency.getKeyword()\n                    ldl = dependency.getLowerDetectionLimit()\n                    udl = dependency.getUpperDetectionLimit()\n                    bdl = dependency.isBelowLowerDetectionLimit()\n                    adl = dependency.isAboveUpperDetectionLimit()\n                    mapping[key] = result\n                    mapping['%s.%s' % (key, 'RESULT')] = result\n                    mapping['%s.%s' % (key, 'LDL')] = ldl\n                    mapping['%s.%s' % (key, 'UDL')] = udl\n                    mapping['%s.%s' % (key, 'BELOWLDL')] = int(bdl)\n                    mapping['%s.%s' % (key, 'ABOVEUDL')] = int(adl)\n                except (TypeError, ValueError):\n                    return False\n\n        # Calculate\n        formula = calc.getMinifiedFormula()\n        formula = formula.replace('[', '%(').replace(']', ')f')\n        try:\n            formula = eval(\"'%s'%%mapping\" % formula,\n                           {\"__builtins__\": None,\n                            'math': math,\n                            'context': self},\n                           {'mapping': mapping})\n            result = eval(formula, calc._getGlobals())\n        except TypeError:\n            self.setResult(\"NA\")\n            return True\n        except ZeroDivisionError:\n            self.setResult('0/0')\n            return True\n        except KeyError:\n            self.setResult(\"NA\")\n            return True\n        except ImportError:\n            self.setResult(\"NA\")\n            return True\n\n        self.setResult(str(result))\n        return True", "response": "Calculates the result for the current analysis. Returns True if the result is valid False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getVATAmount(self):\n        vat = self.getVAT()\n        price = self.getPrice()\n        return Decimal(price) * Decimal(vat) / 100", "response": "Compute the VAT amount without member discount."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getDuration(self):\n        starttime = self.getStartProcessDate()\n        if not starttime:\n            # The analysis is not yet ready to be processed\n            return 0\n        endtime = self.getDateVerified() or DateTime()\n\n        # Duration in minutes\n        duration = (endtime - starttime) * 24 * 60\n        return duration", "response": "Returns the time in minutes taken for this analysis."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the remaining time in minutes before the analysis reaches TAT", "response": "def getEarliness(self):\n        \"\"\"The remaining time in minutes for this analysis to be completed.\n        Returns zero if the analysis is neither 'ready to process' nor a\n        turnaround time is set.\n            earliness = duration - max_turnaround_time\n        The analysis is late if the earliness is negative\n        :return: the remaining time in minutes before the analysis reaches TAT\n        :rtype: int\n        \"\"\"\n        maxtime = self.getMaxTimeAllowed()\n        if not maxtime:\n            # No Turnaround time is set for this analysis\n            return 0\n        return api.to_minutes(**maxtime) - self.getDuration()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef isInstrumentAllowed(self, instrument):\n        if isinstance(instrument, str):\n            uid = instrument\n        else:\n            uid = instrument.UID()\n\n        return uid in self.getAllowedInstrumentUIDs()", "response": "Checks if the passed in instrument is allowed for this analysis."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef isMethodAllowed(self, method):\n        if isinstance(method, str):\n            uid = method\n        else:\n            uid = method.UID()\n\n        return uid in self.getAllowedMethodUIDs()", "response": "Checks if the analysis can follow the specified method"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getAllowedMethods(self):\n        service = self.getAnalysisService()\n        if not service:\n            return []\n\n        methods = []\n        if self.getManualEntryOfResults():\n            methods = service.getMethods()\n        if self.getInstrumentEntryOfResults():\n            for instrument in service.getInstruments():\n                methods.extend(instrument.getMethods())\n\n        return list(set(methods))", "response": "Returns the list of methods allowed for this analysis."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the allowed instruments for this analysis.", "response": "def getAllowedInstruments(self):\n        \"\"\"Returns the allowed instruments for this analysis, either if the\n        instrument was assigned directly (by using \"Allows instrument entry of\n        results\") or indirectly via Method (by using \"Allows manual entry of\n        results\") in Analysis Service edit view.\n        :return: A list of instruments allowed for this Analysis\n        :rtype: list of instruments\n        \"\"\"\n        service = self.getAnalysisService()\n        if not service:\n            return []\n\n        instruments = []\n        if self.getInstrumentEntryOfResults():\n            instruments = service.getInstruments()\n        if self.getManualEntryOfResults():\n            for meth in self.getAllowedMethods():\n                instruments += meth.getInstruments()\n\n        return list(set(instruments))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getExponentialFormatPrecision(self, result=None):\n        if not result or self.getPrecisionFromUncertainty() is False:\n            return self._getExponentialFormatPrecision()\n        else:\n            uncertainty = self.getUncertainty(result)\n            if uncertainty is None:\n                return self._getExponentialFormatPrecision()\n\n            try:\n                float(result)\n            except ValueError:\n                # if analysis result is not a number, then we assume in range\n                return self._getExponentialFormatPrecision()\n\n            return get_significant_digits(uncertainty)", "response": "Returns the precision for the Analysis Service and result."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getFormattedResult(self, specs=None, decimalmark='.', sciformat=1,\n                           html=True):\n        \"\"\"Formatted result:\n        1. If the result is a detection limit, returns '< LDL' or '> UDL'\n        2. Print ResultText of matching ResultOptions\n        3. If the result is not floatable, return it without being formatted\n        4. If the analysis specs has hidemin or hidemax enabled and the\n           result is out of range, render result as '<min' or '>max'\n        5. If the result is below Lower Detection Limit, show '<LDL'\n        6. If the result is above Upper Detecion Limit, show '>UDL'\n        7. Otherwise, render numerical value\n        :param specs: Optional result specifications, a dictionary as follows:\n            {'min': <min_val>,\n             'max': <max_val>,\n             'error': <error>,\n             'hidemin': <hidemin_val>,\n             'hidemax': <hidemax_val>}\n        :param decimalmark: The string to be used as a decimal separator.\n            default is '.'\n        :param sciformat: 1. The sci notation has to be formatted as aE^+b\n                          2. The sci notation has to be formatted as a\u00b710^b\n                          3. As 2, but with super html entity for exp\n                          4. The sci notation has to be formatted as a\u00b710^b\n                          5. As 4, but with super html entity for exp\n                          By default 1\n        :param html: if true, returns an string with the special characters\n            escaped: e.g: '<' and '>' (LDL and UDL for results like < 23.4).\n        \"\"\"\n        result = self.getResult()\n\n        # 1. The result is a detection limit, return '< LDL' or '> UDL'\n        dl = self.getDetectionLimitOperand()\n        if dl:\n            try:\n                res = float(result)  # required, check if floatable\n                res = drop_trailing_zeros_decimal(res)\n                fdm = formatDecimalMark(res, decimalmark)\n                hdl = cgi.escape(dl) if html else dl\n                return '%s %s' % (hdl, fdm)\n            except (TypeError, ValueError):\n                logger.warn(\n                    \"The result for the analysis %s is a detection limit, \"\n                    \"but not floatable: %s\" % (self.id, result))\n                return formatDecimalMark(result, decimalmark=decimalmark)\n\n        choices = self.getResultOptions()\n\n        # 2. Print ResultText of matching ResulOptions\n        match = [x['ResultText'] for x in choices\n                 if str(x['ResultValue']) == str(result)]\n        if match:\n            return match[0]\n\n        # 3. If the result is not floatable, return it without being formatted\n        try:\n            result = float(result)\n        except (TypeError, ValueError):\n            return formatDecimalMark(result, decimalmark=decimalmark)\n\n        # 4. If the analysis specs has enabled hidemin or hidemax and the\n        #    result is out of range, render result as '<min' or '>max'\n        specs = specs if specs else self.getResultsRange()\n        hidemin = specs.get('hidemin', '')\n        hidemax = specs.get('hidemax', '')\n        try:\n            belowmin = hidemin and result < float(hidemin) or False\n        except (TypeError, ValueError):\n            belowmin = False\n        try:\n            abovemax = hidemax and result > float(hidemax) or False\n        except (TypeError, ValueError):\n            abovemax = False\n\n        # 4.1. If result is below min and hidemin enabled, return '<min'\n        if belowmin:\n            fdm = formatDecimalMark('< %s' % hidemin, decimalmark)\n            return fdm.replace('< ', '&lt; ', 1) if html else fdm\n\n        # 4.2. If result is above max and hidemax enabled, return '>max'\n        if abovemax:\n            fdm = formatDecimalMark('> %s' % hidemax, decimalmark)\n            return fdm.replace('> ', '&gt; ', 1) if html else fdm\n\n        # Below Lower Detection Limit (LDL)?\n        ldl = self.getLowerDetectionLimit()\n        if result < ldl:\n            # LDL must not be formatted according to precision, etc.\n            # Drop trailing zeros from decimal\n            ldl = drop_trailing_zeros_decimal(ldl)\n            fdm = formatDecimalMark('< %s' % ldl, decimalmark)\n            return fdm.replace('< ', '&lt; ', 1) if html else fdm\n\n        # Above Upper Detection Limit (UDL)?\n        udl = self.getUpperDetectionLimit()\n        if result > udl:\n            # UDL must not be formatted according to precision, etc.\n            # Drop trailing zeros from decimal\n            udl = drop_trailing_zeros_decimal(udl)\n            fdm = formatDecimalMark('> %s' % udl, decimalmark)\n            return fdm.replace('> ', '&gt; ', 1) if html else fdm\n\n        # Render numerical values\n        return format_numeric_result(self, self.getResult(),\n                                     decimalmark=decimalmark,\n                                     sciformat=sciformat)", "response": "Returns a string that represents the result of the analysis."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the precision for the Analysis.", "response": "def getPrecision(self, result=None):\n        \"\"\"Returns the precision for the Analysis.\n\n        - If ManualUncertainty is set, calculates the precision of the result\n          in accordance with the manual uncertainty set.\n\n        - If Calculate Precision from Uncertainty is set in Analysis Service,\n          calculates the precision in accordance with the uncertainty infered\n          from uncertainties ranges.\n\n        - If neither Manual Uncertainty nor Calculate Precision from\n          Uncertainty are set, returns the precision from the Analysis Service\n\n        - If you have a number with zero uncertainty: If you roll a pair of\n        dice and observe five spots, the number of spots is 5. This is a raw\n        data point, with no uncertainty whatsoever. So just write down the\n        number. Similarly, the number of centimeters per inch is 2.54,\n        by definition, with no uncertainty whatsoever. Again: just write\n        down the number.\n\n        Further information at AbstractBaseAnalysis.getPrecision()\n        \"\"\"\n        allow_manual = self.getAllowManualUncertainty()\n        precision_unc = self.getPrecisionFromUncertainty()\n        if allow_manual or precision_unc:\n            uncertainty = self.getUncertainty(result)\n            if uncertainty is None:\n                return self.getField('Precision').get(self)\n            if uncertainty == 0 and result is None:\n                return self.getField('Precision').get(self)\n            if uncertainty == 0:\n                strres = str(result)\n                numdecimals = strres[::-1].find('.')\n                return numdecimals\n            return get_significant_digits(uncertainty)\n        return self.getField('Precision').get(self)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the stored Analyst or the user who submitted the result", "response": "def getAnalyst(self):\n        \"\"\"Returns the stored Analyst or the user who submitted the result\n        \"\"\"\n        analyst = self.getField(\"Analyst\").get(self)\n        if not analyst:\n            analyst = self.getSubmittedBy()\n        return analyst or \"\""}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getAnalystName(self):\n        analyst = self.getAnalyst()\n        if not analyst:\n            return \"\"\n        user = api.get_user(analyst.strip())\n        return user and user.getProperty(\"fullname\") or analyst", "response": "Returns the name of the currently assigned analyst"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getWorksheet(self):\n        worksheet = self.getBackReferences('WorksheetAnalysis')\n        if not worksheet:\n            return None\n        if len(worksheet) > 1:\n            logger.error(\n                \"Analysis %s is assigned to more than one worksheet.\"\n                % self.getId())\n        return worksheet[0]", "response": "Returns the Worksheet to which this analysis belongs to or None if there is no worksheet."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse to populate metadata so that we don t need full objects of the attachments.", "response": "def getAttachmentUIDs(self):\n        \"\"\"Used to populate metadata, so that we don't need full objects of\n        analyses when working with their attachments.\n        \"\"\"\n        attachments = self.getAttachment()\n        uids = [att.UID() for att in attachments]\n        return uids"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_duplicates(self, ws):\n        for analysis in ws.objectValues():\n            if IDuplicateAnalysis.providedBy(analysis) \\\n                    and analysis.getAnalysis().UID() == self.UID():\n                ws.removeAnalysis(analysis)", "response": "This function is responsible for deleting duplicate analyses from the worksheet."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets a value to an interim of this analysis", "response": "def setInterimValue(self, keyword, value):\n        \"\"\"Sets a value to an interim of this analysis\n        :param keyword: the keyword of the interim\n        :param value: the value for the interim\n        \"\"\"\n        # Ensure result integrity regards to None, empty and 0 values\n        val = str('' if not value and value != 0 else value).strip()\n        interims = self.getInterimFields()\n        for interim in interims:\n            if interim['keyword'] == keyword:\n                interim['value'] = val\n                self.setInterimFields(interims)\n                return\n\n        logger.warning(\"Interim '{}' for analysis '{}' not found\"\n                       .format(keyword, self.getKeyword()))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the value of an interim of this analysis AttributeNames", "response": "def getInterimValue(self, keyword):\n        \"\"\"Returns the value of an interim of this analysis\n        \"\"\"\n        interims = filter(lambda item: item[\"keyword\"] == keyword,\n                          self.getInterimFields())\n        if not interims:\n            logger.warning(\"Interim '{}' for analysis '{}' not found\"\n                           .format(keyword, self.getKeyword()))\n            return None\n        if len(interims) > 1:\n            logger.error(\"More than one interim '{}' found for '{}'\"\n                         .format(keyword, self.getKeyword()))\n            return None\n        return interims[0].get('value', '')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the current user has granted access to the worksheet. Returns False if the user has no access. Returns True if the user has granted access to the worksheet False otherwise", "response": "def checkUserAccess(worksheet, request, redirect=True):\n    \"\"\" Checks if the current user has granted access to the worksheet.\n        If the user is an analyst without LabManager, LabClerk and\n        RegulatoryInspector roles and the option 'Allow analysts\n        only to access to the Worksheets on which they are assigned' is\n        ticked and the above condition is true, it will redirect to\n        the main Worksheets view.\n        Returns False if the user has no access, otherwise returns True\n    \"\"\"\n    # Deny access to foreign analysts\n    allowed = worksheet.checkUserAccess()\n    if allowed == False and redirect == True:\n        msg =  _('You do not have sufficient privileges to view '\n                 'the worksheet ${worksheet_title}.',\n                 mapping={\"worksheet_title\": worksheet.Title()})\n        worksheet.plone_utils.addPortalMessage(msg, 'warning')\n        # Redirect to WS list\n        portal = getToolByName(worksheet, 'portal_url').getPortalObject()\n        destination_url = portal.absolute_url() + \"/worksheets\"\n        request.response.redirect(destination_url)\n\n    return allowed"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the current user has granted access to the worksheet and redirects to the manage_results view.", "response": "def checkUserManage(worksheet, request, redirect=True):\n    \"\"\" Checks if the current user has granted access to the worksheet\n        and if has also privileges for managing it. If the user has no\n        granted access and redirect's value is True, redirects to\n        /manage_results view. Otherwise, does nothing\n    \"\"\"\n    allowed = worksheet.checkUserManage()\n    if allowed == False and redirect == True:\n        # Redirect to /manage_results view\n        destination_url = worksheet.absolute_url() + \"/manage_results\"\n        request.response.redirect(destination_url)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef showRejectionMessage(worksheet):\n    if hasattr(worksheet, 'replaced_by'):\n        uc = getToolByName(worksheet, 'uid_catalog')\n        uid = getattr(worksheet, 'replaced_by')\n        _ws = uc(UID=uid)[0].getObject()\n        msg = _(\"This worksheet has been rejected.  The replacement worksheet is ${ws_id}\",\n                mapping={'ws_id':_ws.getId()})\n        worksheet.plone_utils.addPortalMessage(msg)\n    if hasattr(worksheet, 'replaces_rejected_worksheet'):\n        uc = getToolByName(worksheet, 'uid_catalog')\n        uid = getattr(worksheet, 'replaces_rejected_worksheet')\n        _ws = uc(UID=uid)[0].getObject()\n        msg = _(\"This worksheet has been created to replace the rejected \"\n                \"worksheet at ${ws_id}\",\n                mapping={'ws_id':_ws.getId()})\n        worksheet.plone_utils.addPortalMessage(msg)", "response": "Shows a portalMessage for the rejected worksheet."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntries to return a DateTime. DateTime object from a string or datetime. datetime object.", "response": "def get_date(context, value):\n    \"\"\"Tries to return a DateTime.DateTime object\n    \"\"\"\n    if not value:\n        return None\n    if isinstance(value, DateTime):\n        return value\n    if isinstance(value, datetime):\n        return dt2DT(value)\n    if not isinstance(value, basestring):\n        return None\n\n    def try_parse(date_string, format):\n        if not format:\n            return None\n        try:\n            struct_time = strptime(date_string, format)\n            return datetime(*struct_time[:6])\n        except ValueError:\n            pass\n        return None\n\n    def get_locale_format(key, context):\n        format = context.translate(key, domain=\"senaite.core\", mapping={})\n        # TODO: Is this replacement below strictly necessary?\n        return format.replace(r\"${\", '%').replace('}', '')\n\n    # Try with prioritized formats\n    formats = [get_locale_format(\"date_format_long\", context),\n               get_locale_format(\"date_format_short\", context),\n               \"%Y-%m-%d %H:%M\", \"%Y-%m-%d\", \"%Y-%m-%d %H:%M:%S\"]\n    for pri_format in formats:\n        val = try_parse(value, pri_format)\n        if not val:\n            continue\n        val = dt2DT(val)\n        if val.timezoneNaive():\n            # Use local timezone for tz naive strings\n            # see http://dev.plone.org/plone/ticket/10141\n            zone = val.localZone(safelocaltime(val.timeTime()))\n            parts = val.parts()[:-1] + (zone,)\n            val = DateTime(*parts)\n        return val\n\n    logger.warn(\"Unable to convert {} to datetime\".format(value))\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef python_date_format(self, long_format=None, time_only=False):\n        # get msgid\n        msgid = long_format and 'date_format_long' or 'date_format_short'\n        if time_only:\n            msgid = 'time_format'\n        # get the formatstring\n        formatstring = translate(msgid, domain=\"senaite.core\",\n                                 context=self.request)\n\n        if formatstring is None or formatstring.startswith(\n                'date_') or formatstring.startswith('time_'):\n            self.logger.error(\"bika/%s/%s could not be translated\" %\n                              (self.request.get('LANGUAGE'), msgid))\n            # msg catalog was not able to translate this msgids\n            # use default setting\n            properties = getToolByName(self.context,\n                                       'portal_properties').site_properties\n            if long_format:\n                format = properties.localLongTimeFormat\n            else:\n                if time_only:\n                    format = properties.localTimeOnlyFormat\n                else:\n                    format = properties.localTimeFormat\n            return format\n        return formatstring.replace(r\"${\", '%').replace('}', '')", "response": "This function converts a bika domain date format msgstrs to a Python date format string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the values of the dict values are correct.", "response": "def _check_set_values(instance, dic):\n    \"\"\"\n    This function checks if the dict values are correct.\n    :instance: the object instance. Used for querying\n    :dic: is a dictionary with the following format:\n    {'actions': [{'act_row_idx': 0,\n                   'action': 'repeat',\n                   'an_result_id': 'rep-1',\n                   'analyst': '',\n                   'otherWS': 'current',\n                   'setresultdiscrete': '',\n                   'setresulton': 'original',\n                   'setresultvalue': '',\n                   'worksheettemplate': ''}],\n      'conditions': [{'analysisservice': '52853cf7d5114b5aa8c159afad2f3da1',\n                      'and_or': 'no',\n                      'cond_row_idx': 0,\n                      'discreteresult': '',\n                      'range0': '11',\n                      'range1': '12'}],\n      'mother_service_uid': '52853cf7d5114b5aa8c159afad2f3da1',\n      'rulenumber': '0',\n      'trigger': 'submit'},\n    These are the checking rules:\n        :range0/range1: string or number.\n    They are the numeric range within the action will be\n    carried on. It is needed to keep it as None or '' if the discreteresult\n    is going to be used instead.\n        :discreteresult: string\n    If discreteresult is not Null, ranges have to be Null.\n        :trigger: string.\n    So far there are only two options: 'submit'/'verify'. They are defined\n    in browser/widgets/reflexrulewidget.py/ReflexRuleWidget/getTriggerVoc.\n        :analysisservice: it is the uid of an analysis service\n        :actions: It is a list of dictionaries with the following format:\n    [{'action':'<action_name>', 'act_row_idx':'X',\n                'otherWS':Bool, 'analyst': '<analyst_id>'},\n              {'action':'<action_name>', 'act_row_idx':'X',\n                'otherWS':Bool, 'analyst': '<analyst_id>'},\n        ]\n        :'repetition_max': integer or string representing an integer.\n    <action_name> options are found in\n    browser/widgets/reflexrulewidget.py/ReflexRuleWidget/getActionVoc\n    so far.\n    \"\"\"\n    uc = getToolByName(instance, 'uid_catalog')\n    rulenumber = dic.get('rulenumber', '0')\n    if rulenumber and not(isnumber(rulenumber)):\n        logger.warn('The range must be a number. Now its value is: '\n                    '%s' % (rulenumber))\n        return False\n    trigger = dic.get('trigger', 'submit')\n    if trigger not in ['submit', 'verify']:\n        logger.warn('Only available triggers are \"verify\" or \"submit\". '\n                    '%s has been introduced.' % (trigger))\n        return False\n    mother_service_uid = dic.get('mother_service_uid', '')\n    as_brain = uc(UID=mother_service_uid)\n    if not as_brain:\n        logger.warn(\n            'Not correct analysis service with UID. %s' % (mother_service_uid))\n        return False\n    # Checking the conditions\n    conditions = dic.get('conditions', [])\n    if not conditions or not _check_conditions(instance, conditions):\n        return False\n    # Checking the actions\n    actions = dic.get('actions', [])\n    if not actions or not _check_actions(instance, actions):\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the reflexrule field of the reflex rule.", "response": "def set(self, instance, rules_list, **kwargs):\n        \"\"\"\n        Set the reflexrule field.\n        :rules_list: is a list of dictionaries with the following format:\n        [{'actions': [{'act_row_idx': 0,\n                       'action': 'repeat',\n                       'an_result_id': 'rep-1',\n                       'analyst': '',\n                       'otherWS': 'current',\n                       'setresultdiscrete': '',\n                       'setresulton': 'original',\n                       'setresultvalue': '',\n                       'worksheettemplate': ''}],\n          'conditions': [{'analysisservice': '52853cf7d5114b5aa8c159afad2f3da1',\n                          'and_or': 'no',\n                          'cond_row_idx': 0,\n                          'discreteresult': '',\n                          'range0': '11',\n                          'range1': '12'}],\n          'mother_service_uid': '52853cf7d5114b5aa8c159afad2f3da1',\n          'rulenumber': '0',\n          'trigger': 'submit'},\n         {'actions': [{'act_row_idx': 0,\n                       'action': 'repeat',\n                       'an_result_id': 'rep-2',\n                       'analyst': '',\n                       'otherWS': 'current',\n                       'setresultdiscrete': '',\n                       'setresulton': 'original',\n                       'setresultvalue': '',\n                       'worksheettemplate': ''},\n                      {'act_row_idx': 1,\n                       'action': 'repeat',\n                       'an_result_id': 'rep-4',\n                       'analyst': 'analyst1',\n                       'otherWS': 'to_another',\n                       'setresultdiscrete': '',\n                       'setresulton': 'original',\n                       'setresultvalue': '',\n                       'worksheettemplate': '70d48adfb34c4231a145f76a858e94cf'},\n                      {'act_row_idx': 2,\n                       'action': 'repeat',\n                       'an_result_id': 'rep-5',\n                       'analyst': '',\n                       'otherWS': 'create_another',\n                       'setresultdiscrete': '',\n                       'setresulton': 'original',\n                       'setresultvalue': '',\n                       'worksheettemplate': ''},\n                      {'act_row_idx': 3,\n                       'action': 'repeat',\n                       'an_result_id': 'rep-6',\n                       'analyst': '',\n                       'otherWS': 'no_ws',\n                       'setresultdiscrete': '',\n                       'setresulton': 'original',\n                       'setresultvalue': '',\n                       'worksheettemplate': ''}],\n          'conditions': [{'analysisservice': 'rep-1',\n                          'and_or': 'no',\n                          'cond_row_idx': 0,\n                          'discreteresult': '',\n                          'range0': '12',\n                          'range1': '12'}],\n          'mother_service_uid': '52853cf7d5114b5aa8c159afad2f3da1',\n          'rulenumber': '1',\n          'trigger': 'submit'},\n         {'actions': [{'act_row_idx': 0,\n                       'action': 'repeat',\n                       'an_result_id': 'rep-3',\n                       'analyst': '',\n                       'otherWS': 'current',\n                       'setresultdiscrete': '',\n                       'setresulton': 'original',\n                       'setresultvalue': '',\n                       'worksheettemplate': ''}],\n          'conditions': [{'analysisservice': 'rep-1',\n                          'and_or': 'and',\n                          'cond_row_idx': 0,\n                          'discreteresult': '',\n                          'range0': '12',\n                          'range1': '12'},\n                         {'analysisservice': 'rep-2',\n                          'and_or': 'or',\n                          'cond_row_idx': 1,\n                          'discreteresult': '',\n                          'range0': '115',\n                          'range1': '115'},\n                         {'analysisservice': 'rep-1',\n                          'and_or': 'no',\n                          'cond_row_idx': 2,\n                          'discreteresult': '',\n                          'range0': '14',\n                          'range1': '14'}],\n          'mother_service_uid': '52853cf7d5114b5aa8c159afad2f3da1',\n          'rulenumber': '2',\n          'trigger': 'submit'}]\n        This list of dictionaries is how the system will store the reflexrule\n        field info. This dictionaries must be in sync with the\n        browser/widgets/reflexrulewidget.py/process_form() dictionaries format.\n        \"\"\"\n        for d in rules_list:\n            # Checking if all dictionary items are correct\n            if not _check_set_values(instance, d):\n                RecordsField.set(self, instance, [], **kwargs)\n        RecordsField.set(self, instance, rules_list, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getVATAmount(self):\n        price, vat = self.getPrice(), self.getVAT()\n        return float(price) * (float(vat) / 100)", "response": "Compute VAT Amount from the Price and system configured VAT"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getDiscountedPrice(self):\n        price = self.getPrice()\n        price = price and price or 0\n        discount = self.bika_setup.getMemberDiscount()\n        discount = discount and discount or 0\n        return float(price) - (float(price) * float(discount)) / 100", "response": "Compute discounted price excl. VAT\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute discounted bulk discount excl. VAT", "response": "def getDiscountedBulkPrice(self):\n        \"\"\"Compute discounted bulk discount excl. VAT\n        \"\"\"\n        price = self.getBulkPrice()\n        price = price and price or 0\n        discount = self.bika_setup.getMemberDiscount()\n        discount = discount and discount or 0\n        return float(price) - (float(price) * float(discount)) / 100"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getTotalPrice(self):\n        price = self.getPrice()\n        vat = self.getVAT()\n        price = price and price or 0\n        vat = vat and vat or 0\n        return float(price) + (float(price) * float(vat)) / 100", "response": "Compute total price including VAT"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getTotalBulkPrice(self):\n        price = self.getBulkPrice()\n        vat = self.getVAT()\n        price = price and price or 0\n        vat = vat and vat or 0\n        return float(price) + (float(price) * float(vat)) / 100", "response": "Compute total bulk price of the current asset"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getTotalDiscountedPrice(self):\n        price = self.getDiscountedPrice()\n        vat = self.getVAT()\n        price = price and price or 0\n        vat = vat and vat or 0\n        return float(price) + (float(price) * float(vat)) / 100", "response": "Compute total discounted price of the current asset"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getTotalDiscountedBulkPrice(self):\n        price = self.getDiscountedCorporatePrice()\n        vat = self.getVAT()\n        price = price and price or 0\n        vat = vat and vat or 0\n        return float(price) + (float(price) * float(vat)) / 100", "response": "Compute total discounted corporate bulk price\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getAnalysisCategories(self):\n        bsc = getToolByName(self, 'bika_setup_catalog')\n        cats = bsc(portal_type='AnalysisCategory', is_active=True)\n        items = [(o.UID, o.Title) for o in cats]\n        o = self.getCategory()\n        if o and o.UID() not in [i[0] for i in items]:\n            items.append((o.UID(), o.Title()))\n        items.sort(lambda x, y: cmp(x[1], y[1]))\n        return DisplayList(list(items))", "response": "A vocabulary listing available and activated categories."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getLowerDetectionLimit(self):\n        ldl = self.getField('LowerDetectionLimit').get(self)\n        try:\n            return float(ldl)\n        except ValueError:\n            return 0", "response": "Returns the Lower Detection Limit for this service as a floatable\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the Upper Detection Limit for this service as a floatable", "response": "def getUpperDetectionLimit(self):\n        \"\"\"Returns the Upper Detection Limit for this service as a floatable\n        \"\"\"\n        udl = self.getField('UpperDetectionLimit').get(self)\n        try:\n            return float(udl)\n        except ValueError:\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning if the user that submitted a result for this analysis must have self - verification enabled or false otherwise", "response": "def isSelfVerificationEnabled(self):\n        \"\"\"Returns if the user that submitted a result for this analysis must\n        also be able to verify the result\n        :returns: true or false\n        \"\"\"\n        bsve = self.bika_setup.getSelfVerificationEnabled()\n        vs = self.getSelfVerification()\n        return bsve if vs == -1 else vs == 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _getSelfVerificationVocabulary(self):\n        bsve = self.bika_setup.getSelfVerificationEnabled()\n        bsve = _('Yes') if bsve else _('No')\n        bsval = \"%s (%s)\" % (_(\"System default\"), bsve)\n        items = [(-1, bsval), (0, _('No')), (1, _('Yes'))]\n        return IntDisplayList(list(items))", "response": "Returns a DisplayList with the available options for the\n        self - verification list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getNumberOfRequiredVerifications(self):\n        num = self.getField('NumberOfRequiredVerifications').get(self)\n        if num < 1:\n            return self.bika_setup.getNumberOfRequiredVerifications()\n        return num", "response": "Returns the number of required verifications a test for thisCOOKIE is transitioned to verified state"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _getNumberOfRequiredVerificationsVocabulary(self):\n        bsve = self.bika_setup.getNumberOfRequiredVerifications()\n        bsval = \"%s (%s)\" % (_(\"System default\"), str(bsve))\n        items = [(-1, bsval), (1, '1'), (2, '2'), (3, '3'), (4, '4')]\n        return IntDisplayList(list(items))", "response": "Returns a DisplayList with the available options for the\n        multi - verification list"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the maximum turnaround time for this analysis.", "response": "def getMaxTimeAllowed(self):\n        \"\"\"Returns the maximum turnaround time for this analysis. If no TAT is\n        set for this particular analysis, it returns the value set at setup\n        return: a dictionary with the keys \"days\", \"hours\" and \"minutes\"\n        \"\"\"\n        tat = self.Schema().getField(\"MaxTimeAllowed\").get(self)\n        return tat or self.bika_setup.getDefaultTurnaroundTime()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getSiblings(self, retracted=False):\n        request = self.getRequest()\n        if not request:\n            return []\n\n        siblings = []\n        retracted_states = [STATE_RETRACTED, STATE_REJECTED]\n        for sibling in request.getAnalyses(full_objects=True):\n            if api.get_uid(sibling) == self.UID():\n                # Exclude me from the list\n                continue\n\n            if not retracted:\n                if api.get_workflow_status_of(sibling) in retracted_states:\n                    # Exclude retracted analyses\n                    continue\n\n            siblings.append(sibling)\n\n        return siblings", "response": "Returns the list of siblings for this analysis."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_interims_data(self):\n        form = self.request.form\n        if 'item_data' not in form:\n            return {}\n\n        item_data = {}\n        if type(form['item_data']) == list:\n            for i_d in form['item_data']:\n                for i, d in json.loads(i_d).items():\n                    item_data[i] = d\n            return item_data\n\n        return json.loads(form['item_data'])", "response": "Returns a dictionary with the interims data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all services which depend on this service", "response": "def get_calculation_dependants_for(service):\n    \"\"\"Collect all services which depend on this service\n\n    :param service: Analysis Service Object/ZCatalog Brain\n    :returns: List of services that depend on this service\n    \"\"\"\n\n    def calc_dependants_gen(service, collector=None):\n        \"\"\"Generator for recursive resolution of dependant sevices.\n        \"\"\"\n\n        # The UID of the service\n        service_uid = api.get_uid(service)\n\n        # maintain an internal dependency mapping\n        if collector is None:\n            collector = {}\n\n        # Stop iteration if we processed this service already\n        if service_uid in collector:\n            raise StopIteration\n\n        # Get the dependant calculations of the service\n        # (calculations that use the service in their formula).\n        calc_uids = get_backreferences(\n            service, relationship=\"CalculationDependentServices\")\n\n        for calc_uid in calc_uids:\n            # Get the calculation object\n            calc = api.get_object_by_uid(calc_uid)\n\n            # Get the Analysis Services which have this calculation assigned\n            dep_service_uids = get_backreferences(\n                calc, relationship='AnalysisServiceCalculation')\n\n            for dep_service_uid in dep_service_uids:\n                dep_service = api.get_object_by_uid(dep_service_uid)\n\n                # remember the dependent service\n                collector[dep_service_uid] = dep_service\n\n                # yield the dependent service\n                yield dep_service\n\n                # check the dependants of the dependant services\n                for ddep_service in calc_dependants_gen(\n                        dep_service, collector=collector):\n                    yield ddep_service\n\n    dependants = {}\n    service = api.get_object(service)\n\n    for dep_service in calc_dependants_gen(service):\n        # Skip the initial (requested) service\n        if dep_service == service:\n            continue\n        uid = api.get_uid(dep_service)\n        dependants[uid] = dep_service\n\n    return dependants"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the dependency set for the given service.", "response": "def get_calculation_dependencies_for(service):\n    \"\"\"Calculation dependencies of this service and the calculation of each\n    dependent service (recursively).\n    \"\"\"\n\n    def calc_dependencies_gen(service, collector=None):\n        \"\"\"Generator for recursive dependency resolution.\n        \"\"\"\n\n        # The UID of the service\n        service_uid = api.get_uid(service)\n\n        # maintain an internal dependency mapping\n        if collector is None:\n            collector = {}\n\n        # Stop iteration if we processed this service already\n        if service_uid in collector:\n            raise StopIteration\n\n        # Get the calculation of the service.\n        # The calculation comes either from an assigned method or the user\n        # has set a calculation manually (see content/analysisservice.py).\n        calculation = service.getCalculation()\n\n        # Stop iteration if there is no calculation\n        if not calculation:\n            raise StopIteration\n\n        # The services used in this calculation.\n        # These are the actual dependencies of the used formula.\n        dep_services = calculation.getDependentServices()\n        for dep_service in dep_services:\n            # get the UID of the dependent service\n            dep_service_uid = api.get_uid(dep_service)\n\n            # remember the dependent service\n            collector[dep_service_uid] = dep_service\n\n            # yield the dependent service\n            yield dep_service\n\n            # check the dependencies of the dependent services\n            for ddep_service in calc_dependencies_gen(dep_service,\n                                                      collector=collector):\n                yield ddep_service\n\n    dependencies = {}\n    for dep_service in calc_dependencies_gen(service):\n        # Skip the initial (requested) service\n        if dep_service == service:\n            continue\n        uid = api.get_uid(dep_service)\n        dependencies[uid] = dep_service\n\n    return dependencies"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_service_dependencies_for(service):\n\n    dependants = get_calculation_dependants_for(service)\n    dependencies = get_calculation_dependencies_for(service)\n\n    return {\n        \"dependencies\": dependencies.values(),\n        \"dependants\": dependants.values(),\n    }", "response": "Calculate the dependencies for the given service."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_failed_instruments(self):\n        bsc = api.get_tool(\"bika_setup_catalog\")\n        insts = bsc(portal_type=\"Instrument\", is_active=True)\n        for i in insts:\n            i = i.getObject()\n            instr = {\n                'uid': i.UID(),\n                'title': i.Title(),\n            }\n            if i.isValidationInProgress():\n                instr['link'] = '<a href=\"%s/validations\">%s</a>' % (\n                    i.absolute_url(), i.Title()\n                )\n                self.nr_failed += 1\n                self.failed['validation'].append(instr)\n            elif i.isCalibrationInProgress():\n                instr['link'] = '<a href=\"%s/calibrations\">%s</a>' % (\n                    i.absolute_url(), i.Title()\n                )\n                self.nr_failed += 1\n                self.failed['calibration'].append(instr)\n            elif i.isOutOfDate():\n                instr['link'] = '<a href=\"%s/certifications\">%s</a>' % (\n                    i.absolute_url(), i.Title()\n                )\n                self.nr_failed += 1\n                self.failed['out-of-date'].append(instr)\n            elif not i.isQCValid():\n                instr['link'] = '<a href=\"%s/referenceanalyses\">%s</a>' % (\n                    i.absolute_url(), i.Title()\n                )\n                self.nr_failed += 1\n                self.failed['qc-fail'].append(instr)\n            elif i.getDisposeUntilNextCalibrationTest():\n                instr['link'] = '<a href=\"%s/referenceanalyses\">%s</a>' % (\n                    i.absolute_url(), i.Title()\n                )\n                self.nr_failed += 1\n                self.failed['next-test'].append(instr)", "response": "Return a dictionary of all failed instruments"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef available(self):\n        url = api.get_url(self.context)\n        # render on the portal root\n        if self.context == api.get_portal():\n            return True\n        # render on the front-page\n        if url.endswith(\"/front-page\"):\n            return True\n        # render for manage_results\n        if url.endswith(\"/manage_results\"):\n            return True\n        return False", "response": "Control availability of the viewlet\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_resultsline(self, line):\n\n        splitted = [token.strip() for token in line.split(',')]\n        if self._end_header:\n            resid = splitted[0]\n        rawdict = {'DefaultResult': 'Soln Conc'}\n        rawdict.update(dict(zip(self._resultsheader, splitted)))\n        date_string = \"{Date} {Time}\".format(**rawdict)\n        date_time = DateTime(date_string)\n        rawdict['DateTime'] = date_time\n        element = rawdict.get(\"Element\", \"\").replace(\" \", \"\").replace(\".\", \"\")\n\n        # Set DefaultResult to 0.0 if result is \"0\" or \"--\" or '' or 'ND'\n        result = rawdict[rawdict['DefaultResult']]\n        column_name = rawdict['DefaultResult']\n        result = self.get_result(column_name, result, line)\n        rawdict[rawdict['DefaultResult']] = result\n        #\n\n        val = re.sub(r\"\\W\", \"\", element)\n        self._addRawResult(resid, values={val: rawdict}, override=False)\n\n        self.log(\n            \"End of file reached successfully: ${total_objects} objects, \"\n            \"${total_analyses} analyses, ${total_results} results\",\n            mapping={\"total_objects\": self.getObjectsTotalCount(),\n                     \"total_analyses\": self.getAnalysesTotalCount(),\n                     \"total_results\": self.getResultsTotalCount()}\n        )", "response": "Parse the results line and add it to the internal dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_widget_label_for(self, fieldname, default=None):\n        widget = self.get_widget_for(fieldname)\n        if widget is None:\n            return default\n        return widget.label", "response": "Lookup the widget of the field and return the label"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_localized_time(self, date, **kw):\n        date = api.to_date(date, default=None)\n        if date is None:\n            return \"\"\n        # default options\n        options = {\n            \"long_format\": True,\n            \"time_only\": False,\n            \"context\": self.context,\n            \"request\": self.request,\n            \"domain\": \"senaite.core\",\n        }\n        options.update(kw)\n        return ulocalized_time(date, **options)", "response": "Converts the given date to a localized time string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef translate_state(self, s):\n        if not isinstance(s, basestring):\n            return s\n        s = s.capitalize().replace(\"_\", \" \")\n        return t(_(s))", "response": "Translate the given state string into a human readable string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef folderitems(self):\n        items = []\n        # get the snapshots\n        snapshots = get_snapshots(self.context)\n        # reverse the order to get the most recent change first\n        snapshots = list(reversed(snapshots))\n        # set the total number of items\n        self.total = len(snapshots)\n        # slice a batch\n        batch = snapshots[self.limit_from:self.limit_from+self.pagesize]\n\n        for snapshot in batch:\n            item = self.make_empty_item(**snapshot)\n            # get the version of the snapshot\n            version = get_snapshot_version(self.context, snapshot)\n\n            # Version\n            item[\"version\"] = version\n\n            # get the metadata of the diff\n            metadata = get_snapshot_metadata(snapshot)\n\n            # Modification Date\n            m_date = metadata.get(\"modified\")\n            item[\"modified\"] = self.to_localized_time(m_date)\n\n            # Actor\n            actor = metadata.get(\"actor\")\n            item[\"actor\"] = actor\n\n            # Fullname\n            properties = api.get_user_properties(actor)\n            item[\"fullname\"] = properties.get(\"fullname\", actor)\n\n            # Roles\n            roles = metadata.get(\"roles\", [])\n            item[\"roles\"] = \", \".join(roles)\n\n            # Remote Address\n            remote_address = metadata.get(\"remote_address\")\n            item[\"remote_address\"] = remote_address\n\n            # Action\n            action = metadata.get(\"action\")\n            item[\"action\"] = self.translate_state(action)\n\n            # Review State\n            review_state = metadata.get(\"review_state\")\n            item[\"review_state\"] = self.translate_state(review_state)\n\n            # get the previous snapshot\n            prev_snapshot = get_snapshot_by_version(self.context, version-1)\n            if prev_snapshot:\n                prev_metadata = get_snapshot_metadata(prev_snapshot)\n                prev_review_state = prev_metadata.get(\"review_state\")\n                if prev_review_state != review_state:\n                    item[\"replace\"][\"review_state\"] = \"{} &rarr; {}\".format(\n                        self.translate_state(prev_review_state),\n                        self.translate_state(review_state))\n\n                # Rendered Diff\n                diff = compare_snapshots(snapshot, prev_snapshot)\n                item[\"diff\"] = self.render_diff(diff)\n\n            # append the item\n            items.append(item)\n\n        return items", "response": "Generate the items for each version of the current object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a set of raw results for an object with id = resid and values = values.", "response": "def _addRawResult(self, resid, values={}, override=False):\n        \"\"\" Adds a set of raw results for an object with id=resid\n            resid is usually an Analysis Request ID or Worksheet's Reference\n            Analysis ID. The values are a dictionary in which the keys are\n            analysis service keywords and the values, another dictionary with\n            the key,value results.\n            The column 'DefaultResult' must be provided, because is used to map\n            to the column from which the default result must be retrieved.\n\n            Example:\n            resid  = 'DU13162-001-R1'\n            values = {\n                'D2': {'DefaultResult': 'Final Conc',\n                       'Remarks':       '',\n                       'Resp':          '5816',\n                       'ISTD Resp':     '274638',\n                       'Resp Ratio':    '0.0212',\n                       'Final Conc':    '0.9145',\n                       'Exp Conc':      '1.9531',\n                       'Accuracy':      '98.19' },\n\n                'D3': {'DefaultResult': 'Final Conc',\n                       'Remarks':       '',\n                       'Resp':          '5816',\n                       'ISTD Resp':     '274638',\n                       'Resp Ratio':    '0.0212',\n                       'Final Conc':    '0.9145',\n                       'Exp Conc':      '1.9531',\n                       'Accuracy':      '98.19' }\n                }\n        \"\"\"\n        if override or resid not in self._rawresults.keys():\n            self._rawresults[resid] = [values]\n        else:\n            self._rawresults[resid].append(values)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of analysis service keywords found in the database.", "response": "def getAnalysisKeywords(self):\n        \"\"\" The analysis service keywords found\n        \"\"\"\n        analyses = []\n        for rows in self.getRawResults().values():\n            for row in rows:\n                analyses = list(set(analyses + row.keys()))\n        return analyses"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_file(self, infile):\n        try:\n            encoding = self._encoding if self._encoding else None\n            mode = 'r' if self._encoding else 'rU'\n            with codecs.open(infile.name, mode, encoding=encoding) as f:\n                lines = f.readlines()\n        except AttributeError:\n            lines = infile.readlines()\n        lines = [line.strip() for line in lines]\n        return lines", "response": "Given an input file read its contents strip whitespace from the beginning and end of each line and return a list of the preprocessed\n         lines read."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef attach_attachment(self, analysis, attachment):\n        if not attachment:\n            return\n        if isinstance(attachment, list):\n            for attach in attachment:\n                self.attach_attachment(analysis, attach)\n            return\n        # current attachments\n        an_atts = analysis.getAttachment()\n        atts_filenames = [att.getAttachmentFile().filename for att in an_atts]\n        if attachment.getAttachmentFile().filename not in atts_filenames:\n            an_atts.append(attachment)\n            logger.info(\n                \"Attaching %s to %s\" % (attachment.UID(), analysis))\n            analysis.setAttachment([att.UID() for att in an_atts])\n            analysis.reindexObject()\n        else:\n            self.warn(\"Attachment %s was not linked to analysis %s\" %\n                      (attachment.UID(), analysis))", "response": "Attach an attachment to an analysis"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsearch for analyses from ZODB to be filled with results.", "response": "def _getZODBAnalyses(self, objid):\n        \"\"\" Searches for analyses from ZODB to be filled with results.\n            objid can be either AR ID or Worksheet's Reference Sample IDs.\n            Only analyses that matches with getAnallowedAnalysisStates() will\n            be returned. If not a ReferenceAnalysis, getAllowedARStates() is\n            also checked.\n            Returns empty array if no analyses found\n        \"\"\"\n        # ars = []\n        analyses = []\n        searchcriteria = ['getId', 'getClientSampleID']\n        allowed_ar_states = self.getAllowedARStates()\n        allowed_an_states = self.getAllowedAnalysisStates()\n        # allowed_ar_states_msg = [_(s) for s in allowed_ar_states]\n        allowed_an_states_msg = [_(s) for s in allowed_an_states]\n\n        # Acceleration of searches using priorization\n        if self._priorizedsearchcriteria in ['rgid', 'rid', 'ruid']:\n            # Look from reference analyses\n            analyses = self._getZODBAnalysesFromReferenceAnalyses(\n                    objid, self._priorizedsearchcriteria)\n        if len(analyses) == 0:\n            # Look from ar and derived\n            analyses = self._getZODBAnalysesFromAR(objid,\n                                                   '',\n                                                   searchcriteria,\n                                                   allowed_ar_states)\n\n        # Discard analyses that don't match with allowed_an_states\n        analyses = [analysis for analysis in analyses\n                    if analysis.portal_type != 'Analysis' or\n                    self.wf.getInfoFor(analysis, 'review_state')\n                    in allowed_an_states]\n\n        if len(analyses) == 0:\n            self.warn(\n                \"No analyses '${allowed_analysis_states}' \"\n                \"states found for ${object_id}\",\n                mapping={\"allowed_analysis_states\": ', '.join(\n                    allowed_an_states_msg),\n                         \"object_id\": objid})\n\n        return analyses"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calculateTotalResults(self, objid, analysis):\n        analyses = self._getZODBAnalyses(objid)\n        # Filter Analyses With Calculation\n        analyses_with_calculation = filter(\n                                        lambda an: an.getCalculation(),\n                                        analyses)\n        for analysis_with_calc in analyses_with_calculation:\n            # Get the calculation to get the formula so that we can check\n            # if param analysis keyword is used on the calculation formula\n            calcultion = analysis_with_calc.getCalculation()\n            formula = calcultion.getMinifiedFormula()\n            # The analysis that we are currenly on\n            analysis_keyword = analysis.getKeyword()\n            if analysis_keyword not in formula:\n                continue\n\n            # If the analysis_keyword is in the formula, it means that this\n            # analysis is a dependent on that calculated analysis\n            calc_passed = analysis_with_calc.calculateResult(override=self._override[1])\n            if calc_passed:\n                api.do_transition_for(analysis_with_calc, \"submit\")\n                self.log(\n                    \"${request_id}: calculated result for \"\n                    \"'${analysis_keyword}': '${analysis_result}'\",\n                    mapping={\"request_id\": objid,\n                             \"analysis_keyword\": analysis_with_calc.getKeyword(),\n                             \"analysis_result\": str(analysis_with_calc.getResult())}\n                )", "response": "This method checks if the AR has a calculation and calculates the total results for the current AR."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nduplicate an analysis consist on creating a new analysis with the same analysis service for the same sample. It is used in order to reduce the error procedure probability because both results must be similar. :base: the analysis object used as the creation base.", "response": "def duplicateAnalysis(analysis):\n    \"\"\"\n    Duplicate an analysis consist on creating a new analysis with\n    the same analysis service for the same sample. It is used in\n    order to reduce the error procedure probability because both\n    results must be similar.\n    :base: the analysis object used as the creation base.\n    \"\"\"\n    ar = analysis.aq_parent\n    kw = analysis.getKeyword()\n    # Rename the analysis to make way for it's successor.\n    # Support multiple duplicates by renaming to *-0, *-1, etc\n    cnt = [x for x in ar.objectValues(\"Analysis\") if x.getId().startswith(kw)]\n    a_id = \"{0}-{1}\".format(kw, len(cnt))\n    dup = create_analysis(ar, analysis, id=a_id, Retested=True)\n    return dup"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_analysis(context, source, **kwargs):\n    an_id = kwargs.get('id', source.getKeyword())\n    analysis = _createObjectByType(\"Analysis\", context, an_id)\n    copy_analysis_field_values(source, analysis, **kwargs)\n\n    # AnalysisService field is not present on actual AnalysisServices.\n    if IAnalysisService.providedBy(source):\n        analysis.setAnalysisService(source)\n    else:\n        analysis.setAnalysisService(source.getAnalysisService())\n\n    # Set the interims from the Service\n    service_interims = analysis.getAnalysisService().getInterimFields()\n    # Avoid references from the analysis interims to the service interims\n    service_interims = copy.deepcopy(service_interims)\n    analysis.setInterimFields(service_interims)\n\n    analysis.unmarkCreationFlag()\n    zope.event.notify(ObjectInitializedEvent(analysis))\n    return analysis", "response": "Create a new Analysis object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_significant_digits(numeric_value):\n    try:\n        numeric_value = float(numeric_value)\n    except (TypeError, ValueError):\n        return None\n    if numeric_value == 0:\n        return 0\n    significant_digit = int(math.floor(math.log10(abs(numeric_value))))\n    return 0 if significant_digit > 0 else abs(significant_digit)", "response": "Returns the number of significant digits of a given floatable value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the formatted uncertainty according to the analysis, result and decimal mark specified following these rules: If the \"Calculate precision from uncertainties\" is enabled in the Analysis service, and a) If the the non-decimal number of digits of the result is above the service's ExponentialFormatPrecision, the uncertainty will be formatted in scientific notation. The uncertainty exponential value used will be the same as the one used for the result. The uncertainty will be rounded according to the same precision as the result. Example: Given an Analysis with an uncertainty of 37 for a range of results between 30000 and 40000, with an ExponentialFormatPrecision equal to 4 and a result of 32092, this method will return 0.004E+04 b) If the number of digits of the integer part of the result is below the ExponentialFormatPrecision, the uncertainty will be formatted as decimal notation and the uncertainty will be rounded one position after reaching the last 0 (precision calculated according to the uncertainty value). Example: Given an Analysis with an uncertainty of 0.22 for a range of results between 1 and 10 with an ExponentialFormatPrecision equal to 4 and a result of 5.234, this method will return 0.2 If the \"Calculate precision from Uncertainties\" is disabled in the analysis service, the same rules described above applies, but the precision used for rounding the uncertainty is not calculated from the uncertainty neither the result. The fixed length precision is used instead. For further details, visit https://jira.bikalabs.com/browse/LIMS-1334 If the result is not floatable or no uncertainty defined, returns an empty string. The default decimal mark '.' will be replaced by the decimalmark specified. :param analysis: the analysis from which the uncertainty, precision and other additional info have to be retrieved :param result: result of the analysis. Used to retrieve and/or calculate the precision and/or uncertainty :param decimalmark: decimal mark to use. By default '.' :param sciformat: 1. The sci notation has to be formatted as aE^+b 2. The sci notation has to be formatted as ax10^b 3. As 2, but with super html entity for exp 4. The sci notation has to be formatted as a\u00b710^b 5. As 4, but with super html entity for exp By default 1 :returns: the formatted uncertainty", "response": "def format_uncertainty(analysis, result, decimalmark='.', sciformat=1):\n    \"\"\"\n    Returns the formatted uncertainty according to the analysis, result\n    and decimal mark specified following these rules:\n\n    If the \"Calculate precision from uncertainties\" is enabled in\n    the Analysis service, and\n\n    a) If the the non-decimal number of digits of the result is above\n       the service's ExponentialFormatPrecision, the uncertainty will\n       be formatted in scientific notation. The uncertainty exponential\n       value used will be the same as the one used for the result. The\n       uncertainty will be rounded according to the same precision as\n       the result.\n\n       Example:\n       Given an Analysis with an uncertainty of 37 for a range of\n       results between 30000 and 40000, with an\n       ExponentialFormatPrecision equal to 4 and a result of 32092,\n       this method will return 0.004E+04\n\n    b) If the number of digits of the integer part of the result is\n       below the ExponentialFormatPrecision, the uncertainty will be\n       formatted as decimal notation and the uncertainty will be\n       rounded one position after reaching the last 0 (precision\n       calculated according to the uncertainty value).\n\n       Example:\n       Given an Analysis with an uncertainty of 0.22 for a range of\n       results between 1 and 10 with an ExponentialFormatPrecision\n       equal to 4 and a result of 5.234, this method will return 0.2\n\n    If the \"Calculate precision from Uncertainties\" is disabled in the\n    analysis service, the same rules described above applies, but the\n    precision used for rounding the uncertainty is not calculated from\n    the uncertainty neither the result. The fixed length precision is\n    used instead.\n\n    For further details, visit\n    https://jira.bikalabs.com/browse/LIMS-1334\n\n    If the result is not floatable or no uncertainty defined, returns\n    an empty string.\n\n    The default decimal mark '.' will be replaced by the decimalmark\n    specified.\n\n    :param analysis: the analysis from which the uncertainty, precision\n                     and other additional info have to be retrieved\n    :param result: result of the analysis. Used to retrieve and/or\n                   calculate the precision and/or uncertainty\n    :param decimalmark: decimal mark to use. By default '.'\n    :param sciformat: 1. The sci notation has to be formatted as aE^+b\n                  2. The sci notation has to be formatted as ax10^b\n                  3. As 2, but with super html entity for exp\n                  4. The sci notation has to be formatted as a\u00b710^b\n                  5. As 4, but with super html entity for exp\n                  By default 1\n    :returns: the formatted uncertainty\n    \"\"\"\n    try:\n        result = float(result)\n    except ValueError:\n        return \"\"\n\n    objres = None\n    try:\n        objres = float(analysis.getResult())\n    except ValueError:\n        pass\n\n    if result == objres:\n        # To avoid problems with DLs\n        uncertainty = analysis.getUncertainty()\n    else:\n        uncertainty = analysis.getUncertainty(result)\n\n    if uncertainty is None or uncertainty == 0:\n        return \"\"\n\n    # Scientific notation?\n    # Get the default precision for scientific notation\n    threshold = analysis.getExponentialFormatPrecision()\n    precision = analysis.getPrecision(result)\n    formatted = _format_decimal_or_sci(uncertainty, precision, threshold,\n                                       sciformat)\n    return formatDecimalMark(formatted, decimalmark)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_numeric_result(analysis, result, decimalmark='.', sciformat=1):\n    try:\n        result = float(result)\n    except ValueError:\n        return result\n\n    # continuing with 'nan' result will cause formatting to fail.\n    if math.isnan(result):\n        return result\n\n    # Scientific notation?\n    # Get the default precision for scientific notation\n    threshold = analysis.getExponentialFormatPrecision()\n    precision = analysis.getPrecision(result)\n    formatted = _format_decimal_or_sci(result, precision, threshold, sciformat)\n    return formatDecimalMark(formatted, decimalmark)", "response": "This method formats a numeric result value for the given analysis and result."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_method_instrument_constraints(context, uids):\n    constraints = {}\n    uc = getToolByName(context, 'uid_catalog')\n    analyses = uc(portal_type=['Analysis', 'ReferenceAnalysis'],\n                  UID=uids)\n    cached_servs = {}\n    for analysis in analyses:\n        if not analysis:\n            continue\n        analysis = analysis.getObject()\n        auid = analysis.UID()\n        suid = analysis.getServiceUID()\n        refan = analysis.portal_type == 'ReferenceAnalysis'\n        cachedkey = \"qc\" if refan else \"re\"\n        if suid in cached_servs.get(cachedkey, []):\n            constraints[auid] = cached_servs[cachedkey][suid]\n            continue\n\n        if not cached_servs.get(cachedkey, None):\n            cached_servs[cachedkey] = {suid: {}}\n        else:\n            cached_servs[cachedkey][suid] = {}\n        constraints[auid] = {}\n\n        allowed_instruments = analysis.getAllowedInstruments()\n\n        # Analysis allows manual/instrument entry?\n        s_mentry = analysis.getManualEntryOfResults()\n        s_ientry = analysis.getInstrumentEntryOfResults()\n        s_instrums = allowed_instruments if s_ientry else []\n        s_instrums = [instr.UID() for instr in s_instrums]\n        a_dinstrum = analysis.getInstrument() if s_ientry else None\n        s_methods = analysis.getAllowedMethods()\n        s_dmethod = analysis.getMethod()\n        dmuid = s_dmethod.UID() if s_dmethod else ''\n        diuid = a_dinstrum.UID() if a_dinstrum else ''\n\n        # To take into account ASs with no method assigned by default or\n        # ASs that have an instrument assigned by default that doesn't have\n        # a method associated.\n        if s_mentry or not s_dmethod:\n            s_methods += [None]\n\n        for method in s_methods:\n            # Method manual entry?\n            m_mentry = method.isManualEntryOfResults() if method else True\n\n            instrs = []\n            if method:\n                # Instruments available for this method and analysis?\n                instrs = [i for i in method.getInstruments()\n                          if i.UID() in s_instrums]\n            else:\n                # What about instruments without a method assigned?\n                instrs = [i for i in allowed_instruments\n                          if i.UID() in s_instrums and not i.getMethods()]\n\n            instuids = [i.UID() for i in instrs]\n            v_instrobjs = [i for i in instrs if i.isValid()]\n            v_instrs = [i.UID() for i in v_instrobjs]\n            muid = method.UID() if method else ''\n\n            # PREMISES\n            # p1: Analysis allows manual entry?\n            # p2: Analysis allows instrument entry?\n            # p3: Method selected and non empty?\n            # p4: Method allows manual entry?\n            # p5: At least one instrument available for this method?\n            # p6: Valid instruments available?\n            # p7: All instruments valid?\n            # p8: Methods allow the service's default instrument?\n            # p9: Default instrument valid?\n            premises = [\n                \"R\" if not refan else 'Q',\n                \"Y\" if s_mentry else \"N\",\n                \"Y\" if s_ientry else \"N\",\n                \"Y\" if method else \"N\",\n                \"Y\" if m_mentry else \"N\",\n                \"Y\" if instrs else \"N\",\n                \"Y\" if v_instrs or not instrs else \"N\",\n                \"Y\" if len(v_instrs) == len(instrs) else \"N\",\n                \"Y\" if diuid in instuids else \"N\",\n                \"Y\" if a_dinstrum and a_dinstrum.isValid() else \"N\",\n            ]\n            tprem = ''.join(premises)\n\n            fiuid = v_instrs[0] if v_instrs else ''\n            instrtitle = to_unicode(a_dinstrum.Title()) if a_dinstrum else ''\n            iinstrs = ', '.join([to_unicode(i.Title()) for i in instrs\n                                 if i.UID() not in v_instrs])\n            dmeth = to_unicode(method.Title()) if method else ''\n            m1 = _(\"Invalid instruments are not displayed: %s\") % iinstrs\n            m2 = _(\"Default instrument %s is not valid\") % instrtitle\n            m3 = _(\"No valid instruments available: %s \") % iinstrs\n            m4 = _(\"Manual entry of results for method %s is not allowed \"\n                   \"and no valid instruments found: %s\") % (dmeth, iinstrs)\n            m5 = _(\"The method %s is not valid: no manual entry allowed \"\n                   \"and no instrument assigned\") % dmeth\n            m6 = _(\"The method %s is not valid: only instrument entry for \"\n                   \"this analysis is allowed, but the method has no \"\n                   \"instrument assigned\") % dmeth\n            m7 = _(\"Only instrument entry for this analysis is allowed, \"\n                   \"but there is no instrument assigned\")\n\n            \"\"\"\n            Matrix dict keys char positions: (True: Y, False: N)\n              0: (R)egular analysis or (Q)C analysis\n              1: Analysis allows manual entry?\n              2: Analysis allows instrument entry?\n              3: Method is not None?\n              4: Method allows manual entry?\n              5: At least one instrument avialable for the method?\n              6: Valid instruments available?\n              7: All instruments valid?\n              8: Method allows the service's default instrument?\n              9: Default instrument valid?\n\n            Matrix dict values array indexes:\n              0: Method list visible? YES:1, NO:0, YES(a):2, YES(r):3\n              1: Add \"None\" in methods list? YES:1, NO:0, NO(g):2\n              2: Instr. list visible? YES:1, NO:0\n              3: Add \"None\" in instrums list? YES: 1, NO:0\n              4: UID of the selected instrument or '' if None\n              5: Results field editable? YES: 1, NO:0\n              6: Alert message string\n\n            See docs/imm_results_entry_behaviour.png for further details\n            \"\"\"\n            matrix = {\n                # Regular analyses\n                'RYYYYYYYY': [1, 1, 1, 1, diuid, 1, ''],  # B1\n                'RYYYYYYYN': [1, 1, 1, 1, '', 1, ''],  # B2\n                'RYYYYYYNYY': [1, 1, 1, 1, diuid, 1, m1],  # B3\n                'RYYYYYYNYN': [1, 1, 1, 1, '', 1, m2],  # B4\n                'RYYYYYYNN': [1, 1, 1, 1, '', 1, m1],  # B5\n                'RYYYYYN': [1, 1, 1, 1, '', 1, m3],  # B6\n                'RYYYYN': [1, 1, 1, 1, '', 1, ''],  # B7\n                'RYYYNYYYY': [1, 1, 1, 0, diuid, 1, ''],  # B8\n                'RYYYNYYYN': [1, 1, 1, 0, fiuid, 1, ''],  # B9\n                'RYYYNYYNYY': [1, 1, 1, 0, diuid, 1, m1],  # B10\n                'RYYYNYYNYN': [1, 1, 1, 1, '', 0, m2],  # B11\n                'RYYYNYYNN': [1, 1, 1, 0, fiuid, 1, m1],  # B12\n                'RYYYNYN': [1, 1, 1, 1, '', 0, m4],  # B13\n                'RYYYNN': [1, 1, 1, 1, '', 0, m5],  # B14\n                'RYYNYYYYY': [1, 1, 1, 1, diuid, 1, ''],  # B15\n                'RYYNYYYYN': [1, 1, 1, 1, '', 1, ''],  # B16\n                'RYYNYYYNYY': [1, 1, 1, 1, diuid, 1, m1],  # B17\n                'RYYNYYYNYN': [1, 1, 1, 1, '', 1, m2],  # B18\n                'RYYNYYYNN': [1, 1, 1, 1, '', 1, m1],  # B19\n                'RYYNYYN': [1, 1, 1, 1, '', 1, m3],  # B20\n                'RYYNYN': [1, 1, 1, 1, '', 1, ''],  # B21\n                'RYNY': [2, 0, 0, 0, '', 1, ''],  # B22\n                'RYNN': [0, 0, 0, 0, '', 1, ''],  # B23\n                'RNYYYYYYY': [3, 2, 1, 1, diuid, 1, ''],  # B24\n                'RNYYYYYYN': [3, 2, 1, 1, '', 1, ''],  # B25\n                'RNYYYYYNYY': [3, 2, 1, 1, diuid, 1, m1],  # B26\n                'RNYYYYYNYN': [3, 2, 1, 1, '', 1, m2],  # B27\n                'RNYYYYYNN': [3, 2, 1, 1, '', 1, m1],  # B28\n                'RNYYYYN': [3, 2, 1, 1, '', 1, m3],  # B29\n                'RNYYYN': [3, 2, 1, 1, '', 0, m6],  # B30\n                'RNYYNYYYY': [3, 2, 1, 0, diuid, 1, ''],  # B31\n                'RNYYNYYYN': [3, 2, 1, 0, fiuid, 1, ''],  # B32\n                'RNYYNYYNYY': [3, 2, 1, 0, diuid, 1, m1],  # B33\n                'RNYYNYYNYN': [3, 2, 1, 1, '', 0, m2],  # B34\n                'RNYYNYYNN': [3, 2, 1, 0, fiuid, 1, m1],  # B35\n                'RNYYNYN': [3, 2, 1, 1, '', 0, m3],  # B36\n                'RNYYNN': [3, 2, 1, 1, '', 0, m6],  # B37\n                'RNYNYYYYY': [3, 1, 1, 0, diuid, 1, ''],  # B38\n                'RNYNYYYYN': [3, 1, 1, 0, fiuid, 1, ''],  # B39\n                'RNYNYYYNYY': [3, 1, 1, 0, diuid, 1, m1],  # B40\n                'RNYNYYYNYN': [3, 1, 1, 1, '', 0, m2],  # B41\n                'RNYNYYYNN': [3, 1, 1, 0, fiuid, 1, m1],  # B42\n                'RNYNYYN': [3, 1, 1, 0, '', 0, m3],  # B43\n                'RNYNYN': [3, 1, 1, 0, '', 0, m7],  # B44\n                # QC Analyses\n                'QYYYYYYYY': [1, 1, 1, 1, diuid, 1, ''],  # C1\n                'QYYYYYYYN': [1, 1, 1, 1, '', 1, ''],  # C2\n                'QYYYYYYNYY': [1, 1, 1, 1, diuid, 1, ''],  # C3\n                'QYYYYYYNYN': [1, 1, 1, 1, diuid, 1, ''],  # C4\n                'QYYYYYYNN': [1, 1, 1, 1, '', 1, ''],  # C5\n                'QYYYYYN': [1, 1, 1, 1, '', 1, ''],  # C6\n                'QYYYYN': [1, 1, 1, 1, '', 1, ''],  # C7\n                'QYYYNYYYY': [1, 1, 1, 0, diuid, 1, ''],  # C8\n                'QYYYNYYYN': [1, 1, 1, 0, fiuid, 1, ''],  # C9\n                'QYYYNYYNYY': [1, 1, 1, 0, diuid, 1, ''],  # C10\n                'QYYYNYYNYN': [1, 1, 1, 0, diuid, 1, ''],  # C11\n                'QYYYNYYNN': [1, 1, 1, 0, fiuid, 1, ''],  # C12\n                'QYYYNYN': [1, 1, 1, 0, fiuid, 1, ''],  # C13\n                'QYYYNN': [1, 1, 1, 1, '', 0, m5],  # C14\n                'QYYNYYYYY': [1, 1, 1, 1, diuid, 1, ''],  # C15\n                'QYYNYYYYN': [1, 1, 1, 1, '', 1, ''],  # C16\n                'QYYNYYYNYY': [1, 1, 1, 1, diuid, 1, ''],  # C17\n                'QYYNYYYNYN': [1, 1, 1, 1, diuid, 1, ''],  # C18\n                'QYYNYYYNN': [1, 1, 1, 1, fiuid, 1, ''],  # C19\n                'QYYNYYN': [1, 1, 1, 1, diuid, 1, ''],  # C20\n                'QYYNYN': [1, 1, 1, 1, '', 1, ''],  # C21\n                'QYNY': [2, 0, 0, 0, '', 1, ''],  # C22\n                'QYNN': [0, 0, 0, 0, '', 1, ''],  # C23\n                'QNYYYYYYY': [3, 2, 1, 1, diuid, 1, ''],  # C24\n                'QNYYYYYYN': [3, 2, 1, 1, '', 1, ''],  # C25\n                'QNYYYYYNYY': [3, 2, 1, 1, diuid, 1, ''],  # C26\n                'QNYYYYYNYN': [3, 2, 1, 1, diuid, 1, ''],  # C27\n                'QNYYYYYNN': [3, 2, 1, 1, '', 1, ''],  # C28\n                'QNYYYYN': [3, 2, 1, 1, '', 1, ''],  # C29\n                'QNYYYN': [3, 2, 1, 1, '', 0, m6],  # C30\n                'QNYYNYYYY': [3, 2, 1, 0, diuid, 1, ''],  # C31\n                'QNYYNYYYN': [3, 2, 1, 0, fiuid, 1, ''],  # C32\n                'QNYYNYYNYY': [3, 2, 1, 0, diuid, 1, ''],  # C33\n                'QNYYNYYNYN': [3, 2, 1, 0, diuid, 1, ''],  # C34\n                'QNYYNYYNN': [3, 2, 1, 0, fiuid, 1, ''],  # C35\n                'QNYYNYN': [3, 2, 1, 0, fiuid, 1, ''],  # C36\n                'QNYYNN': [3, 2, 1, 1, '', 0, m5],  # C37\n                'QNYNYYYYY': [3, 1, 1, 0, diuid, 1, ''],  # C38\n                'QNYNYYYYN': [3, 1, 1, 0, fiuid, 1, ''],  # C39\n                'QNYNYYYNYY': [3, 1, 1, 0, diuid, 1, ''],  # C40\n                'QNYNYYYNYN': [3, 1, 1, 0, diuid, 1, ''],  # C41\n                'QNYNYYYNN': [3, 1, 1, 0, fiuid, 1, ''],  # C42\n                'QNYNYYN': [3, 1, 1, 0, fiuid, 1, ''],  # C43\n                'QNYNYN': [3, 1, 1, 1, '', 0, m7],  # C44\n            }\n            targ = [v for k, v in matrix.items() if tprem.startswith(k)]\n            if not targ:\n                targ = [[1, 1, 1, 1, '', 0, 'Key not found: %s' % tprem], ]\n            targ = targ[0]\n            atitle = analysis.Title() if analysis else \"None\"\n            mtitle = method.Title() if method else \"None\"\n            instdi = {}\n            if refan and instrs:\n                instdi = {i.UID(): i.Title() for i in instrs}\n            elif not refan and v_instrobjs:\n                instdi = {i.UID(): i.Title() for i in v_instrobjs}\n            targ += [instdi, mtitle, atitle, tprem]\n            constraints[auid][muid] = targ\n            cached_servs[cachedkey][suid][muid] = targ\n    return constraints", "response": "Returns a dictionary with the constraints and rules for the method instruments and results to be applied to each of the analyses specified in the param uids."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating hook for reference results view", "response": "def update(self):\n        \"\"\"Update hook\n        \"\"\"\n        super(ReferenceResultsView, self).update()\n        self.categories.sort()\n        self.referenceresults = self.get_reference_results()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of items in the folder.", "response": "def folderitems(self):\n        \"\"\"TODO: Refactor to non-classic mode\n        \"\"\"\n        items = super(ReferenceResultsView, self).folderitems()\n        self.categories.sort()\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef folderitem(self, obj, item, index):\n        # ensure we have an object and not a brain\n        obj = api.get_object(obj)\n        uid = api.get_uid(obj)\n        url = api.get_url(obj)\n        title = api.get_title(obj)\n\n        # get the category\n        if self.show_categories_enabled():\n            category = obj.getCategoryTitle()\n            if category not in self.categories:\n                self.categories.append(category)\n            item[\"category\"] = category\n\n        rr = self.referenceresults.get(uid, {})\n        item[\"Title\"] = title\n        item[\"replace\"][\"Title\"] = get_link(url, value=title)\n        item[\"allow_edit\"] = self.get_editable_columns()\n        item[\"required\"] = self.get_required_columns()\n        item[\"selected\"] = rr and True or False\n        item[\"result\"] = rr.get(\"result\", \"\")\n        item[\"min\"] = rr.get(\"min\", \"\")\n        item[\"max\"] = rr.get(\"max\", \"\")\n\n        # Icons\n        after_icons = \"\"\n        if obj.getAccredited():\n            after_icons += get_image(\n                \"accredited.png\", title=_(\"Accredited\"))\n        if obj.getAttachmentOption() == \"r\":\n            after_icons += get_image(\n                \"attach_reqd.png\", title=_(\"Attachment required\"))\n        if obj.getAttachmentOption() == \"n\":\n            after_icons += get_image(\n                \"attach_no.png\", title=_(\"Attachment not permitted\"))\n        if after_icons:\n            item[\"after\"][\"Title\"] = after_icons\n\n        return item", "response": "This method is called by the folderitem service triggered each time an item is iterated in folderitems. It returns the item dict containing the properties of the object and the index of the item."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing the form and return a list of dictionaries fit for ReferenceResultsField consumption.", "response": "def process_form(self, instance, field, form,\n                     empty_marker=None, emptyReturnsMarker=False):\n        \"\"\"Return a list of dictionaries fit for ReferenceResultsField\n        consumption. Only services which have float()able entries in result,min\n        and max field will be included. If any of min, max, or result fields\n        are blank, the row value is ignored here.\n        \"\"\"\n        values = {}\n\n        # Process settings from the reference definition first\n        ref_def = form.get(\"ReferenceDefinition\")\n        ref_def_uid = ref_def and ref_def[0]\n        if ref_def_uid:\n            ref_def_obj = api.get_object_by_uid(ref_def_uid)\n            ref_results = ref_def_obj.getReferenceResults()\n            # store reference results by UID to avoid duplicates\n            rr_by_uid = dict(map(lambda r: (r.get(\"uid\"), r), ref_results))\n            values.update(rr_by_uid)\n\n        # selected services\n        service_uids = form.get(\"uids\", [])\n\n        for uid in service_uids:\n            result = self._get_spec_value(form, uid, \"result\")\n            if not result:\n                # User has to set a value for result subfield at least\n                continue\n\n            # If neither min nor max have been set, assume we only accept a\n            # discrete result (like if % of error was 0).\n            s_min = self._get_spec_value(form, uid, \"min\", result)\n            s_max = self._get_spec_value(form, uid, \"max\", result)\n\n            service = api.get_object_by_uid(uid)\n            values[uid] = {\n                \"keyword\": service.getKeyword(),\n                \"uid\": uid,\n                \"result\": result,\n                \"min\": s_min,\n                \"max\": s_max\n            }\n\n        return values.values(), {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_spec_value(self, form, uid, key, default=''):\n        if not form or not uid:\n            return default\n        values = form.get(key, None)\n        if not values or len(values) == 0:\n            return default\n        value = values[0].get(uid, default)\n        return api.is_floatable(value) and value or default", "response": "Returns the value assigned to the analysis\n        service uid from the passed in form."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ReferenceResults(self, field, allow_edit=False):\n        instance = getattr(self, \"instance\", field.aq_parent)\n        table = api.get_view(\"table_reference_results\",\n                             context=instance,\n                             request=self.REQUEST)\n        # Call listing hooks\n        table.update()\n        table.before_render()\n        return table.ajax_contents_table()", "response": "Render Reference Results Table"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding the value to the existing text stored in the field and also adds a small divider showing username and date of this entry.", "response": "def set(self, instance, value, **kwargs):\n        \"\"\"Adds the value to the existing text stored in the field,\n        along with a small divider showing username and date of this entry.\n        \"\"\"\n        if not value:\n            return\n        value = value.strip()\n        date = DateTime().rfc822()\n        user = getSecurityManager().getUser()\n        username = user.getUserName()\n        divider = \"=== {} ({})\".format(date, username)\n        existing_remarks = instance.getRawRemarks()\n        remarks = '\\n'.join([divider, value, existing_remarks])\n        ObjectField.set(self, instance, remarks)\n        # reindex the object after save to update all catalog metadata\n        instance.reindexObject()\n        # notify object edited event\n        event.notify(ObjectEditedEvent(instance))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the raw field value", "response": "def getRaw(self, instance, **kwargs):\n        \"\"\"Returns raw field value (possible wrapped in BaseUnit)\n        \"\"\"\n        value = ObjectField.get(self, instance, **kwargs)\n        # getattr(instance, \"Remarks\") returns a BaseUnit\n        if callable(value):\n            value = value()\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning before the first import step of the default profile", "response": "def pre_install(portal_setup):\n    \"\"\"Runs before the first import step of the *default* profile\n\n    This handler is registered as a *pre_handler* in the generic setup profile\n\n    :param portal_setup: SetupTool\n    \"\"\"\n    logger.info(\"SENAITE PRE-INSTALL handler [BEGIN]\")\n\n    # https://docs.plone.org/develop/addons/components/genericsetup.html#custom-installer-code-setuphandlers-py\n    profile_id = PROFILE_ID\n\n    context = portal_setup._getImportContext(profile_id)\n    portal = context.getSite()  # noqa\n\n    logger.info(\"SENAITE PRE-INSTALL handler [DONE]\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving default content from the object list", "response": "def remove_default_content(portal):\n    \"\"\"Remove default Plone contents\n    \"\"\"\n    logger.info(\"*** Delete Default Content ***\")\n\n    # Get the list of object ids for portal\n    object_ids = portal.objectIds()\n    delete_ids = filter(lambda id: id in object_ids, CONTENTS_TO_DELETE)\n    portal.manage_delObjects(ids=delete_ids)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhide the root items in navigation", "response": "def hide_navbar_items(portal):\n    \"\"\"Hide root items in navigation\n    \"\"\"\n    logger.info(\"*** Hide Navigation Items ***\")\n\n    # Get the list of object ids for portal\n    object_ids = portal.objectIds()\n    object_ids = filter(lambda id: id in object_ids, NAV_BAR_ITEMS_TO_HIDE)\n    for object_id in object_ids:\n        item = portal[object_id]\n        item.setExcludeFromNav(True)\n        item.reindexObject()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setup_groups(portal):\n    logger.info(\"*** Setup Roles and Groups ***\")\n\n    portal_groups = api.get_tool(\"portal_groups\")\n\n    for gdata in GROUPS:\n        group_id = gdata[\"id\"]\n        # create the group and grant the roles\n        if group_id not in portal_groups.listGroupIds():\n            logger.info(\"+++ Adding group {title} ({id})\".format(**gdata))\n            portal_groups.addGroup(group_id,\n                                   title=gdata[\"title\"],\n                                   roles=gdata[\"roles\"])\n        # grant the roles to the existing group\n        else:\n            ploneapi.group.grant_roles(\n                groupname=gdata[\"id\"],\n                roles=gdata[\"roles\"],)\n            logger.info(\"+++ Granted group {title} ({id}) the roles {roles}\"\n                        .format(**gdata))", "response": "Setup roles and groups for BECHEM\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setup_auditlog_catalog(portal):\n    logger.info(\"*** Setup Audit Log Catalog ***\")\n\n    catalog_id = auditlog_catalog.CATALOG_AUDITLOG\n    catalog = api.get_tool(catalog_id)\n\n    for name, meta_type in auditlog_catalog._indexes.iteritems():\n        indexes = catalog.indexes()\n        if name in indexes:\n            logger.info(\"*** Index '%s' already in Catalog [SKIP]\" % name)\n            continue\n\n        logger.info(\"*** Adding Index '%s' for field '%s' to catalog ...\"\n                    % (meta_type, name))\n\n        catalog.addIndex(name, meta_type)\n\n        # Setup TextIndexNG3 for listings\n        # XXX is there another way to do this?\n        if meta_type == \"TextIndexNG3\":\n            index = catalog._catalog.getIndex(name)\n            index.index.default_encoding = \"utf-8\"\n            index.index.query_parser = \"txng.parsers.en\"\n            index.index.autoexpand = \"always\"\n            index.index.autoexpand_limit = 3\n\n        logger.info(\"*** Added Index '%s' for field '%s' to catalog [DONE]\"\n                    % (meta_type, name))\n\n    # Attach the catalog to all known portal types\n    at = api.get_tool(\"archetype_tool\")\n    pt = api.get_tool(\"portal_types\")\n\n    for portal_type in pt.listContentTypes():\n        catalogs = at.getCatalogsByType(portal_type)\n        if catalog not in catalogs:\n            new_catalogs = map(lambda c: c.getId(), catalogs) + [catalog_id]\n            at.setCatalogsByType(portal_type, new_catalogs)\n            logger.info(\"*** Adding catalog '{}' for '{}'\".format(\n                catalog_id, portal_type))", "response": "Setup the auditlog catalog"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _fetch_analysis_for_local_id(analysis, ans_cond):\n    # Getting the first reflexed analysis from the chain\n    first_reflexed = analysis.getOriginalReflexedAnalysis()\n    # Getting all reflexed analysis created due to this first analysis\n    analyses_catalog = getToolByName(analysis, CATALOG_ANALYSIS_LISTING)\n    derivatives_brains = analyses_catalog(\n        getOriginalReflexedAnalysisUID=first_reflexed.UID()\n    )\n    # From all the related reflexed analysis, return the one that matches\n    # with the local id 'ans_cond'\n    for derivative in derivatives_brains:\n        derivative = derivative.getObject()\n        if derivative.getReflexRuleLocalID() == ans_cond:\n            return derivative\n    return None", "response": "Returns the first reflexed analysis that matches the local id ans_cond"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _createWorksheet(base, worksheettemplate, analyst):\n    if not(analyst):\n        # Get any analyst\n        analyst = getUsers(base, ['Manager', 'LabManager', 'Analyst'])[1]\n    folder = base.bika_setup.worksheets\n    _id = folder.invokeFactory('Worksheet', id=tmpID())\n    ws = folder[_id]\n    ws.unmarkCreationFlag()\n    new_ws_id = renameAfterCreation(ws)\n    ws.edit(\n        Number=new_ws_id,\n        Analyst=analyst,\n        )\n    if worksheettemplate:\n        ws.applyWorksheetTemplate(worksheettemplate)\n    return ws", "response": "This function creates a new worksheet in the system."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef doWorksheetLogic(base, action, analysis):\n    otherWS = action.get('otherWS', False)\n    worksheet_catalog = getToolByName(base, CATALOG_WORKSHEET_LISTING)\n    if otherWS in ['to_another', 'create_another']:\n        # Adds the new analysis inside same worksheet as the previous analysis.\n        # Checking if the actions defines an analyst\n        new_analyst = action.get('analyst', '')\n        # Checking if the action defines a worksheet template\n        worksheettemplate = action.get('worksheettemplate', '')\n        # Creating the query\n        contentFilter = {\n            'review_state': 'open',\n            'sort_on': 'created',\n            'sort_order': 'reverse'}\n        # If a new analyst is defined, add the analysis to the first\n        # analyst's worksheet\n        if new_analyst:\n            # Getting the last worksheet created for the analyst\n            contentFilter['Analyst'] = new_analyst\n        if worksheettemplate:\n            # Adding the worksheettemplate filter\n            contentFilter['getWorksheetTemplateUID'] = worksheettemplate\n        # Run the filter\n        wss = worksheet_catalog(contentFilter)\n        # 'repeat' actions takes advantatge of the 'retract' workflow action.\n        # the retract process assigns the new analysis to the same worksheet\n        # as the base analysis, so we need to desassign it now.\n        ws = analysis.getWorksheet()\n        if ws:\n            ws.removeAnalysis(analysis)\n        # If worksheet found and option 2\n        if len(wss) > 0 and otherWS == 'to_another':\n            # Add the new analysis to the worksheet\n            wss[0].getObject().addAnalysis(analysis)\n        # No worksheet found, but option 2 or 3 selected:\n        elif new_analyst:\n            # Create a new worksheet and add the analysis to it\n            ws = _createWorksheet(base, worksheettemplate, new_analyst)\n            ws.addAnalysis(analysis)\n        elif not new_analyst:\n            # Getting the original analysis to which the rule applies\n            previous_analysis = analysis.getReflexAnalysisOf()\n            # Getting the worksheet of the analysis\n            prev_ws = previous_analysis.getWorksheet()\n            # Getting the analyst from the worksheet\n            prev_analyst = prev_ws.getAnalyst() if prev_ws else ''\n            # If the previous analysis belongs to a worksheet:\n            if prev_analyst:\n                ws = _createWorksheet(base, worksheettemplate, prev_analyst)\n                ws.addAnalysis(analysis)\n            # If the original analysis doesn't have assigned any analyst\n            else:\n                # assign the same analyst that was assigned to the latest\n                # worksheet available\n                prev_analyst = wss[0].getObject().getAnalyst() if wss else ''\n                if prev_analyst:\n                    ws = _createWorksheet(base, worksheettemplate, prev_analyst)\n                    ws.addAnalysis(analysis)\n\n    elif otherWS == 'current':\n        # Getting the base's worksheet\n        ws = base.getWorksheet()\n        if ws:\n            # If the old analysis belongs to a worksheet, add the new\n            # one to it\n            ws.addAnalysis(analysis)\n    # If option 1 selected and no ws found, no worksheet will be assigned to\n    # the analysis.\n    # If option 4 selected, no worksheet will be assigned to the analysis\n    elif otherWS == 'no_ws':\n        ws = analysis.getWorksheet()\n        if ws:\n            ws.removeAnalysis(analysis)", "response": "This function checks if the actions contains a worksheet and if it does then creates a new worksheet and assigns the analysis to the base analysis."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef doReflexRuleAction(base, action_row):\n    for action in action_row:\n        # Do the action\n        analysis = doActionToAnalysis(base, action)\n        # Working with the worksheetlogic\n        doWorksheetLogic(base, action, analysis)\n        # Reindexing both objects in order to fill its metacolumns with\n        # the changes.\n        # TODO: Sometimes, objects are reindexed. Could it be that they\n        # are reindexed in some workflow step that some of them don't do?\n        base.reindexObject()\n        analysis.reindexObject()\n    return True", "response": "This function executes all the reflex rule actions inside the action_row and returns True if all the actions are done."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _areConditionsMet(self, action_set, analysis, forceuid=False):\n        conditions = action_set.get('conditions', [])\n        eval_str = ''\n        # Getting the analysis local id or its uid instead\n        alocalid = analysis.getReflexRuleLocalID() if \\\n            analysis.getIsReflexAnalysis() and not forceuid else analysis.getServiceUID()\n        # Getting the local ids (or analysis service uid) from the condition\n        # with the same local id (or analysis service uid) as the analysis\n        # attribute\n        localids = [cond.get('analysisservice') for cond in conditions\n                    if cond.get('analysisservice', '') == alocalid]\n        # Now the alocalid could be the UID of the analysis service (if\n        # this analysis has not been crated by a previous reflex rule) or it\n        # could be a local_id if the analysis has been created by a reflex\n        # rule.\n        # So, if the analysis was reflexed, and no localid has been found\n        # inside the action_set matching the analysis localid, lets look for\n        # the analysis service UID.\n        # forceuid is True when this second query has been done.\n        if not localids and not forceuid and analysis.getIsReflexAnalysis():\n            return self._areConditionsMet(action_set, analysis, forceuid=True)\n        # action_set will not have any action for this analysis\n        elif not localids:\n            return False\n        # Getting the action_set.rulenumber in order to check the\n        # analysis.ReflexRuleActionsTriggered\n        rulenumber = action_set.get('rulenumber', '')\n        # Getting the reflex rules uid in order to fill the\n        # analysis.ReflexRuleActionsTriggered attribute later\n        rr_uid = self.UID()\n        # Building the possible analysis.ReflexRuleActionsTriggered\n        rr_actions_triggered = '.'.join([rr_uid, rulenumber])\n        # If the follow condition is met, it means that the action_set for this\n        # analysis has already been done by any other analysis from the\n        # action_set. (e.g analsys.local_id =='dup-2', but this action has been\n        # ran by the analysis with local_id=='dup-1', so we do not have to\n        # run it again)\n        if rr_actions_triggered in\\\n                analysis.getReflexRuleActionsTriggered().split('|'):\n            return False\n        # Check that rules are not repited: lets supose that some conditions\n        # are met and an analysis with id analysis-1 is reflexed using a\n        # duplicate action. Now we have analysis-1 and analysis1-dup. If the\n        # same conditions are met while submitting/verifying analysis1-dup, the\n        # duplicated shouldn't trigger the reflex action again.\n        if forceuid and analysis.IsReflexAnalysis and rr_actions_triggered in\\\n                analysis.getReflexRuleActionsTriggered().split('|'):\n            return False\n        # To save the analysis related in the same action_set\n        ans_related_to_set = []\n        for condition in conditions:\n            # analysisservice can be either a service uid (if it is the first\n            # condition in the reflex rule) or it could be a local id such\n            # as 'dup-2' if the analysis_set belongs to a derivate rule.\n            ans_cond = condition.get('analysisservice', '')\n            ans_uid_cond = action_set.get('mother_service_uid', '')\n            # Be aware that we already know that the local_id for 'analysis'\n            # has been found inside the conditions for this action_set\n            if ans_cond != alocalid:\n                # If the 'analysisservice' item from the condition is not the\n                # same as the local_id from the analysis, the system\n                # should look for the possible analysis with this localid\n                # (e.g dup-2) and get its analysis object in order to compare\n                # the results of the 'analysis variable' with the analysis\n                # object obtained here\n                curranalysis = _fetch_analysis_for_local_id(\n                    analysis, ans_cond)\n            else:\n                # if the local_id of the 'analysis' is the same as the\n                # local_id in the condition, we will use it as the current\n                # analysis\n                curranalysis = analysis\n            if not curranalysis:\n                continue\n            ans_related_to_set.append(curranalysis)\n            # the value of the analysis' result as string\n            result = curranalysis.getResult()\n            if len(analysis.getResultOptions()) > 0:\n                # Discrete result as expacted value\n                exp_val = condition.get('discreteresult', '')\n            else:\n                exp_val = (\n                    condition.get('range0', ''),\n                    condition.get('range1', '')\n                    )\n            and_or = condition.get('and_or', '')\n            service_uid = curranalysis.getServiceUID()\n\n            # Resolve the conditions\n            resolution = \\\n                ans_uid_cond == service_uid and\\\n                ((isnumber(result) and isinstance(exp_val, str) and\n                    exp_val == result) or\n                    (isnumber(result) and len(exp_val) == 2 and\n                        isnumber(exp_val[0]) and isnumber(exp_val[1]) and\n                        float(exp_val[0]) <= float(result) and\n                        float(result) <= float(exp_val[1])))\n            # Build a string and then use eval()\n            if and_or == 'no':\n                eval_str += str(resolution)\n            else:\n                eval_str += str(resolution) + ' ' + and_or + ' '\n        if eval_str and eval(eval_str):\n            for an in ans_related_to_set:\n                an.addReflexRuleActionsTriggered(rr_actions_triggered)\n            return True\n        else:\n            return False", "response": "This function checks if the conditions in the action_set are met and returns a boolean as True if the conditions are met."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef workflow_script_import(self):\n        bsc = getToolByName(self, 'bika_setup_catalog')\n        client = self.aq_parent\n\n        title = _('Submitting Sample Import')\n        description = _('Creating and initialising objects')\n        bar = ProgressBar(self, self.REQUEST, title, description)\n        notify(InitialiseProgressBar(bar))\n\n        profiles = [x.getObject() for x in bsc(portal_type='AnalysisProfile')]\n\n        gridrows = self.schema['SampleData'].get(self)\n        row_cnt = 0\n        for therow in gridrows:\n            row = deepcopy(therow)\n            row_cnt += 1\n\n            # Profiles are titles, profile keys, or UIDS: convert them to UIDs.\n            newprofiles = []\n            for title in row['Profiles']:\n                objects = [x for x in profiles\n                           if title in (x.getProfileKey(), x.UID(), x.Title())]\n                for obj in objects:\n                    newprofiles.append(obj.UID())\n            row['Profiles'] = newprofiles\n\n            # Same for analyses\n            newanalyses = set(self.get_row_services(row) +\n                              self.get_row_profile_services(row))\n            # get batch\n            batch = self.schema['Batch'].get(self)\n            if batch:\n                row['Batch'] = batch.UID()\n            # Add AR fields from schema into this row's data\n            row['ClientReference'] = self.getClientReference()\n            row['ClientOrderNumber'] = self.getClientOrderNumber()\n            contact_uid =\\\n                self.getContact().UID() if self.getContact() else None\n            row['Contact'] = contact_uid\n            # Creating analysis request from gathered data\n            ar = create_analysisrequest(\n                client,\n                self.REQUEST,\n                row,\n                analyses=list(newanalyses),)\n\n            # progress marker update\n            progress_index = float(row_cnt) / len(gridrows) * 100\n            progress = ProgressState(self.REQUEST, progress_index)\n            notify(UpdateProgressEvent(progress))\n\n        # document has been written to, and redirect() fails here\n        self.REQUEST.response.write(\n            '<script>document.location.href=\"%s\"</script>' % (\n                self.absolute_url()))", "response": "Create objects from valid ARImport\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_header_values(self):\n        lines = self.getOriginalFile().data.splitlines()\n        reader = csv.reader(lines)\n        header_fields = header_data = []\n        for row in reader:\n            if not any(row):\n                continue\n            if row[0].strip().lower() == 'header':\n                header_fields = [x.strip() for x in row][1:]\n                continue\n            if row[0].strip().lower() == 'header data':\n                header_data = [x.strip() for x in row][1:]\n                break\n        if not (header_data or header_fields):\n            return None\n        if not (header_data and header_fields):\n            self.error(\"File is missing header row or header data\")\n            return None\n        # inject us out of here\n        values = dict(zip(header_fields, header_data))\n        # blank cell from sheet will probably make it in here:\n        if '' in values:\n            del (values[''])\n        return values", "response": "Scrape the Header values from the original input file\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves values from the file s header row into their schema fields.", "response": "def save_header_data(self):\n        \"\"\"Save values from the file's header row into their schema fields.\n        \"\"\"\n        client = self.aq_parent\n\n        headers = self.get_header_values()\n        if not headers:\n            return False\n\n        # Plain header fields that can be set into plain schema fields:\n        for h, f in [\n            ('File name', 'Filename'),\n            ('No of Samples', 'NrSamples'),\n            ('Client name', 'ClientName'),\n            ('Client ID', 'ClientID'),\n            ('Client Order Number', 'ClientOrderNumber'),\n            ('Client Reference', 'ClientReference')\n        ]:\n            v = headers.get(h, None)\n            if v:\n                field = self.schema[f]\n                field.set(self, v)\n            del (headers[h])\n\n        # Primary Contact\n        v = headers.get('Contact', None)\n        contacts = [x for x in client.objectValues('Contact')]\n        contact = [c for c in contacts if c.Title() == v]\n        if contact:\n            self.schema['Contact'].set(self, contact)\n        else:\n            self.error(\"Specified contact '%s' does not exist; using '%s'\"%\n                       (v, contacts[0].Title()))\n            self.schema['Contact'].set(self, contacts[0])\n        del (headers['Contact'])\n\n        # CCContacts\n        field_value = {\n            'CCNamesReport': '',\n            'CCEmailsReport': '',\n            'CCNamesInvoice': '',\n            'CCEmailsInvoice': ''\n        }\n        for h, f in [\n            # csv header name      DataGrid Column ID\n            ('CC Names - Report', 'CCNamesReport'),\n            ('CC Emails - Report', 'CCEmailsReport'),\n            ('CC Names - Invoice', 'CCNamesInvoice'),\n            ('CC Emails - Invoice', 'CCEmailsInvoice'),\n        ]:\n            if h in headers:\n                values = [x.strip() for x in headers.get(h, '').split(\",\")]\n                field_value[f] = values if values else ''\n                del (headers[h])\n        self.schema['CCContacts'].set(self, [field_value])\n\n        if headers:\n            unexpected = ','.join(headers.keys())\n            self.error(\"Unexpected header fields: %s\" % unexpected)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_sample_values(self):\n        res = {'samples': []}\n        lines = self.getOriginalFile().data.splitlines()\n        reader = csv.reader(lines)\n        next_rows_are_sample_rows = False\n        for row in reader:\n            if not any(row):\n                continue\n            if next_rows_are_sample_rows:\n                vals = [x.strip() for x in row]\n                if not any(vals):\n                    continue\n                res['samples'].append(zip(res['headers'], vals))\n            elif row[0].strip().lower() == 'samples':\n                res['headers'] = [x.strip() for x in row]\n            elif row[0].strip().lower() == 'analysis price':\n                res['prices'] = \\\n                    zip(res['headers'], [x.strip() for x in row])\n            elif row[0].strip().lower() == 'total analyses':\n                res['total_analyses'] = \\\n                    zip(res['headers'], [x.strip() for x in row])\n            elif row[0].strip().lower() == 'total price excl tax':\n                res['price_totals'] = \\\n                    zip(res['headers'], [x.strip() for x in row])\n                next_rows_are_sample_rows = True\n        return res", "response": "Read the rows specifying Samples and return a dictionary with the keys of the with\n            related data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save_sample_data(self):\n        bsc = getToolByName(self, 'bika_setup_catalog')\n        keywords = self.bika_setup_catalog.uniqueValuesFor('getKeyword')\n        profiles = []\n        for p in bsc(portal_type='AnalysisProfile'):\n            p = p.getObject()\n            profiles.append(p.Title())\n            profiles.append(p.getProfileKey())\n\n        sample_data = self.get_sample_values()\n        if not sample_data:\n            return False\n\n        # columns that we expect, but do not find, are listed here.\n        # we report on them only once, after looping through sample rows.\n        missing = set()\n\n        # This contains all sample header rows that were not handled\n        # by this code\n        unexpected = set()\n\n        # Save other errors here instead of sticking them directly into\n        # the field, so that they show up after MISSING and before EXPECTED\n        errors = []\n\n        # This will be the new sample-data field value, when we are done.\n        grid_rows = []\n\n        row_nr = 0\n        for row in sample_data['samples']:\n            row = dict(row)\n            row_nr += 1\n\n            # sid is just for referring the user back to row X in their\n            # in put spreadsheet\n            gridrow = {'sid': row['Samples']}\n            del (row['Samples'])\n\n            # We'll use this later to verify the number against selections\n            if 'Total number of Analyses or Profiles' in row:\n                nr_an = row['Total number of Analyses or Profiles']\n                del (row['Total number of Analyses or Profiles'])\n            else:\n                nr_an = 0\n            try:\n                nr_an = int(nr_an)\n            except ValueError:\n                nr_an = 0\n\n            # TODO this is ignored and is probably meant to serve some purpose.\n            del (row['Price excl Tax'])\n\n            # ContainerType - not part of sample or AR schema\n            if 'ContainerType' in row:\n                title = row['ContainerType']\n                if title:\n                    obj = self.lookup(('ContainerType',),\n                                      Title=row['ContainerType'])\n                    if obj:\n                        gridrow['ContainerType'] = obj[0].UID\n                del (row['ContainerType'])\n\n            if 'SampleMatrix' in row:\n                # SampleMatrix - not part of sample or AR schema\n                title = row['SampleMatrix']\n                if title:\n                    obj = self.lookup(('SampleMatrix',),\n                                      Title=row['SampleMatrix'])\n                    if obj:\n                        gridrow['SampleMatrix'] = obj[0].UID\n                del (row['SampleMatrix'])\n\n            # match against sample schema\n            for k, v in row.items():\n                if k in ['Analyses', 'Profiles']:\n                    continue\n                if k in sample_schema:\n                    del (row[k])\n                    if v:\n                        try:\n                            value = self.munge_field_value(\n                                sample_schema, row_nr, k, v)\n                            gridrow[k] = value\n                        except ValueError as e:\n                            errors.append(e.message)\n\n            # match against ar schema\n            for k, v in row.items():\n                if k in ['Analyses', 'Profiles']:\n                    continue\n                if k in ar_schema:\n                    del (row[k])\n                    if v:\n                        try:\n                            value = self.munge_field_value(\n                                ar_schema, row_nr, k, v)\n                            gridrow[k] = value\n                        except ValueError as e:\n                            errors.append(e.message)\n\n            # Count and remove Keywords and Profiles from the list\n            gridrow['Analyses'] = []\n            for k, v in row.items():\n                if k in keywords:\n                    del (row[k])\n                    if str(v).strip().lower() not in ('', '0', 'false'):\n                        gridrow['Analyses'].append(k)\n            gridrow['Profiles'] = []\n            for k, v in row.items():\n                if k in profiles:\n                    del (row[k])\n                    if str(v).strip().lower() not in ('', '0', 'false'):\n                        gridrow['Profiles'].append(k)\n            if len(gridrow['Analyses']) + len(gridrow['Profiles']) != nr_an:\n                errors.append(\n                    \"Row %s: Number of analyses does not match provided value\" %\n                    row_nr)\n\n            grid_rows.append(gridrow)\n\n        self.setSampleData(grid_rows)\n\n        if missing:\n            self.error(\"SAMPLES: Missing expected fields: %s\" %\n                       ','.join(missing))\n\n        for thing in errors:\n            self.error(thing)\n\n        if unexpected:\n            self.error(\"Unexpected header fields: %s\" %\n                       ','.join(unexpected))", "response": "Save values from the file s header row into the DataGrid columns\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_batch_header_values(self):\n        lines = self.getOriginalFile().data.splitlines()\n        reader = csv.reader(lines)\n        batch_headers = batch_data = []\n        for row in reader:\n            if not any(row):\n                continue\n            if row[0].strip().lower() == 'batch header':\n                batch_headers = [x.strip() for x in row][1:]\n                continue\n            if row[0].strip().lower() == 'batch data':\n                batch_data = [x.strip() for x in row][1:]\n                break\n        if not (batch_data or batch_headers):\n            return None\n        if not (batch_data and batch_headers):\n            self.error(\"Missing batch headers or data\")\n            return None\n        # Inject us out of here\n        values = dict(zip(batch_headers, batch_data))\n        return values", "response": "Scrape the Batch Header values from the original input file\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates or reference a batch if possible with specified values.", "response": "def create_or_reference_batch(self):\n        \"\"\"Save reference to batch, if existing batch specified\n        Create new batch, if possible with specified values\n        \"\"\"\n        client = self.aq_parent\n        batch_headers = self.get_batch_header_values()\n        if not batch_headers:\n            return False\n        # if the Batch's Title is specified and exists, no further\n        # action is required. We will just set the Batch field to\n        # use the existing object.\n        batch_title = batch_headers.get('title', False)\n        if batch_title:\n            existing_batch = [x for x in client.objectValues('Batch')\n                              if x.title == batch_title]\n            if existing_batch:\n                self.setBatch(existing_batch[0])\n                return existing_batch[0]\n        # If the batch title is specified but does not exist,\n        # we will attempt to create the bach now.\n        if 'title' in batch_headers:\n            if 'id' in batch_headers:\n                del (batch_headers['id'])\n            if '' in batch_headers:\n                del (batch_headers[''])\n            batch = _createObjectByType('Batch', client, tmpID())\n            batch.processForm()\n            batch.edit(**batch_headers)\n            self.setBatch(batch)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_headers(self):\n\n        pc = getToolByName(self, 'portal_catalog')\n        pu = getToolByName(self, \"plone_utils\")\n\n        client = self.aq_parent\n\n        # Verify Client Name\n        if self.getClientName() != client.Title():\n            self.error(\"%s: value is invalid (%s).\" % (\n                'Client name', self.getClientName()))\n\n        # Verify Client ID\n        if self.getClientID() != client.getClientID():\n            self.error(\"%s: value is invalid (%s).\" % (\n                'Client ID', self.getClientID()))\n\n        existing_arimports = pc(portal_type='ARImport',\n                                review_state=['valid', 'imported'])\n        # Verify Client Order Number\n        for arimport in existing_arimports:\n            if arimport.UID == self.UID() \\\n                    or not arimport.getClientOrderNumber():\n                continue\n            arimport = arimport.getObject()\n\n            if arimport.getClientOrderNumber() == self.getClientOrderNumber():\n                self.error('%s: already used by existing ARImport.' %\n                           'ClientOrderNumber')\n                break\n\n        # Verify Client Reference\n        for arimport in existing_arimports:\n            if arimport.UID == self.UID() \\\n                    or not arimport.getClientReference():\n                continue\n            arimport = arimport.getObject()\n            if arimport.getClientReference() == self.getClientReference():\n                self.error('%s: already used by existing ARImport.' %\n                           'ClientReference')\n                break\n\n        # getCCContacts has no value if object is not complete (eg during test)\n        if self.getCCContacts():\n            cc_contacts = self.getCCContacts()[0]\n            contacts = [x for x in client.objectValues('Contact')]\n            contact_names = [c.Title() for c in contacts]\n            # validate Contact existence in this Client\n            for k in ['CCNamesReport', 'CCNamesInvoice']:\n                for val in cc_contacts[k]:\n                    if val and val not in contact_names:\n                        self.error('%s: value is invalid (%s)' % (k, val))\n        else:\n            cc_contacts = {'CCNamesReport': [],\n                           'CCEmailsReport': [],\n                           'CCNamesInvoice': [],\n                           'CCEmailsInvoice': []\n                           }\n            # validate Contact existence in this Client\n            for k in ['CCEmailsReport', 'CCEmailsInvoice']:\n                for val in cc_contacts.get(k, []):\n                    if val and not pu.validateSingleNormalizedEmailAddress(val):\n                        self.error('%s: value is invalid (%s)' % (k, val))", "response": "Validate the headers fields from schema\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nscan through the SampleData values and make sure that each one is correct", "response": "def validate_samples(self):\n        \"\"\"Scan through the SampleData values and make sure\n        that each one is correct\n        \"\"\"\n\n        bsc = getToolByName(self, 'bika_setup_catalog')\n        keywords = bsc.uniqueValuesFor('getKeyword')\n        profiles = []\n        for p in bsc(portal_type='AnalysisProfile'):\n            p = p.getObject()\n            profiles.append(p.Title())\n            profiles.append(p.getProfileKey())\n\n        row_nr = 0\n        for gridrow in self.getSampleData():\n            row_nr += 1\n\n            # validate against sample and ar schemas\n            for k, v in gridrow.items():\n                if k in ['Analysis', 'Profiles']:\n                    break\n                if k in sample_schema:\n                    try:\n                        self.validate_against_schema(\n                            sample_schema, row_nr, k, v)\n                        continue\n                    except ValueError as e:\n                        self.error(e.message)\n                        break\n                if k in ar_schema:\n                    try:\n                        self.validate_against_schema(\n                            ar_schema, row_nr, k, v)\n                    except ValueError as e:\n                        self.error(e.message)\n\n            an_cnt = 0\n            for v in gridrow['Analyses']:\n                if v and v not in keywords:\n                    self.error(\"Row %s: value is invalid (%s=%s)\" %\n                               ('Analysis keyword', row_nr, v))\n                else:\n                    an_cnt += 1\n            for v in gridrow['Profiles']:\n                if v and v not in profiles:\n                    self.error(\"Row %s: value is invalid (%s=%s)\" %\n                               ('Profile Title', row_nr, v))\n                else:\n                    an_cnt += 1\n            if not an_cnt:\n                self.error(\"Row %s: No valid analyses or profiles\" % row_nr)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lookup(self, allowed_types, **kwargs):\n        at = getToolByName(self, 'archetype_tool')\n        for portal_type in allowed_types:\n            catalog = at.catalog_map.get(portal_type, [None])[0]\n            catalog = getToolByName(self, catalog)\n            kwargs['portal_type'] = portal_type\n            brains = catalog(**kwargs)\n            if brains:\n                return brains", "response": "Lookup an object of type allowed_types. kwargs is sent\n        directly to the catalog."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of services which are referenced in Analyses. Analyses are referenced in Analyses. Analyses are referenced in Analyses. Keyword are used to identify services which are referenced in Analyses. Title or Keyword.", "response": "def get_row_services(self, row):\n        \"\"\"Return a list of services which are referenced in Analyses.\n        values may be UID, Title or Keyword.\n        \"\"\"\n        bsc = getToolByName(self, 'bika_setup_catalog')\n        services = set()\n        for val in row.get('Analyses', []):\n            brains = bsc(portal_type='AnalysisService', getKeyword=val)\n            if not brains:\n                brains = bsc(portal_type='AnalysisService', title=val)\n            if not brains:\n                brains = bsc(portal_type='AnalysisService', UID=val)\n            if brains:\n                services.add(brains[0].UID)\n            else:\n                self.error(\"Invalid analysis specified: %s\" % val)\n        return list(services)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_row_profile_services(self, row):\n        bsc = getToolByName(self, 'bika_setup_catalog')\n        services = set()\n        profiles = [x.getObject() for x in bsc(portal_type='AnalysisProfile')]\n        for val in row.get('Profiles', []):\n            objects = [x for x in profiles\n                       if val in (x.getProfileKey(), x.UID(), x.Title())]\n            if objects:\n                for service in objects[0].getService():\n                    services.add(service.UID())\n            else:\n                self.error(\"Invalid profile specified: %s\" % val)\n        return list(services)", "response": "Return a list of services which are referenced in profiles\n                       values may be UID Title or ProfileKey."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a sample container from a row", "response": "def get_row_container(self, row):\n        \"\"\"Return a sample container\n        \"\"\"\n        bsc = getToolByName(self, 'bika_setup_catalog')\n        val = row.get('Container', False)\n        if val:\n            brains = bsc(portal_type='Container', UID=row['Container'])\n            if brains:\n                brains[0].getObject()\n            brains = bsc(portal_type='ContainerType', UID=row['Container'])\n            if brains:\n                # XXX Cheating.  The calculation of capacity vs. volume  is not done.\n                return brains[0].getObject()\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npresents a DisplayList containing users in the specified list of roles", "response": "def getUsers(context, roles, allow_empty=True):\n    \"\"\" Present a DisplayList containing users in the specified\n        list of roles\n    \"\"\"\n    mtool = getToolByName(context, 'portal_membership')\n    pairs = allow_empty and [['', '']] or []\n    users = mtool.searchForMembers(roles=roles)\n    for user in users:\n        uid = user.getId()\n        fullname = user.getProperty('fullname')\n        if not fullname:\n            fullname = uid\n        pairs.append((uid, fullname))\n    pairs.sort(lambda x, y: cmp(x[1], y[1]))\n    return DisplayList(pairs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nobtain and reformat the from and to dates into a date query construct", "response": "def formatDateQuery(context, date_id):\n    \"\"\" Obtain and reformat the from and to dates\n        into a date query construct\n    \"\"\"\n    from_date = context.REQUEST.get('%s_fromdate' % date_id, None)\n    if from_date:\n        from_date = from_date + ' 00:00'\n    to_date = context.REQUEST.get('%s_todate' % date_id, None)\n    if to_date:\n        to_date = to_date + ' 23:59'\n\n    date_query = {}\n    if from_date and to_date:\n        date_query = {'query': [from_date, to_date],\n                      'range': 'min:max'}\n    elif from_date or to_date:\n        date_query = {'query': from_date or to_date,\n                      'range': from_date and 'min' or 'max'}\n\n    return date_query"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nobtains and reformat the from and to dates into a printable date parameter construct", "response": "def formatDateParms(context, date_id):\n    \"\"\" Obtain and reformat the from and to dates\n        into a printable date parameter construct\n    \"\"\"\n    from_date = context.REQUEST.get('%s_fromdate' % date_id, None)\n    to_date = context.REQUEST.get('%s_todate' % date_id, None)\n\n    date_parms = {}\n    if from_date and to_date:\n        date_parms = 'from %s to %s' % (from_date, to_date)\n    elif from_date:\n        date_parms = 'from %s' % (from_date)\n    elif to_date:\n        date_parms = 'to %s' % (to_date)\n\n    return date_parms"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef formatDecimalMark(value, decimalmark='.'):\n    # We have to consider the possibility of working with decimals such as\n    # X.000 where those decimals are important because of the precission\n    # and significant digits matters\n    # Using 'float' the system delete the extre desimals with 0 as a value\n    # Example: float(2.00) -> 2.0\n    # So we have to save the decimal length, this is one reason we are usnig\n    # strings for results\n    rawval = str(value)\n    try:\n        return decimalmark.join(rawval.split('.'))\n    except:\n        return rawval", "response": "Format a value into a string with a decimal mark"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nencode a HTTP header in quoted - printable encoding.", "response": "def encode_header(header, charset='utf-8'):\n    \"\"\" Will encode in quoted-printable encoding only if header\n    contains non latin characters\n    \"\"\"\n\n    # Return empty headers unchanged\n    if not header:\n        return header\n\n    # return plain header if it does not contain non-ascii characters\n    if hqre.match(header):\n        return header\n\n    quoted = ''\n    # max_encoded = 76 - len(charset) - 7\n    for c in header:\n        # Space may be represented as _ instead of =20 for readability\n        if c == ' ':\n            quoted += '_'\n        # These characters can be included verbatim\n        elif hqre.match(c):\n            quoted += c\n        # Otherwise, replace with hex value like =E2\n        else:\n            quoted += \"=%02X\" % ord(c)\n\n    return '=?%s?q?%s?=' % (charset, quoted)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sortable_title(portal, title):\n    if not title:\n        return ''\n\n    def_charset = portal.plone_utils.getSiteEncoding()\n    sortabletitle = str(title.lower().strip())\n    # Replace numbers with zero filled numbers\n    sortabletitle = num_sort_regex.sub(zero_fill, sortabletitle)\n    # Truncate to prevent bloat\n    for charset in [def_charset, 'latin-1', 'utf-8']:\n        try:\n            sortabletitle = safe_unicode(sortabletitle, charset)[:30]\n            sortabletitle = sortabletitle.encode(def_charset or 'utf-8')\n            break\n        except UnicodeError:\n            pass\n        except TypeError:\n            # If we get a TypeError if we already have a unicode string\n            sortabletitle = sortabletitle[:30]\n            break\n    return sortabletitle", "response": "Convert title to sortable title\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchanging the state of an object in the given workflow", "response": "def changeWorkflowState(content, wf_id, state_id, **kw):\n    \"\"\"Change the workflow state of an object\n    @param content: Content obj which state will be changed\n    @param state_id: name of the state to put on content\n    @param kw: change the values of same name of the state mapping\n    @return: True if succeed. Otherwise, False\n    \"\"\"\n    portal_workflow = api.get_tool(\"portal_workflow\")\n    workflow = portal_workflow.getWorkflowById(wf_id)\n    if not workflow:\n        logger.error(\"%s: Cannot find workflow id %s\" % (content, wf_id))\n        return False\n\n    wf_state = {\n        'action': kw.get(\"action\", None),\n        'actor': kw.get(\"actor\", api.get_current_user().id),\n        'comments': \"Setting state to %s\" % state_id,\n        'review_state': state_id,\n        'time': DateTime()\n    }\n\n    # Change status and update permissions\n    portal_workflow.setStatusOf(wf_id, content, wf_state)\n    workflow.updateRoleMappingsFor(content)\n\n    # Map changes to catalog\n    indexes = [\"allowedRolesAndUsers\", \"review_state\", \"is_active\"]\n    content.reindexObject(idxs=indexes)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef senaite_url_fetcher(url):\n\n    logger.info(\"Fetching URL '{}' for WeasyPrint\".format(url))\n\n    # get the pyhsical path from the URL\n    request = api.get_request()\n    host = request.get_header(\"HOST\")\n    path = \"/\".join(request.physicalPathFromURL(url))\n\n    # fetch the object by sub-request\n    portal = api.get_portal()\n    context = portal.restrictedTraverse(path, None)\n\n    # We double check here to avoid an edge case, where we have the same path\n    # as well in our local site, e.g. we have `/senaite/img/systems/senaite.png`,\n    # but the user requested http://www.ridingbytes.com/img/systems/senaite.png:\n    #\n    # \"/\".join(request.physicalPathFromURL(\"http://www.ridingbytes.com/img/systems/senaite.png\"))\n    # '/senaite/img/systems/senaite.png'\n    if context is None or host not in url:\n        logger.info(\"URL is external, passing over to the default URL fetcher...\")\n        return default_url_fetcher(url)\n\n    logger.info(\"URL is local, fetching data by path '{}' via subrequest\".format(path))\n\n    # get the data via an authenticated subrequest\n    response = subrequest(path)\n\n    # Prepare the return data as required by WeasyPrint\n    string = response.getBody()\n    filename = url.split(\"/\")[-1]\n    mime_type = mimetypes.guess_type(url)[0]\n    redirected_url = url\n\n    return {\n        \"string\": string,\n        \"filename\": filename,\n        \"mime_type\": mime_type,\n        \"redirected_url\": redirected_url,\n    }", "response": "Uses plone. request to fetch an internal image resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a PDF from some HTML report", "response": "def createPdf(htmlreport, outfile=None, css=None, images={}):\n    \"\"\"create a PDF from some HTML.\n    htmlreport: rendered html\n    outfile: pdf filename; if supplied, caller is responsible for creating\n             and removing it.\n    css: remote URL of css file to download\n    images: A dictionary containing possible URLs (keys) and local filenames\n            (values) with which they may to be replaced during rendering.\n    # WeasyPrint will attempt to retrieve images directly from the URL\n    # referenced in the HTML report, which may refer back to a single-threaded\n    # (and currently occupied) zeoclient, hanging it.  All image source\n    # URL's referenced in htmlreport should be local files.\n    \"\"\"\n    # A list of files that should be removed after PDF is written\n    cleanup = []\n    css_def = ''\n    if css:\n        if css.startswith(\"http://\") or css.startswith(\"https://\"):\n            # Download css file in temp dir\n            u = urllib2.urlopen(css)\n            _cssfile = tempfile.mktemp(suffix='.css')\n            localFile = open(_cssfile, 'w')\n            localFile.write(u.read())\n            localFile.close()\n            cleanup.append(_cssfile)\n        else:\n            _cssfile = css\n        cssfile = open(_cssfile, 'r')\n        css_def = cssfile.read()\n\n    htmlreport = to_utf8(htmlreport)\n\n    for (key, val) in images.items():\n        htmlreport = htmlreport.replace(key, val)\n\n    # render\n    htmlreport = to_utf8(htmlreport)\n    renderer = HTML(string=htmlreport, url_fetcher=senaite_url_fetcher, encoding='utf-8')\n    pdf_fn = outfile if outfile else tempfile.mktemp(suffix=\".pdf\")\n    if css:\n        renderer.write_pdf(pdf_fn, stylesheets=[CSS(string=css_def)])\n    else:\n        renderer.write_pdf(pdf_fn)\n    # return file data\n    pdf_data = open(pdf_fn, \"rb\").read()\n    if outfile is None:\n        os.remove(pdf_fn)\n    for fn in cleanup:\n        os.remove(fn)\n    return pdf_data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dicts_to_dict(dictionaries, key_subfieldname):\n    result = {}\n    for d in dictionaries:\n        result[d[key_subfieldname]] = d\n    return result", "response": "Convert a list of dictionaries into a dictionary of dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntransform the text adding sub and super html scripts and return the formatted text.", "response": "def format_supsub(text):\n    \"\"\"\n    Mainly used for Analysis Service's unit. Transform the text adding\n    sub and super html scripts:\n    For super-scripts, use ^ char\n    For sub-scripts, use _ char\n    The expression \"cm^2\" will be translated to \"cm\u00b2\" and the\n    expression \"b_(n-1)\" will be translated to \"b n-1\".\n    The expression \"n_(fibras)/cm^3\" will be translated as\n    \"n fibras / cm\u00b3\"\n    :param text: text to be formatted\n    \"\"\"\n    out = []\n    subsup = []\n    clauses = []\n    insubsup = True\n    for c in str(text):\n        if c == '(':\n            if insubsup is False:\n                out.append(c)\n                clauses.append(')')\n            else:\n                clauses.append('')\n\n        elif c == ')':\n            if len(clauses) > 0:\n                out.append(clauses.pop())\n                if len(subsup) > 0:\n                    out.append(subsup.pop())\n\n        elif c == '^':\n            subsup.append('</sup>')\n            out.append('<sup>')\n            insubsup = True\n            continue\n\n        elif c == '_':\n            subsup.append('</sub>')\n            out.append('<sub>')\n            insubsup = True\n            continue\n\n        elif c == ' ':\n            if insubsup is True:\n                out.append(subsup.pop())\n            else:\n                out.append(c)\n        elif c in ['+', '-']:\n            if len(clauses) == 0 and len(subsup) > 0:\n                out.append(subsup.pop())\n            out.append(c)\n        else:\n            out.append(c)\n\n        insubsup = False\n\n    while True:\n        if len(subsup) == 0:\n            break\n        out.append(subsup.pop())\n\n    return ''.join(out)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndrops the trailinz zeros from decimal value. Returns a string.", "response": "def drop_trailing_zeros_decimal(num):\n    \"\"\" Drops the trailinz zeros from decimal value.\n        Returns a string\n    \"\"\"\n    out = str(num)\n    return out.rstrip('0').rstrip('.') if '.' in out else out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if a user has the given permissions for a given object.", "response": "def checkPermissions(permissions=[], obj=None):\n    \"\"\"\n    Checks if a user has permissions for a given object.\n\n    Args:\n        permissions: The permissions the current user must be compliant with\n        obj: The object for which the permissions apply\n\n    Returns:\n        1 if the user complies with all the permissions for the given object.\n        Otherwise, it returns empty.\n    \"\"\"\n    if not obj:\n        return False\n    sm = getSecurityManager()\n    for perm in permissions:\n        if not sm.checkPermission(perm, obj):\n            return ''\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_link(href, value=None, **kwargs):\n    if not href:\n        return \"\"\n    anchor_value = value and value or href\n    attr = render_html_attributes(**kwargs)\n    return '<a href=\"{}\" {}>{}</a>'.format(href, attr, anchor_value)", "response": "Returns a well - formed link."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_email_link(email, value=None):\n    if not email:\n        return \"\"\n    mailto = 'mailto:{}'.format(email)\n    link_value = value and value or email\n    return get_link(mailto, link_value)", "response": "Returns a well - formed link to an email address."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a well - formed image", "response": "def get_image(name, **kwargs):\n    \"\"\"Returns a well-formed image\n    :param name: file name of the image\n    :param kwargs: additional attributes and values\n    :return: a well-formed html img\n    \"\"\"\n    if not name:\n        return \"\"\n    portal_url = api.get_url(api.get_portal())\n    attr = render_html_attributes(**kwargs)\n    html = '<img src=\"{}/++resource++bika.lims.images/{}\" {}/>'\n    return html.format(portal_url, name, attr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef render_html_attributes(**kwargs):\n    attr = list()\n    if kwargs:\n        attr = ['{}=\"{}\"'.format(key, val) for key, val in kwargs.items()]\n    return \" \".join(attr).replace(\"css_class\", \"class\")", "response": "Returns a well - formed string representation of attributes for html entities\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the utility for IRegistry and returns the value for the key passed in.", "response": "def get_registry_value(key, default=None):\n    \"\"\"\n    Gets the utility for IRegistry and returns the value for the key passed in.\n    If there is no value for the key passed in, returns default value\n    :param key: the key in the registry to look for\n    :param default: default value if the key is not registered\n    :return: value in the registry for the key passed in\n    \"\"\"\n    registry = queryUtility(IRegistry)\n    return registry.get(key, default)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if the current user has rights for the permission passed in against the object passed in obj", "response": "def check_permission(permission, obj):\n    \"\"\"\n    Returns if the current user has rights for the permission passed in against\n    the obj passed in\n    :param permission: name of the permission\n    :param obj: the object to check the permission against for the current user\n    :return: 1 if the user has rights for this permission for the passed in obj\n    \"\"\"\n    mtool = api.get_tool('portal_membership')\n    object = api.get_object(obj)\n    return mtool.checkPermission(permission, object)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_int(value, default=0):\n    try:\n        return int(value)\n    except (TypeError, ValueError):\n        return to_int(default, default=0)", "response": "Tries to convert the value passed in as an int. If no conversion succeeds returns the default value passed in."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_strings(data):\n    # if this is a unicode string, return its string representation\n    if isinstance(data, unicode):\n        return data.encode('utf-8')\n\n    # if this is a list of values, return list of string values\n    if isinstance(data, list):\n        return [get_strings(item) for item in data]\n\n    # if this is a dictionary, return dictionary of string keys and values\n    if isinstance(data, dict):\n        return {\n            get_strings(key): get_strings(value)\n            for key, value in data.iteritems()\n        }\n    # if it's anything else, return it in its original form\n    return data", "response": "Convert unicode values to strings even if they belong to lists or dicts."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting string values to unicode even if they belong to lists or dicts.", "response": "def get_unicode(data):\n    \"\"\"\n    Convert string values to unicode even if they belong to lists or dicts.\n    :param data: an object.\n    :return: The object with all string values converted to unicode.\n    \"\"\"\n    # if this is a common string, return its unicode representation\n    if isinstance(data, str):\n        return safe_unicode(data)\n\n    # if this is a list of values, return list of unicode values\n    if isinstance(data, list):\n        return [get_unicode(item) for item in data]\n\n    # if this is a dictionary, return dictionary of unicode keys and values\n    if isinstance(data, dict):\n        return {\n            get_unicode(key): get_unicode(value)\n            for key, value in data.iteritems()\n        }\n    # if it's anything else, return it in its original form\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a DisplayList with the items sorted by TitlebyTitle ascending", "response": "def get_display_list(brains_or_objects=None, none_item=False):\n    \"\"\"\n    Returns a DisplayList with the items sorted by Title\n    :param brains_or_objects: list of brains or objects\n    :param none_item: adds an item with empty uid and text \"Select..\" in pos 0\n    :return: DisplayList (uid, title) sorted by title ascending\n    :rtype: DisplayList\n    \"\"\"\n    if brains_or_objects is None:\n        return get_display_list(list(), none_item)\n\n    items = list()\n    for brain in brains_or_objects:\n        uid = api.get_uid(brain)\n        if not uid:\n            continue\n        title = api.get_title(brain)\n        items.append((uid, title))\n\n    # Sort items by title ascending\n    items.sort(lambda x, y: cmp(x[1], y[1]))\n\n    # Add the first item?\n    if none_item:\n        items.insert(0, ('', t('Select...')))\n\n    return DisplayList(items)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_data_line(self, sline):\n        # if there are less values founded than headers, it's an error\n        if len(sline) != len(self._columns):\n            self.err(\"One data line has the wrong number of items\")\n            return -1\n        values = {}\n        remark = ''\n        date = ''\n        resid = ''\n        for idx, result in enumerate(sline):\n            if self._columns[idx] == 'Date':\n                date = self.csvDate2BikaDate(result)\n            elif self._columns[idx] == 'Patient no.':\n                resid = result\n            elif self._columns[idx] == 'Customer no.':\n                remark = result\n            elif self._columns[idx] != '':\n                values[self._columns[idx]] = {\n                    'result': result,\n                    'DefaultResult': 'result',\n                    'Remarks': remark,\n                    'DateTime': date,\n                }\n        self._addRawResult(resid, values, False)\n        return 0", "response": "Parses the data line and builds the dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getWorksheet(self):\n        ws = None\n        if self._current_ws_index < len(self._worksheets):\n            ws = self._ws_data(self._worksheets[self._current_ws_index])\n        return ws", "response": "Returns the current worksheet from the list. Returns None when the iterator reaches the end of the array. Returns None when the iterator reaches the end of the array."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef splitList(self, elements, chunksnum):\n        if len(elements) < chunksnum:\n            return [elements]\n        groups = zip(*[elements[i::chunksnum] for i in range(chunksnum)])\n        if len(groups) * chunksnum < len(elements):\n            groups.extend([elements[-(len(elements) - len(groups) * chunksnum):]])\n        return groups", "response": "Splits a list into n lists with chunksnum number of elements\n            each one."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _lab_data(self):\n        portal = self.context.portal_url.getPortalObject()\n        lab = self.context.bika_setup.laboratory\n        lab_address = lab.getPostalAddress() \\\n            or lab.getBillingAddress() \\\n            or lab.getPhysicalAddress()\n        if lab_address:\n            _keys = ['address', 'city', 'state', 'zip', 'country']\n            _list = [\"<div>%s</div>\" % lab_address.get(v) for v in _keys\n                     if lab_address.get(v)]\n            lab_address = \"\".join(_list)\n        else:\n            lab_address = ''\n\n        return {'obj': lab,\n                'title': to_utf8(lab.Title()),\n                'url': to_utf8(lab.getLabURL()),\n                'address': to_utf8(lab_address),\n                'confidence': lab.getConfidence(),\n                'accredited': lab.getLaboratoryAccredited(),\n                'accreditation_body': to_utf8(lab.getAccreditationBody()),\n                'accreditation_logo': lab.getAccreditationBodyLogo(),\n                'logo': \"%s/logo_print.png\" % portal.absolute_url()}", "response": "Returns a dictionary that represents the lab object\n            Keys : obj title url address confidence accredited accreditation_body accreditation_logo logo"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dict of all the data for each specific resource in the given Worksheet.", "response": "def _ws_data(self, ws):\n        \"\"\" Creates an ws dict, accessible from the view and from each\n            specific template.\n            Keys: obj, id, url, template_title, remarks, date_printed,\n                ars, createdby, analyst, printedby, analyses_titles,\n                portal, laboratory\n        \"\"\"\n        data = {\n            'obj': ws,\n            'id': ws.id,\n            'url': ws.absolute_url(),\n            'template_title': ws.getWorksheetTemplateTitle(),\n            'remarks': ws.getRemarks(),\n            'date_printed': self.ulocalized_time(DateTime(), long_format=1),\n            'date_created': self.ulocalized_time(ws.created(), long_format=1),\n        }\n\n        # Sub-objects\n        data['ars'] = self._analyses_data(ws)\n        data['createdby'] = self._createdby_data(ws)\n        data['analyst'] = self._analyst_data(ws)\n        data['printedby'] = self._printedby_data(ws)\n\n        # Unify the analyses titles for the template\n        # N.B. The Analyses come in sorted, so don't use a set() to unify them,\n        #      because it sorts the Analyses alphabetically\n        ans = []\n        for ar in data['ars']:\n            for an in ar['analyses']:\n                title = an[\"title\"]\n                if title in ans:\n                    continue\n                ans.append(title)\n        data['analyses_titles'] = ans\n\n        portal = self.context.portal_url.getPortalObject()\n        data['portal'] = {'obj': portal,\n                          'url': portal.absolute_url()}\n        data['laboratory'] = self._lab_data()\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _createdby_data(self, ws):\n        username = ws.getOwner().getUserName()\n        return {'username': username,\n                'fullname': to_utf8(self.user_fullname(username)),\n                'email': to_utf8(self.user_email(username))}", "response": "Returns a dict that represents the user who created the ws\n            Keys : username fullmame email\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _analyst_data(self, ws):\n        username = ws.getAnalyst()\n        return {'username': username,\n                'fullname': to_utf8(self.user_fullname(username)),\n                'email': to_utf8(self.user_email(username))}", "response": "Returns a dict that represents the analyst assigned to the the\n            worksheet."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dict that represents the user who prints the ws Keys : username fullname email", "response": "def _printedby_data(self, ws):\n        \"\"\" Returns a dict that represents the user who prints the ws\n            Keys: username, fullname, email\n        \"\"\"\n        data = {}\n        member = self.context.portal_membership.getAuthenticatedMember()\n        if member:\n            username = member.getUserName()\n            data['username'] = username\n            data['fullname'] = to_utf8(self.user_fullname(username))\n            data['email'] = to_utf8(self.user_email(username))\n\n            c = [x for x in self.bika_setup_catalog(portal_type='LabContact')\n                 if x.getObject().getUsername() == username]\n            if c:\n                sf = c[0].getObject().getSignature()\n                if sf:\n                    data['signature'] = sf.absolute_url() + \"/Signature\"\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of dicts. Each dict represents an analysis assigned to the worksheet and each dict represents an analysis that is assigned to the worksheet .", "response": "def _analyses_data(self, ws):\n        \"\"\" Returns a list of dicts. Each dict represents an analysis\n            assigned to the worksheet\n        \"\"\"\n        ans = ws.getAnalyses()\n        layout = ws.getLayout()\n        pos_count = 0\n        prev_pos = 0\n        ars = {}\n\n        # mapping of analysis UID -> position in layout\n        uid_to_pos_mapping = dict(\n            map(lambda row: (row[\"analysis_uid\"], row[\"position\"]), layout))\n\n        for an in ans:\n            # Build the analysis-specific dict\n            if an.portal_type == \"DuplicateAnalysis\":\n                andict = self._analysis_data(an.getAnalysis())\n                andict['id'] = an.getReferenceAnalysesGroupID()\n                andict['obj'] = an\n                andict['type'] = \"DuplicateAnalysis\"\n                andict['reftype'] = 'd'\n            else:\n                andict = self._analysis_data(an)\n\n            # Analysis position\n            pos = uid_to_pos_mapping.get(an.UID(), 0)\n\n            # compensate for possible bad data (dbw#104)\n            if isinstance(pos, (list, tuple)) and pos[0] == 'new':\n                pos = prev_pos\n\n            pos = int(pos)\n            prev_pos = pos\n\n            # This will allow to sort automatically all the analyses,\n            # also if they have the same initial position.\n            andict['tmp_position'] = (pos * 100) + pos_count\n            andict['position'] = pos\n            pos_count += 1\n\n            # Look for the analysis request, client and sample info and\n            # group the analyses per Analysis Request\n            reqid = andict['request_id']\n            if an.portal_type in (\"ReferenceAnalysis\", \"DuplicateAnalysis\"):\n                reqid = an.getReferenceAnalysesGroupID()\n\n            if reqid not in ars:\n                arobj = an.aq_parent\n                if an.portal_type == \"DuplicateAnalysis\":\n                    arobj = an.getAnalysis().aq_parent\n\n                ar = self._ar_data(arobj)\n                ar['client'] = self._client_data(arobj.aq_parent)\n                ar[\"sample\"] = dict()\n                if IReferenceSample.providedBy(an):\n                    ar['sample'] = self._sample_data(an.getSample())\n                else:\n                    ar['sample'] = self._sample_data(an.getRequest())\n                ar['analyses'] = []\n                ar['tmp_position'] = andict['tmp_position']\n                ar['position'] = andict['position']\n                if an.portal_type in (\"ReferenceAnalysis\", \"DuplicateAnalysis\"):\n                    ar['id'] = an.getReferenceAnalysesGroupID()\n                    ar['url'] = an.absolute_url()\n            else:\n                ar = ars[reqid]\n                if (andict['tmp_position'] < ar['tmp_position']):\n                    ar['tmp_position'] = andict['tmp_position']\n                    ar['position'] = andict['position']\n\n            # Sort analyses by position\n            ans = ar['analyses']\n            ans.append(andict)\n            ans.sort(lambda x, y: cmp(x.get('tmp_position'), y.get('tmp_position')))\n            ar['analyses'] = ans\n            ars[reqid] = ar\n\n        ars = [a for a in ars.itervalues()]\n\n        # Sort analysis requests by position\n        ars.sort(lambda x, y: cmp(x.get('tmp_position'), y.get('tmp_position')))\n        return ars"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _analysis_data(self, analysis):\n        decimalmark = analysis.aq_parent.aq_parent.getDecimalMark()\n        keyword = analysis.getKeyword()\n        andict = {\n            'obj': analysis,\n            'id': analysis.id,\n            'title': analysis.Title(),\n            'keyword': keyword,\n            'scientific_name': analysis.getScientificName(),\n            'accredited': analysis.getAccredited(),\n            'point_of_capture': to_utf8(POINTS_OF_CAPTURE.getValue(analysis.getPointOfCapture())),\n            'category': to_utf8(analysis.getCategoryTitle()),\n            'result': analysis.getResult(),\n            'unit': to_utf8(analysis.getUnit()),\n            'formatted_unit': format_supsub(to_utf8(analysis.getUnit())),\n            'capture_date': analysis.getResultCaptureDate(),\n            'request_id': analysis.aq_parent.getId(),\n            'formatted_result': '',\n            'uncertainty': analysis.getUncertainty(),\n            'formatted_uncertainty': '',\n            'retested': analysis.isRetest(),\n            'remarks': to_utf8(analysis.getRemarks()),\n            'outofrange': False,\n            'type': analysis.portal_type,\n            'reftype': analysis.getReferenceType() if hasattr(\n                analysis, 'getReferenceType') else None,\n            'worksheet': None,\n            'specs': {},\n            'formatted_specs': '',\n            'review_state': api.get_workflow_status_of(analysis),\n        }\n\n        andict['refsample'] = analysis.getSample().id \\\n            if IReferenceAnalysis.providedBy(analysis) \\\n            else analysis.getRequestID()\n\n        specs = analysis.getResultsRange()\n        andict['specs'] = specs\n        scinot = self.context.bika_setup.getScientificNotationReport()\n        andict['formatted_result'] = analysis.getFormattedResult(specs=specs, sciformat=int(scinot), decimalmark=decimalmark)\n\n        fs = ''\n        if specs.get('min', None) and specs.get('max', None):\n            fs = '%s - %s' % (specs['min'], specs['max'])\n        elif specs.get('min', None):\n            fs = '> %s' % specs['min']\n        elif specs.get('max', None):\n            fs = '< %s' % specs['max']\n        andict['formatted_specs'] = formatDecimalMark(fs, decimalmark)\n        andict['formatted_uncertainty'] = format_uncertainty(analysis, analysis.getResult(), decimalmark=decimalmark, sciformat=int(scinot))\n\n        # Out of range?\n        andict['outofrange'] = is_out_of_range(analysis)[0]\n        return andict", "response": "Returns a dict that represents the analysis"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dict that represents the sample object Keys are obj id url sample_type sample_point composite date_expired date_disposed date_sampled date_received date_sampled composite date_expired date_disposed date_expired date_disposed date_expired date_disposed date_expired date_disposed date_expired date_disposed date", "response": "def _sample_data(self, sample):\n        \"\"\" Returns a dict that represents the sample\n            Keys: obj, id, url, client_sampleid, date_sampled,\n                  sampling_date, sampler, date_received, composite,\n                  date_expired, date_disposal, date_disposed, remarks\n        \"\"\"\n        data = {}\n        if sample:\n            data = {'obj': sample,\n                    'id': sample.id,\n                    'url': sample.absolute_url(),\n                    'date_sampled': self.ulocalized_time(\n                        sample.getDateSampled(), long_format=True),\n                    'date_received': self.ulocalized_time(\n                        sample.getDateReceived(), long_format=0),\n                    }\n\n            if sample.portal_type == \"ReferenceSample\":\n                data['sample_type'] = None\n                data['sample_point'] = None\n            else:\n                data['sample_type'] = self._sample_type(sample)\n                data['sample_point'] = self._sample_point(sample)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _sample_type(self, sample=None):\n        data = {}\n        sampletype = sample.getSampleType() if sample else None\n        if sampletype:\n            data = {'obj': sampletype,\n                    'id': sampletype.id,\n                    'title': sampletype.Title(),\n                    'url': sampletype.absolute_url()}\n        return data", "response": "Returns a dict that represents the sample type assigned to\n            the sample specified\n            Keys are obj id title url"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _sample_point(self, sample=None):\n        samplepoint = sample.getSamplePoint() if sample else None\n        data = {}\n        if samplepoint:\n            data = {'obj': samplepoint,\n                    'id': samplepoint.id,\n                    'title': samplepoint.Title(),\n                    'url': samplepoint.absolute_url()}\n        return data", "response": "Returns a dict that represents the sample point assigned to\n            the sample specified\n            Keys : obj id title url"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ar_data(self, ar):\n        if not ar:\n            return {}\n\n        if ar.portal_type == \"AnalysisRequest\":\n            return {'obj': ar,\n                    'id': ar.getId(),\n                    'date_received': self.ulocalized_time(\n                        ar.getDateReceived(), long_format=0),\n                    'date_sampled': self.ulocalized_time(\n                        ar.getDateSampled(), long_format=True),\n                    'url': ar.absolute_url(), }\n        elif ar.portal_type == \"ReferenceSample\":\n            return {'obj': ar,\n                    'id': ar.id,\n                    'date_received': self.ulocalized_time(\n                        ar.getDateReceived(), long_format=0),\n                    'date_sampled': self.ulocalized_time(\n                        ar.getDateSampled(), long_format=True),\n                    'url': ar.absolute_url(), }\n        else:\n            return {'obj': ar,\n                    'id': ar.id,\n                    'date_received': \"\",\n                    'date_sampled': \"\",\n                    'url': ar.absolute_url(), }", "response": "Returns a dict that represents the analysis request"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dict that represents the client specified Keys : obj id url name", "response": "def _client_data(self, client):\n        \"\"\" Returns a dict that represents the client specified\n            Keys: obj, id, url, name\n        \"\"\"\n        data = {}\n        if client:\n            data['obj'] = client\n            data['id'] = client.id\n            data['url'] = client.absolute_url()\n            data['name'] = to_utf8(client.getName())\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getAvailableMethods(self):\n        methods = self.getMethods()\n        muids = [m.UID() for m in methods]\n        if self.getInstrumentEntryOfResults():\n            # Add the methods from the instruments capable to perform\n            # this analysis service\n            for ins in self.getInstruments():\n                for method in ins.getMethods():\n                    if method and method.UID() not in muids:\n                        methods.append(method)\n                        muids.append(method.UID())\n\n        return methods", "response": "Returns the methods available for this analysis."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the available instruments for this service.", "response": "def getAvailableInstruments(self):\n        \"\"\" Returns the instruments available for this service.\n            If the service has the getInstrumentEntryOfResults(), returns\n            the instruments capable to perform this service. Otherwhise,\n            returns an empty list.\n        \"\"\"\n        instruments = self.getInstruments() \\\n            if self.getInstrumentEntryOfResults() is True \\\n            else None\n        return instruments if instruments else []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a DisplayList with the available Instruments", "response": "def _getAvailableInstrumentsDisplayList(self):\n        \"\"\" Returns a DisplayList with the available Instruments\n            registered in Bika-Setup. Only active Instruments are\n            fetched. Used to fill the Instruments MultiSelectionWidget\n        \"\"\"\n        bsc = getToolByName(self, 'bika_setup_catalog')\n        items = [(i.UID, i.Title)\n                 for i in bsc(portal_type='Instrument',\n                              is_active=True)]\n        items.sort(lambda x, y: cmp(x[1], y[1]))\n        return DisplayList(list(items))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the data line and builds the dictionary.", "response": "def parse_data_line(self, sline):\n        \"\"\"\n        Parses the data line and builds the dictionary.\n        :param sline: a split data line to parse\n        :returns: the number of rows to jump and parse the next data line or return the code error -1\n        \"\"\"\n        # if there are less values founded than headers, it's an error\n        if len(sline) != len(self._columns):\n            self.err(\"One data line has the wrong number of items\")\n            return -1\n        rawdict = {}\n        for idx, result in enumerate(sline):\n            rawdict[self._columns[idx]] = result.replace(' ', '')\n\n        resid = rawdict['Sample ID'].replace('.', '')\n        rawdict['DefaultResult'] = 'Result'\n        rawdict['Remarks'] = rawdict['Sample Details']\n        del rawdict['Sample Details']\n        name = rawdict['Test Name'].replace('.', '')\n        del rawdict['Test Name']\n        self._addRawResult(resid, {name: rawdict}, False)\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the results Range from the AR", "response": "def get_results_range(self):\n        \"\"\"Get the results Range from the AR\n        \"\"\"\n        spec = self.context.getResultsRange()\n        if spec:\n            return dicts_to_dict(spec, \"keyword\")\n        return ResultsRangeDict()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of all items in the folder.", "response": "def folderitems(self):\n        \"\"\"XXX refactor if possible to non-classic mode\n        \"\"\"\n        items = super(AnalysisRequestAnalysesView, self).folderitems()\n        self.categories.sort()\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef folderitem(self, obj, item, index):\n        # ensure we have an object and not a brain\n        obj = api.get_object(obj)\n        uid = api.get_uid(obj)\n\n        # settings for this analysis\n        service_settings = self.context.getAnalysisServiceSettings(uid)\n        hidden = service_settings.get(\"hidden\", obj.getHidden())\n\n        # get the category\n        category = obj.getCategoryTitle()\n        item[\"category\"] = category\n        if category not in self.categories:\n            self.categories.append(category)\n\n        price = obj.getPrice()\n        keyword = obj.getKeyword()\n\n        if uid in self.analyses:\n            analysis = self.analyses[uid]\n            # Might differ from the service keyword\n            keyword = analysis.getKeyword()\n            # Mark the row as disabled if the analysis is not in an open state\n            item[\"disabled\"] = not any([analysis.isOpen(),\n                                        analysis.isRegistered()])\n            # get the hidden status of the analysis\n            hidden = analysis.getHidden()\n            # get the price of the analysis\n            price = analysis.getPrice()\n\n        # get the specification of this object\n        rr = self.get_results_range()\n        spec = rr.get(keyword, ResultsRangeDict())\n\n        item[\"Title\"] = obj.Title()\n        item[\"Unit\"] = obj.getUnit()\n        item[\"Price\"] = price\n        item[\"before\"][\"Price\"] = self.get_currency_symbol()\n        item[\"allow_edit\"] = self.get_editable_columns(obj)\n        item[\"selected\"] = uid in self.selected\n        item[\"min\"] = str(spec.get(\"min\", \"\"))\n        item[\"max\"] = str(spec.get(\"max\", \"\"))\n        item[\"warn_min\"] = str(spec.get(\"warn_min\", \"\"))\n        item[\"warn_max\"] = str(spec.get(\"warn_max\", \"\"))\n        item[\"Hidden\"] = hidden\n\n        # Append info link before the service\n        # see: bika.lims.site.coffee for the attached event handler\n        item[\"before\"][\"Title\"] = get_link(\n            \"analysisservice_info?service_uid={}\".format(uid),\n            value=\"<span class='glyphicon glyphicon-info-sign'></span>\",\n            css_class=\"service_info\")\n\n        # Icons\n        after_icons = \"\"\n        if obj.getAccredited():\n            after_icons += get_image(\n                \"accredited.png\", title=t(_(\"Accredited\")))\n        if obj.getAttachmentOption() == \"r\":\n            after_icons += get_image(\n                \"attach_reqd.png\", title=t(_(\"Attachment required\")))\n        if obj.getAttachmentOption() == \"n\":\n            after_icons += get_image(\n                \"attach_no.png\", title=t(_('Attachment not permitted')))\n        if after_icons:\n            item[\"after\"][\"Title\"] = after_icons\n\n        return item", "response": "This method is called by the base class to add the object to the item dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getAuthenticatedMember(self):\n    '''\n    Returns the currently authenticated member object\n    or the Anonymous User.  Never returns None.\n    This caches the value in the reqeust...\n    '''\n    if not \"_c_authenticatedUser\" in self.REQUEST:\n        u = _getAuthenticatedUser(self)\n        if u is None:\n            u = nobody\n        if str(u) not in ('Anonymous User',):\n            self.REQUEST['_c_authenticatedUser'] = u\n    else:\n        u = self.REQUEST['_c_authenticatedUser']\n    return self.wrapUser(u)", "response": "Returns the currently authenticated member object or the Anonymous User."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if an analyst assignment is allowed", "response": "def is_analyst_assignment_allowed(self):\n        \"\"\"Check if the analyst can be assigned\n        \"\"\"\n        if not self.allow_edit:\n            return False\n        if not self.can_manage:\n            return False\n        if self.filter_by_user:\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nallows Analyst reassignment for the current locale.", "response": "def allow_analyst_reassignment(self):\n        \"\"\"Allow the Analyst reassignment\n        \"\"\"\n        reassing_analyst_transition = {\n            \"id\": \"reassign\",\n            \"title\": _(\"Reassign\")}\n        for rs in self.review_states:\n            if rs[\"id\"] not in [\"default\", \"mine\", \"open\", \"all\"]:\n                continue\n            rs[\"custom_transitions\"].append(reassing_analyst_transition)\n        self.show_select_column = True\n        self.show_workflow_action_buttons = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the current selected state", "response": "def get_selected_state(self):\n        \"\"\"Returns the current selected state\n        \"\"\"\n        form_key = \"{}_review_state\".format(self.form_id)\n        return self.request.get(form_key, \"default\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nservice triggered each time an item is iterated in folderitems. The use of this service prevents the extra-loops in child objects. :obj: the instance of the class to be foldered :item: dict containing the properties of the object to be used by the template :index: current index of the item", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Service triggered each time an item is iterated in folderitems.\n\n        The use of this service prevents the extra-loops in child objects.\n\n        :obj: the instance of the class to be foldered\n        :item: dict containing the properties of the object to be used by\n            the template\n        :index: current index of the item\n        \"\"\"\n\n        layout = obj.getLayout\n        title = api.get_title(obj)\n        url = api.get_url(obj)\n\n        item[\"CreationDate\"] = self.ulocalized_time(obj.created)\n        if len(obj.getAnalysesUIDs) == 0:\n            item[\"table_row_class\"] = \"state-empty-worksheet\"\n\n        title_link = \"{}/{}\".format(url, \"add_analyses\")\n        if len(layout) > 0:\n            title_link = \"{}/{}\".format(url, \"manage_results\")\n\n        item[\"Title\"] = title\n        item[\"replace\"][\"Title\"] = get_link(title_link, value=title)\n\n        pos_parent = {}\n        for slot in layout:\n            # compensate for bad data caused by a stupid bug.\n            if type(slot[\"position\"]) in (list, tuple):\n                slot[\"position\"] = slot[\"position\"][0]\n            if slot[\"position\"] == \"new\":\n                continue\n            if slot[\"position\"] in pos_parent:\n                continue\n            pos_parent[slot[\"position\"]] =\\\n                self.rc.lookupObject(slot.get(\"container_uid\"))\n\n        # Total QC Analyses\n        item[\"NumQCAnalyses\"] = str(obj.getNumberOfQCAnalyses)\n        # Total Routine Analyses\n        item[\"NumRegularAnalyses\"] = str(obj.getNumberOfRegularAnalyses)\n        # Total Number of Samples\n        item[\"NumRegularSamples\"] = str(obj.getNumberOfRegularSamples)\n\n        # Progress\n        progress_perc = obj.getProgressPercentage\n        item[\"replace\"][\"Progress\"] = get_progress_bar_html(progress_perc)\n\n        review_state = item[\"review_state\"]\n        if self.can_reassign and review_state == \"open\":\n            item[\"Analyst\"] = obj.getAnalyst\n            item[\"allow_edit\"] = [\"Analyst\"]\n            item[\"required\"] = [\"Analyst\"]\n            item[\"choices\"] = {\"Analyst\": self.analyst_choices}\n        else:\n            fullname = user_fullname(self.context, obj.getAnalyst)\n            item[\"Analyst\"] = fullname\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getTemplateInstruments(self):\n        items = dict()\n        templates = self._get_worksheet_templates_brains()\n        for template in templates:\n            template_obj = api.get_object(template)\n            uid_template = api.get_uid(template_obj)\n            instrument = template_obj.getInstrument()\n            uid_instrument = \"\"\n            if instrument:\n                uid_instrument = api.get_uid(instrument)\n            items[uid_template] = uid_instrument\n\n        return json.dumps(items)", "response": "Returns worksheet templates as JSON\nalides"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\naction to be done when an analysis is added in an Analysis Request", "response": "def ObjectInitializedEventHandler(analysis, event):\n    \"\"\"Actions to be done when an analysis is added in an Analysis Request\n    \"\"\"\n\n    # Initialize the analysis if it was e.g. added by Manage Analysis\n    wf.doActionFor(analysis, \"initialize\")\n\n    # Try to transition the analysis_request to \"sample_received\". There are\n    # some cases that can end up with an inconsistent state between the AR\n    # and the analyses it contains: retraction of an analysis when the state\n    # of the AR was \"to_be_verified\", addition of a new analysis when the\n    # state was \"to_be_verified\", etc.\n    request = analysis.getRequest()\n    wf.doActionFor(request, \"rollback_to_receive\")\n\n    # Reindex the indexes for UIDReference fields on creation!\n    analysis.reindexObject(idxs=\"getServiceUID\")\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nactions to be done when an analysis is removed from an Analysis Request", "response": "def ObjectRemovedEventHandler(analysis, event):\n    \"\"\"Actions to be done when an analysis is removed from an Analysis Request\n    \"\"\"\n    # If all the remaining analyses have been submitted (or verified), try to\n    # promote the transition to the Analysis Request\n    # Note there is no need to check if the Analysis Request allows a given\n    # transition, cause this is already managed by doActionFor\n    analysis_request = analysis.getRequest()\n    wf.doActionFor(analysis_request, \"submit\")\n    wf.doActionFor(analysis_request, \"verify\")\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef before_render(self):\n        # Render the Add button if the user has the AddClient permission\n        if check_permission(AddClient, self.context):\n            self.context_actions[_(\"Add\")] = {\n                \"url\": \"createObject?type_name=Client\",\n                \"icon\": \"++resource++bika.lims.images/add.png\"\n            }\n\n        # Display a checkbox next to each client in the list if the user has\n        # rights for ModifyPortalContent\n        self.show_select_column = check_permission(ModifyPortalContent,\n                                                   self.context)", "response": "Hook to render the Add button if the user has the AddClient permission for the item in the list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\napplying new properties to the item that is currently being rendered as a row in the list", "response": "def folderitem(self, obj, item, index):\n        \"\"\"Applies new properties to the item (Client) that is currently being\n        rendered as a row in the list\n\n        :param obj: client to be rendered as a row in the list\n        :param item: dict representation of the client, suitable for the list\n        :param index: current position of the item within the list\n        :type obj: ATContentType/DexterityContentType\n        :type item: dict\n        :type index: int\n        :return: the dict representation of the item\n        :rtype: dict\n        \"\"\"\n        # render a link to the defined start page\n        link_url = \"{}/{}\".format(item[\"url\"], self.landing_page)\n        item[\"replace\"][\"title\"] = get_link(link_url, item[\"title\"])\n        item[\"replace\"][\"getClientID\"] = get_link(link_url, item[\"getClientID\"])\n        # render an email link\n        item[\"replace\"][\"EmailAddress\"] = get_email_link(item[\"EmailAddress\"])\n        # translate True/FALSE values\n        item[\"replace\"][\"BulkDiscount\"] = obj.getBulkDiscount() and _(\"Yes\") or _(\"No\")\n        item[\"replace\"][\"MemberDiscountApplies\"] = obj.getMemberDiscountApplies() and _(\"Yes\") or _(\"No\")\n        # render a phone link\n        phone = obj.getPhone()\n        if phone:\n            item[\"replace\"][\"Phone\"] = get_link(\"tel:{}\".format(phone), phone)\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fixPath(path):\n  path = os.path.abspath(os.path.expanduser(path))\n  if path.startswith(\"\\\\\"):\n    return \"C:\" + path\n\n  return path", "response": "Fixes paths that start with C"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds the requirements. txt file and parse into requirements for setup s install_requirements option.", "response": "def findRequirements(platform):\n  \"\"\"\n  Read the requirements.txt file and parse into requirements for setup's\n  install_requirements option.\n  \"\"\"\n  includePycapnp = platform not in WINDOWS_PLATFORMS\n  requirementsPath = fixPath(os.path.join(PY_BINDINGS, \"requirements.txt\"))\n  return [\n    line.strip()\n    for line in open(requirementsPath).readlines()\n    if not line.startswith(\"#\") and (not line.startswith(\"pycapnp\") or includePycapnp)\n  ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main():\n  engine.Network.registerPyRegion(__name__,\n                                    SerializationTestPyRegion.__name__)\n\n  try:\n    _runTest()\n  finally:\n    engine.Network.unregisterPyRegion(SerializationTestPyRegion.__name__)", "response": "This function is used to measure capnp serialization performance of a simple\n"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef checkImportBindingsExtensions():\n  import nupic.bindings.math\n  import nupic.bindings.algorithms\n  import nupic.bindings.engine_internal", "response": "Checks that nupic. bindings extension libraries can be imported."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getParameter(self, name, index):\n    if name.startswith('_'):\n      raise Exception('Parameter name must not start with an underscore')\n\n    value = getattr(self, name)\n    return value", "response": "Default implementation that returns an attribute with the requested name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getParameterArrayCount(self, name, index):\n    if name.startswith('_'):\n      raise Exception('Parameter name must not start with an underscore')\n\n    return len(self.parameters[name])", "response": "Default implementation that returns the length of the attribute."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getParameterArray(self, name, index, array):\n    if name.startswith('_'):\n      raise Exception('Parameter name must not start with an underscore')\n\n    v = getattr(self, name)\n    # Not performing sanity checks for performance reasons.\n    #assert array.dtype == v.dtype\n    #assert len(array) == len(v)\n    array[:] = v", "response": "Default implementation that returns an attribute with the requested name and index and writes it into the output numpy array."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite the region to the given proto.", "response": "def write(self, proto):\n    \"\"\"\n    Calls :meth:`~nupic.bindings.regions.PyRegion.PyRegion.writeToProto`\n    on subclass after converting proto to specific type using\n    :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getSchema`.\n\n    :param proto: PyRegionProto capnproto object\n    \"\"\"\n    regionImpl = proto.regionImpl.as_struct(self.getSchema())\n    self.writeToProto(regionImpl)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a single object from a proto.", "response": "def read(cls, proto):\n    \"\"\"\n    Calls :meth:`~nupic.bindings.regions.PyRegion.PyRegion.readFromProto`\n    on subclass after converting proto to specific type using\n    :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getSchema`.\n\n    :param proto: PyRegionProto capnproto object\n    \"\"\"\n    regionImpl = proto.regionImpl.as_struct(cls.getSchema())\n    return cls.readFromProto(regionImpl)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes a method named methodName with the specified arguments.", "response": "def executeMethod(self, methodName, args):\n    \"\"\"Executes a method named ``methodName`` with the specified arguments.\n\n    This method is called when the user executes a command as defined in\n    the node spec. It provides a perfectly reasonble implementation\n    of the command mechanism. As a sub-class developer you just need to\n    implement a method for each command in the node spec. Note that due to\n    the command mechanism only unnamed argument are supported.\n\n    :param methodName: (string) the name of the method that correspond to a\n           command in the spec.\n    :param args: (list) of arguments that will be passed to the method\n    \"\"\"\n    if not hasattr(self, methodName):\n      raise Exception('Missing command method: ' + methodName)\n\n    m = getattr(self, methodName)\n    if not hasattr(m, '__call__'):\n      raise Exception('Command: ' + methodName + ' must be callable')\n\n    return m(*args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the value of a sparse array.", "response": "def setSparseOutput(outputs, name, value):\n    \"\"\"\n    Set region sparse output value.\n\n    The region output memory is owned by the c++ caller and cannot be changed  \n    directly from python. Use this method to update the sparse output fields in  \n    the \"outputs\" array so it can be resized from the c++ code.\n\n    :param outputs: (dict) of numpy arrays. This is the original outputs dict \n           owned by the C++ caller, passed to region via the compute method to \n           be updated.\n    :param name: (string) name of an existing output to modify\n    :param value: (list) list of UInt32 indices of all the nonzero entries \n           representing the sparse array to be set \n    \"\"\"\n    # The region output memory is owned by the c++ and cannot be changed from\n    # python. We use a special attribule named \"__{name}_len__\" to pass\n    # the sparse array length back to c++\n    lenAttr = \"__{}_len__\".format(name)\n    if lenAttr not in outputs:\n      raise Exception(\"Output {} is not a valid sparse output\".format(name))\n\n    if outputs[name].size < value.size:\n      raise Exception(\n        \"Output {} must be less than {}. Given value size is {}\".format(\n          name, outputs[name].size, value.size))\n          \n    outputs[lenAttr][0] = value.size\n    outputs[name][:value.size] = value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmeasure capnp serialization performance of Random \u00b0", "response": "def main():\n  \"\"\"Measure capnp serialization performance of Random\n  \"\"\"\n  r = Random(42)\n\n  # Measure serialization\n  startSerializationTime = time.time()\n\n  for i in xrange(_SERIALIZATION_LOOPS):\n    # NOTE pycapnp's builder.from_dict (used in nupic.bindings) leaks\n    # memory if called on the same builder more than once, so we construct a\n    # fresh builder here\n    builderProto = RandomProto.new_message()\n    r.write(builderProto)\n\n  elapsedSerializationTime = time.time() - startSerializationTime\n\n\n  builderBytes = builderProto.to_bytes()\n\n\n  # Measure deserialization\n  startDeserializationTime = time.time()\n\n  deserializationCount = 0\n  while deserializationCount < _DESERIALIZATION_LOOPS:\n    # NOTE: periodicaly create a new reader to avoid \"Exceeded message traversal\n    # limit\" error\n    readerProto = RandomProto.from_bytes(\n      builderBytes,\n      traversal_limit_in_words=_TRAVERSAL_LIMIT_IN_WORDS,\n      nesting_limit=_NESTING_LIMIT)\n\n    numReads = min(_DESERIALIZATION_LOOPS - deserializationCount,\n                     _MAX_DESERIALIZATION_LOOPS_PER_READER)\n    for _ in xrange(numReads):\n      r.read(readerProto)\n\n    deserializationCount += numReads\n\n  elapsedDeserializationTime = time.time() - startDeserializationTime\n\n  # Print report\n  print _SERIALIZATION_LOOPS, \"Serialization loops in\", \\\n        elapsedSerializationTime, \"seconds.\"\n  print \"\\t\", elapsedSerializationTime/_SERIALIZATION_LOOPS, \"seconds per loop.\"\n\n  print deserializationCount, \"Deserialization loops in\", \\\n        elapsedDeserializationTime, \"seconds.\"\n  print \"\\t\", elapsedDeserializationTime/deserializationCount, \"seconds per loop.\""}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates Steam 2FA code for a given timestamp.", "response": "def generate_twofactor_code_for_time(shared_secret, timestamp):\n    \"\"\"Generate Steam 2FA code for timestamp\n\n    :param shared_secret: authenticator shared secret\n    :type shared_secret: bytes\n    :param timestamp: timestamp to use, if left out uses current time\n    :type timestamp: int\n    :return: steam two factor code\n    :rtype: str\n    \"\"\"\n    hmac = hmac_sha1(bytes(shared_secret),\n                     struct.pack('>Q', int(timestamp)//30))  # this will NOT stop working in 2038\n\n    start = ord(hmac[19:20]) & 0xF\n    codeint = struct.unpack('>I', hmac[start:start+4])[0] & 0x7fffffff\n\n    charset = '23456789BCDFGHJKMNPQRTVWXY'\n    code = ''\n\n    for _ in range(5):\n        codeint, i = divmod(codeint, len(charset))\n        code += charset[i]\n\n    return code"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_confirmation_key(identity_secret, tag, timestamp):\n    data = struct.pack('>Q', int(timestamp)) + tag.encode('ascii') # this will NOT stop working in 2038\n    return hmac_sha1(bytes(identity_secret), data)", "response": "Generate confirmation key for trades. Can only be used once."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_time_offset():\n    try:\n        resp = webapi.post('ITwoFactorService', 'QueryTime', 1, params={'http_timeout': 10})\n    except:\n        return None\n\n    ts = int(time())\n    return int(resp.get('response', {}).get('server_time', ts)) - ts", "response": "Get time offset from Steam WebAPI s server time"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_device_id(steamid):\n    h = hexlify(sha1_hash(str(steamid).encode('ascii'))).decode('ascii')\n    return \"android:%s-%s-%s-%s-%s\" % (h[:8], h[8:12], h[12:16], h[16:20], h[20:32])", "response": "Generate Android device id from Steam ID."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract_secrets_from_android_rooted(adb_path='adb'):\n    data = subprocess.check_output([\n        adb_path, 'shell', 'su', '-c',\n        \"'cat /data/data/com.valvesoftware.android.steam.community/files/Steamguard*'\"\n        ])\n\n    # When adb daemon is not running, `adb` will print a couple of lines before our data.\n    # The data doesn't have new lines and its always on the last line.\n    data = data.decode('utf-8').split('\\n')[-1]\n\n    if data[0] != \"{\":\n        raise RuntimeError(\"Got invalid data: %s\" % repr(data))\n\n    return {int(x['steamid']): x\n            for x in map(json.loads, data.replace(\"}{\", '}|||||{').split('|||||'))}", "response": "Extract Steam Authenticator secrets from a rooted Android device."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_time(self):\n        if (self.steam_time_offset is None\n           or (self.align_time_every and (time() - self._offset_last_check) > self.align_time_every)\n           ):\n            self.steam_time_offset = get_time_offset()\n\n            if self.steam_time_offset is not None:\n                self._offset_last_check = time()\n\n        return int(time() + (self.steam_time_offset or 0))", "response": "Get the time of Steam aligned timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_code(self, timestamp=None):\n        return generate_twofactor_code_for_time(b64decode(self.shared_secret),\n                                                self.get_time() if timestamp is None else timestamp)", "response": "returns two factor code for the given time"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_confirmation_key(self, tag='', timestamp=None):\n        return generate_confirmation_key(b64decode(self.identity_secret), tag,\n                                         self.get_time() if timestamp is None else timestamp)", "response": "generate confirmation key for this value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add(self):\n        if not self.has_phone_number():\n            raise SteamAuthenticatorError(\"Account doesn't have a verified phone number\")\n\n        resp = self._send_request('AddAuthenticator', {\n            'steamid': self.backend.steam_id,\n            'authenticator_time': int(time()),\n            'authenticator_type': int(ETwoFactorTokenType.ValveMobileApp),\n            'device_identifier': generate_device_id(self.backend.steam_id),\n            'sms_phone_id': '1',\n        })\n\n        if resp['status'] != EResult.OK:\n            raise SteamAuthenticatorError(\"Failed to add authenticator. Error: %s\" % repr(EResult(resp['status'])))\n\n        self.secrets = resp\n        self.steam_time_offset = int(resp['server_time']) - time()", "response": "Add authenticator to an account."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef finalize(self, activation_code):\n        resp = self._send_request('FinalizeAddAuthenticator', {\n            'steamid': self.backend.steam_id,\n            'authenticator_time': int(time()),\n            'authenticator_code': self.get_code(),\n            'activation_code': activation_code,\n        })\n\n        if resp['status'] != EResult.TwoFactorActivationCodeMismatch and resp.get('want_more', False) and self._finalize_attempts:\n            self.steam_time_offset += 30\n            self._finalize_attempts -= 1\n            self.finalize(activation_code)\n            return\n        elif not resp['success']:\n            self._finalize_attempts = 5\n            raise SteamAuthenticatorError(\"Failed to finalize authenticator. Error: %s\" % repr(EResult(resp['status'])))\n\n        self.steam_time_offset = int(resp['server_time']) - time()", "response": "Finalize authenticator with received SMS code."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove(self):\n        if not self.secrets:\n            raise SteamAuthenticatorError(\"No authenticator secrets available?\")\n        if not isinstance(self.backend, MobileWebAuth):\n            raise SteamAuthenticatorError(\"Only available via MobileWebAuth\")\n\n        resp = self._send_request('RemoveAuthenticator', {\n            'steamid': self.backend.steam_id,\n            'revocation_code': self.revocation_code,\n            'steamguard_scheme': 1,\n        })\n\n        if not resp['success']:\n            raise SteamAuthenticatorError(\"Failed to remove authenticator. (attempts remaining: %s)\" % (\n                resp['revocation_attempts_remaining'],\n                ))\n\n        self.secrets.clear()", "response": "Remove authenticator from the store."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating emergency codes for a specific SMS code.", "response": "def create_emergency_codes(self, code=None):\n        \"\"\"Generate emergency codes\n\n        :param code: SMS code\n        :type code: str\n        :raises: :class:`SteamAuthenticatorError`\n        :return: list of codes\n        :rtype: list\n\n        .. note::\n            A confirmation code is required to generate emergency codes and this method needs\n            to be called twice as shown below.\n\n        .. code:: python\n\n            sa.create_emergency_codes()              # request a SMS code\n            sa.create_emergency_codes(code='12345')  # creates emergency codes\n        \"\"\"\n        if code:\n            return self._send_request('createemergencycodes', {'code': code}).get('codes', [])\n        else:\n            self._send_request('createemergencycodes', {})\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_web_session(self):\n        if isinstance(self.backend, MobileWebAuth):\n            return self.backend.session\n        else:\n            if self.backend.logged_on:\n                sess = self.backend.get_web_session()\n\n                if sess is None:\n                    raise RuntimeError(\"Failed to get a web session. Try again in a few minutes\")\n                else:\n                    return sess\n            else:\n                raise RuntimeError(\"SteamClient instance is not connected\")", "response": "Get authenticated web session"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a phone number to the account s account s account s account record.", "response": "def add_phone_number(self, phone_number):\n        \"\"\"Add phone number to account\n\n        Then confirm it via :meth:`confirm_phone_number()`\n\n        :param phone_number: phone number with country code\n        :type  phone_number: :class:`str`\n        :return: success (returns ``False`` on request fail/timeout)\n        :rtype: :class:`bool`\n        \"\"\"\n        sess = self._get_web_session()\n\n        try:\n            resp = sess.post('https://steamcommunity.com/steamguard/phoneajax',\n                            data={\n                                'op': 'add_phone_number',\n                                'arg': phone_number,\n                                'checkfortos': 0,\n                                'skipvoip': 0,\n                                'sessionid': sess.cookies.get('sessionid', domain='steamcommunity.com'),\n                                },\n                            timeout=15).json()\n        except:\n            return False\n\n        return (resp or {}).get('success', False)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconfirming phone number with the recieved SMS code.", "response": "def confirm_phone_number(self, sms_code):\n        \"\"\"Confirm phone number with the recieved SMS code\n\n        :param sms_code: sms code\n        :type  sms_code: :class:`str`\n        :return: success (returns ``False`` on request fail/timeout)\n        :rtype: :class:`bool`\n        \"\"\"\n        sess = self._get_web_session()\n\n        try:\n            resp = sess.post('https://steamcommunity.com/steamguard/phoneajax',\n                            data={\n                                'op': 'check_sms_code',\n                                'arg': sms_code,\n                                'checkfortos': 1,\n                                'skipvoip': 1,\n                                'sessionid': sess.cookies.get('sessionid', domain='steamcommunity.com'),\n                                },\n                            timeout=15).json()\n        except:\n            return False\n\n        return (resp or {}).get('success', False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking whether the account has a verified phone number.", "response": "def has_phone_number(self):\n        \"\"\"Check whether the account has a verified phone number\n\n        :return: result\n        :rtype: :class:`bool` or :class:`None`\n\n        .. note::\n            Retruns `None` if the web requests fails for any reason\n        \"\"\"\n        sess = self._get_web_session()\n\n        try:\n            resp = sess.post('https://steamcommunity.com/steamguard/phoneajax',\n                            data={\n                                'op': 'has_phone',\n                                'arg': '0',\n                                'checkfortos': 0,\n                                'skipvoip': 1,\n                                'sessionid': sess.cookies.get('sessionid', domain='steamcommunity.com'),\n                                },\n                            timeout=15).json()\n        except:\n            raise\n\n        if resp['success'] == True:\n            return resp['has_phone']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting rsa key for a given username", "response": "def get_rsa_key(self, username):\n        \"\"\"Get rsa key for a given username\n\n        :param username: username\n        :type  username: :class:`str`\n        :return: json response\n        :rtype: :class:`dict`\n        :raises HTTPError: any problem with http request, timeouts, 5xx, 4xx etc\n        \"\"\"\n        try:\n            resp = self.session.post('https://steamcommunity.com/login/getrsakey/',\n                                     timeout=15,\n                                     data={\n                                         'username': username,\n                                         'donotchache': int(time() * 1000),\n                                         },\n                                     ).json()\n        except requests.exceptions.RequestException as e:\n            raise HTTPError(str(e))\n\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nattempting web login and returns on a session with cookies set", "response": "def login(self, password='', captcha='', email_code='', twofactor_code='', language='english'):\n        \"\"\"Attempts web login and returns on a session with cookies set\n\n        :param password: password, if it wasn't provided on instance init\n        :type  password: :class:`str`\n        :param captcha: text reponse for captcha challenge\n        :type  captcha: :class:`str`\n        :param email_code: email code for steam guard\n        :type  email_code: :class:`str`\n        :param twofactor_code: 2FA code for steam guard\n        :type  twofactor_code: :class:`str`\n        :param language: select language for steam web pages (sets language cookie)\n        :type  language: :class:`str`\n        :return: a session on success and :class:`None` otherwise\n        :rtype: :class:`requests.Session`, :class:`None`\n        :raises HTTPError: any problem with http request, timeouts, 5xx, 4xx etc\n        :raises LoginIncorrect: wrong username or password\n        :raises CaptchaRequired: when captcha is needed\n        :raises CaptchaRequiredLoginIncorrect: when captcha is needed and login is incorrect\n        :raises EmailCodeRequired: when email is needed\n        :raises TwoFactorCodeRequired: when 2FA is needed\n        \"\"\"\n        if self.logged_on:\n            return self.session\n\n        if password:\n            self.password = password\n        else:\n            if self.password:\n                password = self.password\n            else:\n                raise LoginIncorrect(\"password is not specified\")\n\n        if not captcha and self.captcha_code:\n            captcha = self.captcha_code\n\n        self._load_key()\n        resp = self._send_login(password=password, captcha=captcha, email_code=email_code, twofactor_code=twofactor_code)\n\n        if resp['success'] and resp['login_complete']:\n            self.logged_on = True\n            self.password = self.captcha_code = ''\n            self.captcha_gid = -1\n\n            for cookie in list(self.session.cookies):\n                for domain in ['store.steampowered.com', 'help.steampowered.com', 'steamcommunity.com']:\n                    self.session.cookies.set(cookie.name, cookie.value, domain=domain, secure=cookie.secure)\n\n            self.session_id = generate_session_id()\n\n            for domain in ['store.steampowered.com', 'help.steampowered.com', 'steamcommunity.com']:\n                self.session.cookies.set('Steam_Language', language, domain=domain)\n                self.session.cookies.set('birthtime', '-3333', domain=domain)\n                self.session.cookies.set('sessionid', self.session_id, domain=domain)\n\n            self._finalize_login(resp)\n\n            return self.session\n        else:\n            if resp.get('captcha_needed', False):\n                self.captcha_gid = resp['captcha_gid']\n                self.captcha_code = ''\n\n                if resp.get('clear_password_field', False):\n                    self.password = ''\n                    raise CaptchaRequiredLoginIncorrect(resp['message'])\n                else:\n                    raise CaptchaRequired(resp['message'])\n            elif resp.get('emailauth_needed', False):\n                self.steam_id = SteamID(resp['emailsteamid'])\n                raise EmailCodeRequired(resp['message'])\n            elif resp.get('requires_twofactor', False):\n                raise TwoFactorCodeRequired(resp['message'])\n            else:\n                self.password = ''\n                raise LoginIncorrect(resp['message'])\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates CLI prompts to perform the entire login process :param password: password, if it wasn't provided on instance init :type password: :class:`str` :param captcha: text reponse for captcha challenge :type captcha: :class:`str` :param email_code: email code for steam guard :type email_code: :class:`str` :param twofactor_code: 2FA code for steam guard :type twofactor_code: :class:`str` :param language: select language for steam web pages (sets language cookie) :type language: :class:`str` :return: a session on success and :class:`None` otherwise :rtype: :class:`requests.Session`, :class:`None` .. code:: python In [3]: user.cli_login() Enter password for 'steamuser': Solve CAPTCHA at https://steamcommunity.com/login/rendercaptcha/?gid=1111111111111111111 CAPTCHA code: 123456 Invalid password for 'steamuser'. Enter password: Solve CAPTCHA at https://steamcommunity.com/login/rendercaptcha/?gid=2222222222222222222 CAPTCHA code: abcdef Enter 2FA code: AB123 Out[3]: <requests.sessions.Session at 0x6fffe56bef0>", "response": "def cli_login(self, password='', captcha='', email_code='', twofactor_code='', language='english'):\n        \"\"\"Generates CLI prompts to perform the entire login process\n\n        :param password: password, if it wasn't provided on instance init\n        :type  password: :class:`str`\n        :param captcha: text reponse for captcha challenge\n        :type  captcha: :class:`str`\n        :param email_code: email code for steam guard\n        :type  email_code: :class:`str`\n        :param twofactor_code: 2FA code for steam guard\n        :type  twofactor_code: :class:`str`\n        :param language: select language for steam web pages (sets language cookie)\n        :type  language: :class:`str`\n        :return: a session on success and :class:`None` otherwise\n        :rtype: :class:`requests.Session`, :class:`None`\n\n        .. code:: python\n\n            In [3]: user.cli_login()\n            Enter password for 'steamuser':\n            Solve CAPTCHA at https://steamcommunity.com/login/rendercaptcha/?gid=1111111111111111111\n            CAPTCHA code: 123456\n            Invalid password for 'steamuser'. Enter password:\n            Solve CAPTCHA at https://steamcommunity.com/login/rendercaptcha/?gid=2222222222222222222\n            CAPTCHA code: abcdef\n            Enter 2FA code: AB123\n            Out[3]: <requests.sessions.Session at 0x6fffe56bef0>\n\n        \"\"\"\n\n        # loop until successful login\n        while True:\n            try:\n                return self.login(password, captcha, email_code, twofactor_code, language)\n            except (LoginIncorrect, CaptchaRequired) as exp:\n                email_code = twofactor_code = ''\n\n                if isinstance(exp, LoginIncorrect):\n                    prompt = (\"Enter password for %s: \" if not password else\n                              \"Invalid password for %s. Enter password: \")\n                    password = getpass(prompt % repr(self.username))\n                if isinstance(exp, CaptchaRequired):\n                    prompt = \"Solve CAPTCHA at %s\\nCAPTCHA code: \" % self.captcha_url\n                    captcha = _cli_input(prompt)\n                else:\n                    captcha = ''\n            except EmailCodeRequired:\n                prompt = (\"Enter email code: \" if not email_code else\n                          \"Incorrect code. Enter email code: \")\n                email_code, twofactor_code = _cli_input(prompt), ''\n            except TwoFactorCodeRequired:\n                prompt = (\"Enter 2FA code: \" if not twofactor_code else\n                          \"Incorrect code. Enter 2FA code: \")\n                email_code, twofactor_code = '', _cli_input(prompt)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_leaderboard(self, app_id, name):\n        message = MsgProto(EMsg.ClientLBSFindOrCreateLB)\n        message.header.routing_appid = app_id\n        message.body.app_id = app_id\n        message.body.leaderboard_name = name\n        message.body.create_if_not_found = False\n\n        resp = self.send_job_and_wait(message, timeout=15)\n\n        if not resp:\n            raise LookupError(\"Didn't receive response within 15seconds :(\")\n        if resp.eresult != EResult.OK:\n            raise LookupError(EResult(resp.eresult))\n\n        return SteamLeaderboard(self, app_id, name, resp)", "response": "Find a leaderboard by application id and name."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets leaderboard entries. :param start: start entry, not index (e.g. rank 1 is ``start=1``) :type start: :class:`int` :param end: end entry, not index (e.g. only one entry then ``start=1,end=1``) :type end: :class:`int` :param data_request: data being requested :type data_request: :class:`steam.enums.common.ELeaderboardDataRequest` :param steam_ids: list of steam ids when using :prop:`.ELeaderboardDataRequest.Users` :type steamids: :class:`list` :return: a list of entries, see ``CMsgClientLBSGetLBEntriesResponse`` :rtype: :class:`list` :raises: :class:`LookupError` on message timeout or error", "response": "def get_entries(self, start=0, end=0, data_request=None, steam_ids=None):\n        \"\"\"Get leaderboard entries.\n\n        :param start: start entry, not index (e.g. rank 1 is ``start=1``)\n        :type start: :class:`int`\n        :param end: end entry, not index (e.g. only one entry then ``start=1,end=1``)\n        :type end: :class:`int`\n        :param data_request: data being requested\n        :type data_request: :class:`steam.enums.common.ELeaderboardDataRequest`\n        :param steam_ids: list of steam ids when using :prop:`.ELeaderboardDataRequest.Users`\n        :type steamids: :class:`list`\n        :return: a list of entries, see ``CMsgClientLBSGetLBEntriesResponse``\n        :rtype: :class:`list`\n        :raises: :class:`LookupError` on message timeout or error\n        \"\"\"\n        message = MsgProto(EMsg.ClientLBSGetLBEntries)\n        message.body.app_id = self.app_id\n        message.body.leaderboard_id = self.id\n        message.body.range_start = start\n        message.body.range_end = end\n        message.body.leaderboard_data_request = self.data_request if data_request is None else data_request\n\n        if steam_ids:\n            message.body.steamids.extend(steam_ids)\n\n        resp = self._steam.send_job_and_wait(message, timeout=15)\n\n        if not resp:\n            raise LookupError(\"Didn't receive response within 15seconds :(\")\n        if resp.eresult != EResult.OK:\n            raise LookupError(EResult(resp.eresult))\n\n        if resp.HasField('leaderboard_entry_count'):\n            self.entry_count = resp.leaderboard_entry_count\n\n        return resp.entries"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_iter(self, times, seconds, chunk_size=2000):\n        def entry_generator():\n            with ConstantRateLimit(times, seconds, sleep_func=self._steam.sleep) as r:\n                for entries in chunks(self, chunk_size):\n                    if not entries:\n                        return\n                    for entry in entries:\n                        yield entry\n                    r.wait()\n        return entry_generator()", "response": "Make an iterator over the entries\nCOOKIES"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connect(self, *args, **kwargs):\n        self._bootstrap_cm_list_from_file()\n        CMClient.connect(self, *args, **kwargs)", "response": "Attempt to establish connection to the CM and initialize the CM list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef disconnect(self, *args, **kwargs):\n        self.logged_on = False\n        CMClient.disconnect(self, *args, **kwargs)", "response": "Disconnect from the CMClient"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimplement explonential backoff delay before attempting to connect.", "response": "def reconnect(self, maxdelay=30, retry=0):\n        \"\"\"Implements explonential backoff delay before attempting to connect.\n        It is otherwise identical to calling :meth:`.CMClient.connect`.\n        The delay is reset upon a successful login.\n\n        :param maxdelay: maximum delay in seconds before connect (0-120s)\n        :type maxdelay: :class:`int`\n        :param retry: see :meth:`.CMClient.connect`\n        :type retry: :class:`int`\n        :return: successful connection\n        :rtype: :class:`bool`\n        \"\"\"\n        delay_seconds = 2**self._reconnect_backoff_c - 1\n\n        if delay_seconds < maxdelay:\n            self._reconnect_backoff_c = min(7, self._reconnect_backoff_c + 1)\n\n        return self.connect(delay=delay_seconds, retry=retry)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwaits for a message or return None.", "response": "def wait_msg(self, event, timeout=None, raises=None):\n        \"\"\"Wait for a message, similiar to :meth:`.wait_event`\n\n        :param event: :class:`.EMsg' or job id\n        :param timeout: seconds to wait before timeout\n        :type timeout: :class:`int`\n        :param raises: On timeout when ``False` returns :class:`None`, else raise :class:`gevent.Timeout`\n        :type raises: :class:`bool`\n        :return: returns a message or :class:`None`\n        :rtype: :class:`None`, or `proto message`\n        :raises: ``gevent.Timeout``\n        \"\"\"\n        resp = self.wait_event(event, timeout, raises)\n\n        if resp is not None:\n            return resp[0]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send(self, message, body_params=None):\n        if not self.connected:\n            self._LOG.debug(\"Trying to send message when not connected. (discarded)\")\n        else:\n            if body_params and isinstance(message, MsgProto):\n                proto_fill_from_dict(message.body, body_params)\n\n            CMClient.send(self, message)", "response": ".. versionchanged :: 0. 8. 4. 4"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_job(self, message, body_params=None):\n        jobid = self.current_jobid = ((self.current_jobid + 1) % 10000) or 1\n        self.remove_all_listeners('job_%d' % jobid)\n\n        if message.proto:\n            message.header.jobid_source = jobid\n        else:\n            message.header.sourceJobID = jobid\n\n        self.send(message, body_params)\n\n        return \"job_%d\" % jobid", "response": "Send a message as a job."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_job_and_wait(self, message, body_params=None, timeout=None, raises=False):\n        job_id = self.send_job(message, body_params)\n        response = self.wait_event(job_id, timeout, raises=raises)\n        if response is None:\n            return None\n        return response[0].body", "response": "Send a message as a job and wait for the response."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_message_and_wait(self, message, response_emsg, body_params=None, timeout=None, raises=False):\n        self.send(message, body_params)\n        response = self.wait_event(response_emsg, timeout, raises=raises)\n        if response is None:\n            return None\n        return response[0].body", "response": ".. versionchanged :: 0. 8. 4. 4 Send a message to CM and wait for an answer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn contents of sentry file for the given username.", "response": "def get_sentry(self, username):\n        \"\"\"\n        Returns contents of sentry file for the given username\n\n        .. note::\n            returns ``None`` if :attr:`credential_location` is not set, or file is not found/inaccessible\n\n        :param username: username\n        :type username: :class:`str`\n        :return: sentry file contents, or ``None``\n        :rtype: :class:`bytes`, :class:`None`\n        \"\"\"\n        filepath = self._get_sentry_path(username)\n\n        if filepath and os.path.isfile(filepath):\n            try:\n                with open(filepath, 'rb') as f:\n                    return f.read()\n            except IOError as e:\n                self._LOG.error(\"get_sentry: %s\" % str(e))\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstores sentry bytes under a username", "response": "def store_sentry(self, username, sentry_bytes):\n        \"\"\"\n        Store sentry bytes under a username\n\n        :param username: username\n        :type username: :class:`str`\n        :return: Whenver the operation succeed\n        :rtype: :class:`bool`\n        \"\"\"\n        filepath = self._get_sentry_path(username)\n        if filepath:\n            try:\n                with open(filepath, 'wb') as f:\n                    f.write(sentry_bytes)\n                return True\n            except IOError as e:\n                self._LOG.error(\"store_sentry: %s\" % str(e))\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlogs-in without needing credentials essentially remember password.", "response": "def relogin(self):\n        \"\"\"Login without needing credentials, essentially remember password.\n        The :attr:`login_key` is acquired after successful login and it will be\n        automatically acknowledged. Listen for the ``new_login_key`` event.\n        After that the client can relogin using this method.\n\n        .. note::\n            Only works when :attr:`relogin_available` is ``True``.\n\n        .. code:: python\n\n            if client.relogin_available: client.relogin()\n            else:\n                client.login(user, pass)\n        \"\"\"\n        if self.relogin_available:\n            self.login(self.username, '', self.login_key)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef login(self, username, password='', login_key=None, auth_code=None, two_factor_code=None, login_id=None):\n        self._LOG.debug(\"Attempting login\")\n\n        if not self._pre_login():\n            return EResult.TryAnotherCM\n\n        self.username = username\n\n        message = MsgProto(EMsg.ClientLogon)\n        message.header.steamid = SteamID(type='Individual', universe='Public')\n        message.body.protocol_version = 65579\n        message.body.client_package_version = 1771\n        message.body.client_os_type = EOSType.Windows10\n        message.body.client_language = \"english\"\n        message.body.should_remember_password = True\n        message.body.supports_rate_limit_response = True\n\n        if login_id is None:\n            message.body.obfustucated_private_ip = ip_to_int(self.connection.local_address) ^ 0xF00DBAAD\n        else:\n            message.body.obfustucated_private_ip = login_id\n\n        message.body.account_name = username\n\n        if login_key:\n            message.body.login_key = login_key\n        else:\n            message.body.password = password\n\n        sentry = self.get_sentry(username)\n        if sentry is None:\n            message.body.eresult_sentryfile = EResult.FileNotFound\n        else:\n            message.body.eresult_sentryfile = EResult.OK\n            message.body.sha_sentryfile = sha1_hash(sentry)\n\n        if auth_code:\n            message.body.auth_code = auth_code\n        if two_factor_code:\n            message.body.two_factor_code = two_factor_code\n\n        self.send(message)\n\n        resp = self.wait_msg(EMsg.ClientLogOnResponse, timeout=30)\n\n        if resp and resp.body.eresult == EResult.OK:\n            self.sleep(0.5)\n\n        return EResult(resp.body.eresult) if resp else EResult.Fail", "response": "Login as a specific user"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlogs-in as anonymous user", "response": "def anonymous_login(self):\n        \"\"\"Login as anonymous user\n\n        :return: logon result, see `CMsgClientLogonResponse.eresult <https://github.com/ValvePython/steam/blob/513c68ca081dc9409df932ad86c66100164380a6/protobufs/steammessages_clientserver.proto#L95-L118>`_\n        :rtype: :class:`.EResult`\n        \"\"\"\n        self._LOG.debug(\"Attempting Anonymous login\")\n\n        self._pre_login()\n\n        self.username = None\n        self.login_key = None\n\n        message = MsgProto(EMsg.ClientLogon)\n        message.header.steamid = SteamID(type='AnonUser', universe='Public')\n        message.body.protocol_version = 65579\n        self.send(message)\n\n        resp = self.wait_msg(EMsg.ClientLogOnResponse, timeout=30)\n        return EResult(resp.body.eresult) if resp else EResult.Fail"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlogging in to the specified user and returns the logon result.", "response": "def cli_login(self, username='', password=''):\n        \"\"\"Generates CLI prompts to complete the login process\n\n        :param username: optionally provide username\n        :type  username: :class:`str`\n        :param password: optionally provide password\n        :type  password: :class:`str`\n        :return: logon result, see `CMsgClientLogonResponse.eresult <https://github.com/ValvePython/steam/blob/513c68ca081dc9409df932ad86c66100164380a6/protobufs/steammessages_clientserver.proto#L95-L118>`_\n        :rtype: :class:`.EResult`\n\n        Example console output after calling :meth:`cli_login`\n\n        .. code:: python\n\n            In [5]: client.cli_login()\n            Steam username: myusername\n            Password:\n            Steam is down. Keep retrying? [y/n]: y\n            Invalid password for 'myusername'. Enter password:\n            Enter email code: 123\n            Incorrect code. Enter email code: K6VKF\n            Out[5]: <EResult.OK: 1>\n        \"\"\"\n        if not username:\n            username = _cli_input(\"Username: \")\n        if not password:\n            password = getpass()\n\n        auth_code = two_factor_code = None\n        prompt_for_unavailable = True\n\n        result = self.login(username, password)\n\n        while result in (EResult.AccountLogonDenied, EResult.InvalidLoginAuthCode,\n                         EResult.AccountLoginDeniedNeedTwoFactor, EResult.TwoFactorCodeMismatch,\n                         EResult.TryAnotherCM, EResult.ServiceUnavailable,\n                         EResult.InvalidPassword,\n                         ):\n            self.sleep(0.1)\n\n            if result == EResult.InvalidPassword:\n                password = getpass(\"Invalid password for %s. Enter password: \" % repr(username))\n\n            elif result in (EResult.AccountLogonDenied, EResult.InvalidLoginAuthCode):\n                prompt = (\"Enter email code: \" if result == EResult.AccountLogonDenied else\n                          \"Incorrect code. Enter email code: \")\n                auth_code, two_factor_code = _cli_input(prompt), None\n\n            elif result in (EResult.AccountLoginDeniedNeedTwoFactor, EResult.TwoFactorCodeMismatch):\n                prompt = (\"Enter 2FA code: \" if result == EResult.AccountLoginDeniedNeedTwoFactor else\n                          \"Incorrect code. Enter 2FA code: \")\n                auth_code, two_factor_code = None, _cli_input(prompt)\n\n            elif result in (EResult.TryAnotherCM, EResult.ServiceUnavailable):\n                if prompt_for_unavailable and result == EResult.ServiceUnavailable:\n                    while True:\n                        answer = _cli_input(\"Steam is down. Keep retrying? [y/n]: \").lower()\n                        if answer in 'yn': break\n\n                    prompt_for_unavailable = False\n                    if answer == 'n': break\n\n                self.reconnect(maxdelay=15)  # implements reconnect throttling\n\n            result = self.login(username, password, None, auth_code, two_factor_code)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connect(self, retry=0, delay=0):\n        if self.connected:\n            self._LOG.debug(\"Connect called, but we are connected?\")\n            return\n        if self._connecting:\n            self._LOG.debug(\"Connect called, but we are already connecting.\")\n            return\n        self._connecting = True\n\n        if delay:\n            self._LOG.debug(\"Delayed connect: %d seconds\" % delay)\n            self.emit(self.EVENT_RECONNECT, delay)\n            self.sleep(delay)\n\n        self._LOG.debug(\"Connect initiated.\")\n\n        for i, server_addr in enumerate(self.cm_servers):\n            if retry and i > retry:\n                return False\n\n            start = time()\n\n            if self.connection.connect(server_addr):\n                break\n\n            diff = time() - start\n\n            self._LOG.debug(\"Failed to connect. Retrying...\")\n\n            if diff < 5:\n                self.sleep(5 - diff)\n\n        self.current_server_addr = server_addr\n        self.connected = True\n        self.emit(self.EVENT_CONNECTED)\n        self._recv_loop = gevent.spawn(self._recv_messages)\n        self._connecting = False\n        return True", "response": "Initiate a connection to CM."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disconnect(self):\n\n        if not self.connected:\n            return\n        self.connected = False\n\n        self.connection.disconnect()\n\n        if self._heartbeat_loop:\n            self._heartbeat_loop.kill()\n        self._recv_loop.kill()\n\n        self._reset_attributes()\n\n        self.emit(self.EVENT_DISCONNECTED)", "response": "Disconnect from the broker."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send(self, message):\n        if not isinstance(message, (Msg, MsgProto)):\n            raise ValueError(\"Expected Msg or MsgProto, got %s\" % message)\n\n        if self.steam_id:\n            message.steamID = self.steam_id\n        if self.session_id:\n            message.sessionID = self.session_id\n\n        if self.verbose_debug:\n            self._LOG.debug(\"Outgoing: %s\\n%s\" % (repr(message), str(message)))\n        else:\n            self._LOG.debug(\"Outgoing: %s\", repr(message))\n\n        data = message.serialize()\n\n        if self.channel_key:\n            if self.channel_hmac:\n                data = crypto.symmetric_encrypt_HMAC(data, self.channel_key, self.channel_hmac)\n            else:\n                data = crypto.symmetric_encrypt(data, self.channel_key)\n\n        self.connection.put_message(data)", "response": "Send a message to the instance of the current instance of the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clear(self):\n        if len(self.list):\n            self._LOG.debug(\"List cleared.\")\n        self.list.clear()", "response": "Clears the server list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bootstrap_from_webapi(self, cellid=0):\n        self._LOG.debug(\"Attempting bootstrap via WebAPI\")\n\n        from steam import webapi\n        try:\n            resp = webapi.get('ISteamDirectory', 'GetCMList', 1, params={'cellid': cellid})\n        except Exception as exp:\n            self._LOG.error(\"WebAPI boostrap failed: %s\" % str(exp))\n            return False\n\n        result = EResult(resp['response']['result'])\n\n        if result != EResult.OK:\n            self._LOG.error(\"GetCMList failed with %s\" % repr(result))\n            return False\n\n        serverlist = resp['response']['serverlist']\n        self._LOG.debug(\"Recieved %d servers from WebAPI\" % len(serverlist))\n\n        def str_to_tuple(serveraddr):\n            ip, port = serveraddr.split(':')\n            return str(ip), int(port)\n\n        self.clear()\n        self.merge_list(map(str_to_tuple, serverlist))\n\n        return True", "response": "Fetches CM server list from WebAPI and replaces the current one with the ones from the current one."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reset_all(self):\n\n        self._LOG.debug(\"Marking all CMs as Good.\")\n\n        for key in self.list:\n            self.mark_good(key)", "response": "Reset all server status for all servers in the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmarks server address as good", "response": "def mark_good(self, server_addr):\n        \"\"\"Mark server address as good\n\n        :param server_addr: (ip, port) tuple\n        :type server_addr: :class:`tuple`\n        \"\"\"\n        self.list[server_addr].update({'quality': CMServerList.Good, 'timestamp': time()})"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmark server address as bad.", "response": "def mark_bad(self, server_addr):\n        \"\"\"Mark server address as bad, when unable to connect for example\n\n        :param server_addr: (ip, port) tuple\n        :type server_addr: :class:`tuple`\n        \"\"\"\n        self._LOG.debug(\"Marking %s as Bad.\" % repr(server_addr))\n        self.list[server_addr].update({'quality': CMServerList.Bad, 'timestamp': time()})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding new CM servers to the list.", "response": "def merge_list(self, new_list):\n        \"\"\"Add new CM servers to the list\n\n        :param new_list: a list of ``(ip, port)`` tuples\n        :type new_list: :class:`list`\n        \"\"\"\n        total = len(self.list)\n\n        for ip, port in new_list:\n            if (ip, port) not in self.list:\n                self.mark_good((ip, port))\n\n        if len(self.list) > total:\n            self._LOG.debug(\"Added %d new CM addresses.\" % (len(self.list) - total))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a new user to the friend.", "response": "def add(self, steamid_or_accountname_or_email):\n        \"\"\"\n        Add/Accept a steam user to be your friend.\n        When someone sends you an invite, use this method to accept it.\n\n        :param steamid_or_accountname_or_email: steamid, account name, or email\n        :type steamid_or_accountname_or_email: :class:`int`, :class:`.SteamID`, :class:`.SteamUser`, :class:`str`\n\n        .. note::\n            Adding by email doesn't not work. It's only mentioned for the sake of completeness.\n        \"\"\"\n        m = MsgProto(EMsg.ClientAddFriend)\n\n        if isinstance(steamid_or_accountname_or_email, (intBase, int)):\n            m.body.steamid_to_add = steamid_or_accountname_or_email\n        elif isinstance(steamid_or_accountname_or_email, SteamUser):\n            m.body.steamid_to_add = steamid_or_accountname_or_email.steam_id\n        else:\n            m.body.accountname_or_email_to_add = steamid_or_accountname_or_email\n\n        self._steam.send_job(m)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove a friend from the user list", "response": "def remove(self, steamid):\n        \"\"\"\n        Remove a friend\n\n        :param steamid: their steamid\n        :type steamid: :class:`int`, :class:`.SteamID`, :class:`.SteamUser`\n        \"\"\"\n        if isinstance(steamid, SteamUser):\n            steamid = steamid.steam_id\n\n        self._steam.send(MsgProto(EMsg.ClientRemoveFriend), {'friendid': steamid})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef webapi_request(url, method='GET', caller=None, session=None, params=None):\n    if method not in ('GET', 'POST'):\n        raise NotImplemented(\"HTTP method: %s\" % repr(self.method))\n    if params is None:\n        params = {}\n\n    onetime = {}\n    for param in DEFAULT_PARAMS:\n        params[param] = onetime[param] = params.get(param, DEFAULT_PARAMS[param])\n    for param in ('raw', 'apihost', 'https', 'http_timeout'):\n        del params[param]\n\n    if onetime['format'] not in ('json', 'vdf', 'xml'):\n        raise ValueError(\"Expected format to be json,vdf or xml; got %s\" % onetime['format'])\n\n    for k, v in list(params.items()): # serialize some types\n        if isinstance(v, bool): params[k] = 1 if v else 0\n        elif isinstance(v, dict): params[k] = _json.dumps(v)\n        elif isinstance(v, list):\n            del params[k]\n            for i, lvalue in enumerate(v):\n                params[\"%s[%d]\" % (k, i)] = lvalue\n\n    kwargs = {'params': params} if method == \"GET\" else {'data': params} # params to data for POST\n\n    if session is None: session = _make_session()\n\n    f = getattr(session, method.lower())\n    resp = f(url, stream=False, timeout=onetime['http_timeout'], **kwargs)\n\n    # we keep a reference of the last response instance on the caller\n    if caller is not None: caller.last_response = resp\n    # 4XX and 5XX will cause this to raise\n    resp.raise_for_status()\n\n    if onetime['raw']:\n        return resp.text\n    elif onetime['format'] == 'json':\n        return resp.json()\n    elif onetime['format'] == 'xml':\n        from lxml import etree as _etree\n        return _etree.fromstring(resp.content)\n    elif onetime['format'] == 'vdf':\n        import vdf as _vdf\n        return _vdf.loads(resp.text)", "response": "This method is used to make a request to Steam s WebAPI."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(interface, method, version=1,\n        apihost=DEFAULT_PARAMS['apihost'], https=DEFAULT_PARAMS['https'],\n        caller=None, session=None, params=None):\n    \"\"\"Send GET request to an API endpoint\n\n    .. versionadded:: 0.8.3\n\n    :param interface: interface name\n    :type interface: str\n    :param method: method name\n    :type method: str\n    :param version: method version\n    :type version: int\n    :param apihost: API hostname\n    :type apihost: str\n    :param https: whether to use HTTPS\n    :type https: bool\n    :param params: parameters for endpoint\n    :type params: dict\n    :return: endpoint response\n    :rtype: :class:`dict`, :class:`lxml.etree.Element`, :class:`str`\n    \"\"\"\n    url = u\"%s://%s/%s/%s/v%s/\" % (\n        'https' if https else 'http', apihost, interface, method, version)\n    return webapi_request(url, 'GET', caller=caller, session=session, params=params)", "response": "Send GET request to an API endpoint"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend POST request to an API endpoint", "response": "def post(interface, method, version=1,\n         apihost=DEFAULT_PARAMS['apihost'], https=DEFAULT_PARAMS['https'],\n         caller=None, session=None, params=None):\n    \"\"\"Send POST request to an API endpoint\n\n    .. versionadded:: 0.8.3\n\n    :param interface: interface name\n    :type interface: str\n    :param method: method name\n    :type method: str\n    :param version: method version\n    :type version: int\n    :param apihost: API hostname\n    :type apihost: str\n    :param https: whether to use HTTPS\n    :type https: bool\n    :param params: parameters for endpoint\n    :type params: dict\n    :return: endpoint response\n    :rtype: :class:`dict`, :class:`lxml.etree.Element`, :class:`str`\n    \"\"\"\n    url = \"%s://%s/%s/%s/v%s/\" % (\n        'https' if https else 'http', apihost, interface, method, version)\n    return webapi_request(url, 'POST', caller=caller, session=session, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetch_interfaces(self):\n        return get('ISteamWebAPIUtil', 'GetSupportedAPIList', 1,\n            https=self.https,\n            apihost=self.apihost,\n            caller=None,\n            session=self.session,\n            params={'format': 'json',\n                    'key': self.key,\n                    },\n            )", "response": "Fetch all interfaces and methods from GetSupportedAPIList"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_interfaces(self, interfaces_dict):\n        if interfaces_dict.get('apilist', {}).get('interfaces', None) is None:\n            raise ValueError(\"Invalid response for GetSupportedAPIList\")\n\n        interfaces = interfaces_dict['apilist']['interfaces']\n        if len(interfaces) == 0:\n            raise ValueError(\"API returned not interfaces; probably using invalid key\")\n\n        # clear existing interface instances\n        for interface in self.interfaces:\n            delattr(self, interface.name)\n        self.interfaces = []\n\n        # create interface instances from response\n        for interface in interfaces:\n            obj = WebAPIInterface(interface, parent=self)\n            self.interfaces.append(obj)\n            setattr(self, obj.name, obj)", "response": "Load the API interfaces from the response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake an API call for specific method_path.", "response": "def call(self, method_path, **kwargs):\n        \"\"\"\n        Make an API call for specific method\n\n        :param method_path: format ``Interface.Method`` (e.g. ``ISteamWebAPIUtil.GetServerInfo``)\n        :type method_path: :class:`str`\n        :param kwargs: keyword arguments for the specific method\n        :return: response\n        :rtype: :class:`dict`, :class:`lxml.etree.Element` or :class:`str`\n        \"\"\"\n\n        interface, method = method_path.split('.', 1)\n        return getattr(getattr(self, interface), method)(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a string that contains the documentation for all interfaces and their methods", "response": "def doc(self):\n        \"\"\"\n        :return: Documentation for all interfaces and their methods\n        :rtype: str\n        \"\"\"\n        doc = \"Steam Web API - List of all interfaces\\n\\n\"\n        for interface in self.interfaces:\n            doc += interface.__doc__\n        return doc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting request proto instance for given methed name", "response": "def get(self, method_name):\n        \"\"\"Get request proto instance for given methed name\n\n        :param method_name: name for the method (e.g. ``Player.GetGameBadgeLevels#1``)\n        :type method_name: :class:`str`\n        :return: proto message instance, or :class:`None` if not found\n        \"\"\"\n        proto = get_um(method_name)\n        if proto is None:\n            return None\n        message = proto()\n        self._data[message] = method_name\n        return message"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send(self, message, params=None):\n        if isinstance(message, str):\n            message = self.get(message)\n        if message not in self._data:\n            raise ValueError(\"Supplied message is invalid. Use 'get' method.\")\n\n        if params:\n            proto_fill_from_dict(message, params)\n\n        capsule = MsgProto(EMsg.ClientServiceMethod)\n        capsule.body.method_name = self._data[message]\n        capsule.body.serialized_method = message.SerializeToString()\n\n        return self._steam.send_job(capsule)", "response": "Send a message to the service."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a message to the service method and wait for the response.", "response": "def send_and_wait(self, message, params=None, timeout=10, raises=False):\n        \"\"\"Send service method request and wait for response\n\n        :param message:\n            proto message instance (use :meth:`SteamUnifiedMessages.get`)\n            or method name (e.g. ``Player.GetGameBadgeLevels#1``)\n        :type message: :class:`str`, proto message instance\n        :param params: message parameters\n        :type params: :class:`dict`\n        :param timeout: (optional) seconds to wait\n        :type timeout: :class:`int`\n        :param raises: (optional) On timeout if :class:`False` return :class:`None`, else raise :class:`gevent.Timeout`\n        :type raises: :class:`bool`\n        :return: response proto message instance\n        :rtype: (proto message, :class:`.UnifiedMessageError`)\n        :raises: :class:`gevent.Timeout`\n        \"\"\"\n        job_id = self.send(message, params)\n        resp = self.wait_event(job_id, timeout, raises=raises)\n        return (None, None) if resp is None else resp"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns n bytes from the current offset", "response": "def read(self, n=1):\n        \"\"\"Return n bytes\n\n        :param n: number of bytes to return\n        :type  n: :class:`int`\n        :return: bytes\n        :rtype: :class:`bytes`\n        \"\"\"\n        self.offset += n\n        return self.data[self.offset - n:self.offset]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread a single null termianted string.", "response": "def read_cstring(self, terminator=b'\\x00'):\n        \"\"\"Reads a single null termianted string\n\n        :return: string without bytes\n        :rtype: :class:`bytes`\n        \"\"\"\n        null_index = self.data.find(terminator, self.offset)\n        if null_index == -1:\n            raise RuntimeError(\"Reached end of buffer\")\n        result = self.data[self.offset:null_index]  # bytes without the terminator\n        self.offset = null_index + len(terminator)  # advance offset past terminator\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unpack(self, format_text):\n        data = _unpack_from(format_text, self.data, self.offset)\n        self.offset += _calcsize(format_text)\n        return data", "response": "Unpack bytes using struct modules format\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a random session key for the next random universe element in the sequence.", "response": "def generate_session_key(hmac_secret=b''):\n    \"\"\"\n    :param hmac_secret: optional HMAC\n    :type hmac_secret: :class:`bytes`\n    :return: (session_key, encrypted_session_key) tuple\n    :rtype: :class:`tuple`\n    \"\"\"\n    session_key = random_bytes(32)\n    encrypted_session_key = PKCS1_OAEP.new(UniverseKey.Public, SHA1)\\\n                                      .encrypt(session_key + hmac_secret)\n\n    return (session_key, encrypted_session_key)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef symmetric_decrypt_HMAC(cyphertext, key, hmac_secret):\n    iv = symmetric_decrypt_iv(cyphertext, key)\n    message = symmetric_decrypt_with_iv(cyphertext, key, iv)\n\n    hmac = hmac_sha1(hmac_secret, iv[-3:] + message)\n\n    if iv[:13] != hmac[:13]:\n        raise RuntimeError(\"Unable to decrypt message. HMAC does not match.\")\n\n    return message", "response": "Decrypt a message using a symmetric key and HMAC secret."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a protobuf message instance to a dict.", "response": "def proto_to_dict(message):\n    \"\"\"Converts protobuf message instance to dict\n\n    :param message: protobuf message instance\n    :return: parameters and their values\n    :rtype: dict\n    :raises: :class:`.TypeError` if ``message`` is not a proto message\n    \"\"\"\n    if not isinstance(message, _ProtoMessageType):\n        raise TypeError(\"Expected `message` to be a instance of protobuf message\")\n\n    data = {}\n\n    for desc, field in message.ListFields():\n        if desc.type == desc.TYPE_MESSAGE:\n            if desc.label == desc.LABEL_REPEATED:\n                data[desc.name] = list(map(proto_to_dict, field))\n            else:\n                data[desc.name] = proto_to_dict(field)\n        else:\n            data[desc.name] = list(field) if desc.label == desc.LABEL_REPEATED else field\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfill protobuf message parameters inplace from a dict.", "response": "def proto_fill_from_dict(message, data, clear=True):\n    \"\"\"Fills protobuf message parameters inplace from a :class:`dict`\n\n    :param message: protobuf message instance\n    :param data: parameters and values\n    :type data: dict\n    :param clear: whether clear exisiting values\n    :type clear: bool\n    :return: value of message paramater\n    :raises: incorrect types or values will raise\n    \"\"\"\n    if not isinstance(message, _ProtoMessageType):\n        raise TypeError(\"Expected `message` to be a instance of protobuf message\")\n    if not isinstance(data, dict):\n        raise TypeError(\"Expected `data` to be of type `dict`\")\n\n    if clear: message.Clear()\n    field_descs = message.DESCRIPTOR.fields_by_name\n\n    for key, val in data.items():\n        desc = field_descs[key]\n\n        if desc.type == desc.TYPE_MESSAGE:\n            if desc.label == desc.LABEL_REPEATED:\n                if not isinstance(val, _list_types):\n                    raise TypeError(\"Expected %s to be of type list, got %s\" % (repr(key), type(val)))\n\n                list_ref = getattr(message, key)\n\n                # Takes care of overwriting list fields when merging partial data (clear=False)\n                if not clear: del list_ref[:]  # clears the list\n\n                for item in val:\n                    item_message = getattr(message, key).add()\n                    proto_fill_from_dict(item_message, item)\n            else:\n                if not isinstance(val, dict):\n                    raise TypeError(\"Expected %s to be of type dict, got %s\" % (repr(key), type(dict)))\n\n                proto_fill_from_dict(getattr(message, key), val)\n        else:\n            if isinstance(val, _list_types):\n                list_ref = getattr(message, key)\n                if not clear: del list_ref[:]  # clears the list\n                list_ref.extend(val)\n            else:\n                setattr(message, key, val)\n\n    return message"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsplits a list into chunks of size size", "response": "def chunks(arr, size):\n    \"\"\"Splits a list into chunks\n\n    :param arr: list to split\n    :type arr: :class:`list`\n    :param size: number of elements in each chunk\n    :type size: :class:`int`\n    :return: generator object\n    :rtype: :class:`generator`\n    \"\"\"\n    for i in _range(0, len(arr), size):\n        yield arr[i:i+size]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a message to the GC broker.", "response": "def send(self, header, body):\n        \"\"\"\n        Send a message to GC\n\n        :param header: message header\n        :type header: :class:`steam.core.msg.GCMsgHdr`\n        :param body: serialized body of the message\n        :type body: :class:`bytes`\n        \"\"\"\n        message = MsgProto(EMsg.ClientToGC)\n        message.header.routing_appid = self.app_id\n        message.body.appid = self.app_id\n        message.body.msgtype = (set_proto_bit(header.msg)\n                                if header.proto\n                                else header.msg\n                                )\n        message.body.payload = header.serialize() + body\n        self.steam.send(message)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_um(method_name, response=False):\n    key = (method_name, response)\n\n    if key not in method_lookup:\n        match = re.findall(r'^([a-z]+)\\.([a-z]+)#(\\d)?$', method_name, re.I)\n        if not match:\n            return None\n\n        interface, method, version = match[0]\n\n        if interface not in service_lookup:\n            return None\n\n        package = import_module(service_lookup[interface])\n\n        service = getattr(package, interface, None)\n        if service is None:\n            return None\n\n        for method_desc in service.GetDescriptor().methods:\n            name = \"%s.%s#%d\" % (interface, method_desc.name, 1)\n\n            method_lookup[(name, False)] = getattr(package, method_desc.input_type.full_name, None)\n            method_lookup[(name, True)] = getattr(package, method_desc.output_type.full_name, None)\n\n    return method_lookup[key]", "response": "Get protobuf for given method name"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new version of the ISO64 - encoded version of the internal structure of the internal structure of the internal structure.", "response": "def make_steam64(id=0, *args, **kwargs):\n    \"\"\"\n    Returns steam64 from various other representations.\n\n    .. code:: python\n\n        make_steam64()  # invalid steamid\n        make_steam64(12345)  # accountid\n        make_steam64('12345')\n        make_steam64(id=12345, type='Invalid', universe='Invalid', instance=0)\n        make_steam64(103582791429521412)  # steam64\n        make_steam64('103582791429521412')\n        make_steam64('STEAM_1:0:2')  # steam2\n        make_steam64('[g:1:4]')  # steam3\n    \"\"\"\n\n    accountid = id\n    etype = EType.Invalid\n    universe = EUniverse.Invalid\n    instance = None\n\n    if len(args) == 0 and len(kwargs) == 0:\n        value = str(accountid)\n\n        # numeric input\n        if value.isdigit():\n            value = int(value)\n\n            # 32 bit account id\n            if 0 < value < 2**32:\n                accountid = value\n                etype = EType.Individual\n                universe = EUniverse.Public\n            # 64 bit\n            elif value < 2**64:\n                return value\n\n        # textual input e.g. [g:1:4]\n        else:\n            result = steam2_to_tuple(value) or steam3_to_tuple(value)\n\n            if result:\n                (accountid,\n                 etype,\n                 universe,\n                 instance,\n                 ) = result\n            else:\n                accountid = 0\n\n    elif len(args) > 0:\n        length = len(args)\n        if length == 1:\n            etype, = args\n        elif length == 2:\n            etype, universe = args\n        elif length == 3:\n            etype, universe, instance = args\n        else:\n            raise TypeError(\"Takes at most 4 arguments (%d given)\" % length)\n\n    if len(kwargs) > 0:\n        etype = kwargs.get('type', etype)\n        universe = kwargs.get('universe', universe)\n        instance = kwargs.get('instance', instance)\n\n    etype = (EType(etype)\n             if isinstance(etype, (int, EType))\n             else EType[etype]\n             )\n\n    universe = (EUniverse(universe)\n                if isinstance(universe, (int, EUniverse))\n                else EUniverse[universe]\n                )\n\n    if instance is None:\n        instance = 1 if etype in (EType.Individual, EType.GameServer) else 0\n\n    assert instance <= 0xffffF, \"instance larger than 20bits\"\n\n    return (universe << 56) | (etype << 52) | (instance << 32) | accountid"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef steam2_to_tuple(value):\n    match = re.match(r\"^STEAM_(?P<universe>\\d+)\"\n                     r\":(?P<reminder>[0-1])\"\n                     r\":(?P<id>\\d+)$\", value\n                     )\n\n    if not match:\n        return None\n\n    steam32 = (int(match.group('id')) << 1) | int(match.group('reminder'))\n    universe = int(match.group('universe'))\n\n    # Games before orange box used to incorrectly display universe as 0, we support that\n    if universe == 0:\n        universe = 1\n\n    return (steam32, EType(1), EUniverse(universe), 1)", "response": "Convert a string representation of a Steam 2 account into a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a string representation of a Steam 3 account to a tuple.", "response": "def steam3_to_tuple(value):\n    \"\"\"\n    :param value: steam3 (e.g. ``[U:1:1234]``)\n    :type value: :class:`str`\n    :return: (accountid, type, universe, instance)\n    :rtype: :class:`tuple` or :class:`None`\n    \"\"\"\n    match = re.match(r\"^\\[\"\n                     r\"(?P<type>[i%s]):\"        # type char\n                     r\"(?P<universe>[0-4]):\"     # universe\n                     r\"(?P<id>\\d{1,10})\"            # accountid\n                     r\"(:(?P<instance>\\d+))?\"  # instance\n                     r\"\\]$\" % ETypeChars,\n                     value\n                     )\n    if not match:\n        return None\n\n    steam32 = int(match.group('id'))\n    universe = EUniverse(int(match.group('universe')))\n    typechar = match.group('type').replace('i', 'I')\n    etype = EType(ETypeChar[typechar])\n    instance = match.group('instance')\n\n    if typechar in 'gT':\n        instance = 0\n    elif instance is not None:\n        instance = int(instance)\n    elif typechar == 'L':\n        instance = EInstanceFlag.Lobby\n    elif typechar == 'c':\n        instance = EInstanceFlag.Clan\n    elif etype in (EType.Individual, EType.GameServer):\n        instance = 1\n    else:\n        instance = 0\n\n    instance = int(instance)\n\n    return (steam32, etype, universe, instance)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef steam64_from_url(url, http_timeout=30):\n\n    match = re.match(r'^(?P<clean_url>https?://steamcommunity.com/'\n                     r'(?P<type>profiles|id|gid|groups)/(?P<value>.*?))(?:/(?:.*)?)?$', url)\n\n    if not match:\n        return None\n\n    web = make_requests_session()\n\n    try:\n        # user profiles\n        if match.group('type') in ('id', 'profiles'):\n            text = web.get(match.group('clean_url'), timeout=http_timeout).text\n            data_match = re.search(\"g_rgProfileData = (?P<json>{.*?});[ \\t\\r]*\\n\", text)\n\n            if data_match:\n                data = json.loads(data_match.group('json'))\n                return int(data['steamid'])\n        # group profiles\n        else:\n            text = web.get(match.group('clean_url'), timeout=http_timeout).text\n            data_match = re.search(\"'steam://friends/joinchat/(?P<steamid>\\d+)'\", text)\n\n            if data_match:\n                return int(data_match.group('steamid'))\n    except requests.exceptions.RequestException:\n        return None", "response": "Takes a Steam Community url and returns a Steam64 object or None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the current object as a steam3 string.", "response": "def as_steam3(self):\n        \"\"\"\n        :return: steam3 format (e.g ``[U:1:1234]``)\n        :rtype: :class:`str`\n        \"\"\"\n        typechar = str(ETypeChar(self.type))\n        instance = None\n\n        if self.type in (EType.AnonGameServer, EType.Multiseat):\n            instance = self.instance\n        elif self.type == EType.Individual:\n            if self.instance != 1:\n                instance = self.instance\n        elif self.type == EType.Chat:\n            if self.instance & EInstanceFlag.Clan:\n                typechar = 'c'\n            elif self.instance & EInstanceFlag.Lobby:\n                typechar = 'L'\n            else:\n                typechar = 'T'\n\n        parts = [typechar, int(self.universe), self.id]\n\n        if instance is not None:\n            parts.append(instance)\n\n        return '[%s]' % (':'.join(map(str, parts)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_valid(self):\n        if self.type == EType.Invalid or self.type >= EType.Max:\n            return False\n\n        if self.universe == EUniverse.Invalid or self.universe >= EUniverse.Max:\n            return False\n\n        if self.type == EType.Individual:\n            if self.id == 0 or self.instance > 4:\n                return False\n\n        if self.type == EType.Clan:\n            if self.id == 0 or self.instance != 0:\n                return False\n\n        if self.type == EType.GameServer:\n            if self.id == 0:\n                return False\n\n        if self.type == EType.AnonGameServer:\n            if self.id == 0 and self.instance == 0:\n                return False\n\n        return True", "response": "Check whether this SteamID is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef query(self, filter_text, max_servers=10, timeout=30, **kw):\n        if 'geo_location_ip' in kw:\n            kw['geo_location_ip'] = ip_to_int(kw['geo_location_ip'])\n\n        kw['filter_text'] = filter_text\n        kw['max_servers'] = max_servers\n\n        resp = self._s.send_job_and_wait(MsgProto(EMsg.ClientGMSServerQuery),\n                                         kw,\n                                         timeout=timeout,\n                                         )\n\n        if resp is None:\n            return None\n\n        resp = proto_to_dict(resp)\n\n        for server in resp['servers']:\n            server['server_ip'] = ip_from_int(server['server_ip'])\n\n        return resp['servers']", "response": "r Query game servers and return a list of servers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting list of servers.", "response": "def get_server_list(self, filter_text, max_servers=10, timeout=20):\n        \"\"\"\n        Get list of servers. Works similiarly to :meth:`query`, but the response has more details.\n\n        :param filter_text: filter for servers\n        :type  filter_text: str\n        :param max_servers: (optional) number of servers to return\n        :type  max_servers: int\n        :param timeout: (optional) timeout for request in seconds\n        :type  timeout: int\n        :returns: list of servers, see below. (``None`` is returned steam doesn't respond)\n        :rtype: :class:`list`, :class:`None`\n        :raises: :class:`.UnifiedMessageError`\n\n\n        Sample response:\n\n        .. code:: python\n\n            [{'addr': '1.2.3.4:27067',\n              'appid': 730,\n              'bots': 0,\n              'dedicated': True,\n              'gamedir': 'csgo',\n              'gameport': 27067,\n              'gametype': 'valve_ds,empty,secure',\n              'map': 'de_dust2',\n              'max_players': 10,\n              'name': 'Valve CS:GO Asia Server (srcdsXXX.XXX.XXX)',\n              'os': 'l',\n              'players': 0,\n              'product': 'csgo',\n              'region': 5,\n              'secure': True,\n              'steamid': SteamID(id=3279818759, type='AnonGameServer', universe='Public', instance=7011),\n              'version': '1.35.4.0'}\n            ]\n        \"\"\"\n        resp, error = self._um.send_and_wait(\"GameServers.GetServerList#1\",\n                                             {\n                                              \"filter\": filter_text,\n                                              \"limit\": max_servers,\n                                             },\n                                             timeout=20,\n                                             )\n\n        if error:\n            raise error\n        if resp is None:\n            return None\n\n        resp = proto_to_dict(resp)\n\n        if not resp:\n            return []\n        else:\n            for server in resp['servers']:\n                server['steamid'] = SteamID(server['steamid'])\n\n            return resp['servers']"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresolving IPs from SteamIDs", "response": "def get_ips_from_steamids(self, server_steam_ids, timeout=30):\n        \"\"\"Resolve IPs from SteamIDs\n\n        :param server_steam_ids: a list of steamids\n        :type  server_steam_ids: list\n        :param timeout: (optional) timeout for request in seconds\n        :type  timeout: int\n        :return: map of ips to steamids\n        :rtype: dict\n        :raises: :class:`.UnifiedMessageError`\n\n        Sample response:\n\n        .. code:: python\n\n            {SteamID(id=123456, type='AnonGameServer', universe='Public', instance=1234): '1.2.3.4:27060'}\n        \"\"\"\n        resp, error = self._um.send_and_wait(\"GameServers.GetServerIPsBySteamID#1\",\n                                             {\"server_steamids\": server_steam_ids},\n                                             timeout=timeout,\n                                             )\n        if error:\n            raise error\n        if resp is None:\n            return None\n\n        return {SteamID(server.steamid): server.addr for server in resp.servers}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_steamids_from_ips(self, server_ips, timeout=30):\n        resp, error = self._um.send_and_wait(\"GameServers.GetServerSteamIDsByIP#1\",\n                                             {\"server_ips\": server_ips},\n                                             timeout=timeout,\n                                             )\n\n        if error:\n            raise error\n        if resp is None:\n            return None\n\n        return {server.addr: SteamID(server.steamid) for server in resp.servers}", "response": "Resolve SteamIDs from IPs in a list of servers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef request_validation_mail(self):\n        message = Msg(EMsg.ClientRequestValidationMail, extended=True)\n\n        resp = self.send_job_and_wait(message, timeout=10)\n\n        if resp is None:\n            return EResult.Timeout\n        else:\n            return EResult(resp.eresult)", "response": "Send a request to get the validation mail"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrequests password change mail.", "response": "def request_password_change_mail(self, password):\n        \"\"\"Request password change mail\n\n        :param password: current account password\n        :type  password: :class:`str`\n        :return: result\n        :rtype: :class:`.EResult`\n        \"\"\"\n        message = Msg(EMsg.ClientRequestChangeMail, extended=True)\n        message.body.password = password\n\n        resp = self.send_job_and_wait(message, timeout=10)\n\n        if resp is None:\n            return EResult.Timeout\n        else:\n            return EResult(resp.eresult)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchanges the password of the current account.", "response": "def change_password(self, password, new_password, email_code):\n        \"\"\"Change account's password\n\n        :param password: current account password\n        :type  password: :class:`str`\n        :param new_password: new account password\n        :type  new_password: :class:`str`\n        :param email_code: confirmation code from email\n        :type  email_code: :class:`str`\n        :return: result\n        :rtype: :class:`.EResult`\n\n        .. note::\n            First request change mail via :meth:`request_password_change_mail()`\n            to get the email code\n        \"\"\"\n        message = Msg(EMsg.ClientPasswordChange3, extended=True)\n        message.body.password = password\n        message.body.new_password = new_password\n        message.body.code = email_code\n\n        resp = self.send_job_and_wait(message, timeout=10)\n\n        if resp is None:\n            return EResult.Timeout\n        else:\n            return EResult(resp.eresult)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_ps(self, field_name, wait_pstate=True):\n        if not wait_pstate or self._pstate_ready.wait(timeout=5):\n            if self._pstate is None and wait_pstate:\n                self._steam.request_persona_state([self.steam_id])\n                self._pstate_ready.wait(timeout=5)\n\n            return getattr(self._pstate, field_name)\n        return None", "response": "Get property from PersonaState object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef state(self):\n        state = self.get_ps('persona_state', False)\n        return EPersonaState(state) if state else EPersonaState.Offline", "response": "Get the persona state of the current resource."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rich_presence(self):\n        kvs = self.get_ps('rich_presence')\n        data = {}\n\n        if kvs:\n            for kv in kvs:\n                data[kv.key] = kv.value\n\n        return data", "response": "Contains Rich Presence key - values\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_avatar_url(self, size=2):\n        hashbytes = self.get_ps('avatar_hash')\n\n        if hashbytes != \"\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\":\n            ahash = hexlify(hashbytes).decode('ascii')\n        else:\n            ahash = 'fef49e7fa7e1997310d705b2a6158ff8dc1cdfeb'\n\n        sizes = {\n            0: '',\n            1: '_medium',\n            2: '_full',\n        }\n        url = \"http://cdn.akamai.steamstatic.com/steamcommunity/public/images/avatars/%s/%s%s.jpg\"\n\n        return url % (ahash[:2], ahash, sizes[size])", "response": "Get URL to avatar picture\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_message(self, message):\n        self._steam.send(MsgProto(EMsg.ClientFriendMsg), {\n            'steamid': self.steam_id,\n            'chat_entry_type': EChatEntryType.ChatMsg,\n            'message': message.encode('utf8'),\n            })", "response": "Send chat message to this user"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef wait(self):\n        now = _monotonic()\n        if now < self._ref:\n            delay = max(0, self._ref - now)\n            self.sleep_func(delay)\n        self._update_ref()", "response": "Blocks until the rate is met"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_player_count(self, app_id, timeout=5):\n        resp = self.send_job_and_wait(MsgProto(EMsg.ClientGetNumberOfCurrentPlayersDP),\n                                      {'appid': app_id},\n                                      timeout=timeout\n                                     )\n        if resp is None:\n            return EResult.Timeout\n        elif resp.eresult == EResult.OK:\n            return resp.player_count\n        else:\n            return EResult(resp.eresult)", "response": "Get number of players for app id"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_product_info(self, apps=[], packages=[], timeout=15):\n        if not apps and not packages: return\n\n        message = MsgProto(EMsg.ClientPICSProductInfoRequest)\n\n        for app in apps:\n                app_info = message.body.apps.add()\n                app_info.only_public = False\n                if isinstance(app, tuple):\n                        app_info.appid, app_info.access_token = app\n                else:\n                        app_info.appid = app\n\n        for package in packages:\n                package_info = message.body.packages.add()\n                if isinstance(package, tuple):\n                        package_info.appid, package_info.access_token = package\n                else:\n                        package_info.packageid = package\n\n        message.body.meta_data_only = False\n\n        job_id = self.send_job(message)\n\n        data = dict(apps={}, packages={})\n\n        while True:\n            chunk = self.wait_event(job_id, timeout=timeout)\n\n            if chunk is None: return\n            chunk = chunk[0].body\n\n            for app in chunk.apps:\n                data['apps'][app.appid] = vdf.loads(app.buffer[:-1].decode('utf-8', 'replace'))['appinfo']\n            for pkg in chunk.packages:\n                data['packages'][pkg.packageid] = vdf.binary_loads(pkg.buffer[4:])[str(pkg.packageid)]\n\n            if not chunk.response_pending:\n                break\n\n        return data", "response": "Get product info for apps and packages."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_changes_since(self, change_number, app_changes=True, package_changes=False):\n        return self.send_job_and_wait(MsgProto(EMsg.ClientPICSChangesSinceRequest),\n                                      {\n                                       'since_change_number': change_number,\n                                       'send_app_info_changes': app_changes,\n                                       'send_package_info_changes': package_changes,\n                                      },\n                                      timeout=15\n                                     )", "response": "Get changes since a change number."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_app_ticket(self, app_id):\n        return self.send_job_and_wait(MsgProto(EMsg.ClientGetAppOwnershipTicket),\n                                      {'app_id': app_id},\n                                      timeout=15\n                                     )", "response": "Get app ownership ticket"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_depot_key(self, depot_id, app_id=0):\n        return self.send_job_and_wait(MsgProto(EMsg.ClientGetDepotDecryptionKey),\n                                      {\n                                       'depot_id': depot_id,\n                                       'app_id': app_id,\n                                      },\n                                      timeout=15\n                                     )", "response": "Get the depot decryption key for a specific app."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_cdn_auth_token(self, app_id, hostname):\n        return self.send_job_and_wait(MsgProto(EMsg.ClientGetCDNAuthToken),\n                                      {\n                                       'app_id': app_id,\n                                       'host_name': hostname,\n                                      },\n                                      timeout=15\n                                     )", "response": "Get CDN authentication token."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting access tokens for all products in the specified list of app ids and package ids.", "response": "def get_product_access_tokens(self, app_ids=[], package_ids=[]):\n        \"\"\"Get access tokens\n\n        :param app_ids: list of app ids\n        :type app_ids: :class:`list`\n        :param package_ids: list of package ids\n        :type package_ids: :class:`list`\n        :return: dict with ``apps`` and ``packages`` containing their access tokens, see example below\n        :rtype: :class:`dict`, :class:`None`\n\n        .. code:: python\n\n            {'apps':     {123: 'token', ...},\n             'packages': {456: 'token', ...}\n            }\n        \"\"\"\n        if not app_ids and not package_ids: return\n\n        resp = self.send_job_and_wait(MsgProto(EMsg.ClientPICSAccessTokenRequest),\n                                      {\n                                       'appids': map(int, app_ids),\n                                       'packageids': map(int, package_ids),\n                                      },\n                                      timeout=15\n                                     )\n\n        if resp:\n            return {'apps': dict(map(lambda app: (app.appid, app.access_token), resp.app_access_tokens)),\n                    'packages': dict(map(lambda pkg: (pkg.appid, pkg.access_token), resp.package_access_tokens)),\n                   }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering a CD - Key and redeem a CD - Key.", "response": "def register_product_key(self, key):\n        \"\"\"Register/Redeem a CD-Key\n\n        :param key: CD-Key\n        :type  key: :class:`str`\n        :return: format ``(eresult, result_details, receipt_info)``\n        :rtype: :class:`tuple`\n\n        Example ``receipt_info``:\n\n        .. code:: python\n\n            {'BasePrice': 0,\n              'CurrencyCode': 0,\n              'ErrorHeadline': '',\n              'ErrorLinkText': '',\n              'ErrorLinkURL': '',\n              'ErrorString': '',\n              'LineItemCount': 1,\n              'PaymentMethod': 1,\n              'PurchaseStatus': 1,\n              'ResultDetail': 0,\n              'Shipping': 0,\n              'Tax': 0,\n              'TotalDiscount': 0,\n              'TransactionID': UINT_64(111111111111111111),\n              'TransactionTime': 1473000000,\n              'lineitems': {'0': {'ItemDescription': 'Half-Life 3',\n                'TransactionID': UINT_64(11111111111111111),\n                'packageid': 1234}},\n              'packageid': -1}\n        \"\"\"\n        resp = self.send_job_and_wait(MsgProto(EMsg.ClientRegisterKey),\n                                      {'key': key},\n                                      timeout=30,\n                                      )\n\n        if resp:\n            details = vdf.binary_loads(resp.purchase_receipt_info).get('MessageObject', None)\n            return EResult(resp.eresult), resp.purchase_result_details, details\n        else:\n            return EResult.Timeout, None, None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget information from a server.", "response": "def a2s_info(server_addr, timeout=2, force_goldsrc=False):\n    \"\"\"Get information from a server\n\n    .. note::\n        All ``GoldSrc`` games have been updated to reply in ``Source`` format.\n        ``GoldSrc`` format is essentially DEPRECATED.\n        By default the function will prefer to return ``Source`` format, and will\n        automatically fallback to ``GoldSrc`` if available.\n\n    :param server_addr: (ip, port) for the server\n    :type  server_addr: tuple\n    :param force_goldsrc: (optional) only accept ``GoldSrc`` response format\n    :type  force_goldsrc: :class:`bool`\n    :param timeout: (optional) timeout in seconds\n    :type  timeout: float\n    :raises: :class:`RuntimeError`, :class:`socket.timeout`\n    :returns: a dict with information or `None` on timeout\n    :rtype: :class:`dict`\n    \"\"\"\n    ss = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    ss.connect(server_addr)\n    ss.settimeout(timeout)\n\n    # request server info\n    ss.send(_pack('<lc', -1, b'T') + b'Source Engine Query\\x00')\n    start = _time()\n\n    # handle response(s)\n    try:\n        data = _handle_a2s_response(ss)\n    except:\n        ss.close()\n        raise\n\n    ping = max(0.0, _time() - start) * 1000\n\n    if force_goldsrc:\n        if data[4:5] != b'm':\n            ss.close()\n            raise socket.timeout('time out')\n    else:\n        # we got a valid GoldSrc response, check if it is followed by Source response\n        if data[4:5] == b'm':\n            ss.settimeout(0.3)\n            try:\n                data = _handle_a2s_response(ss)\n            except socket.timeout:\n                pass\n\n    ss.close()\n    data = StructReader(data)\n    header, = data.unpack('<4xc')\n\n    # invalid header\n    if header not in b'mI':\n        raise RuntimeError(\"Invalid reponse header - %s\" % repr(header))\n    # GoldSrc response\n    elif header == b'm':\n        info = {\n            '_ping': ping,\n            '_type': 'goldsrc',\n            'address': data.read_cstring(),\n            'name': data.read_cstring(),\n            'map': data.read_cstring(),\n            'folder': data.read_cstring(),\n            'game': data.read_cstring(),\n        }\n\n        (info['players'],\n         info['max_players'],\n         info['protocol'],\n         info['server_type'],\n         info['environment'],\n         info['visibility'],\n         info['mod'],\n         ) = data.unpack('<BBBccBB')\n\n        info['server_type'] = _u(info['server_type'])\n        info['environment'] = _u(info['environment'])\n\n        if info['mod'] == 1:\n            info['link'] = data.read_cstring()\n            info['download_link'] = data.read_cstring()\n\n            (info['version'],\n             info['size'],\n             info['type'],\n             info['ddl'],\n             ) = data.unpack('<xLLBB')\n\n        info['vac'], info['bots'] = data.unpack('<BB')\n    # Source response\n    elif header == b'I':\n        info = {\n            '_ping': ping,\n            '_type': 'source',\n            'protocol': data.unpack('<b')[0],\n            'name': data.read_cstring(),\n            'map': data.read_cstring(),\n            'folder': data.read_cstring(),\n            'game': data.read_cstring(),\n        }\n\n        (info['app_id'],\n         info['players'],\n         info['max_players'],\n         info['bots'],\n         info['server_type'],\n         info['environment'],\n         info['visibility'],\n         info['vac'],\n         ) = data.unpack('<HBBBccBB')\n\n        info['server_type'] = _u(info['server_type'])\n        info['environment'] = _u(info['environment'])\n\n        if info['app_id'] == 2400:\n            (info['mode'],\n             info['witnesses'],\n             info['duration'],\n             ) = data.unpack('<BBB')\n\n        info['version'] = data.read_cstring()\n\n        if data.rlen():\n            edf, = data.unpack('<B')\n            info['edf'] = edf\n\n            if edf & 0x80:\n                info['port'], = data.unpack('<H')\n            if edf & 0x10:\n                info['steam_id'], = data.unpack('<Q')\n            if edf & 0x40:\n                info['sourcetv_port'], = data.unpack('<H')\n                info['sourcetv_name'] = data.read_cstring()\n            if edf & 0x20:\n                info['keywords'] = data.read_cstring()\n            if edf & 0x01:\n                info['game_id'], = data.unpack('<Q')\n                info['app_id'] = info['game_id'] & 0xFFFFFF\n\n    return info"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets list of players and their info from server.", "response": "def a2s_players(server_addr, timeout=2, challenge=0):\n    \"\"\"Get list of players and their info\n\n    :param server_addr: (ip, port) for the server\n    :type  server_addr: tuple\n    :param timeout: (optional) timeout in seconds\n    :type  timeout: float\n    :param challenge: (optional) challenge number\n    :type  challenge: int\n    :raises: :class:`RuntimeError`, :class:`socket.timeout`\n    :returns: a list of players\n    :rtype: :class:`list`\n    \"\"\"\n    ss = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    ss.connect(server_addr)\n    ss.settimeout(timeout)\n\n    # request challenge number\n    header = None\n\n    if challenge in (-1, 0):\n        ss.send(_pack('<lci', -1, b'U', challenge))\n        try:\n            data = ss.recv(512)\n            _, header, challenge = _unpack_from('<lcl', data)\n        except:\n            ss.close()\n            raise\n\n        if header not in b'AD':  # work around for CSGO sending only max players\n            raise RuntimeError(\"Unexpected challenge response - %s\" % repr(header))\n\n    # request player info\n    if header == b'D':  # work around for CSGO sending only max players\n        data = StructReader(data)\n    else:\n        ss.send(_pack('<lci', -1, b'U', challenge))\n\n        try:\n            data = StructReader(_handle_a2s_response(ss))\n        finally:\n            ss.close()\n\n    header, num_players = data.unpack('<4xcB')\n\n    if header != b'D':\n        raise RuntimeError(\"Invalid reponse header - %s\" % repr(header))\n\n    players = []\n\n    while len(players) < num_players:\n        player = dict()\n        player['index'] = data.unpack('<B')[0]\n        player['name'] = data.read_cstring()\n        player['score'], player['duration'] = data.unpack('<lf')\n        players.append(player)\n\n    if data.rlen() / 8 == num_players:  # assume the ship server\n        for player in players:\n            player['deaths'], player['money'] = data.unpack('<ll')\n\n    return players"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets rules from a server", "response": "def a2s_rules(server_addr, timeout=2, challenge=0):\n    \"\"\"Get rules from server\n\n    :param server_addr: (ip, port) for the server\n    :type  server_addr: tuple\n    :param timeout: (optional) timeout in seconds\n    :type  timeout: float\n    :param challenge: (optional) challenge number\n    :type  challenge: int\n    :raises: :class:`RuntimeError`, :class:`socket.timeout`\n    :returns: a list of players\n    :rtype: :class:`list`\n    \"\"\"\n    ss = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    ss.connect(server_addr)\n    ss.settimeout(timeout)\n\n    # request challenge number\n    if challenge in (-1, 0):\n        ss.send(_pack('<lci', -1, b'V', challenge))\n        try:\n            _, header, challenge = _unpack_from('<lcl', ss.recv(512))\n        except:\n            ss.close()\n            raise\n\n        if header != b'A':\n            raise RuntimeError(\"Unexpected challenge response\")\n\n    # request player info\n    ss.send(_pack('<lci', -1, b'V', challenge))\n\n    try:\n        data = StructReader(_handle_a2s_response(ss))\n    finally:\n        ss.close()\n\n    header, num_rules = data.unpack('<4xcH')\n\n    if header != b'E':\n        raise RuntimeError(\"Invalid reponse header - %s\" % repr(header))\n\n    rules = {}\n\n    while len(rules) != num_rules:\n        name = data.read_cstring()\n        value = data.read_cstring()\n\n        if _re_match(r'^\\-?[0-9]+$', value):\n            value = int(value)\n        elif _re_match(r'^\\-?[0-9]+\\.[0-9]+$', value):\n            value = float(value)\n\n        rules[name] = value\n\n    return rules"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nping a server and return the ping response in milliseconds.", "response": "def a2s_ping(server_addr, timeout=2):\n    \"\"\"Ping a server\n\n    .. warning::\n        This method for pinging is considered deprecated and may not work on certian servers.\n        Use :func:`.a2s_info` instead.\n\n    :param server_addr: (ip, port) for the server\n    :type  server_addr: tuple\n    :param timeout: (optional) timeout in seconds\n    :type  timeout: float\n    :raises: :class:`RuntimeError`, :class:`socket.timeout`\n    :returns: ping response in milliseconds or `None` for timeout\n    :rtype: :class:`float`\n    \"\"\"\n    ss = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    ss.connect(server_addr)\n    ss.settimeout(timeout)\n\n    ss.send(_pack('<lc', -1, b'i'))\n    start = _time()\n\n    try:\n        data = _handle_a2s_response(ss)\n    finally:\n        ss.close()\n\n    ping = max(0.0, _time() - start) * 1000\n\n    if data[4:5] == b'j':\n        return ping"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget protobuf for a given EMsg object", "response": "def get_cmsg(emsg):\n    \"\"\"Get protobuf for a given EMsg\n\n    :param emsg: EMsg\n    :type emsg: :class:`steam.enums.emsg.EMsg`, :class:`int`\n    :return: protobuf message\n    \"\"\"\n    if not isinstance(emsg, EMsg):\n        emsg = EMsg(emsg)\n\n    if emsg in cmsg_lookup_predefined:\n        return cmsg_lookup_predefined[emsg]\n    else:\n        enum_name = emsg.name.lower()\n        if enum_name.startswith(\"econ\"):  # special case for 'EconTrading_'\n            enum_name = enum_name[4:]\n        cmsg_name = \"cmsg\" + enum_name\n\n    return cmsg_lookup.get(cmsg_name, None)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_web_session_cookies(self):\n        if not self.logged_on: return None\n\n        resp = self.send_job_and_wait(MsgProto(EMsg.ClientRequestWebAPIAuthenticateUserNonce), timeout=7)\n\n        if resp is None: return None\n\n        skey, ekey = generate_session_key()\n\n        data = {\n            'steamid': self.steam_id,\n            'sessionkey': ekey,\n            'encrypted_loginkey': symmetric_encrypt(resp.webapi_authenticate_user_nonce.encode('ascii'), skey),\n        }\n\n        try:\n            resp = webapi.post('ISteamUserAuth', 'AuthenticateUser', 1, params=data)\n        except Exception as exp:\n            self._LOG.debug(\"get_web_session_cookies error: %s\" % str(exp))\n            return None\n\n        return {\n            'steamLogin': resp['authenticateuser']['token'],\n            'steamLoginSecure': resp['authenticateuser']['tokensecure'],\n        }", "response": "Get web authentication cookies via WebAPI s AuthenticateUser method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a requests. Session that is ready for use for use.", "response": "def get_web_session(self, language='english'):\n        \"\"\"Get a :class:`requests.Session` that is ready for use\n\n        See :meth:`get_web_session_cookies`\n\n        .. note::\n            Auth cookies will only be send to ``(help|store).steampowered.com`` and ``steamcommunity.com`` domains\n\n        .. note::\n            The session is valid only while :class:`.SteamClient` instance is logged on.\n\n        :param language: localization language for steam pages\n        :type language: :class:`str`\n        :return: authenticated Session ready for use\n        :rtype: :class:`requests.Session`, :class:`None`\n        \"\"\"\n        if self._web_session:\n            return self._web_session\n\n        cookies = self.get_web_session_cookies()\n        if cookies is None:\n            return None\n\n        self._web_session = session = make_requests_session()\n        session_id = generate_session_id()\n\n        for domain in ['store.steampowered.com', 'help.steampowered.com', 'steamcommunity.com']:\n            for name, val in cookies.items():\n                secure = (name == 'steamLoginSecure')\n                session.cookies.set(name, val, domain=domain, secure=secure)\n\n            session.cookies.set('Steam_Language', language, domain=domain)\n            session.cookies.set('birthtime', '-3333', domain=domain)\n            session.cookies.set('sessionid', session_id, domain=domain)\n\n        return session"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a requests session for the availabe", "response": "def make_requests_session():\n    \"\"\"\n    :returns: requests session\n    :rtype: :class:`requests.Session`\n    \"\"\"\n    session = requests.Session()\n\n    version = __import__('steam').__version__\n    ua = \"python-steam/{0} {1}\".format(version,\n                                session.headers['User-Agent'])\n    session.headers['User-Agent'] = ua\n\n    return session"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchanges the status of the current user s persona.", "response": "def change_status(self, **kwargs):\n        \"\"\"\n        Set name, persona state, flags\n\n        .. note::\n            Changing persona state will also change :attr:`persona_state`\n\n        :param persona_state: persona state (Online/Offlane/Away/etc)\n        :type persona_state: :class:`.EPersonaState`\n        :param player_name: profile name\n        :type player_name: :class:`str`\n        :param persona_state_flags: persona state flags\n        :type persona_state_flags: :class:`.EPersonaStateFlag`\n        \"\"\"\n        if not kwargs: return\n\n        self.persona_state = kwargs.get('persona_state', self.persona_state)\n\n        message = MsgProto(EMsg.ClientChangeStatus)\n        proto_fill_from_dict(message.body, kwargs)\n        self.send(message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef request_persona_state(self, steam_ids, state_flags=863):\n        m = MsgProto(EMsg.ClientRequestFriendData)\n        m.body.persona_state_requested = state_flags\n        m.body.friends.extend(steam_ids)\n        self.send(m)", "response": "Request persona state data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a SteamUser instance for the given steam id.", "response": "def get_user(self, steam_id, fetch_persona_state=True):\n        \"\"\"Get :class:`.SteamUser` instance for ``steam id``\n\n        :param steam_id: steam id\n        :type steam_id: :class:`int`, :class:`.SteamID`\n        :param fetch_persona_state: whether to request person state when necessary\n        :type fetch_persona_state: :class:`bool`\n        :return: SteamUser instance\n        :rtype: :class:`.SteamUser`\n        \"\"\"\n        steam_id = int(steam_id)\n        suser = self._user_cache.get(steam_id, None)\n\n        if suser is None:\n            suser = SteamUser(steam_id, self)\n            self._user_cache[steam_id] = suser\n\n            if fetch_persona_state:\n                self.request_persona_state([steam_id])\n\n        return suser"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the apps being played by the user.", "response": "def games_played(self, app_ids):\n        \"\"\"\n        Set the apps being played by the user\n\n        :param app_ids: a list of application ids\n        :type app_ids: :class:`list`\n\n        These app ids will be recorded in :attr:`current_games_played`.\n        \"\"\"\n        if not isinstance(app_ids, list):\n            raise ValueError(\"Expected app_ids to be of type list\")\n\n        self.current_games_played = app_ids = list(map(int, app_ids))\n\n        self.send(MsgProto(EMsg.ClientGamesPlayed),\n                  {'games_played': [{'game_id': app_id} for app_id in app_ids]}\n                  )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef new(sequence_count, start_time, process_id, box_id):\n        if not isinstance(start_time, datetime):\n            start_time = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n        start_time_seconds = int((start_time - datetime(2005, 1, 1)).total_seconds())\n\n        return (box_id << 54) | (process_id << 50) | (start_time_seconds << 20) | sequence_count", "response": "Make a new GlobalID"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_shared_people(self):\n        people = []\n        output = self._get_data()\n        self._logger.debug(output)\n        shared_entries = output[0] or []\n        for info in shared_entries:\n            try:\n                people.append(Person(info))\n            except InvalidData:\n                self._logger.debug('Missing location or other info, dropping person with info: %s', info)\n        return people", "response": "Retrieves all people that share their location with this account"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_authenticated_person(self):\n        try:\n            output = self._get_data()\n            self._logger.debug(output)\n            person = Person([\n                self.email,\n                output[9][1],\n                None,\n                None,\n                None,\n                None,\n                [\n                    None,\n                    None,\n                    self.email,\n                    self.email\n                ],\n                None,\n                None,\n                None,\n                None,\n                None,\n                None,\n                None,\n            ])\n        except (IndexError, TypeError, InvalidData):\n            self._logger.debug('Missing essential info, cannot instantiate authenticated person')\n            return None\n        return person", "response": "Retrieves the authenticated person associated with this account"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves a person by nickname", "response": "def get_person_by_nickname(self, nickname):\n        \"\"\"Retrieves a person by nickname\"\"\"\n        return next((person for person in self.get_all_people()\n                     if person.nickname.lower() == nickname.lower()), None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_person_by_full_name(self, name):\n        return next((person for person in self.get_all_people()\n                     if person.full_name.lower() == name.lower()), None)", "response": "Retrieves a person by full name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_coordinates_by_nickname(self, nickname):\n        person = self.get_person_by_nickname(nickname)\n        if not person:\n            return '', ''\n        return person.latitude, person.longitude", "response": "Retrieves a person s coordinates by nickname"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve a person s coordinates by full name", "response": "def get_coordinates_by_full_name(self, name):\n        \"\"\"Retrieves a person's coordinates by full name\"\"\"\n        person = self.get_person_by_full_name(name)\n        if not person:\n            return '', ''\n        return person.latitude, person.longitude"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncrosses - platform version of os. path. isabs.", "response": "def xisabs(filename):\n  \"\"\" Cross-platform version of `os.path.isabs()`\n      Returns True if `filename` is absolute on\n      Linux, OS X or Windows.\n  \"\"\"\n  if filename.startswith(b'/'):     # Linux/Unix\n    return True\n  elif filename.startswith(b'\\\\'):  # Windows\n    return True\n  elif re.match(b'\\\\w:[\\\\\\\\/]', filename): # Windows\n    return True\n  return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef xnormpath(path):\n  # replace escapes and Windows slashes\n  normalized = posixpath.normpath(path).replace(b'\\\\', b'/')\n  # fold the result\n  return posixpath.normpath(normalized)", "response": "Cross - platform version of os. path. normpath."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking relative path out of absolute by stripping prefixes used on Linux OS X and Windows.", "response": "def xstrip(filename):\n  \"\"\" Make relative path out of absolute by stripping\n      prefixes used on Linux, OS X and Windows.\n\n      This function is critical for security.\n  \"\"\"\n  while xisabs(filename):\n    # strip windows drive with all slashes\n    if re.match(b'\\\\w:[\\\\\\\\/]', filename):\n      filename = re.sub(b'^\\\\w+:[\\\\\\\\/]+', b'', filename)\n    # strip all slashes\n    elif re.match(b'[\\\\\\\\/]', filename):\n      filename = re.sub(b'^[\\\\\\\\/]+', b'', filename)\n  return filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fromfile(filename):\n  patchset = PatchSet()\n  debug(\"reading %s\" % filename)\n  fp = open(filename, \"rb\")\n  res = patchset.parse(fp)\n  fp.close()\n  if res == True:\n    return patchset\n  return False", "response": "Parse a file into a new PatchSet object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a text string and return a PatchSet object or False if parsing fails.", "response": "def fromstring(s):\n  \"\"\" Parse text string and return PatchSet()\n      object (or False if parsing fails)\n  \"\"\"\n  ps = PatchSet( StringIO(s) )\n  if ps.errors == 0:\n    return ps\n  return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a patch from a URL.", "response": "def fromurl(url):\n  \"\"\" Parse patch from an URL, return False\n      if an error occured. Note that this also\n      can throw urlopen() exceptions.\n  \"\"\"\n  ps = PatchSet( urllib_request.urlopen(url) )\n  if ps.errors == 0:\n    return ps\n  return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstripping n leading components from the given path.", "response": "def pathstrip(path, n):\n  \"\"\" Strip n leading components from the given path \"\"\"\n  pathlist = [path]\n  while os.path.dirname(pathlist[0]) != b'':\n    pathlist[0:1] = os.path.split(pathlist[0])\n  return b'/'.join(pathlist[n:])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(self, stream):\n    lineends = dict(lf=0, crlf=0, cr=0)\n    nexthunkno = 0    #: even if index starts with 0 user messages number hunks from 1\n\n    p = None\n    hunk = None\n    # hunkactual variable is used to calculate hunk lines for comparison\n    hunkactual = dict(linessrc=None, linestgt=None)\n\n\n    class wrapumerate(enumerate):\n      \"\"\"Enumerate wrapper that uses boolean end of stream status instead of\n      StopIteration exception, and properties to access line information.\n      \"\"\"\n\n      def __init__(self, *args, **kwargs):\n        # we don't call parent, it is magically created by __new__ method\n\n        self._exhausted = False\n        self._lineno = False     # after end of stream equal to the num of lines\n        self._line = False       # will be reset to False after end of stream\n\n      def next(self):\n        \"\"\"Try to read the next line and return True if it is available,\n           False if end of stream is reached.\"\"\"\n        if self._exhausted:\n          return False\n\n        try:\n          self._lineno, self._line = compat_next(super(wrapumerate, self))\n        except StopIteration:\n          self._exhausted = True\n          self._line = False\n          return False\n        return True\n\n      @property\n      def is_empty(self):\n        return self._exhausted\n\n      @property\n      def line(self):\n        return self._line\n\n      @property\n      def lineno(self):\n        return self._lineno\n\n    # define states (possible file regions) that direct parse flow\n    headscan  = True  # start with scanning header\n    filenames = False # lines starting with --- and +++\n\n    hunkhead = False  # @@ -R +R @@ sequence\n    hunkbody = False  #\n    hunkskip = False  # skipping invalid hunk mode\n\n    hunkparsed = False # state after successfully parsed hunk\n\n    # regexp to match start of hunk, used groups - 1,3,4,6\n    re_hunk_start = re.compile(b\"^@@ -(\\d+)(,(\\d+))? \\+(\\d+)(,(\\d+))? @@\")\n\n    self.errors = 0\n    # temp buffers for header and filenames info\n    header = []\n    srcname = None\n    tgtname = None\n\n    # start of main cycle\n    # each parsing block already has line available in fe.line\n    fe = wrapumerate(stream)\n    while fe.next():\n\n      # -- deciders: these only switch state to decide who should process\n      # --           line fetched at the start of this cycle\n      if hunkparsed:\n        hunkparsed = False\n        if re_hunk_start.match(fe.line):\n            hunkhead = True\n        elif fe.line.startswith(b\"--- \"):\n            filenames = True\n        else:\n            headscan = True\n      # -- ------------------------------------\n\n      # read out header\n      if headscan:\n        while not fe.is_empty and not fe.line.startswith(b\"--- \"):\n            header.append(fe.line)\n            fe.next()\n        if fe.is_empty:\n            if p == None:\n              debug(\"no patch data found\")  # error is shown later\n              self.errors += 1\n            else:\n              info(\"%d unparsed bytes left at the end of stream\" % len(b''.join(header)))\n              self.warnings += 1\n              # TODO check for \\No new line at the end..\n              # TODO test for unparsed bytes\n              # otherwise error += 1\n            # this is actually a loop exit\n            continue\n\n        headscan = False\n        # switch to filenames state\n        filenames = True\n\n      line = fe.line\n      lineno = fe.lineno\n\n\n      # hunkskip and hunkbody code skipped until definition of hunkhead is parsed\n      if hunkbody:\n        # [x] treat empty lines inside hunks as containing single space\n        #     (this happens when diff is saved by copy/pasting to editor\n        #      that strips trailing whitespace)\n        if line.strip(b\"\\r\\n\") == b\"\":\n            debug(\"expanding empty line in a middle of hunk body\")\n            self.warnings += 1\n            line = b' ' + line\n\n        # process line first\n        if re.match(b\"^[- \\\\+\\\\\\\\]\", line):\n            # gather stats about line endings\n            if line.endswith(b\"\\r\\n\"):\n              p.hunkends[\"crlf\"] += 1\n            elif line.endswith(b\"\\n\"):\n              p.hunkends[\"lf\"] += 1\n            elif line.endswith(b\"\\r\"):\n              p.hunkends[\"cr\"] += 1\n\n            if line.startswith(b\"-\"):\n              hunkactual[\"linessrc\"] += 1\n            elif line.startswith(b\"+\"):\n              hunkactual[\"linestgt\"] += 1\n            elif not line.startswith(b\"\\\\\"):\n              hunkactual[\"linessrc\"] += 1\n              hunkactual[\"linestgt\"] += 1\n            hunk.text.append(line)\n            # todo: handle \\ No newline cases\n        else:\n            warning(\"invalid hunk no.%d at %d for target file %s\" % (nexthunkno, lineno+1, p.target))\n            # add hunk status node\n            hunk.invalid = True\n            p.hunks.append(hunk)\n            self.errors += 1\n            # switch to hunkskip state\n            hunkbody = False\n            hunkskip = True\n\n        # check exit conditions\n        if hunkactual[\"linessrc\"] > hunk.linessrc or hunkactual[\"linestgt\"] > hunk.linestgt:\n            warning(\"extra lines for hunk no.%d at %d for target %s\" % (nexthunkno, lineno+1, p.target))\n            # add hunk status node\n            hunk.invalid = True\n            p.hunks.append(hunk)\n            self.errors += 1\n            # switch to hunkskip state\n            hunkbody = False\n            hunkskip = True\n        elif hunk.linessrc == hunkactual[\"linessrc\"] and hunk.linestgt == hunkactual[\"linestgt\"]:\n            # hunk parsed successfully\n            p.hunks.append(hunk)\n            # switch to hunkparsed state\n            hunkbody = False\n            hunkparsed = True\n\n            # detect mixed window/unix line ends\n            ends = p.hunkends\n            if ((ends[\"cr\"]!=0) + (ends[\"crlf\"]!=0) + (ends[\"lf\"]!=0)) > 1:\n              warning(\"inconsistent line ends in patch hunks for %s\" % p.source)\n              self.warnings += 1\n            if debugmode:\n              debuglines = dict(ends)\n              debuglines.update(file=p.target, hunk=nexthunkno)\n              debug(\"crlf: %(crlf)d  lf: %(lf)d  cr: %(cr)d\\t - file: %(file)s hunk: %(hunk)d\" % debuglines)\n            # fetch next line\n            continue\n\n      if hunkskip:\n        if re_hunk_start.match(line):\n          # switch to hunkhead state\n          hunkskip = False\n          hunkhead = True\n        elif line.startswith(b\"--- \"):\n          # switch to filenames state\n          hunkskip = False\n          filenames = True\n          if debugmode and len(self.items) > 0:\n            debug(\"- %2d hunks for %s\" % (len(p.hunks), p.source))\n\n      if filenames:\n        if line.startswith(b\"--- \"):\n          if srcname != None:\n            # XXX testcase\n            warning(\"skipping false patch for %s\" % srcname)\n            srcname = None\n            # XXX header += srcname\n            # double source filename line is encountered\n            # attempt to restart from this second line\n          re_filename = b\"^--- ([^\\t]+)\"\n          match = re.match(re_filename, line)\n          # todo: support spaces in filenames\n          if match:\n            srcname = match.group(1).strip()\n          else:\n            warning(\"skipping invalid filename at line %d\" % (lineno+1))\n            self.errors += 1\n            # XXX p.header += line\n            # switch back to headscan state\n            filenames = False\n            headscan = True\n        elif not line.startswith(b\"+++ \"):\n          if srcname != None:\n            warning(\"skipping invalid patch with no target for %s\" % srcname)\n            self.errors += 1\n            srcname = None\n            # XXX header += srcname\n            # XXX header += line\n          else:\n            # this should be unreachable\n            warning(\"skipping invalid target patch\")\n          filenames = False\n          headscan = True\n        else:\n          if tgtname != None:\n            # XXX seems to be a dead branch\n            warning(\"skipping invalid patch - double target at line %d\" % (lineno+1))\n            self.errors += 1\n            srcname = None\n            tgtname = None\n            # XXX header += srcname\n            # XXX header += tgtname\n            # XXX header += line\n            # double target filename line is encountered\n            # switch back to headscan state\n            filenames = False\n            headscan = True\n          else:\n            re_filename = b\"^\\+\\+\\+ ([^\\t]+)\"\n            match = re.match(re_filename, line)\n            if not match:\n              warning(\"skipping invalid patch - no target filename at line %d\" % (lineno+1))\n              self.errors += 1\n              srcname = None\n              # switch back to headscan state\n              filenames = False\n              headscan = True\n            else:\n              if p: # for the first run p is None\n                self.items.append(p)\n              p = Patch()\n              p.source = srcname\n              srcname = None\n              p.target = match.group(1).strip()\n              p.header = header\n              header = []\n              # switch to hunkhead state\n              filenames = False\n              hunkhead = True\n              nexthunkno = 0\n              p.hunkends = lineends.copy()\n              continue\n\n      if hunkhead:\n        match = re.match(b\"^@@ -(\\d+)(,(\\d+))? \\+(\\d+)(,(\\d+))? @@(.*)\", line)\n        if not match:\n          if not p.hunks:\n            warning(\"skipping invalid patch with no hunks for file %s\" % p.source)\n            self.errors += 1\n            # XXX review switch\n            # switch to headscan state\n            hunkhead = False\n            headscan = True\n            continue\n          else:\n            # TODO review condition case\n            # switch to headscan state\n            hunkhead = False\n            headscan = True\n        else:\n          hunk = Hunk()\n          hunk.startsrc = int(match.group(1))\n          hunk.linessrc = 1\n          if match.group(3): hunk.linessrc = int(match.group(3))\n          hunk.starttgt = int(match.group(4))\n          hunk.linestgt = 1\n          if match.group(6): hunk.linestgt = int(match.group(6))\n          hunk.invalid = False\n          hunk.desc = match.group(7)[1:].rstrip()\n          hunk.text = []\n\n          hunkactual[\"linessrc\"] = hunkactual[\"linestgt\"] = 0\n\n          # switch to hunkbody state\n          hunkhead = False\n          hunkbody = True\n          nexthunkno += 1\n          continue\n\n    # /while fe.next()\n\n    if p:\n      self.items.append(p)\n\n    if not hunkparsed:\n      if hunkskip:\n        warning(\"warning: finished with errors, some hunks may be invalid\")\n      elif headscan:\n        if len(self.items) == 0:\n          warning(\"error: no patch data found!\")\n          return False\n        else: # extra data at the end of file\n          pass\n      else:\n        warning(\"error: patch stream is incomplete!\")\n        self.errors += 1\n        if len(self.items) == 0:\n          return False\n\n    if debugmode and len(self.items) > 0:\n        debug(\"- %2d hunks for %s\" % (len(p.hunks), p.source))\n\n    # XXX fix total hunks calculation\n    debug(\"total files: %d  total hunks: %d\" % (len(self.items),\n        sum(len(p.hunks) for p in self.items)))\n\n    # ---- detect patch and patchset types ----\n    for idx, p in enumerate(self.items):\n      self.items[idx].type = self._detect_type(p)\n\n    types = set([p.type for p in self.items])\n    if len(types) > 1:\n      self.type = MIXED\n    else:\n      self.type = types.pop()\n    # --------\n\n    self._normalize_filenames()\n\n    return (self.errors == 0)", "response": "parse unified diff into a new unified diff"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetects and return type for the specified Patch object", "response": "def _detect_type(self, p):\n    \"\"\" detect and return type for the specified Patch object\n        analyzes header and filenames info\n\n        NOTE: must be run before filenames are normalized\n    \"\"\"\n\n    # check for SVN\n    #  - header starts with Index:\n    #  - next line is ===... delimiter\n    #  - filename is followed by revision number\n    # TODO add SVN revision\n    if (len(p.header) > 1 and p.header[-2].startswith(b\"Index: \")\n          and p.header[-1].startswith(b\"=\"*67)):\n        return SVN\n\n    # common checks for both HG and GIT\n    DVCS = ((p.source.startswith(b'a/') or p.source == b'/dev/null')\n        and (p.target.startswith(b'b/') or p.target == b'/dev/null'))\n\n    # GIT type check\n    #  - header[-2] is like \"diff --git a/oldname b/newname\"\n    #  - header[-1] is like \"index <hash>..<hash> <mode>\"\n    # TODO add git rename diffs and add/remove diffs\n    #      add git diff with spaced filename\n    # TODO http://www.kernel.org/pub/software/scm/git/docs/git-diff.html\n\n    # Git patch header len is 2 min\n    if len(p.header) > 1:\n      # detect the start of diff header - there might be some comments before\n      for idx in reversed(range(len(p.header))):\n        if p.header[idx].startswith(b\"diff --git\"):\n          break\n      if p.header[idx].startswith(b'diff --git a/'):\n        if (idx+1 < len(p.header)\n            and re.match(b'index \\\\w{7}..\\\\w{7} \\\\d{6}', p.header[idx+1])):\n          if DVCS:\n            return GIT\n\n    # HG check\n    #\n    #  - for plain HG format header is like \"diff -r b2d9961ff1f5 filename\"\n    #  - for Git-style HG patches it is \"diff --git a/oldname b/newname\"\n    #  - filename starts with a/, b/ or is equal to /dev/null\n    #  - exported changesets also contain the header\n    #    # HG changeset patch\n    #    # User name@example.com\n    #    ...\n    # TODO add MQ\n    # TODO add revision info\n    if len(p.header) > 0:\n      if DVCS and re.match(b'diff -r \\\\w{12} .*', p.header[-1]):\n        return HG\n      if DVCS and p.header[-1].startswith(b'diff --git a/'):\n        if len(p.header) == 1:  # native Git patch header len is 2\n          return HG\n        elif p.header[0].startswith(b'# HG changeset patch'):\n          return HG\n\n    return PLAIN"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsanitize filenames and return a list of dicts", "response": "def _normalize_filenames(self):\n    \"\"\" sanitize filenames, normalizing paths, i.e.:\n        1. strip a/ and b/ prefixes from GIT and HG style patches\n        2. remove all references to parent directories (with warning)\n        3. translate any absolute paths to relative (with warning)\n\n        [x] always use forward slashes to be crossplatform\n            (diff/patch were born as a unix utility after all)\n\n        return None\n    \"\"\"\n    if debugmode:\n      debug(\"normalize filenames\")\n    for i,p in enumerate(self.items):\n      # if debugmode:\n        # debug(\"    patch type = \" + p.type)\n        # debug(\"    source = \" + p.source.)\n        # debug(\"    target = \" + p.target)\n      if p.type in (HG, GIT):\n        # TODO: figure out how to deal with /dev/null entries\n        debug(\"stripping a/ and b/ prefixes\")\n        if p.source != '/dev/null':\n          if not p.source.startswith(b\"a/\"):\n            warning(\"invalid source filename\")\n          else:\n            p.source = p.source[2:]\n        if p.target != '/dev/null':\n          if not p.target.startswith(b\"b/\"):\n            warning(\"invalid target filename\")\n          else:\n            p.target = p.target[2:]\n\n      p.source = xnormpath(p.source)\n      p.target = xnormpath(p.target)\n\n      sep = b'/'  # sep value can be hardcoded, but it looks nice this way\n\n      # references to parent are not allowed\n      if p.source.startswith(b\"..\" + sep):\n        warning(\"error: stripping parent path for source file patch no.%d\" % (i+1))\n        self.warnings += 1\n        while p.source.startswith(b\"..\" + sep):\n          p.source = p.source.partition(sep)[2]\n      if p.target.startswith(b\"..\" + sep):\n        warning(\"error: stripping parent path for target file patch no.%d\" % (i+1))\n        self.warnings += 1\n        while p.target.startswith(b\"..\" + sep):\n          p.target = p.target.partition(sep)[2]\n      # absolute paths are not allowed\n      if xisabs(p.source) or xisabs(p.target):\n        warning(\"error: absolute paths are not allowed - file no.%d\" % (i+1))\n        self.warnings += 1\n        if xisabs(p.source):\n          warning(\"stripping absolute path from source name '%s'\" % p.source)\n          p.source = xstrip(p.source)\n        if xisabs(p.target):\n          warning(\"stripping absolute path from target name '%s'\" % p.target)\n          p.target = xstrip(p.target)\n\n      self.items[i].source = p.source\n      self.items[i].target = p.target"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating diffstat and return as a string", "response": "def diffstat(self):\n    \"\"\" calculate diffstat and return as a string\n        Notes:\n          - original diffstat ouputs target filename\n          - single + or - shouldn't escape histogram\n    \"\"\"\n    names = []\n    insert = []\n    delete = []\n    delta = 0    # size change in bytes\n    namelen = 0\n    maxdiff = 0  # max number of changes for single file\n                 # (for histogram width calculation)\n    for patch in self.items:\n      i,d = 0,0\n      for hunk in patch.hunks:\n        for line in hunk.text:\n          if line.startswith(b'+'):\n            i += 1\n            delta += len(line)-1\n          elif line.startswith(b'-'):\n            d += 1\n            delta -= len(line)-1\n      names.append(patch.target)\n      insert.append(i)\n      delete.append(d)\n      namelen = max(namelen, len(patch.target))\n      maxdiff = max(maxdiff, i+d)\n    output = ''\n    statlen = len(str(maxdiff))  # stats column width\n    for i,n in enumerate(names):\n      # %-19s | %-4d %s\n      format = \" %-\" + str(namelen) + \"s | %\" + str(statlen) + \"s %s\\n\"\n\n      hist = ''\n      # -- calculating histogram --\n      width = len(format % ('', '', ''))\n      histwidth = max(2, 80 - width)\n      if maxdiff < histwidth:\n        hist = \"+\"*insert[i] + \"-\"*delete[i]\n      else:\n        iratio = (float(insert[i]) / maxdiff) * histwidth\n        dratio = (float(delete[i]) / maxdiff) * histwidth\n\n        # make sure every entry gets at least one + or -\n        iwidth = 1 if 0 < iratio < 1 else int(iratio)\n        dwidth = 1 if 0 < dratio < 1 else int(dratio)\n        #print(iratio, dratio, iwidth, dwidth, histwidth)\n        hist = \"+\"*int(iwidth) + \"-\"*int(dwidth)\n      # -- /calculating +- histogram --\n      output += (format % (tostr(names[i]), str(insert[i] + delete[i]), hist))\n\n    output += (\" %d files changed, %d insertions(+), %d deletions(-), %+d bytes\"\n               % (len(names), sum(insert), sum(delete), delta))\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the file that should be patched", "response": "def findfile(self, old, new):\n    \"\"\" return name of file to be patched or None \"\"\"\n    if exists(old):\n      return old\n    elif exists(new):\n      return new\n    else:\n      # [w] Google Code generates broken patches with its online editor\n      debug(\"broken patch from Google Code, stripping prefixes..\")\n      if old.startswith(b'a/') and new.startswith(b'b/'):\n        old, new = old[2:], new[2:]\n        debug(\"   %s\" % old)\n        debug(\"   %s\" % new)\n        if exists(old):\n          return old\n        elif exists(new):\n          return new\n      return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying parsed patch to a list of items.", "response": "def apply(self, strip=0, root=None):\n    \"\"\" Apply parsed patch, optionally stripping leading components\n        from file paths. `root` parameter specifies working dir.\n        return True on success\n    \"\"\"\n    if root:\n      prevdir = os.getcwd()\n      os.chdir(root)\n\n    total = len(self.items)\n    errors = 0\n    if strip:\n      # [ ] test strip level exceeds nesting level\n      #   [ ] test the same only for selected files\n      #     [ ] test if files end up being on the same level\n      try:\n        strip = int(strip)\n      except ValueError:\n        errors += 1\n        warning(\"error: strip parameter '%s' must be an integer\" % strip)\n        strip = 0\n\n    #for fileno, filename in enumerate(self.source):\n    for i,p in enumerate(self.items):\n      if strip:\n        debug(\"stripping %s leading component(s) from:\" % strip)\n        debug(\"   %s\" % p.source)\n        debug(\"   %s\" % p.target)\n        old = pathstrip(p.source, strip)\n        new = pathstrip(p.target, strip)\n      else:\n        old, new = p.source, p.target\n\n      filename = self.findfile(old, new)\n\n      if not filename:\n          warning(\"source/target file does not exist:\\n  --- %s\\n  +++ %s\" % (old, new))\n          errors += 1\n          continue\n      if not isfile(filename):\n        warning(\"not a file - %s\" % filename)\n        errors += 1\n        continue\n\n      # [ ] check absolute paths security here\n      debug(\"processing %d/%d:\\t %s\" % (i+1, total, filename))\n\n      # validate before patching\n      f2fp = open(filename, 'rb')\n      hunkno = 0\n      hunk = p.hunks[hunkno]\n      hunkfind = []\n      hunkreplace = []\n      validhunks = 0\n      canpatch = False\n      for lineno, line in enumerate(f2fp):\n        if lineno+1 < hunk.startsrc:\n          continue\n        elif lineno+1 == hunk.startsrc:\n          hunkfind = [x[1:].rstrip(b\"\\r\\n\") for x in hunk.text if x[0] in b\" -\"]\n          hunkreplace = [x[1:].rstrip(b\"\\r\\n\") for x in hunk.text if x[0] in b\" +\"]\n          #pprint(hunkreplace)\n          hunklineno = 0\n\n          # todo \\ No newline at end of file\n\n        # check hunks in source file\n        if lineno+1 < hunk.startsrc+len(hunkfind)-1:\n          if line.rstrip(b\"\\r\\n\") == hunkfind[hunklineno]:\n            hunklineno+=1\n          else:\n            info(\"file %d/%d:\\t %s\" % (i+1, total, filename))\n            info(\" hunk no.%d doesn't match source file at line %d\" % (hunkno+1, lineno+1))\n            info(\"  expected: %s\" % hunkfind[hunklineno])\n            info(\"  actual  : %s\" % line.rstrip(b\"\\r\\n\"))\n            # not counting this as error, because file may already be patched.\n            # check if file is already patched is done after the number of\n            # invalid hunks if found\n            # TODO: check hunks against source/target file in one pass\n            #   API - check(stream, srchunks, tgthunks)\n            #           return tuple (srcerrs, tgterrs)\n\n            # continue to check other hunks for completeness\n            hunkno += 1\n            if hunkno < len(p.hunks):\n              hunk = p.hunks[hunkno]\n              continue\n            else:\n              break\n\n        # check if processed line is the last line\n        if lineno+1 == hunk.startsrc+len(hunkfind)-1:\n          debug(\" hunk no.%d for file %s  -- is ready to be patched\" % (hunkno+1, filename))\n          hunkno+=1\n          validhunks+=1\n          if hunkno < len(p.hunks):\n            hunk = p.hunks[hunkno]\n          else:\n            if validhunks == len(p.hunks):\n              # patch file\n              canpatch = True\n              break\n      else:\n        if hunkno < len(p.hunks):\n          warning(\"premature end of source file %s at hunk %d\" % (filename, hunkno+1))\n          errors += 1\n\n      f2fp.close()\n\n      if validhunks < len(p.hunks):\n        if self._match_file_hunks(filename, p.hunks):\n          warning(\"already patched  %s\" % filename)\n        else:\n          warning(\"source file is different - %s\" % filename)\n          errors += 1\n      if canpatch:\n        backupname = filename+b\".orig\"\n        if exists(backupname):\n          warning(\"can't backup original file to %s - aborting\" % backupname)\n        else:\n          import shutil\n          shutil.move(filename, backupname)\n          if self.write_hunks(backupname, filename, p.hunks):\n            info(\"successfully patched %d/%d:\\t %s\" % (i+1, total, filename))\n            os.unlink(backupname)\n          else:\n            errors += 1\n            warning(\"error patching file %s\" % filename)\n            shutil.copy(filename, filename+\".invalid\")\n            warning(\"invalid version is saved to %s\" % filename+\".invalid\")\n            # todo: proper rejects\n            shutil.move(backupname, filename)\n\n    if root:\n      os.chdir(prevdir)\n\n    # todo: check for premature eof\n    return (errors == 0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _reverse(self):\n    for p in self.items:\n      for h in p.hunks:\n        h.startsrc, h.starttgt = h.starttgt, h.startsrc\n        h.linessrc, h.linestgt = h.linestgt, h.linessrc\n        for i,line in enumerate(h.text):\n          # need to use line[0:1] here, because line[0]\n          # returns int instead of bytes on Python 3\n          if line[0:1] == b'+':\n            h.text[i] = b'-' + line[1:]\n          elif line[0:1] == b'-':\n            h.text[i] = b'+' +line[1:]", "response": "reverse patch direction (this doesn't touch filenames)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef revert(self, strip=0, root=None):\n    reverted = copy.deepcopy(self)\n    reverted._reverse()\n    return reverted.apply(strip, root)", "response": "apply patch in reverse order"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if specified filename can be patched. Returns None if file can t be patched.", "response": "def can_patch(self, filename):\n    \"\"\" Check if specified filename can be patched. Returns None if file can\n    not be found among source filenames. False if patch can not be applied\n    clearly. True otherwise.\n\n    :returns: True, False or None\n    \"\"\"\n    filename = abspath(filename)\n    for p in self.items:\n      if filename == abspath(p.source):\n        return self._match_file_hunks(filename, p.hunks)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the software name and returns the path of the binary.", "response": "def get_binary_path(executable, logging_level='INFO'):\n    \"\"\"Gets the software name and returns the path of the binary.\"\"\"\n    if sys.platform == 'win32':\n        if executable == 'start':\n            return executable\n        executable = executable + '.exe'\n        if executable in os.listdir('.'):\n            binary = os.path.join(os.getcwd(), executable)\n        else:\n            binary = next((os.path.join(path, executable)\n                           for path in os.environ['PATH'].split(os.pathsep)\n                           if os.path.isfile(os.path.join(path, executable))), None)\n    else:\n        venv_parent = get_venv_parent_path()\n        venv_bin_path = os.path.join(venv_parent, '.venv', 'bin')\n        if not venv_bin_path in os.environ.get('PATH'):\n            if logging_level == 'DEBUG':\n                print(f'Adding path {venv_bin_path} to environment PATH variable')\n            os.environ['PATH'] = os.pathsep.join([os.environ['PATH'], venv_bin_path])\n        binary = shutil.which(executable)\n    return binary if binary else None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nchange into a given directory and cleans up after it is done", "response": "def cd(new_directory, clean_up=lambda: True):  # pylint: disable=invalid-name\n    \"\"\"Changes into a given directory and cleans up after it is done\n\n    Args:\n        new_directory: The directory to change to\n        clean_up: A method to clean up the working directory once done\n\n    \"\"\"\n    previous_directory = os.getcwd()\n    os.chdir(os.path.expanduser(new_directory))\n    try:\n        yield\n    finally:\n        os.chdir(previous_directory)\n        clean_up()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a temporary directory", "response": "def tempdir():\n    \"\"\"Creates a temporary directory\"\"\"\n    directory_path = tempfile.mkdtemp()\n\n    def clean_up():  # pylint: disable=missing-docstring\n        shutil.rmtree(directory_path, onerror=on_error)\n\n    with cd(directory_path, clean_up):\n        yield directory_path"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nputs the two whole widgets in the correct position depending on compound.", "response": "def _grid_widgets(self):\n        \"\"\"Puts the two whole widgets in the correct position depending on compound.\"\"\"\n        scrollbar_column = 0 if self.__compound is tk.LEFT else 2\n        self.listbox.grid(row=0, column=1, sticky=\"nswe\")\n        self.scrollbar.grid(row=0, column=scrollbar_column, sticky=\"ns\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _on_click(self, event):\n        if callable(self.__callback):\n            self.__callback(self.selection)", "response": "Function bound to event of selection in the Combobox calls callback if callable otherwise calls self. selection"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef left_press(self, event):\n        self.current_coords = self.canvas.canvasx(event.x), self.canvas.canvasy(event.y)\n        self.set_current()\n        if self.current:\n            self.canvas.itemconfigure(self.current, fill=self.item_colors[self.current][1])\n            self.current = None\n            return\n        results = self.canvas.find_withtag(tk.CURRENT)\n        if len(results) is 0:\n            return\n        self.current = results[0]\n        self.canvas.itemconfigure(self.current, fill=self.item_colors[self.current][2])", "response": "Callback for the left mouse button. Selects a new item and sets its highlightcolor."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a new item on the Canvas.", "response": "def add_item(self, text, font=(\"default\", 12, \"bold\"), backgroundcolor=\"yellow\", textcolor=\"black\",\n                 highlightcolor=\"blue\"):\n        \"\"\"\n        Add a new item on the Canvas.\n        \n        :param text: text to display\n        :type text: str\n        :param font: font of the text\n        :type font: tuple or :class:`~tkinter.font.Font`\n        :param backgroundcolor: background color\n        :type  backgroundcolor: str\n        :param textcolor: text color\n        :type  textcolor: str\n        :param highlightcolor: the color of the text when the item is selected\n        :type  highlightcolor: str\n        \"\"\"\n        item = self.canvas.create_text(0, 0, anchor=tk.NW, text=text, font=font, fill=textcolor, tag=\"item\")\n        rectangle = self.canvas.create_rectangle(self.canvas.bbox(item), fill=backgroundcolor)\n        self.canvas.tag_lower(rectangle, item)\n        self.items[item] = rectangle\n        if callable(self._callback_add):\n            self._callback_add(item, rectangle)\n        self.item_colors[item] = (backgroundcolor, textcolor, highlightcolor)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef del_item(self):\n        item = self.current\n        rectangle = self.items[item]\n        self.canvas.delete(item, rectangle)\n        if callable(self._callback_del):\n            self._callback_del(item, rectangle)", "response": "Delete the current item on the Canvas."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the background image of the Canvas.", "response": "def set_background(self, image=None, path=None, resize=True):\n        \"\"\"\n        Set the background image of the Canvas.\n        \n        :param image: background image\n        :type image: PhotoImage\n        :param path: background image path\n        :type path: str\n        :param resize: whether to resize the image to the Canvas size\n        :type resize: bool\n        \"\"\"\n        if not image and not path:\n            raise ValueError(\"You must either pass a PhotoImage object or a path object\")\n        if image and path:\n            raise ValueError(\"You must pass either a PhotoImage or str path, not both\")\n        if image is not None and not isinstance(image, tk.PhotoImage) and not isinstance(image, ImageTk.PhotoImage):\n            raise ValueError(\"The image passed is not a PhotoImage object\")\n        if path is not None and not isinstance(path, str):\n            raise ValueError(\"The image path passed is not of str type: {0}\".format(path))\n        if path and not os.path.exists(path):\n            raise ValueError(\"The image path passed is not valid: {0}\".format(path))\n        if image is not None:\n            self._image = image\n        elif path is not None:\n            img = Image.open(path)\n            if resize:\n                img = img.resize((self._canvaswidth, self._canvasheight), Image.ANTIALIAS)\n            self._image = ImageTk.PhotoImage(img)\n        self._background = self.canvas.create_image(0, 0, image=self._image, anchor=tk.NW, tag=\"background\")\n        self.canvas.tag_lower(\"background\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nquery widget option. cget", "response": "def cget(self, key):\n        \"\"\"\n        Query widget option.\n\n        :param key: option name\n        :type key: str\n        :return: value of the option\n\n        To get the list of options for this widget, call the method :meth:`~ItemsCanvas.keys`.\n        \"\"\"\n        if key is \"canvaswidth\":\n            return self._canvaswidth\n        elif key is \"canvasheight\":\n            return self._canvasheight\n        elif key is \"function_new\":\n            return self._function_new\n        elif key is \"callback_add\":\n            return self._callback_add\n        elif key is \"callback_del\":\n            return self._callback_del\n        elif key is \"callback_move\":\n            return self._callback_move\n        else:\n            ttk.Frame.cget(self, key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconfigures resources of the widget.", "response": "def config(self, **kwargs):\n        \"\"\"\n        Configure resources of the widget.\n\n        To get the list of options for this widget, call the method :meth:`~ItemsCanvas.keys`.\n        See :meth:`~ItemsCanvas.__init__` for a description of the widget specific option.\n        \"\"\"\n        self._canvaswidth = kwargs.pop(\"canvaswidth\", self._canvaswidth)\n        self._canvasheight = kwargs.pop(\"canvasheight\", self._canvasheight)\n        self.canvas.config(width=self._canvaswidth, height=self._canvasheight)\n        self._function_new = kwargs.pop(\"function_new\", self._function_new)\n        self._callback_add = kwargs.pop(\"callback_add\", self._callback_add)\n        self._callback_del = kwargs.pop(\"callback_del\", self._callback_del)\n        self._callback_move = kwargs.pop(\"callback_move\", self._callback_move)\n        ttk.Frame.config(self, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef quit(self):\n        sys.stdout = self._oldstdout\n        sys.stderr = self._oldstderr\n        self.destroy()", "response": "Restore previous stdout and stderr and destroy the window."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self):\n        val = tk.StringVar.get(self)\n        try:\n            val = int(val)\n            if val < self._from:\n                val = self._from\n                self.set(val)\n            elif val > self._to:\n                val = self._to\n                self.set(val)\n        except ValueError:\n            val = 0\n            self.set(0)\n        return val", "response": "Returns the value of the attribute as an integer between the limits of the variable and the value of the attribute."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_completion_list(self, completion_list):\n        self._completion_list = sorted(completion_list, key=str.lower)  # Work with a sorted list\n        self._hits = []\n        self._hit_index = 0\n        self.position = 0\n        self.bind('<KeyRelease>', self.handle_keyrelease)", "response": "Set a new list of auto completion values for a specific locale."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef askcolor(color=\"red\", parent=None, title=_(\"Color Chooser\"), alpha=False):\n    col = ColorPicker(parent, color, alpha, title)\n    col.wait_window(col)\n    res = col.get_color()\n    if res:\n        return res[0], res[2]\n    else:\n        return None, None", "response": "Open a ColorPicker dialog and return the chosen color."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _unfocus(self, event):\n        w = self.focus_get()\n        if w != self and 'spinbox' not in str(w) and 'entry' not in str(w):\n            self.focus_set()", "response": "Unfocus palette items when click on bar or square."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reset_preview(self, event):\n        label = event.widget\n        label.master.focus_set()\n        label.master.configure(relief=\"sunken\")\n        args = self._old_color\n        if self.alpha_channel:\n            args += (self._old_alpha,)\n            self.alpha.set(self._old_alpha)\n            self.alphabar.set_color(args)\n        color = rgb_to_hexa(*args)\n        h, s, v = rgb_to_hsv(*self._old_color)\n        self.red.set(self._old_color[0])\n        self.green.set(self._old_color[1])\n        self.blue.set(self._old_color[2])\n        self.hue.set(h)\n        self.saturation.set(s)\n        self.value.set(v)\n        self.hexa.delete(0, \"end\")\n        self.hexa.insert(0, color.upper())\n        self.bar.set(h)\n        self.square.set_hsv((h, s, v))\n        self._update_preview()", "response": "Respond to user click on a palette item."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrespond to user click on a palette item.", "response": "def _palette_cmd(self, event):\n        \"\"\"Respond to user click on a palette item.\"\"\"\n        label = event.widget\n        label.master.focus_set()\n        label.master.configure(relief=\"sunken\")\n        r, g, b = self.winfo_rgb(label.cget(\"background\"))\n        r = round2(r * 255 / 65535)\n        g = round2(g * 255 / 65535)\n        b = round2(b * 255 / 65535)\n        args = (r, g, b)\n        if self.alpha_channel:\n            a = self.alpha.get()\n            args += (a,)\n            self.alphabar.set_color(args)\n        color = rgb_to_hexa(*args)\n        h, s, v = rgb_to_hsv(r, g, b)\n        self.red.set(r)\n        self.green.set(g)\n        self.blue.set(b)\n        self.hue.set(h)\n        self.saturation.set(s)\n        self.value.set(v)\n        self.hexa.delete(0, \"end\")\n        self.hexa.insert(0, color.upper())\n        self.bar.set(h)\n        self.square.set_hsv((h, s, v))\n        self._update_preview()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresponds to motion of the color selection cross.", "response": "def _change_sel_color(self, event):\n        \"\"\"Respond to motion of the color selection cross.\"\"\"\n        (r, g, b), (h, s, v), color = self.square.get()\n        self.red.set(r)\n        self.green.set(g)\n        self.blue.set(b)\n        self.saturation.set(s)\n        self.value.set(v)\n        self.hexa.delete(0, \"end\")\n        self.hexa.insert(0, color.upper())\n        if self.alpha_channel:\n            self.alphabar.set_color((r, g, b))\n            self.hexa.insert('end',\n                             (\"%2.2x\" % self.alpha.get()).upper())\n        self._update_preview()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _change_color(self, event):\n        h = self.bar.get()\n        self.square.set_hue(h)\n        (r, g, b), (h, s, v), sel_color = self.square.get()\n        self.red.set(r)\n        self.green.set(g)\n        self.blue.set(b)\n        self.hue.set(h)\n        self.saturation.set(s)\n        self.value.set(v)\n        self.hexa.delete(0, \"end\")\n        self.hexa.insert(0, sel_color.upper())\n        if self.alpha_channel:\n            self.alphabar.set_color((r, g, b))\n            self.hexa.insert('end',\n                             (\"%2.2x\" % self.alpha.get()).upper())\n        self._update_preview()", "response": "Respond to motion of the hsv cursor."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the color of the current HEX entry.", "response": "def _update_color_hexa(self, event=None):\n        \"\"\"Update display after a change in the HEX entry.\"\"\"\n        color = self.hexa.get().upper()\n        self.hexa.delete(0, 'end')\n        self.hexa.insert(0, color)\n        if re.match(r\"^#[0-9A-F]{6}$\", color):\n            r, g, b = hexa_to_rgb(color)\n            self.red.set(r)\n            self.green.set(g)\n            self.blue.set(b)\n            h, s, v = rgb_to_hsv(r, g, b)\n            self.hue.set(h)\n            self.saturation.set(s)\n            self.value.set(v)\n            self.bar.set(h)\n            self.square.set_hsv((h, s, v))\n            if self.alpha_channel:\n                a = self.alpha.get()\n                self.hexa.insert('end', (\"%2.2x\" % a).upper())\n                self.alphabar.set_color((r, g, b, a))\n        elif self.alpha_channel and re.match(r\"^#[0-9A-F]{8}$\", color):\n            r, g, b, a = hexa_to_rgb(color)\n            self.red.set(r)\n            self.green.set(g)\n            self.blue.set(b)\n            self.alpha.set(a)\n            self.alphabar.set_color((r, g, b, a))\n            h, s, v = rgb_to_hsv(r, g, b)\n            self.hue.set(h)\n            self.saturation.set(s)\n            self.value.set(v)\n            self.bar.set(h)\n            self.square.set_hsv((h, s, v))\n        else:\n            self._update_color_rgb()\n        self._update_preview()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the alpha spinbox.", "response": "def _update_alpha(self, event=None):\n        \"\"\"Update display after a change in the alpha spinbox.\"\"\"\n        a = self.alpha.get()\n        hexa = self.hexa.get()\n        hexa = hexa[:7] + (\"%2.2x\" % a).upper()\n        self.hexa.delete(0, 'end')\n        self.hexa.insert(0, hexa)\n        self.alphabar.set(a)\n        self._update_preview()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _update_color_hsv(self, event=None):\n        if event is None or event.widget.old_value != event.widget.get():\n            h = self.hue.get()\n            s = self.saturation.get()\n            v = self.value.get()\n            sel_color = hsv_to_rgb(h, s, v)\n            self.red.set(sel_color[0])\n            self.green.set(sel_color[1])\n            self.blue.set(sel_color[2])\n            if self.alpha_channel:\n                sel_color += (self.alpha.get(),)\n                self.alphabar.set_color(sel_color)\n            hexa = rgb_to_hexa(*sel_color)\n            self.hexa.delete(0, \"end\")\n            self.hexa.insert(0, hexa)\n            self.square.set_hsv((h, s, v))\n            self.bar.set(h)\n            self._update_preview()", "response": "Update color in the HSV spinboxes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _update_color_rgb(self, event=None):\n        if event is None or event.widget.old_value != event.widget.get():\n            r = self.red.get()\n            g = self.green.get()\n            b = self.blue.get()\n            h, s, v = rgb_to_hsv(r, g, b)\n            self.hue.set(h)\n            self.saturation.set(s)\n            self.value.set(v)\n            args = (r, g, b)\n            if self.alpha_channel:\n                args += (self.alpha.get(),)\n                self.alphabar.set_color(args)\n            hexa = rgb_to_hexa(*args)\n            self.hexa.delete(0, \"end\")\n            self.hexa.insert(0, hexa)\n            self.square.set_hsv((h, s, v))\n            self.bar.set(h)\n            self._update_preview()", "response": "Update the color of the color spinboxes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ok(self):\n        rgb, hsv, hexa = self.square.get()\n        if self.alpha_channel:\n            hexa = self.hexa.get()\n            rgb += (self.alpha.get(),)\n        self.color = rgb, hsv, hexa\n        self.destroy()", "response": "Validate color selection and destroy dialog."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndrawing the gradient and put the cursor on alpha.", "response": "def _draw_gradient(self, alpha, color):\n        \"\"\"Draw the gradient and put the cursor on alpha.\"\"\"\n        self.delete(\"gradient\")\n        self.delete(\"cursor\")\n        del self.gradient\n        width = self.winfo_width()\n        height = self.winfo_height()\n\n        bg = create_checkered_image(width, height)\n        r, g, b = color\n        w = width - 1.\n        gradient = Image.new(\"RGBA\", (width, height))\n        for i in range(width):\n            for j in range(height):\n                gradient.putpixel((i, j), (r, g, b, round2(i / w * 255)))\n        self.gradient = ImageTk.PhotoImage(Image.alpha_composite(bg, gradient),\n                                           master=self)\n\n        self.create_image(0, 0, anchor=\"nw\", tags=\"gardient\",\n                          image=self.gradient)\n        self.lower(\"gradient\")\n\n        x = alpha / 255. * width\n        h, s, v = rgb_to_hsv(r, g, b)\n        if v < 50:\n            fill = \"gray80\"\n        else:\n            fill = 'black'\n        self.create_line(x, 0, x, height, width=2, tags='cursor', fill=fill)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset cursor position on the color corresponding to the alpha value.", "response": "def set(self, alpha):\n        \"\"\"\n        Set cursor position on the color corresponding to the alpha value.\n\n        :param alpha: new alpha value (between 0 and 255)\n        :type alpha: int\n        \"\"\"\n        if alpha > 255:\n            alpha = 255\n        elif alpha < 0:\n            alpha = 0\n        x = alpha / 255. * self.winfo_width()\n        self.coords('cursor', x, 0, x, self.winfo_height())\n        self._variable.set(alpha)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchanging gradient color and change cursor position if an alpha value is supplied.", "response": "def set_color(self, color):\n        \"\"\"\n        Change gradient color and change cursor position if an alpha value is supplied.\n\n        :param color: new gradient color in RGB(A) format\n        :type color: tuple[int]\n        \"\"\"\n        if len(color) == 3:\n            alpha = self.get()\n        else:\n            alpha = color[3]\n        self._draw_gradient(alpha, color[:3])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_keyrelease(self, event):\n        if event.keysym == \"BackSpace\":\n            self.delete(self.index(tk.INSERT), tk.END)\n            self.position = self.index(tk.END)\n        if event.keysym == \"Left\":\n            if self.position < self.index(tk.END):  # delete the selection\n                self.delete(self.position, tk.END)\n            else:\n                self.position -= 1  # delete one character\n                self.delete(self.position, tk.END)\n        if event.keysym == \"Right\":\n            self.position = self.index(tk.END)  # go to end (no selection)\n        if event.keysym == \"Return\":\n            self.handle_return(None)\n            return\n        if len(event.keysym) == 1:\n            self.autocomplete()", "response": "Handles the keyrelease event on this widget."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nputs all the child widgets in the correct position.", "response": "def _grid_widgets(self):\n        \"\"\"Puts all the child widgets in the correct position.\"\"\"\n        self._font_family_header.grid(row=0, column=1, sticky=\"nswe\", padx=5, pady=5)\n        self._font_label.grid(row=1, column=1, sticky=\"nswe\", padx=5, pady=(0, 5))\n        self._font_family_list.grid(row=2, rowspan=3, column=1, sticky=\"nswe\", padx=5, pady=(0, 5))\n        self._font_properties_header.grid(row=0, column=2, sticky=\"nswe\", padx=5, pady=5)\n        self._font_properties_frame.grid(row=1, rowspan=2, column=2, sticky=\"we\", padx=5, pady=5)\n        self._font_size_header.grid(row=3, column=2, sticky=\"we\", padx=5, pady=5)\n        self._size_dropdown.grid(row=4, column=2, sticky=\"we\", padx=5, pady=5)\n        self._example_label.grid(row=5, column=1, columnspan=2, sticky=\"nswe\", padx=5, pady=5)\n        self._ok_button.grid(row=6, column=2, sticky=\"nswe\", padx=5, pady=5)\n        self._cancel_button.grid(row=6, column=1, sticky=\"nswe\", padx=5, pady=5)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _on_properties(self, properties):\n        self._bold, self._italic, self._underline, self._overstrike = properties\n        self._on_change()", "response": "Callback for properties change."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __generate_font_tuple(self):\n        if not self._family:\n            return None\n        font = [self._family, self._size]\n        if self._bold:\n            font.append(\"bold\")\n        if self._italic:\n            font.append(\"italic\")\n        if self._underline:\n            font.append(\"underline\")\n        if self._overstrike:\n            font.append(\"overstrike\")\n        return tuple(font)", "response": "Generate a font tuple for tkinter widgets based on the user s entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns tuple of font name size options", "response": "def font(self):\n        \"\"\"\n        Selected font.\n        \n        :return: font tuple (family_name, size, \\*options), :class:`~font.Font` object\n        \"\"\"\n        if self._family is None:\n            return None, None\n        else:\n            font_tuple = self.__generate_font_tuple()\n            font_obj = font.Font(family=self._family, size=self._size,\n                                 weight=font.BOLD if self._bold else font.NORMAL,\n                                 slant=font.ITALIC if self._italic else font.ROMAN,\n                                 underline=1 if self._underline else 0,\n                                 overstrike=1 if self._overstrike else 0)\n            return font_tuple, font_obj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nchange style on focus out events.", "response": "def focusout(event):\n        \"\"\"Change style on focus out events.\"\"\"\n        w = event.widget.spinbox\n        bc = w.style.lookup(\"TEntry\", \"bordercolor\", (\"!focus\",))\n        dc = w.style.lookup(\"TEntry\", \"darkcolor\", (\"!focus\",))\n        lc = w.style.lookup(\"TEntry\", \"lightcolor\", (\"!focus\",))\n        w.style.configure(\"%s.spinbox.TFrame\" % event.widget, bordercolor=bc,\n                          darkcolor=dc, lightcolor=lc)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchanging style on focus in events.", "response": "def focusin(event):\n        \"\"\"Change style on focus in events.\"\"\"\n        w = event.widget.spinbox\n        w.old_value = w.get()\n        bc = w.style.lookup(\"TEntry\", \"bordercolor\", (\"focus\",))\n        dc = w.style.lookup(\"TEntry\", \"darkcolor\", (\"focus\",))\n        lc = w.style.lookup(\"TEntry\", \"lightcolor\", (\"focus\",))\n        w.style.configure(\"%s.spinbox.TFrame\" % event.widget, bordercolor=bc,\n                          darkcolor=dc, lightcolor=lc)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndraws the gradient and put the cursor on hue.", "response": "def _draw_gradient(self, hue):\n        \"\"\"Draw the gradient and put the cursor on hue.\"\"\"\n        self.delete(\"gradient\")\n        self.delete(\"cursor\")\n        del self.gradient\n        width = self.winfo_width()\n        height = self.winfo_height()\n\n        self.gradient = tk.PhotoImage(master=self, width=width, height=height)\n\n        line = []\n        for i in range(width):\n            line.append(rgb_to_hexa(*hue2col(float(i) / width * 360)))\n        line = \"{\" + \" \".join(line) + \"}\"\n        self.gradient.put(\" \".join([line for j in range(height)]))\n        self.create_image(0, 0, anchor=\"nw\", tags=\"gardient\",\n                          image=self.gradient)\n        self.lower(\"gradient\")\n\n        x = hue / 360. * width\n        self.create_line(x, 0, x, height, width=2, tags='cursor')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _on_click(self, event):\n        x = event.x\n        self.coords('cursor', x, 0, x, self.winfo_height())\n        self._variable.set(round2((360. * x) / self.winfo_width()))", "response": "Move selection cursor on click."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _on_move(self, event):\n        w = self.winfo_width()\n        x = min(max(event.x, 0), w)\n        self.coords('cursor', x, 0, x, self.winfo_height())\n        self._variable.set(round2((360. * x) / w))", "response": "Make selection cursor follow the cursor."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _grid_widgets(self):\n        self._family_dropdown.grid(row=0, column=0, sticky=\"nswe\")\n        self._size_dropdown.grid(row=0, column=1, sticky=\"nswe\")\n        self._properties_frame.grid(row=0, column=2, sticky=\"nswe\")", "response": "Grids all the widgets in the correct place."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling callback if any property is changed.", "response": "def _on_change(self):\n        \"\"\"Call callback if any property is changed.\"\"\"\n        if callable(self.__callback):\n            self.__callback((self._family, self._size, self._bold, self._italic, self._underline, self._overstrike))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the text color to the hover color.", "response": "def _on_enter(self, *args):\n        \"\"\"Set the text color to the hover color.\"\"\"\n        self.config(foreground=self._hover_color, cursor=self._cursor)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _on_leave(self, *args):\n        if self.__clicked:\n            self.config(foreground=self._clicked_color)\n        else:\n            self.config(foreground=self._normal_color)\n        self.config(cursor=\"\")", "response": "Set the text color to either the normal color when not clicked or the clicked color when clicked."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nopening the link in the web browser.", "response": "def open_link(self, *args):\n        \"\"\"Open the link in the web browser.\"\"\"\n        if \"disabled\" not in self.state():\n            webbrowser.open(self._link)\n            self.__clicked = True\n            self._on_leave()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cget(self, key):\n        if key is \"link\":\n            return self._link\n        elif key is \"hover_color\":\n            return self._hover_color\n        elif key is \"normal_color\":\n            return self._normal_color\n        elif key is \"clicked_color\":\n            return self._clicked_color\n        else:\n            return ttk.Label.cget(self, key)", "response": "Query widget option. cget"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef configure(self, **kwargs):\n        self._link = kwargs.pop(\"link\", self._link)\n        self._hover_color = kwargs.pop(\"hover_color\", self._hover_color)\n        self._normal_color = kwargs.pop(\"normal_color\", self._normal_color)\n        self._clicked_color = kwargs.pop(\"clicked_color\", self._clicked_color)\n        ttk.Label.configure(self, **kwargs)\n        self._on_leave()", "response": "Configure resources of the widget."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef keys(self):\n        keys = ttk.Label.keys(self)\n        keys.extend([\"link\", \"normal_color\", \"hover_color\", \"clicked_color\"])\n        return keys", "response": "Return a list of all resource names of this widget."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nswapping dragged column with its side ( = left or right neighbor.", "response": "def _swap_columns(self, side):\n        \"\"\"Swap dragged column with its side (=left/right) neighbor.\"\"\"\n        displayed_cols = self._displayed_cols\n        i1 = self._dragged_col_index\n        i2 = i1 + 1 if side == 'right' else i1 - 1\n        if 0 <= i2 < len(displayed_cols):\n            # there is a neighbor, swap columns:\n            displayed_cols[i1] = displayed_cols[i2]\n            displayed_cols[i2] = self._dragged_col\n            self[\"displaycolumns\"] = displayed_cols\n            if side == 'left':\n                right = self._dragged_col_neighbor_widths[0]\n                self._dragged_col_x -= right  # update dragged column x coordinate\n                # set new left neighbor width\n                if i2 > 0:\n                    left = ttk.Treeview.column(self, displayed_cols[i2 - 1], 'width')\n                else:\n                    left = None\n            else:\n                left = self._dragged_col_neighbor_widths[1]\n                self._dragged_col_x += left  # update x coordinate of dragged column\n                # set new right neighbor width\n                if i2 < len(displayed_cols) - 1:\n                    right = ttk.Treeview.column(self, displayed_cols[i2 + 1], 'width')\n                else:\n                    right = None\n            self._dragged_col_index = i2  # update dragged column index\n            self._dragged_col_neighbor_widths = (left, right)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninserting dragged row at item s position.", "response": "def _move_dragged_row(self, item):\n        \"\"\"Insert dragged row at item's position.\"\"\"\n        self.move(self._dragged_row, '', self.index(item))\n        self.see(self._dragged_row)\n        bbox = self.bbox(self._dragged_row)\n        self._dragged_row_y = bbox[1]\n        self._dragged_row_height = bbox[3]\n        self._visual_drag.see(self._dragged_row)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _on_press(self, event):\n        if tk.DISABLED in self.state():\n            return\n\n        region = self.identify_region(event.x, event.y)\n\n        if self._drag_cols and region == 'heading':\n            self._start_drag_col(event)\n        elif self._drag_rows and region == 'cell':\n            self._start_drag_row(event)", "response": "Start dragging column or row on left click."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart dragging a column", "response": "def _start_drag_col(self, event):\n        \"\"\"Start dragging a column\"\"\"\n        # identify dragged column\n        col = self.identify_column(event.x)\n        self._dragged_col = ttk.Treeview.column(self, col, 'id')\n        # get column width\n        self._dragged_col_width = w = ttk.Treeview.column(self, col, 'width')\n        # get x coordinate of the left side of the column\n        x = event.x\n        while self.identify_region(x, event.y) == 'heading':\n            # decrease x until reaching the separator\n            x -= 1\n        x_sep = x\n        w_sep = 0\n        # determine separator width\n        while self.identify_region(x_sep, event.y) == 'separator':\n            w_sep += 1\n            x_sep -= 1\n        if event.x - x <= self._im_drag.width():\n            # start dragging if mouse click was on dragging icon\n            x = x - w_sep // 2 - 1\n            self._dragged_col_x = x\n            # get neighboring column widths\n            displayed_cols = self._displayed_cols\n            self._dragged_col_index = i1 = displayed_cols.index(self._dragged_col)\n            if i1 > 0:\n                left = ttk.Treeview.column(self, displayed_cols[i1 - 1], 'width')\n            else:\n                left = None\n            if i1 < len(displayed_cols) - 1:\n                right = ttk.Treeview.column(self, displayed_cols[i1 + 1], 'width')\n            else:\n                right = None\n            self._dragged_col_neighbor_widths = (left, right)\n            self._dx = x - event.x  # distance between cursor and column left border\n            # configure dragged column preview\n            self._visual_drag.column(self._dragged_col, width=w)\n            self._visual_drag.configure(displaycolumns=[self._dragged_col])\n            if 'headings' in tuple(str(p) for p in self['show']):\n                self._visual_drag.configure(show='headings')\n            else:\n                self._visual_drag.configure(show='')\n            self._visual_drag.place(in_=self, x=x, y=0, anchor='nw',\n                                    width=w + 2, relheight=1)\n            self._visual_drag.state(('active',))\n            self._visual_drag.update_idletasks()\n            self._visual_drag.yview_moveto(self.yview()[0])\n        else:\n            self._dragged_col = None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _start_drag_row(self, event):\n        self._dragged_row = self.identify_row(event.y)  # identify dragged row\n        bbox = self.bbox(self._dragged_row)\n        self._dy = bbox[1] - event.y  # distance between cursor and row upper border\n        self._dragged_row_y = bbox[1]  # y coordinate of dragged row upper border\n        self._dragged_row_height = bbox[3]\n        # configure dragged row preview\n        self._visual_drag.configure(displaycolumns=self['displaycolumns'],\n                                    height=1)\n        for col in self['columns']:\n            self._visual_drag.column(col, width=self.column(col, 'width'))\n        if 'tree' in tuple(str(p) for p in self['show']):\n            self._visual_drag.configure(show='tree')\n        else:\n            self._visual_drag.configure(show='')\n        self._visual_drag.place(in_=self, x=0, y=bbox[1],\n                                height=self._visual_drag.winfo_reqheight() + 2,\n                                anchor='nw', relwidth=1)\n        self._visual_drag.selection_add(self._dragged_row)\n        self.selection_remove(self._dragged_row)\n        self._visual_drag.update_idletasks()\n        self._visual_drag.see(self._dragged_row)\n        self._visual_drag.update_idletasks()\n        self._visual_drag.xview_moveto(self.xview()[0])", "response": "Start dragging a row"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndrag around label if visible.", "response": "def _on_motion(self, event):\n        \"\"\"Drag around label if visible.\"\"\"\n        if not self._visual_drag.winfo_ismapped():\n            return\n\n        if self._drag_cols and self._dragged_col is not None:\n            self._drag_col(event)\n        elif self._drag_rows and self._dragged_row is not None:\n            self._drag_row(event)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncontinuing dragging a column", "response": "def _drag_col(self, event):\n        \"\"\"Continue dragging a column\"\"\"\n        x = self._dx + event.x  # get dragged column new left x coordinate\n        self._visual_drag.place_configure(x=x)  # update column preview position\n        # if one border of the dragged column is beyon the middle of the\n        # neighboring column, swap them\n        if (self._dragged_col_neighbor_widths[0] is not None and\n               x < self._dragged_col_x - self._dragged_col_neighbor_widths[0] / 2):\n            self._swap_columns('left')\n        elif (self._dragged_col_neighbor_widths[1] is not None and\n               x > self._dragged_col_x + self._dragged_col_neighbor_widths[1] / 2):\n            self._swap_columns('right')\n        # horizontal scrolling if the cursor reaches the side of the table\n        if x < 0 and self.xview()[0] > 0:\n            # scroll left and update dragged column x coordinate\n            self.xview_scroll(-10, 'units')\n            self._dragged_col_x += 10\n        elif x + self._dragged_col_width / 2 > self.winfo_width() and self.xview()[1] < 1:\n            # scroll right and update dragged column x coordinate\n            self.xview_scroll(10, 'units')\n            self._dragged_col_x -= 10"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _drag_row(self, event):\n        y = self._dy + event.y  # get dragged row new upper y coordinate\n        self._visual_drag.place_configure(y=y)  # update row preview position\n\n        if y > self._dragged_row_y:\n            # moving downward\n            item = self.identify_row(y + self._dragged_row_height)\n            if item != '':\n                bbox = self.bbox(item)\n                if not bbox:\n                    # the item is not visible so make it visible\n                    self.see(item)\n                    self.update_idletasks()\n                    bbox = self.bbox(item)\n                if y > self._dragged_row_y + bbox[3] / 2:\n                    # the row is beyond half of item, so insert it below\n                    self._move_dragged_row(item)\n                elif item != self.next(self._dragged_row):\n                    # item is not the lower neighbor of the dragged row so insert the row above\n                    self._move_dragged_row(self.prev(item))\n        elif y < self._dragged_row_y:\n            # moving upward\n            item = self.identify_row(y)\n            if item != '':\n                bbox = self.bbox(item)\n                if not bbox:\n                    # the item is not visible so make it visible\n                    self.see(item)\n                    self.update_idletasks()\n                    bbox = self.bbox(item)\n                if y < self._dragged_row_y - bbox[3] / 2:\n                    # the row is beyond half of item, so insert it above\n                    self._move_dragged_row(item)\n                elif item != self.prev(self._dragged_row):\n                    # item is not the upper neighbor of the dragged row so insert the row below\n                    self._move_dragged_row(self.next(item))\n        self.selection_remove(self._dragged_row)", "response": "Continue dragging a row"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsorts a column by its values", "response": "def _sort_column(self, column, reverse):\n        \"\"\"Sort a column by its values\"\"\"\n        if tk.DISABLED in self.state():\n            return\n        # get list of (value, item) tuple where value is the value in column for the item\n        l = [(self.set(child, column), child) for child in self.get_children('')]\n        # sort list using the column type\n        l.sort(reverse=reverse, key=lambda x: self._column_types[column](x[0]))\n        # reorder items\n        for index, (val, child) in enumerate(l):\n            self.move(child, \"\", index)\n        # reverse sorting direction for the next time\n        self.heading(column, command=lambda: self._sort_column(column, not reverse))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nqueries widget option. cget", "response": "def cget(self, key):\n        \"\"\"\n        Query widget option.\n\n        :param key: option name\n        :type key: str\n        :return: value of the option\n\n        To get the list of options for this widget, call the method :meth:`~Table.keys`.\n        \"\"\"\n        if key == 'sortable':\n            return self._sortable\n        elif key == 'drag_cols':\n            return self._drag_cols\n        elif key == 'drag_rows':\n            return self._drag_rows\n        else:\n            return ttk.Treeview.cget(self, key)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef column(self, column, option=None, **kw):\n        config = False\n        if option == 'type':\n            return self._column_types[column]\n        elif 'type' in kw:\n            config = True\n            self._column_types[column] = kw.pop('type')\n        if kw:\n            self._visual_drag.column(ttk.Treeview.column(self, column, 'id'), option, **kw)\n        if kw or option:\n            return ttk.Treeview.column(self, column, option, **kw)\n        elif not config:\n            res = ttk.Treeview.column(self, column, option, **kw)\n            res['type'] = self._column_types[column]\n            return res", "response": "Query or modify the options for the specified column."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconfiguring the resources of the current instance of the current instance of the current instance of the current class.", "response": "def configure(self, cnf=None, **kw):\n        \"\"\"\n        Configure resources of the widget.\n\n        To get the list of options for this widget, call the method :meth:`~Table.keys`.\n        See :meth:`~Table.__init__` for a description of the widget specific option.\n        \"\"\"\n        if cnf == 'drag_cols':\n            return 'drag_cols', self._drag_cols\n        elif cnf == 'drag_rows':\n            return 'drag_rows', self._drag_rows\n        elif cnf == 'sortable':\n            return 'sortable', self._sortable\n\n        if isinstance(cnf, dict):\n            kwargs = cnf.copy()\n            kwargs.update(kw)  # keyword arguments override cnf content\n            cnf = {}   # everything is in kwargs so no need of cnf\n            cnf2 = {}  # to configure the preview\n        else:\n            kwargs = kw\n            cnf2 = cnf\n\n        sortable = bool(kwargs.pop(\"sortable\", self._sortable))\n        if sortable != self._sortable:\n            self._config_sortable(sortable)\n        drag_cols = bool(kwargs.pop(\"drag_cols\", self._drag_cols))\n        if drag_cols != self._drag_cols:\n            self._config_drag_cols(drag_cols)\n        self._drag_rows = bool(kwargs.pop(\"drag_rows\", self._drag_rows))\n        if 'columns' in kwargs:\n            # update column type dict\n            for col in list(self._column_types.keys()):\n                if col not in kwargs['columns']:\n                    del self._column_types[col]\n            for col in kwargs['columns']:\n                if col not in self._column_types:\n                    self._column_types[col] = str\n        # Remove some keywords from the preview configuration dict\n        kw2 = kwargs.copy()\n        kw2.pop('displaycolumns', None)\n        kw2.pop('xscrollcommand', None)\n        kw2.pop('yscrollcommand', None)\n        self._visual_drag.configure(cnf2, **kw2)\n        if len(kwargs) != 0:\n            return ttk.Treeview.configure(self, cnf, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies options set in attributes to Treeview", "response": "def _config_options(self):\n        \"\"\"Apply options set in attributes to Treeview\"\"\"\n        self._config_sortable(self._sortable)\n        self._config_drag_cols(self._drag_cols)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconfigures a new sortable state", "response": "def _config_sortable(self, sortable):\n        \"\"\"Configure a new sortable state\"\"\"\n        for col in self[\"columns\"]:\n            command = (lambda c=col: self._sort_column(c, True)) if sortable else \"\"\n            self.heading(col, command=command)\n        self._sortable = sortable"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _config_drag_cols(self, drag_cols):\n        self._drag_cols = drag_cols\n        # remove/display drag icon\n        if self._drag_cols:\n            self._im_drag.paste(self._im_draggable)\n        else:\n            self._im_drag.paste(self._im_not_draggable)\n        self.focus_set()\n        self.update_idletasks()", "response": "Configure a new drag_cols state"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting all specified items and all their descendants.", "response": "def delete(self, *items):\n        \"\"\"\n        Delete all specified items and all their descendants. The root item may not be deleted.\n\n        :param items: list of item identifiers\n        :type items: sequence[str]\n        \"\"\"\n        self._visual_drag.delete(*items)\n        ttk.Treeview.delete(self, *items)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef detach(self, *items):\n        self._visual_drag.detach(*items)\n        ttk.Treeview.detach(self, *items)", "response": "Detach all of the specified items from the treeview."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef heading(self, column, option=None, **kw):\n        if kw:\n            # Set the default image of the heading to the drag icon\n            kw.setdefault(\"image\", self._im_drag)\n            self._visual_drag.heading(ttk.Treeview.column(self, column, 'id'), option, **kw)\n        return ttk.Treeview.heading(self, column, option, **kw)", "response": "Query or modify the heading options for the specified column."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef insert(self, parent, index, iid=None, **kw):\n        self._visual_drag.insert(parent, index, iid, **kw)\n        return ttk.Treeview.insert(self, parent, index, iid, **kw)", "response": "Creates a new item and returns the item identifier of the newly created item."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquery or modify the options for the specified item.", "response": "def item(self, item, option=None, **kw):\n        \"\"\"\n        Query or modify the options for the specified item.\n\n        If no options are given, a dict with options/values for the item is returned.\n        If option is specified then the value for that option is returned.\n        Otherwise, sets the options to the corresponding values as given by `kw`.\n        \n        :param text: item's label\n        :type text: str\n        :param image: image to be displayed on the left of the item's label\n        :type image: PhotoImage\n        :param values: values to put in the columns\n        :type values: sequence\n        :param open: whether the item's children should be displayed\n        :type open: bool\n        :param tags: list of tags associated with this item\n        :type tags: sequence[str]\n        \"\"\"\n        if kw:\n            self._visual_drag.item(item, option, **kw)\n        return ttk.Treeview.item(self, item, option, **kw)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmove item to position index in parent s list of children.", "response": "def move(self, item, parent, index):\n        \"\"\"\n        Moves item to position index in parent\u2019s list of children.\n\n        It is illegal to move an item under one of its descendants. If index is\n        less than or equal to zero, item is moved to the beginning, if greater\n        than or equal to the number of children, it is moved to the end.\n        If item was detached it is reattached.\n\n        :param item: item's identifier\n        :type item: str\n        :param parent: new parent of item\n        :type parent: str\n        :param index: where in the list of parent\u2019s children to insert item\n        :type index: int of \"end\"\n        \"\"\"\n        self._visual_drag.move(item, parent, index)\n        ttk.Treeview.move(self, item, parent, index)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nquery or set the value of the item s identifier column.", "response": "def set(self, item, column=None, value=None):\n        \"\"\"\n        Query or set the value of given item.\n\n        With one argument, return a dictionary of column/value pairs for the\n        specified item. With two arguments, return the current value of the\n        specified column. With three arguments, set the value of given column\n        in given item to the specified value.\n\n        :param item: item's identifier\n        :type item: str\n        :param column: column's identifier\n        :type column: str, int or None\n        :param value: new value\n        \"\"\"\n        if value is not None:\n            self._visual_drag.set(item, ttk.Treeview.column(self, column, 'id'), value)\n        return ttk.Treeview.set(self, item, column, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_children(self, item, *newchildren):\n        self._visual_drag.set_children(item, *newchildren)\n        ttk.Treeview.set_children(self, item, *newchildren)", "response": "Replaces item s children with newchildren."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _grid_widgets(self):\n        orient = str(self._scale.cget('orient'))\n        self._scale.grid(row=2, column=2, sticky='ew' if orient == tk.HORIZONTAL else 'ns',\n                         padx=(0, self.__entryscalepad) if self.__compound is tk.RIGHT else\n                              (self.__entryscalepad, 0) if self.__compound is tk.LEFT else 0,\n                         pady=(0, self.__entryscalepad) if self.__compound is tk.BOTTOM else\n                              (self.__entryscalepad, 0) if self.__compound is tk.TOP else 0)\n        self._entry.grid(row=1 if self.__compound is tk.TOP else 3 if self.__compound is tk.BOTTOM else 2,\n                         column=1 if self.__compound is tk.LEFT else 3 if self.__compound is tk.RIGHT else 2)\n\n        if orient == tk.HORIZONTAL:\n            self.columnconfigure(0, weight=0)\n            self.columnconfigure(2, weight=1)\n            self.columnconfigure(4, weight=0)\n            self.rowconfigure(0, weight=1)\n            self.rowconfigure(2, weight=0)\n            self.rowconfigure(4, weight=1)\n\n        else:\n            self.rowconfigure(0, weight=0)\n            self.rowconfigure(2, weight=1)\n            self.rowconfigure(4, weight=0)\n            self.columnconfigure(0, weight=1)\n            self.columnconfigure(2, weight=0)\n            self.columnconfigure(4, weight=1)", "response": "Put the widgets in the correct position based on self. __compound."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cget(self, key):\n        if key == 'scalewidth':\n            return self._scale.cget('length')\n        elif key == 'from':\n            return self._scale.cget('from')\n        elif key == 'to':\n            return self._scale.cget('to')\n        elif key == 'entrywidth':\n            return self._entry.cget('width')\n        elif key == 'entryscalepad':\n            return self.__entryscalepad\n        elif key == 'compound':\n            return self.__compound\n        elif key == 'orient':\n            return str(self._scale.cget('orient'))\n        else:\n            return ttk.Frame.cget(self, key)", "response": "Query widget option. cget"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef configure(self, cnf={}, **kw):\n        kw.update(cnf)\n        reinit = False\n        if 'scalewidth' in kw:\n            self._scale.configure(length=kw.pop('scalewidth'))\n        if 'from' in kw:\n            from_ = kw.pop('from')\n            self._scale.configure(from_=from_)\n            self._variable.configure(low=from_)\n        if 'from_' in kw:\n            from_ = kw.pop('from_')\n            self._scale.configure(from_=from_)\n            self._variable.configure(low=from_)\n        if 'to' in kw:\n            to = kw.pop('to')\n            self._scale.configure(to=to)\n            self._variable.configure(high=to)\n        if 'entrywidth' in kw:\n            self._entry.configure(width=kw.pop('entrywidth'))\n        if 'compound' in kw:\n            compound = kw.pop('compound')\n            if compound is not tk.RIGHT and compound is not tk.LEFT and \\\n               compound is not tk.TOP and compound is not tk.BOTTOM:\n                raise ValueError(\"Invalid value for compound passed {0}\".format(compound))\n            else:\n                self.__compound = compound\n                reinit = True\n        if 'orient' in kw:\n            self._scale.configure(orient=kw.pop('orient'))\n            reinit = True\n        if 'entryscalepad' in kw:\n            entryscalepad = kw.pop('entryscalepad')\n            try:\n                self.__entryscalepad = int(entryscalepad)\n                reinit = True\n            except ValueError:\n                raise ValueError(\"Invalid value for entryscalepad passed {0}\".format(entryscalepad))\n        ttk.Frame.configure(self, kw)\n        if reinit:\n            self._grid_widgets()", "response": "Configure resources of the current instance of the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef config_scale(self, cnf={}, **kwargs):\n        self._scale.config(cnf, **kwargs)\n        # Update self._variable limits in case the ones of the scale have changed\n        self._variable.configure(high=self._scale['to'],\n                                 low=self._scale['from'])\n        if 'orient' in cnf or 'orient' in kwargs:\n            self._grid_widgets()", "response": "Configure resources of the Scale widget."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfunctioning bound to double click on Listbox that calls the callback if a valid callback object is passed in.", "response": "def _on_click(self, *args):\n        \"\"\"\n        Function bound to double click on Listbox that calls the callback if a valid callback object is passed\n        \n        :param args: Tkinter event\n        \"\"\"\n        if callable(self._callback):\n            self._callback(self.selection)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef selection(self):\n        selection = self.listbox.curselection()\n        if len(selection) is 0:\n            return None\n        return self.font_indexes[self.listbox.curselection()[0]]", "response": "Return the current font name if one is selected."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplaces the widgets in the Toplevel.", "response": "def _grid_widgets(self):\n        \"\"\"Place the widgets in the Toplevel.\"\"\"\n        self._canvas.grid(sticky=\"nswe\")\n        self.header_label.grid(row=1, column=1, sticky=\"nswe\", pady=5, padx=5)\n        self.text_label.grid(row=3, column=1, sticky=\"nswe\", pady=6, padx=5)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a delayed callback for the enter event.", "response": "def _on_enter(self, event):\n        \"\"\"Creates a delayed callback for the :obj:`<Enter>` event.\"\"\"\n        self._id = self.master.after(int(self._timeout * 1000), func=self.show)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nshow the current cursor.", "response": "def show(self):\n        \"\"\"\n        Create the Toplevel widget and its child widgets to show in the spot of the cursor.\n\n        This is the callback for the delayed :obj:`<Enter>` event (see :meth:`~Balloon._on_enter`). \n        \"\"\"\n        self._toplevel = tk.Toplevel(self.master)\n        self._canvas = tk.Canvas(self._toplevel, background=self.__background)\n        self.header_label = ttk.Label(self._canvas, text=self.__headertext, background=self.__background,\n                                      image=self._photo_image, compound=tk.LEFT)\n        self.text_label = ttk.Label(self._canvas, text=self.__text, wraplength=self.__width,\n                                    background=self.__background)\n        self._toplevel.attributes(\"-topmost\", True)\n        self._toplevel.overrideredirect(True)\n        self._grid_widgets()\n        x, y = self.master.winfo_pointerxy()\n        self._canvas.update()\n        # Update the Geometry of the Toplevel to update its position and size\n        self._toplevel.geometry(\"{0}x{1}+{2}+{3}\".format(self._canvas.winfo_width(), self._canvas.winfo_height(),\n                                                         x + 2, y + 2))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cget(self, key):\n        if key == \"headertext\":\n            return self.__headertext\n        elif key == \"text\":\n            return self.__text\n        elif key == \"width\":\n            return self.__width\n        elif key == \"timeout\":\n            return self._timeout\n        elif key == \"background\":\n            return self.__background\n        else:\n            return ttk.Frame.cget(self, key)", "response": "Query widget option. cget"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconfigures the resources of the current instance of the Balloon.", "response": "def config(self, **kwargs):\n        \"\"\"\n        Configure resources of the widget.\n\n        To get the list of options for this widget, call the method :meth:`~Balloon.keys`.\n        See :meth:`~Balloon.__init__` for a description of the widget specific option.\n        \"\"\"\n        self.__headertext = kwargs.pop(\"headertext\", self.__headertext)\n        self.__text = kwargs.pop(\"text\", self.__text)\n        self.__width = kwargs.pop(\"width\", self.__width)\n        self._timeout = kwargs.pop(\"timeout\", self._timeout)\n        self.__background = kwargs.pop(\"background\", self.__background)\n        if self._toplevel:\n            self._on_leave(None)\n            self.show()\n        ttk.Frame.config(self, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplace the widgets in the correct positions :return: None", "response": "def _grid_widgets(self):\n        \"\"\"\n        Place the widgets in the correct positions\n        :return: None\n        \"\"\"\n        if self.__label:\n            self._header_label.grid(row=0, column=1, columnspan=3, sticky=\"nw\", padx=5, pady=(5, 0))\n        self._bold_button.grid(row=1, column=1, sticky=\"nswe\", padx=5, pady=2)\n        self._italic_button.grid(row=1, column=2, sticky=\"nswe\", padx=(0, 5), pady=2)\n        self._underline_button.grid(row=1, column=3, sticky=\"nswe\", padx=(0, 5), pady=2)\n        self._overstrike_button.grid(row=1, column=4, sticky=\"nswe\", padx=(0, 5), pady=2)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _on_click(self):\n        if callable(self.__callback):\n            self.__callback((self.bold, self.italic, self.underline, self.overstrike))", "response": "Handles clicks and calls callback."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rgb_to_hsv(r, g, b):\n    h, s, v = colorsys.rgb_to_hsv(r / 255., g / 255., b / 255.)\n    return round2(h * 360), round2(s * 100), round2(v * 100)", "response": "Convert RGB color to HSV."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert RGB ( A ) color to hexadecimal.", "response": "def rgb_to_hexa(*args):\n    \"\"\"Convert RGB(A) color to hexadecimal.\"\"\"\n    if len(args) == 3:\n        return (\"#%2.2x%2.2x%2.2x\" % tuple(args)).upper()\n    elif len(args) == 4:\n        return (\"#%2.2x%2.2x%2.2x%2.2x\" % tuple(args)).upper()\n    else:\n        raise ValueError(\"Wrong number of arguments.\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert hexadecimal color to RGB.", "response": "def hexa_to_rgb(color):\n    \"\"\"Convert hexadecimal color to RGB.\"\"\"\n    r = int(color[1:3], 16)\n    g = int(color[3:5], 16)\n    b = int(color[5:7], 16)\n    if len(color) == 7:\n        return r, g, b\n    elif len(color) == 9:\n        return r, g, b, int(color[7:9], 16)\n    else:\n        raise ValueError(\"Invalid hexadecimal notation.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn hue value corresponding to given RGB color.", "response": "def col2hue(r, g, b):\n    \"\"\"Return hue value corresponding to given RGB color.\"\"\"\n    return round2(180 / pi * atan2(sqrt(3) * (g - b), 2 * r - g - b) + 360) % 360"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_checkered_image(width, height, c1=(154, 154, 154, 255),\n                           c2=(100, 100, 100, 255), s=6):\n    \"\"\"\n    Return a checkered image of size width x height.\n\n    Arguments:\n        * width: image width\n        * height: image height\n        * c1: first color (RGBA)\n        * c2: second color (RGBA)\n        * s: size of the squares\n    \"\"\"\n    im = Image.new(\"RGBA\", (width, height), c1)\n    draw = ImageDraw.Draw(im, \"RGBA\")\n    for i in range(s, width, 2 * s):\n        for j in range(0, height, 2 * s):\n            draw.rectangle(((i, j), ((i + s - 1, j + s - 1))), fill=c2)\n    for i in range(0, width, 2 * s):\n        for j in range(s, height, 2 * s):\n            draw.rectangle(((i, j), ((i + s - 1, j + s - 1))), fill=c2)\n    return im", "response": "Create a checkered image with size width x height."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noverlay a rectangle on the image and return the result.", "response": "def overlay(image, color):\n    \"\"\"\n    Overlay a rectangle of color (RGBA) on the image and return the result.\n    \"\"\"\n    width, height = image.size\n    im = Image.new(\"RGBA\", (width, height), color)\n    preview = Image.alpha_composite(image, im)\n    return preview"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconfiguring all widgets using the grid geometry manager.", "response": "def grid_widgets(self):\n        \"\"\"\n        Configure all widgets using the grid geometry manager\n\n        Automatically called by the :meth:`__init__` method.\n        Does not have to be called by the user except in extraordinary\n        cases.\n        \"\"\"\n        # Categories\n        for index, label in enumerate(self._category_labels.values()):\n            label.grid(column=0, row=index, padx=5, sticky=\"nw\", pady=(1, 0) if index == 0 else 0)\n        # Canvas widgets\n        self._canvas_scroll.grid(column=1, row=0, padx=(0, 5), pady=5, sticky=\"nswe\")\n        self._canvas_ticks.grid(column=1, row=1, padx=(0, 5), pady=(0, 5), sticky=\"nswe\")\n        self._scrollbar_timeline.grid(column=1, row=2, padx=(0, 5), pady=(0, 5), sticky=\"we\")\n        # Zoom widgets\n        self._button_zoom_in.grid(row=0, column=0, pady=5, sticky=\"nswe\")\n        self._button_zoom_out.grid(row=1, column=0, pady=(0, 5), sticky=\"nswe\")\n        self._button_zoom_reset.grid(row=2, column=0, pady=(0, 5), sticky=\"nswe\")\n        # Frames\n        self._canvas_categories.grid(column=0, row=0, padx=5, pady=5, sticky=\"nswe\")\n        self._scrollbar_vertical.grid(column=2, row=0, pady=5, padx=(0, 5), sticky=\"ns\")\n        self._frame_zoom.grid(column=3, row=0, rowspan=2, padx=(0, 5), pady=5, sticky=\"nswe\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndraws the contents of the whole TimeLine Canvas", "response": "def draw_timeline(self):\n        \"\"\"Draw the contents of the whole TimeLine Canvas\"\"\"\n        # Configure the canvas\n        self.clear_timeline()\n        self.create_scroll_region()\n        self._timeline.config(width=self.pixel_width)\n        self._canvas_scroll.config(width=self._width, height=self._height)\n        # Generate the Y-coordinates for each of the rows and create the lines indicating the rows\n        self.draw_separators()\n        # Create the markers on the timeline\n        self.draw_markers()\n        # Create the ticks in the _canvas_ticks\n        self.draw_ticks()\n        self.draw_time_marker()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef draw_time_marker(self):\n        self._time_marker_image = self._canvas_ticks.create_image((2, 16), image=self._time_marker)\n        self._time_marker_line = self._timeline.create_line(\n            (2, 0, 2, self._timeline.winfo_height()), fill=\"#016dc9\", width=2)\n        self._timeline.lift(self._time_marker_line)\n        self._timeline.tag_lower(\"marker\")", "response": "Draw the time marker on the TimeLine Canvas"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndraw the category labels on the Canvas", "response": "def draw_categories(self):\n        \"\"\"Draw the category labels on the Canvas\"\"\"\n        for label in self._category_labels.values():\n            label.destroy()\n        self._category_labels.clear()\n        canvas_width = 0\n        for category in (sorted(self._categories.keys() if isinstance(self._categories, dict) else self._categories)\n                         if not isinstance(self._categories, OrderedDict)\n                         else self._categories):\n            kwargs = self._categories[category] if isinstance(self._categories, dict) else {\"text\": category}\n            kwargs[\"background\"] = kwargs.get(\"background\", self._background)\n            kwargs[\"justify\"] = kwargs.get(\"justify\", tk.LEFT)\n            label = ttk.Label(self._frame_categories, **kwargs)\n            width = label.winfo_reqwidth()\n            canvas_width = width if width > canvas_width else canvas_width\n            self._category_labels[category] = label\n        self._canvas_categories.create_window(0, 0, window=self._frame_categories, anchor=tk.NW)\n        self._canvas_categories.config(width=canvas_width + 5, height=self._height)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clear_timeline(self):\n        self._timeline.delete(tk.ALL)\n        self._canvas_ticks.delete(tk.ALL)", "response": "Clear the contents of the TimeLine Canvas and all the canvas ticks."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef draw_ticks(self):\n        self._canvas_ticks.create_line((0, 10, self.pixel_width, 10), fill=\"black\")\n        self._ticks = list(TimeLine.range(self._start, self._finish, self._tick_resolution / self._zoom_factor))\n        for tick in self._ticks:\n            string = TimeLine.get_time_string(tick, self._unit)\n            x = self.get_time_position(tick)\n            x_tick = x + 1 if x == 0 else (x - 1 if x == self.pixel_width else x)\n            x_text = x + 15 if x - 15 <= 0 else (x - 15 if x + 15 >= self.pixel_width else x)\n            self._canvas_ticks.create_text((x_text, 20), text=string, fill=\"black\", font=(\"default\", 10))\n            self._canvas_ticks.create_line((x_tick, 5, x_tick, 15), fill=\"black\")\n        self._canvas_ticks.config(scrollregion=\"0 0 {0} {1}\".format(self.pixel_width, 30))", "response": "Draw the time tick markers on the TimeLine Canvas"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef draw_separators(self):\n        total = 1\n        self._timeline.create_line((0, 1, self.pixel_width, 1))\n        for index, (category, label) in enumerate(self._category_labels.items()):\n            height = label.winfo_reqheight()\n            self._rows[category] = (total, total + height)\n            total += height\n            self._timeline.create_line((0, total, self.pixel_width, total))\n        pixel_height = total\n        self._timeline.config(height=pixel_height)", "response": "Draw the lines separating the categories on the Canvas"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef draw_markers(self):\n        self._canvas_markers.clear()\n        for marker in self._markers.values():\n            self.create_marker(marker[\"category\"], marker[\"start\"], marker[\"finish\"], marker)", "response": "Draw all created markers on the TimeLine Canvas"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __configure_timeline(self, *args):\n        # Resize the canvas scrollregion to fit the entire frame\n        (size_x, size_y) = (self._timeline.winfo_reqwidth(), self._timeline.winfo_reqheight())\n        self._canvas_scroll.config(scrollregion=\"0 0 {0} {1}\".format(size_x, size_y - 5))", "response": "Function from ScrolledFrame adapted for the _timeline"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_marker(self, category, start, finish, marker=None, **kwargs):\n        kwargs = kwargs if marker is None else marker\n        if category not in self._categories:\n            raise ValueError(\"category argument not a valid category: {}\".format(category))\n        if start < self._start or finish > self._finish:\n            raise ValueError(\"time out of bounds\")\n        self.check_marker_kwargs(kwargs)\n        # Update the options based on the tags. The last tag always takes precedence over the ones before it, and the\n        # marker specific options take precedence over tag options\n        tags = kwargs.get(\"tags\", ())\n        options = kwargs.copy()\n        # Check the tags\n        for tag in tags:\n            # Update the options\n            kwargs.update(self._tags[tag])\n        # Update with the specific marker options\n        kwargs.update(options)\n        # Process the other options\n        iid = kwargs.pop(\"iid\", str(self._iid))\n        background = kwargs.get(\"background\", \"default\")\n        foreground = kwargs.get(\"foreground\", \"default\")\n        outline = kwargs.get(\"outline\", \"default\")\n        font = kwargs.get(\"font\", \"default\")\n        border = kwargs.get(\"border\", \"default\")\n        move = kwargs.get(\"move\", \"default\")\n        change_category = kwargs.get(\"change_category\", \"default\")\n        allow_overlap = kwargs.get(\"allow_overlap\", \"default\")\n        snap_to_ticks = kwargs.get(\"snap_to_ticks\", \"default\")\n        # Calculate pixel positions\n        x1 = start / self._resolution * self._zoom_factor\n        x2 = finish / self._resolution * self._zoom_factor\n        y1, y2 = self._rows[category]\n        # Create the rectangle\n        rectangle_id = self._timeline.create_rectangle(\n            (x1, y1, x2, y2),\n            fill=background if background != \"default\" else self._marker_background,\n            outline=outline if outline != \"default\" else self._marker_outline,\n            tags=(\"marker\",),\n            width=border if border != \"default\" else self._marker_border\n        )\n        # Create the text\n        text = kwargs.get(\"text\", None)\n        text_id = self._draw_text((x1, y1, x2, y2), text, foreground, font) if text is not None else None\n        # Save the marker\n        locals_ = locals()\n        self._markers[iid] = {\n            key: (\n                locals_[key.replace(\"hover_\", \"\").replace(\"active_\", \"\")] if key in (\n                    prefix + color for prefix in [\"\", \"hover_\", \"active_\"]\n                    for color in [\"background\", \"foreground\", \"outline\", \"border\"]\n                ) and key not in kwargs else (locals_[key] if key in locals_ else kwargs[key])\n            ) for key in self.marker_options\n        }\n        # Save the marker's Canvas IDs\n        self._canvas_markers[rectangle_id] = iid\n        self._canvas_markers[text_id] = iid\n        self._timeline.tag_lower(\"marker\")\n        # Attempt to prevent duplicate iids\n        while str(self._iid) in self.markers:\n            self._iid += 1\n        return iid", "response": "Create a new marker in the TimeLine with the specified options."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndraw the text and shorten it if required", "response": "def _draw_text(self, coords, text, foreground, font):\n        \"\"\"Draw the text and shorten it if required\"\"\"\n        if text is None:\n            return None\n        x1_r, _, x2_r, _ = coords\n        while True:\n            text_id = self._timeline.create_text(\n                (0, 0), text=text,\n                fill=foreground if foreground != \"default\" else self._marker_foreground,\n                font=font if font != \"default\" else self._marker_font,\n                tags=(\"marker\",)\n            )\n            x1_t, _, x2_t, _ = self._timeline.bbox(text_id)\n            if (x2_t - x1_t) < (x2_r - x1_r):\n                break\n            self._timeline.delete(text_id)\n            text = text[:-4] + \"...\"\n        x, y = TimeLine.calculate_text_coords(coords)\n        self._timeline.coords(text_id, (x, y))\n        return text_id"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the options for a certain marker and redraw the marker", "response": "def update_marker(self, iid, **kwargs):\n        \"\"\"\n        Change the options for a certain marker and redraw the marker\n\n        :param iid: identifier of the marker to change\n        :type iid: str\n        :param kwargs: Dictionary of options to update\n        :type kwargs: dict\n        :raises: ValueError\n        \"\"\"\n        if iid not in self._markers:\n            raise ValueError(\"Unknown iid passed as argument: {}\".format(iid))\n        self.check_kwargs(kwargs)\n        marker = self._markers[iid]\n        marker.update(kwargs)\n        self.delete_marker(iid)\n        return self.create_marker(marker[\"category\"], marker[\"start\"], marker[\"finish\"], marker)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_marker(self, iid):\n        if iid == tk.ALL:\n            for iid in self.markers.keys():\n                self.delete_marker(iid)\n            return\n        options = self._markers[iid]\n        rectangle_id, text_id = options[\"rectangle_id\"], options[\"text_id\"]\n        del self._canvas_markers[rectangle_id]\n        del self._canvas_markers[text_id]\n        del self._markers[iid]\n        self._timeline.delete(rectangle_id, text_id)", "response": "Delete a marker from the TimeLine."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef zoom_in(self):\n        index = self._zoom_factors.index(self._zoom_factor)\n        if index + 1 == len(self._zoom_factors):\n            # Already zoomed in all the way\n            return\n        self._zoom_factor = self._zoom_factors[index + 1]\n        if self._zoom_factors.index(self.zoom_factor) + 1 == len(self._zoom_factors):\n            self._button_zoom_in.config(state=tk.DISABLED)\n        self._button_zoom_out.config(state=tk.NORMAL)\n        self.draw_timeline()", "response": "Increase zoom factor and redraw TimeLine"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecrease zoom factor and redraw TimeLine", "response": "def zoom_out(self):\n        \"\"\"Decrease zoom factor and redraw TimeLine\"\"\"\n        index = self._zoom_factors.index(self._zoom_factor)\n        if index == 0:\n            # Already zoomed out all the way\n            return\n        self._zoom_factor = self._zoom_factors[index - 1]\n        if self._zoom_factors.index(self._zoom_factor) == 0:\n            self._button_zoom_out.config(state=tk.DISABLED)\n        self._button_zoom_in.config(state=tk.NORMAL)\n        self.draw_timeline()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef zoom_reset(self):\n        self._zoom_factor = self._zoom_factors[0] if self._zoom_default == 0 else self._zoom_default\n        if self._zoom_factors.index(self._zoom_factor) == 0:\n            self._button_zoom_out.config(state=tk.DISABLED)\n            self._button_zoom_in.config(state=tk.NORMAL)\n        elif self._zoom_factors.index(self.zoom_factor) + 1 == len(self._zoom_factors):\n            self._button_zoom_out.config(state=tk.NORMAL)\n            self._button_zoom_in.config(state=tk.DISABLED)\n        self.draw_timeline()", "response": "Reset the zoom factor to default and redraw TimeLine"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the time marker to a specific time", "response": "def set_time(self, time):\n        \"\"\"\n        Set the time marker to a specific time\n\n        :param time: Time to set for the time marker on the TimeLine\n        :type time: float\n        \"\"\"\n        x = self.get_time_position(time)\n        _, y = self._canvas_ticks.coords(self._time_marker_image)\n        self._canvas_ticks.coords(self._time_marker_image, x, y)\n        self._timeline.coords(self._time_marker_line, x, 0, x, self._timeline.winfo_height())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _time_marker_move(self, event):\n        limit = self.pixel_width\n        x = self._canvas_ticks.canvasx(event.x)\n        x = min(max(x, 0), limit)\n        _, y = self._canvas_ticks.coords(self._time_marker_image)\n        self._canvas_ticks.coords(self._time_marker_image, x, y)\n        self._timeline.coords(self._time_marker_line, x, 0, x, self._timeline.winfo_height())\n        self._time_show()", "response": "Callback for the time marker move event"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _time_marker_release(self, event):\n        if not self._time_visible:\n            return\n        self._time_label.destroy()\n        self._time_window.destroy()\n        self._time_label = None\n        self._time_window = None\n        self._time_visible = False", "response": "Callback for the time marker release event."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nshowing the time marker window", "response": "def _time_show(self):\n        \"\"\"Show the time marker window\"\"\"\n        if not self._time_visible:\n            self._time_visible = True\n            self._time_window = tk.Toplevel(self)\n            self._time_window.attributes(\"-topmost\", True)\n            self._time_window.overrideredirect(True)\n            self._time_label = ttk.Label(self._time_window)\n            self._time_label.grid()\n            self._time_window.lift()\n        x, y = self.master.winfo_pointerxy()\n        geometry = \"{0}x{1}+{2}+{3}\".format(\n            self._time_label.winfo_width(),\n            self._time_label.winfo_height(),\n            x - 15,\n            self._canvas_ticks.winfo_rooty() - 10)\n        self._time_window.wm_geometry(geometry)\n        self._time_label.config(text=TimeLine.get_time_string(self.time, self._unit))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tag_configure(self, tag_name, **kwargs):\n        callbacks = [\n            kwargs.get(\"move_callback\", None),\n            kwargs.get(\"left_callback\", None),\n            kwargs.get(\"right_callback\", None)\n        ]\n        for callback in callbacks:\n            if callback is not None and not callable(callback):\n                raise ValueError(\"One or more callbacks is not a callable object\")\n        self._tags[tag_name] = kwargs", "response": "Configure the tags for a specific tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef marker_tags(self, iid):\n        tags = self._markers[iid][\"tags\"]\n        for tag in tags:\n            yield tag", "response": "Generator for all the tags of a certain marker"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nscrolls both Canvas and scrolling container", "response": "def _set_scroll_v(self, *args):\n        \"\"\"Scroll both categories Canvas and scrolling container\"\"\"\n        self._canvas_categories.yview(*args)\n        self._canvas_scroll.yview(*args)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _mouse_scroll_h(self, event):\n        args = (int(-1 * (event.delta / 120)), \"units\")\n        self._canvas_scroll.xview_scroll(*args)\n        self._canvas_ticks.xview_scroll(*args)", "response": "Callback for mouse wheel event for horizontal scrolling"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _mouse_scroll_v(self, event):\n        args = (int(-1 * (event.delta / 120)), \"units\")\n        self._canvas_scroll.yview_scroll(*args)\n        self._canvas_categories.yview_scroll(*args)", "response": "Callback for mouse wheel event for vertical scrolling"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting horizontal scroll of scroll container and ticks Canvas", "response": "def _set_scroll(self, *args):\n        \"\"\"Set horizontal scroll of scroll container and ticks Canvas\"\"\"\n        self._canvas_scroll.xview(*args)\n        self._canvas_ticks.xview(*args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_time_position(self, time):\n        if time < self._start or time > self._finish:\n            raise ValueError(\"time argument out of bounds\")\n        return (time - self._start) / (self._resolution / self._zoom_factor)", "response": "Get the x - coordinate of the time - series for the given time."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_position_time(self, position):\n        return self._start + position * (self._resolution / self._zoom_factor)", "response": "Get time for the given x - coordinate"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a properly formatted string given a time and unit", "response": "def get_time_string(time, unit):\n        \"\"\"\n        Create a properly formatted string given a time and unit\n\n        :param time: Time to format\n        :type time: float\n        :param unit: Unit to apply format of. Only supports hours ('h')\n            and minutes ('m').\n        :type unit: str\n        :return: A string in format '{whole}:{part}'\n        :rtype: str\n        \"\"\"\n        supported_units = [\"h\", \"m\"]\n        if unit not in supported_units:\n            return \"{}\".format(round(time, 2))\n        hours, minutes = str(time).split(\".\")\n        hours = int(hours)\n        minutes = int(round(float(\"0.{}\".format(minutes)) * 60))\n        return \"{:02d}:{:02d}\".format(hours, minutes)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfunction bound to right click event for marker canvas", "response": "def _right_click(self, event):\n        \"\"\"Function bound to right click event for marker canvas\"\"\"\n        iid = self.current_iid\n        if iid is None:\n            if self._menu is not None:\n                self._menu.post(event.x, event.y)\n            return\n        args = (iid, (event.x_root, event.y_root))\n        self.call_callbacks(iid, \"right_callback\", args)\n        tags = list(self.marker_tags(iid))\n        if len(tags) == 0:\n            return\n        menu = self._tags[tags[-1]].get(\"menu\", None)\n        if menu is None or not isinstance(menu, tk.Menu):\n            return\n        menu.post(event.x_root, event.y_root)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _left_click(self, event):\n        self.update_active()\n        iid = self.current_iid\n        if iid is None:\n            return\n        args = (iid, event.x_root, event.y_root)\n        self.call_callbacks(iid, \"left_callback\", args)", "response": "Function bound to left click event for marker canvas"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning bound to left motion event for marker canvas", "response": "def _left_motion(self, event):\n        \"\"\"Function bound to move event for marker canvas\"\"\"\n        iid = self.current_iid\n        if iid is None:\n            return\n        marker = self._markers[iid]\n        if marker[\"move\"] is False:\n            return\n        delta = marker[\"finish\"] - marker[\"start\"]\n        # Limit x to 0\n        x = max(self._timeline.canvasx(event.x), 0)\n        # Check if the timeline needs to be extended\n        limit = self.get_time_position(self._finish - delta)\n        if self._extend is False:\n            x = min(x, limit)\n        elif x > limit:  # self._extend is True\n            self.configure(finish=(self.get_position_time(x) + (marker[\"finish\"] - marker[\"start\"])) * 1.1)\n        # Get the new start value\n        start = self.get_position_time(x)\n        finish = start + (marker[\"finish\"] - marker[\"start\"])\n        rectangle_id, text_id = marker[\"rectangle_id\"], marker[\"text_id\"]\n        if rectangle_id not in self._timeline.find_all():\n            return\n        x1, y1, x2, y2 = self._timeline.coords(rectangle_id)\n        # Overlap protection\n        allow_overlap = marker[\"allow_overlap\"]\n        allow_overlap = self._marker_allow_overlap if allow_overlap == \"default\" else allow_overlap\n        if allow_overlap is False:\n            for marker_dict in self.markers.values():\n                if marker_dict[\"allow_overlap\"] is True:\n                    continue\n                if marker[\"iid\"] != marker_dict[\"iid\"] and marker[\"category\"] == marker_dict[\"category\"]:\n                    if marker_dict[\"start\"] < start < marker_dict[\"finish\"]:\n                        start = marker_dict[\"finish\"] if start < marker_dict[\"finish\"] else marker_dict[\"start\"]\n                        finish = start + (marker[\"finish\"] - marker[\"start\"])\n                        x = self.get_time_position(start)\n                        break\n                    if marker_dict[\"start\"] < finish < marker_dict[\"finish\"]:\n                        finish = marker_dict[\"finish\"] if finish > marker_dict[\"finish\"] else marker_dict[\"start\"]\n                        start = finish - (marker_dict[\"finish\"] - marker_dict[\"start\"])\n                        x = self.get_time_position(start)\n                        break\n        # Vertical movement\n        if marker[\"change_category\"] is True or \\\n                (marker[\"change_category\"] == \"default\" and self._marker_change_category):\n            y = max(self._timeline.canvasy(event.y), 0)\n            category = min(self._rows.keys(), key=lambda category: abs(self._rows[category][0] - y))\n            marker[\"category\"] = category\n            y1, y2 = self._rows[category]\n        # Snapping to ticks\n        if marker[\"snap_to_ticks\"] is True or (marker[\"snap_to_ticks\"] == \"default\" and self._marker_snap_to_ticks):\n            # Start is prioritized over finish\n            for tick in self._ticks:\n                tick = self.get_time_position(tick)\n                # Start\n                if abs(x - tick) < self._snap_margin:\n                    x = tick\n                    break\n                # Finish\n                x_finish = x + delta\n                if abs(x_finish - tick) < self._snap_margin:\n                    delta = self.get_time_position(marker[\"finish\"] - marker[\"start\"])\n                    x = tick - delta\n                    break\n        rectangle_coords = (x, y1, x2 + (x - x1), y2)\n        self._timeline.coords(rectangle_id, *rectangle_coords)\n        if text_id is not None:\n            text_x, text_y = TimeLine.calculate_text_coords(rectangle_coords)\n            self._timeline.coords(text_id, text_x, text_y)\n        if self._after_id is not None:\n            self.after_cancel(self._after_id)\n        args = (iid, (marker[\"start\"], marker[\"finish\"]), (start, finish))\n        self._after_id = self.after(10, self._after_handler(iid, \"move_callback\", args))\n        marker[\"start\"] = start\n        marker[\"finish\"] = finish"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _leave_handler(self, event):\n        iid = self.current_iid\n        if iid is None or self.active == iid:\n            return\n        self.update_state(iid, \"normal\")", "response": "Callback for the leave event on marker to set normal options"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_state(self, iid, state):\n        if state not in [\"normal\", \"hover\", \"active\"]:\n            raise ValueError(\"Invalid state: {}\".format(state))\n        marker = self._markers[iid]\n        rectangle_id, text_id = marker[\"rectangle_id\"], marker[\"text_id\"]\n        state = \"\" if state == \"normal\" else state + \"_\"\n        colors = {}\n        for color_type in [\"background\", \"foreground\", \"outline\", \"border\"]:\n            value = marker[state + color_type]\n            attribute = \"_marker_{}\".format(color_type)\n            colors[color_type] = getattr(self, attribute) if value == \"default\" else value\n        self._timeline.itemconfigure(rectangle_id, fill=colors[\"background\"], width=colors[\"border\"],\n                                     outline=colors[\"outline\"])\n        self._timeline.itemconfigure(text_id, fill=colors[\"foreground\"])", "response": "Update the state of the marker with the new state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_active(self):\n        if self.active is not None:\n            self.update_state(self.active, \"normal\")\n        if self.current_iid == self.active:\n            self._active = None\n            return\n        self._active = self.current_iid\n        if self.active is not None:\n            self.update_state(self.active, \"active\")", "response": "Update the active marker on the marker Canvas"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling the callbacks for a certain marker", "response": "def call_callbacks(self, iid, type, args):\n        \"\"\"\n        Call the available callbacks for a certain marker\n\n        :param iid: marker identifier\n        :type iid: str\n        :param type: type of callback (key in tag dictionary)\n        :type type: str\n        :param args: arguments for the callback\n        :type args: tuple\n        :return: amount of callbacks called\n        :rtype: int\n        \"\"\"\n        amount = 0\n        for tag in self.marker_tags(iid):\n            callback = self._tags[tag].get(type, None)\n            if callback is not None:\n                amount += 1\n                callback(*args)\n        return amount"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef time(self):\n        x, _, = self._canvas_ticks.coords(self._time_marker_image)\n        return self.get_position_time(x)", "response": "Get the current time of the time marker"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef current(self):\n        results = self._timeline.find_withtag(tk.CURRENT)\n        return results[0] if len(results) != 0 else None", "response": "Returns the currently active item on the _timeline"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef current_iid(self):\n        current = self.current\n        if current is None or current not in self._canvas_markers:\n            return None\n        return self._canvas_markers[current]", "response": "Currently active item s iid\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the width of the whole TimeLine in pixels.", "response": "def pixel_width(self):\n        \"\"\"\n        Width of the whole TimeLine in pixels\n\n        :rtype: int\n        \"\"\"\n        return self.zoom_factor * ((self._finish - self._start) / self._resolution)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconfigures the TimeLine widget.", "response": "def configure(self, cnf={}, **kwargs):\n        \"\"\"Update options of the TimeLine widget\"\"\"\n        kwargs.update(cnf)\n        TimeLine.check_kwargs(kwargs)\n        scrollbars = 'autohidescrollbars' in kwargs\n        for option in self.options:\n            attribute = \"_\" + option\n            setattr(self, attribute, kwargs.pop(option, getattr(self, attribute)))\n        if scrollbars:\n            self._scrollbar_timeline.destroy()\n            self._scrollbar_vertical.destroy()\n            if self._autohidescrollbars:\n                self._scrollbar_timeline = AutoHideScrollbar(self, command=self._set_scroll, orient=tk.HORIZONTAL)\n                self._scrollbar_vertical = AutoHideScrollbar(self, command=self._set_scroll_v, orient=tk.VERTICAL)\n            else:\n                self._scrollbar_timeline = ttk.Scrollbar(self, command=self._set_scroll, orient=tk.HORIZONTAL)\n                self._scrollbar_vertical = ttk.Scrollbar(self, command=self._set_scroll_v, orient=tk.VERTICAL)\n            self._canvas_scroll.config(xscrollcommand=self._scrollbar_timeline.set,\n                                       yscrollcommand=self._scrollbar_vertical.set)\n            self._canvas_categories.config(yscrollcommand=self._scrollbar_vertical.set)\n            self._scrollbar_timeline.grid(column=1, row=2, padx=(0, 5), pady=(0, 5), sticky=\"we\")\n            self._scrollbar_vertical.grid(column=2, row=0, pady=5, padx=(0, 5), sticky=\"ns\")\n        ttk.Frame.configure(self, **kwargs)\n        self.draw_timeline()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cget(self, item):\n        return getattr(self, \"_\" + item) if item in self.options else ttk.Frame.cget(self, item)", "response": "Return the value of an option"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconfigures options of items drawn on the Canvas.", "response": "def itemconfigure(self, iid, rectangle_options, text_options):\n        \"\"\"\n        Configure options of items drawn on the Canvas\n\n        Low-level access to the individual elements of markers and other\n        items drawn on the timeline Canvas. All modifications are\n        overwritten when the TimeLine is redrawn.\n        \"\"\"\n        rectangle_id, text_id = self._markers[iid][\"rectangle_id\"], self._markers[iid][\"text_id\"]\n        if len(rectangle_options) != 0:\n            self._timeline.itemconfigure(rectangle_id, **rectangle_options)\n        if len(text_options) != 0:\n            self._timeline.itemconfigure(text_id, **text_options)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates Canvas text coordinates based on rectangle coords", "response": "def calculate_text_coords(rectangle_coords):\n        \"\"\"Calculate Canvas text coordinates based on rectangle coords\"\"\"\n        return (int(rectangle_coords[0] + (rectangle_coords[2] - rectangle_coords[0]) / 2),\n                int(rectangle_coords[1] + (rectangle_coords[3] - rectangle_coords[1]) / 2))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef range(start, finish, step):\n        value = start\n        while value <= finish:\n            yield value\n            value += step", "response": "Yields the values from start to finish in a given step."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_kwargs(kwargs):\n        # width, height\n        width = kwargs.get(\"width\", 400)\n        height = kwargs.get(\"height\", 200)\n        if not isinstance(width, int) or not isinstance(height, int):\n            raise TypeError(\"width and/or height arguments not of int type\")\n        if not width > 0 or not height > 0:\n            raise ValueError(\"width and/or height arguments not larger than zero\")\n        # start, finish\n        start = kwargs.get(\"start\", 0.0)\n        finish = kwargs.get(\"finish\", 10.0)\n        if not isinstance(start, float) or not isinstance(finish, float):\n            raise TypeError(\"start and/or finish arguments not of float type\")\n        # resolutions\n        resolution = kwargs.get(\"resolution\", 0.01)\n        tick_resolution = kwargs.get(\"tick_resolution\", 1.0)\n        if not isinstance(resolution, float) or not isinstance(tick_resolution, float):\n            raise TypeError(\"resolution and/or tick_resolution arguments not of float type\")\n        if not resolution > 0 or not tick_resolution > 0:\n            raise ValueError(\"resolution and/or tick_resolution arguments not larger than zero\")\n        # unit\n        unit = kwargs.get(\"unit\", \"\")\n        if not isinstance(unit, str):\n            raise TypeError(\"unit argument not of str type\")\n        # zoom\n        zoom_enabled = kwargs.get(\"zoom_enabled\", True)\n        zoom_factors = kwargs.get(\"zoom_factors\", (1, 2, 5))\n        zoom_default = kwargs.get(\"zoom_default\", 0)\n        if not isinstance(zoom_enabled, bool):\n            raise TypeError(\"zoom_enabled argument not of bool type\")\n        if not isinstance(zoom_factors, tuple):\n            raise TypeError(\"zoom_factors argument not of tuple type\")\n        if not len(zoom_factors) > 0:\n            raise ValueError(\"zoom_factors argument is empty tuple\")\n        if sum(1 for factor in zoom_factors if isinstance(factor, (int, float))) != len(zoom_factors):\n            raise ValueError(\"one or more values in zoom_factors argument not of int or float type\")\n        if not isinstance(zoom_default, (int, float)):\n            raise TypeError(\"zoom_default argument is not int or float type\")\n        if not zoom_default >= 0:\n            raise ValueError(\"zoom_default argument does not have a valid value\")\n        # categories\n        categories = kwargs.get(\"categories\", {})\n        if not isinstance(categories, (dict, tuple)):\n            raise TypeError(\"categories argument not of dict or tuple type\")\n        # background\n        background = kwargs.get(\"background\", \"gray90\")\n        if not isinstance(background, str):\n            raise TypeError(\"background argument not of str type\")\n        # style\n        style = kwargs.get(\"style\", \"TimeLine.TFrame\")\n        if not isinstance(style, str):\n            raise TypeError(\"style argument is not of str type\")\n        # extend\n        extend = kwargs.get(\"extend\", False)\n        if not isinstance(extend, bool):\n            raise TypeError(\"extend argument is not of bool type\")\n        snap_margin = kwargs.get(\"snap_margin\", 10)\n        if not isinstance(snap_margin, int):\n            raise TypeError(\"snap_margin argument is not of int type\")\n        menu = kwargs.get(\"menu\", None)\n        if menu is not None and not isinstance(menu, tk.Menu):\n            raise TypeError(\"menu argument is not a tk.Menu widget\")\n        autohidescrollbars = kwargs.get(\"autohidescrollbars\", False)\n        if not isinstance(autohidescrollbars, bool):\n            raise TypeError(\"autohidescrollbars argument is not of bool type\")\n        # marker options\n        marker_font = kwargs.get(\"marker_font\", (\"default\", 10))\n        marker_background = kwargs.get(\"marker_background\", \"lightblue\")\n        marker_foreground = kwargs.get(\"marker_foreground\", \"black\")\n        marker_outline = kwargs.get(\"marker_outline\", \"black\")\n        marker_border = kwargs.get(\"marker_border\", 0)\n        marker_move = kwargs.get(\"marker_move\", True)\n        marker_change_category = kwargs.get(\"marker_change_category\", False)\n        marker_allow_overlap = kwargs.get(\"marker_allow_overlap\", False)\n        marker_snap_to_ticks = kwargs.get(\"marker_snap_to_ticks\", True)\n        if not isinstance(marker_font, tuple) or len(marker_font) == 0:\n            raise ValueError(\"marker_font argument not a valid font tuple\")\n        if not isinstance(marker_background, str) or not isinstance(marker_foreground, str):\n            raise TypeError(\"marker_background and/or marker_foreground argument(s) not of str type\")\n        if not isinstance(marker_outline, str):\n            raise TypeError(\"marker_outline argument not of str type\")\n        if not isinstance(marker_border, int):\n            raise TypeError(\"marker_border argument is not of int type\")\n        if not marker_border >= 0:\n            raise ValueError(\"marker_border argument is smaller than zero\")\n        if not isinstance(marker_move, bool):\n            raise TypeError(\"marker_move argument is not of bool type\")\n        if not isinstance(marker_change_category, bool):\n            raise TypeError(\"marker_change_category argument is not of bool type\")\n        if not isinstance(marker_allow_overlap, bool):\n            raise TypeError(\"marker_allow_overlap argument is not of bool type\")\n        if not isinstance(marker_snap_to_ticks, bool):\n            raise TypeError(\"marker_snap_to_ticks argument is not of bool type\")\n        return", "response": "Check the type and values of keyword arguments to : meth : __init__"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck the types of the keyword arguments for marker creation", "response": "def check_marker_kwargs(self, kwargs):\n        \"\"\"\n        Check the types of the keyword arguments for marker creation\n\n        :param kwargs: dictionary of options for marker creation\n        :type kwargs: dict\n        :raises: TypeError, ValueError\n        \"\"\"\n        text = kwargs.get(\"text\", \"\")\n        if not isinstance(text, str) and text is not None:\n            raise TypeError(\"text argument is not of str type\")\n        for color in (item for item in (prefix + color for prefix in [\"active_\", \"hover_\", \"\"]\n                                        for color in [\"background\", \"foreground\", \"outline\"])):\n            value = kwargs.get(color, \"\")\n            if value == \"default\":\n                continue\n            if not isinstance(value, str):\n                raise TypeError(\"{} argument not of str type\".format(color))\n        font = kwargs.get(\"font\", (\"default\", 10))\n        if (not isinstance(font, tuple) or not len(font) > 0 or not isinstance(font[0], str)) and font != \"default\":\n            raise ValueError(\"font argument is not a valid font tuple\")\n        for border in (prefix + \"border\" for prefix in [\"active_\", \"hover_\", \"\"]):\n            border_v = kwargs.get(border, 0)\n            if border_v == \"default\":\n                continue\n            if not isinstance(border_v, int) or border_v < 0:\n                raise ValueError(\"{} argument is not of int type or smaller than zero\".format(border))\n        iid = kwargs.get(\"iid\", \"-1\")\n        if not isinstance(iid, str):\n            raise TypeError(\"iid argument not of str type\")\n        if iid == \"\":\n            raise ValueError(\"iid argument empty string\")\n        for boolean_arg in [\"move\", \"category_change\", \"allow_overlap\", \"snap_to_ticks\"]:\n            value = kwargs.get(boolean_arg, False)\n            if value == \"default\":\n                continue\n            if not isinstance(value, bool):\n                raise TypeError(\"{} argument is not of bool type\".format(boolean_arg))\n        tags = kwargs.get(\"tags\", ())\n        if not isinstance(tags, tuple):\n            raise TypeError(\"tags argument is not of tuple type\")\n        for tag in tags:\n            if not isinstance(tag, str):\n                raise TypeError(\"one or more values in tags argument is not of str type\")\n            if tag not in self._tags:\n                raise ValueError(\"unknown tag in tags argument\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplace all the child widgets in the appropriate positions.", "response": "def __grid_widgets(self):\n        \"\"\"Places all the child widgets in the appropriate positions.\"\"\"\n        scrollbar_column = 0 if self.__compound is tk.LEFT else 2\n        self._canvas.grid(row=0, column=1, sticky=\"nswe\")\n        self._scrollbar.grid(row=0, column=scrollbar_column, sticky=\"ns\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __configure_interior(self, *args):\n        # Resize the canvas scrollregion to fit the entire frame\n        (size_x, size_y) = (self.interior.winfo_reqwidth(), self.interior.winfo_reqheight())\n        self._canvas.config(scrollregion=\"0 0 {0} {1}\".format(size_x, size_y))\n        if self.interior.winfo_reqwidth() is not self._canvas.winfo_width():\n            # If the interior Frame is wider than the canvas, automatically resize the canvas to fit the frame\n            self._canvas.config(width=self.interior.winfo_reqwidth())", "response": "Private function to configure the interior Frame."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __configure_canvas(self, *args):\n        if self.interior.winfo_reqwidth() is not self._canvas.winfo_width():\n            self._canvas.configure(width=self.interior.winfo_reqwidth())", "response": "Private function to configure the internal Canvas."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfunction for the user to resize the internal Canvas widget if desired.", "response": "def resize_canvas(self, height=400, width=400):\n        \"\"\"\n        Function for the user to resize the internal Canvas widget if desired.\n        \n        :param height: new height in pixels\n        :type height: int\n        :param width: new width in pixels\n        :type width: int\n        \"\"\"\n        self._canvas.configure(width=width, height=height)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the fractional values of the slider position.", "response": "def set(self, lo, hi):\n        \"\"\"\n        Set the fractional values of the slider position.\n        \n        :param lo: lower end of the scrollbar (between 0 and 1)\n        :type lo: float\n        :param hi: upper end of the scrollbar (between 0 and 1)\n        :type hi: float\n        \"\"\"\n        if float(lo) <= 0.0 and float(hi) >= 1.0:\n            if self._layout == 'place':\n                self.place_forget()\n            elif self._layout == 'pack':\n                self.pack_forget()\n            else:\n                self.grid_remove()\n        else:\n            if self._layout == 'place':\n                self.place(**self._place_kw)\n            elif self._layout == 'pack':\n                self.pack(**self._pack_kw)\n            else:\n                self.grid()\n        ttk.Scrollbar.set(self, lo, hi)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_info(self, layout):\n        info = str(self.tk.call(layout, 'info', self._w)).split(\"-\")\n        dic = {}\n        for i in info:\n            if i:\n                key, val = i.strip().split()\n                dic[key] = val\n        return dic", "response": "Alternative to pack_info and place_info in case of bug."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplace a widget in the parent widget.", "response": "def place(self, **kw):\n        \"\"\"\n        Place a widget in the parent widget.\n        \n        :param in\\_: master relative to which the widget is placed\n        :type in\\_: widget\n        :param x: locate anchor of this widget at position x of master\n        :type x: int\n        :param y: locate anchor of this widget at positiony of master\n        :type y: int\n        :param relx: locate anchor of this widget between 0 and 1\n                      relative to width of master (1 is right edge)\n        :type relx: float\n        :param rely: locate anchor of this widget between 0 and 1\n                      relative to height of master (1 is bottom edge)\n        :type rely: float\n        :param anchor: position anchor according to given direction \n                        (\"n\", \"s\", \"e\", \"w\" or combinations)\n        :type anchor: str\n        :param width: width of this widget in pixel\n        :type width: int\n        :param height: height of this widget in pixel\n        :type height: int\n        :param relwidth: width of this widget between 0.0 and 1.0\n                          relative to width of master (1.0 is the same width\n                          as the master)\n        :type relwidth: float\n        :param relheight: height of this widget between 0.0 and 1.0\n                           relative to height of master (1.0 is the same\n                           height as the master)\n        :type relheight: float\n        :param bordermode: \"inside\" or \"outside\": whether to take border width of master widget into account\n        :type bordermode: str\n        \"\"\"\n        ttk.Scrollbar.place(self, **kw)\n        try:\n            self._place_kw = self.place_info()\n        except TypeError:\n            # bug in some tkinter versions\n            self._place_kw = self._get_info(\"place\")\n        self._layout = 'place'"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pack(self, **kw):\n        ttk.Scrollbar.pack(self, **kw)\n        try:\n            self._pack_kw = self.pack_info()\n        except TypeError:\n            # bug in some tkinter versions\n            self._pack_kw = self._get_info(\"pack\")\n        self._layout = 'pack'", "response": "pack a widget in the parent widget."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npositions a widget in the parent widget in a grid.", "response": "def grid(self, **kw):\n        \"\"\"\n        Position a widget in the parent widget in a grid. \n        \n        :param column: use cell identified with given column (starting with 0)\n        :type column: int\n        :param columnspan: this widget will span several columns\n        :type columnspan: int\n        :param in\\_: widget to use as container\n        :type in\\_: widget\n        :param ipadx: add internal padding in x direction\n        :type ipadx: int\n        :param ipady: add internal padding in y direction\n        :type ipady: int\n        :param padx: add padding in x direction\n        :type padx: int\n        :param pady: add padding in y irection\n        :type pady: int\n        :param row: use cell identified with given row (starting with 0)\n        :type row: int\n        :param rowspan: this widget will span several rows\n        :type rowspan: int\n        :param sticky: \"n\", \"s\", \"e\", \"w\" or combinations: if cell is \n                       larger on which sides will this widget stick to \n                       the cell boundary\n        :type sticky: str\n        \"\"\"\n        ttk.Scrollbar.grid(self, **kw)\n        self._layout = 'grid'"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef toggle(self):\n        if self._open:\n            self._open = False\n            self.__checkbutton_var.set(False)\n            self.interior.grid_forget()\n            self._checkbutton.config(image=self._closed_image)\n        else:\n            self._open = True\n            self.__checkbutton_var.set(True)\n            self.interior.grid(row=1, column=0, sticky=\"nswe\")\n            self._checkbutton.config(image=self._open_image)", "response": "Toggle : obj : ToggledFrame. interior opened or closed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _on_select(self, *args):\n        if callable(self.__callback):\n            self.__callback(self.selection)", "response": "Function bound to event of selection in the Combobox calls callback if callable\n            is not None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef selection(self):\n        if self._font.get() is \"\" or self._font.get() not in self._fonts:\n            return None\n        else:\n            return self._font.get()", "response": "Return the current font name if one is selected."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef state(self, statespec=None):\n        if statespec:\n            if \"disabled\" in statespec:\n                self.bind('<Button-1>', lambda e: 'break')\n            elif \"!disabled\" in statespec:\n                self.unbind(\"<Button-1>\")\n                self.bind(\"<Button-1>\", self._box_click, True)\n            return ttk.Treeview.state(self, statespec)\n        else:\n            return ttk.Treeview.state(self)", "response": "Modify or inquire widget state."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchanging the state of the item.", "response": "def change_state(self, item, state):\n        \"\"\"\n        Replace the current state of the item.\n\n        i.e. replace the current state tag but keeps the other tags.\n        \n        :param item: item id\n        :type item: str\n        :param state: \"checked\", \"unchecked\" or \"tristate\": new state of the item \n        :type state: str\n        \"\"\"\n        tags = self.item(item, \"tags\")\n        states = (\"checked\", \"unchecked\", \"tristate\")\n        new_tags = [t for t in tags if t not in states]\n        new_tags.append(state)\n        self.item(item, tags=tuple(new_tags))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a tag to the tags of item.", "response": "def tag_add(self, item, tag):\n        \"\"\"\n        Add tag to the tags of item.\n        \n        :param item: item identifier\n        :type item: str\n        :param tag: tag name\n        :type tag: str\n        \"\"\"\n        tags = self.item(item, \"tags\")\n        self.item(item, tags=tags + (tag,))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves a tag from the tags of item.", "response": "def tag_del(self, item, tag):\n        \"\"\"\n        Remove tag from the tags of item.\n        \n        :param item: item identifier\n        :type item: str\n        :param tag: tag name\n        :type tag: str\n        \"\"\"\n        tags = list(self.item(item, \"tags\"))\n        if tag in tags:\n            tags.remove(tag)\n            self.item(item, tags=tuple(tags))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new item and returns the item identifier of the newly created item.", "response": "def insert(self, parent, index, iid=None, **kw):\n        \"\"\"\n        Creates a new item and return the item identifier of the newly created item.\n        \n        :param parent: identifier of the parent item\n        :type parent: str\n        :param index: where in the list of parent's children to insert the new item\n        :type index: int or \"end\"\n        :param iid: item identifier, iid must not already exist in the tree. If iid is None a new unique identifier is generated.\n        :type iid: None or str\n        :param kw: other options to be passed on to the :meth:`ttk.Treeview.insert` method\n        \n        :return: the item identifier of the newly created item\n        :rtype: str\n\n        .. note:: Same method as for the standard :class:`ttk.Treeview` but \n                  add the tag for the box state accordingly to the parent \n                  state if no tag among \n                  ('checked', 'unchecked', 'tristate') is given.\n        \"\"\"\n        if self.tag_has(\"checked\", parent):\n            tag = \"checked\"\n        else:\n            tag = 'unchecked'\n        if \"tags\" not in kw:\n            kw[\"tags\"] = (tag,)\n        elif not (\"unchecked\" in kw[\"tags\"] or \"checked\" in kw[\"tags\"] or\n                  \"tristate\" in kw[\"tags\"]):\n            kw[\"tags\"] += (tag,)\n\n        return ttk.Treeview.insert(self, parent, index, iid, **kw)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_checked(self):\n        checked = []\n\n        def get_checked_children(item):\n            if not self.tag_has(\"unchecked\", item):\n                ch = self.get_children(item)\n                if not ch and self.tag_has(\"checked\", item):\n                    checked.append(item)\n                else:\n                    for c in ch:\n                        get_checked_children(c)\n\n        ch = self.get_children(\"\")\n        for c in ch:\n            get_checked_children(c)\n        return checked", "response": "Return the list of checked items that do not have any child."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck the boxes of item s descendants.", "response": "def _check_descendant(self, item):\n        \"\"\"Check the boxes of item's descendants.\"\"\"\n        children = self.get_children(item)\n        for iid in children:\n            self.change_state(iid, \"checked\")\n            self._check_descendant(iid)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchanging the state of the box of item to tristate and change the state of the boxes of item s ancestors accordingly.", "response": "def _tristate_parent(self, item):\n        \"\"\"\n        Put the box of item in tristate and change the state of the boxes of\n        item's ancestors accordingly.\n        \"\"\"\n        self.change_state(item, \"tristate\")\n        parent = self.parent(item)\n        if parent:\n            self._tristate_parent(parent)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _box_click(self, event):\n        x, y, widget = event.x, event.y, event.widget\n        elem = widget.identify(\"element\", x, y)\n        if \"image\" in elem:\n            # a box was clicked\n            item = self.identify_row(y)\n            if self.tag_has(\"unchecked\", item) or self.tag_has(\"tristate\", item):\n                self._check_ancestor(item)\n                self._check_descendant(item)\n            else:\n                self._uncheck_descendant(item)\n                self._uncheck_ancestor(item)", "response": "Check or uncheck box when clicked."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the value of the option with the given name.", "response": "def cget(self, key):\n        \"\"\"\n        Query widget option.\n\n        :param key: option name\n        :type key: str\n        :return: value of the option\n\n        To get the list of options for this widget, call the method :meth:`~TickScale.keys`.\n        \"\"\"\n        if key == 'showvalue':\n            return self._showvalue\n        elif key == 'tickinterval':\n            return self._tickinterval\n        elif key == 'tickpos':\n            return self._tickpos\n        elif key == 'labelpos':\n            return self._labelpos\n        elif key == 'digits':\n            return self._digits\n        elif key == 'resolution':\n            return self._resolution\n        else:\n            return self.scale.cget(key)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconfiguring the resources of the current instance of this class.", "response": "def configure(self, cnf={}, **kw):\n        \"\"\"\n        Configure resources of the widget.\n\n        To get the list of options for this widget, call the method :meth:`~TickScale.keys`.\n        See :meth:`~TickScale.__init__` for a description of the widget specific option.\n        \"\"\"\n        kw.update(cnf)\n        reinit = False\n        if 'orient' in kw:\n            if kw['orient'] == 'vertical':\n                self._style_name = self._style_name.replace('Horizontal', 'Vertical')\n                if 'tickpos' not in kw:\n                    self._tickpos = 'w'\n            else:\n                self._style_name = self._style_name.replace('Vertical', 'Horizontal')\n                if 'tickpos' not in kw:\n                    self._tickpos = 's'\n            self.scale.configure(style=self._style_name)\n            reinit = True\n        if 'showvalue' in kw:\n            self._showvalue = bool(kw.pop('showvalue'))\n            reinit = True\n        if 'tickinterval' in kw:\n            self._tickinterval = kw.pop('tickinterval')\n            reinit = True\n        if 'tickpos' in kw:\n            tickpos = kw.pop('tickpos')\n            orient = kw.get('orient', str(self.cget('orient')))\n            if orient == 'vertical' and tickpos not in ['w', 'e']:\n                raise ValueError(\"For a vertical TickScale, 'tickpos' must be 'w' or 'e'.\")\n            elif orient == 'horizontal' and tickpos not in ['n', 's']:\n                raise ValueError(\"For a horizontal TickScale, 'tickpos' must be 'n' or 's'.\")\n            elif orient in ['vertical', 'horizontal']:\n                self._tickpos = tickpos\n                reinit = True\n        if 'labelpos' in kw:\n            labelpos = kw.pop('labelpos')\n            if labelpos not in ['w', 'e', 'n', 's']:\n                raise ValueError(\"'labelpos' must be 'n', 's', 'e' or 'w'.\")\n            else:\n                self._labelpos = labelpos\n                reinit = True\n        if 'resolution' in kw:\n            try:\n                self._resolution = float(kw.pop('resolution'))\n                if self._resolution < 0:\n                    raise ValueError(\"'resolution' must be non negative.\")\n            except ValueError:\n                raise TypeError(\"'resolution' must be a float.\")\n        if self._tickinterval != 0 and self._resolution > self._tickinterval:\n            self._tickinterval = self._resolution\n            reinit = True\n        if 'digits' in kw:\n            digits = kw.pop('digits')\n            if not isinstance(digits, int):\n                raise TypeError(\"'digits' must be an integer.\")\n            elif digits < 0:\n                self._digits = digits\n                self._formatter = '{:g}'\n                reinit = True\n            else:\n                self._digits = digits\n                self._formatter = '{:.' + str(self._digits) + 'f}'\n                interv = self._get_precision(self._tickinterval)\n                resol = self._get_precision(self._resolution)\n                start = kw.get('from', kw.get('from_', self._start))\n                end = kw.get('to', self.scale.cget('to'))\n                from_ = self._get_precision(start)\n                to = self._get_precision(end)\n                d = max(interv, resol, from_, to)\n                if self._digits < d:\n                    self._resolution = float('1e-{}'.format(self._digits))\n                    self._tickinterval = round(self._tickinterval, self._digits)\n                    if self._resolution > self._tickinterval:\n                        self._tickinterval = self._resolution\n                    kw['to'] = round(end, self._digits)\n                    if 'from_' in kw:\n                        del kw['from_']\n                    kw['from'] = round(start, self._digits)\n                reinit = True\n        elif self._digits > 0:\n            start = kw.get('from', kw.get('from_', self._start))\n            end = kw.get('to', self.scale.cget('to'))\n            from_ = self._get_precision(start)\n            to = self._get_precision(end)\n            interv = self._get_precision(self._tickinterval)\n            resol = self._get_precision(self._resolution)\n            digits = max(self._digits, interv, resol, from_, to)\n            if digits != self._digits:\n                self._digits = digits\n                self._formatter = '{:.' + str(self._digits) + 'f}'\n                reinit = True\n        if 'variable' in kw:\n            self._var = kw['variable']\n            if not self._var:\n                self._var = tk.DoubleVar(self, self.get())\n                kw['variable'] = self._var\n            try:\n                self._var.trace_add('write', self._increment)\n            except AttributeError:\n                # backward compatibility\n                self._var.trace('w', self._increment)\n\n        self.scale.configure(**kw)\n\n        if 'from_' in kw or 'from' in kw or 'to' in kw:\n            self._extent = self.scale.cget('to') - self.scale.cget('from')\n            self._start = self.scale.cget('from')\n            reinit = True\n        if 'style' in kw:\n            self._style_name = kw['style']\n            if not self._style_name:\n                self._style_name = '%s.TScale' % (str(self.scale.cget('orient')).capitalize())\n        if reinit:\n            self._init()\n        if 'orient' in kw:\n            # needed after the reinitialization in case of orientation change\n            self._apply_style()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert value in the scale s unit into a position in pixels.", "response": "def convert_to_pixels(self, value):\n        \"\"\"\n        Convert value in the scale's unit into a position in pixels.\n        \n        :param value: value to convert\n        :type value: float\n        \n        :return: the corresponding position in pixels\n        :rtype: float\n        \"\"\"\n        percent = ((value - self._start) / self._extent)\n        return percent * (self.get_scale_length() - self._sliderlength) + self._sliderlength / 2"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the length of the slider in the horizontal screen.", "response": "def _update_slider_length_horizontal(self):\n        \"\"\"\n        Measure the length of the slider and update the value of self._sliderlength.\n\n        self.scale.identify(x, y) is used to find the first and last pixels of\n        the slider. Indeed, self.scale.identify(x, y) returns the element\n        of the ttk.Scale to which the pixel (x, y) belongs. So, the length of\n        the slider is determined by scanning horizontally the pixels of the scale.\n        \"\"\"\n        if not self.scale.identify(2, 2):\n            # if self.scale.identify(2, 2) is an empty string it means that the scale\n            # is not displayed yet so we cannot measure the length of the slider,\n            # so wait for the scale to be properly displayed.\n            # binding to <Map> event does not work, it can still be to soon to\n            # get any result from identify\n            self.after(10, self._update_slider_length_horizontal)\n        else:\n            w = self.scale.winfo_width()\n            i = 0\n            # find the first pixel of the slider\n            while i < w and 'slider' not in self.scale.identify(i, 2):\n                # increment i until the pixel (i, 2) belongs to the slider\n                i += 1\n            j = i\n            # find the last pixel of the slider\n            while j < w and 'slider' in self.scale.identify(j, 2):\n                # increment j until the pixel (2, j) no longer belongs to the slider\n                j += 1\n            if j == i:\n                # the length of the slider was not determined properly,\n                # so the value of the sliderlength from the style is used\n                self._sliderlength = self.style.lookup(self._style_name, 'sliderlength', default=30)\n            else:\n                # update ticks and label placement\n                self._sliderlength = j - i\n            self._update_display()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _update_slider_length_vertical(self):\n        if not self.scale.identify(2, 2):\n            # if self.scale.identify(2, 2) is an empty string it means that the scale\n            # is not displayed yet so we cannot measure the length of the slider,\n            # so wait for the scale to be properly displayed.\n            # binding to <Map> event does not work, it can still be to soon to\n            # get any result from identify\n            self.after(10, self._update_slider_length_vertical)\n        else:\n            h = self.scale.winfo_height()\n            i = 0\n            # find the first pixel of the slider\n            while i < h and 'slider' not in self.scale.identify(2, i):\n                # increment i until the pixel (2, i) belongs to the slider\n                i += 1\n            j = i\n            # find the last pixel of the slider\n            while j < h and 'slider' in self.scale.identify(2, j):\n                # increment j until the pixel (2, j) no longer belongs to the slider\n                j += 1\n            if j == i:\n                # the length of the slider was not determined properly,\n                # so the value of the sliderlength from the style is used\n                self._sliderlength = self.style.lookup(self._style_name, 'sliderlength', default=30)\n            else:\n                self._sliderlength = j - i\n            # update ticks and label placement\n            self._update_display()", "response": "Update the length of the slider in the vertical screen."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies the scale style to the frame and labels.", "response": "def _apply_style(self):\n        \"\"\"Apply the scale style to the frame and labels.\"\"\"\n        ttk.Frame.configure(self, style=self._style_name + \".TFrame\")\n        self.label.configure(style=self._style_name + \".TLabel\")\n        bg = self.style.lookup('TFrame', 'background', default='light grey')\n        for label in self.ticklabels:\n            label.configure(style=self._style_name + \".TLabel\")\n        self.style.configure(self._style_name + \".TFrame\",\n                             background=self.style.lookup(self._style_name,\n                                                          'background',\n                                                          default=bg))\n        self.style.map(self._style_name + \".TFrame\",\n                       background=self.style.map(self._style_name, 'background'))\n        self.style.configure(self._style_name + \".TLabel\",\n                             font=self.style.lookup(self._style_name, 'font', default='TkDefaultFont'),\n                             background=self.style.lookup(self._style_name, 'background', default=bg),\n                             foreground=self.style.lookup(self._style_name, 'foreground', default='black'))\n        self.style.map(self._style_name + \".TLabel\",\n                       font=self.style.map(self._style_name, 'font'),\n                       background=self.style.map(self._style_name, 'background'),\n                       foreground=self.style.map(self._style_name, 'foreground'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates and grid the widgets.", "response": "def _init(self):\n        \"\"\"Create and grid the widgets.\"\"\"\n        for label in self.ticklabels:\n            label.destroy()\n        self.label.place_forget()\n        self.ticks = []\n        self.ticklabels = []\n        if self._resolution > 0:\n            nb_steps = round((self.scale.cget('to') - self.scale.cget('from')) / self._resolution)\n            self.scale.configure(to=self.scale.cget('from') + nb_steps * self._resolution)\n            self._extent = self.scale.cget('to') - self.scale.cget('from')\n        if str(self.scale.cget('orient')) == \"horizontal\":\n            self.get_scale_length = self.scale.winfo_width\n            self.display_value = self._display_value_horizontal\n            self._update_slider_length = self._update_slider_length_horizontal\n            self.place_ticks = self._place_ticks_horizontal\n            self._init_horizontal()\n        else:\n            self.get_scale_length = self.scale.winfo_height\n            self.display_value = self._display_value_vertical\n            self._update_slider_length = self._update_slider_length_vertical\n            self.place_ticks = self._place_ticks_vertical\n            self._init_vertical()\n        self.scale.lift()\n        try:\n            self._var.trace_remove('write', self._trace)\n            self._trace = self._var.trace_add('write', self._increment)\n        except AttributeError:\n            # backward compatibility\n            self._var.trace_vdelete('w', self._trace)\n            self._trace = self._var.trace('w', self._increment)\n        self._update_slider_length()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating and grid the widgets for a vertical orientation.", "response": "def _init_vertical(self):\n        \"\"\"Create and grid the widgets for a vertical orientation.\"\"\"\n        self.scale.grid(row=0, sticky='ns')\n        # showvalue\n        padx1, padx2 = 0, 0\n        pady1, pady2 = 0, 0\n        if self._showvalue:\n            self.label.configure(text=self._formatter.format(self._start))\n            if self._labelpos == 'w':\n                self.label.place(in_=self.scale, bordermode='outside',\n                                 relx=0, y=0, anchor='e')\n                self.update_idletasks()\n                padx1 = self.label.winfo_width()\n                self.label.configure(text=self._formatter.format(self._start + self._extent))\n                self.update_idletasks()\n                padx1 = max(self.label.winfo_width(), padx1)\n            elif self._labelpos == 'e':\n                self.label.place(in_=self.scale, bordermode='outside',\n                                 relx=1, y=1, anchor='w')\n                self.update_idletasks()\n                padx2 = self.label.winfo_width()\n                self.label.configure(text=self._formatter.format(self._start + self._extent))\n                self.update_idletasks()\n                padx2 = max(self.label.winfo_width(), padx2)\n            else:  # self._labelpos in ['n', 's']:\n                if self._labelpos == 'n':\n                    rely = 0\n                    anchor = 's'\n                    pady1 = self.label.winfo_reqheight()\n                else:\n                    rely = 1\n                    anchor = 'n'\n                    pady2 = self.label.winfo_reqheight()\n                self.label.place(in_=self.scale, bordermode='outside', relx=0.5,\n                                 rely=rely, anchor=anchor)\n                self.update_idletasks()\n                w = self.label.winfo_width()\n                self.label.configure(text=self._formatter.format(self._start + self._extent))\n                self.update_idletasks()\n                w = max(w, self.label.winfo_width())\n                ws = self.scale.winfo_reqwidth()\n                if w > ws:\n                    padx = (w - ws) // 2\n                    if self._tickinterval:\n                        if self._tickpos == 'e':\n                            padx1 = padx\n                        else:   # self._tickpos == 'w'\n                            padx2 = padx\n                    else:\n                        padx1, padx2 = padx, padx\n        # ticks\n        padx1_2, padx2_2 = 0, 0\n        if self._tickinterval:\n            nb_interv = int(self._extent / self._tickinterval)\n            if self._tickpos == 'w':\n                for i in range(nb_interv + 1):\n                    tick = self._start + i * self._tickinterval\n                    self.ticks.append(tick)\n                    self.ticklabels.append(ttk.Label(self,\n                                                     style=self._style_name + \".TLabel\",\n                                                     text=self._formatter.format(tick)))\n                    self.ticklabels[i].place(in_=self.scale, bordermode='outside',\n                                             x=-1 - padx1, y=0,\n                                             anchor='e')\n                    self.update_idletasks()\n                    padx1_2 = max(self.ticklabels[i].winfo_width(), padx1_2)\n            else:  # self._tickpos == 'e'\n                w = self.scale.winfo_reqwidth()\n                for i in range(nb_interv + 1):\n                    tick = self._start + i * self._tickinterval\n                    self.ticks.append(tick)\n                    self.ticklabels.append(ttk.Label(self,\n                                                     style=self._style_name + \".TLabel\",\n                                                     text=self._formatter.format(tick)))\n                    self.ticklabels[i].place(in_=self.scale, bordermode='outside',\n                                             x=w + 1 + padx2, y=0,\n                                             anchor='w')\n                    self.update_idletasks()\n                    padx2_2 = max(self.ticklabels[i].winfo_width(), padx2_2)\n        self.scale.grid_configure(padx=(padx1 + padx1_2 + 1, padx2 + padx2_2 + 1),\n                                  pady=(pady1, pady2))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _init_horizontal(self):\n        self.scale.grid(row=0, sticky='ew')\n        padx1, padx2 = 0, 0\n        pady1, pady2 = 0, 0\n        # showvalue\n        if self._showvalue:\n            self.label.configure(text=self._formatter.format(self._start))\n            self.update_idletasks()\n            if self._labelpos == 'n':\n                self.label.place(in_=self.scale, bordermode='outside',\n                                 rely=0, x=0, anchor='s')\n                pady1 = self.label.winfo_reqheight()\n            elif self._labelpos == 's':\n                self.label.place(in_=self.scale, bordermode='outside',\n                                 rely=1, x=0, anchor='n')\n                pady2 = self.label.winfo_reqheight()\n            elif self._labelpos in ['w', 'e']:\n                padx = self.label.winfo_reqwidth()\n                self.label.configure(text=self._formatter.format(self._start + self._extent))\n                self.update_idletasks()\n                padx = max(padx, self.label.winfo_reqwidth())\n                if self._labelpos == 'w':\n                    anchor = 'e'\n                    relx = 0\n                    padx1 = padx\n                else:\n                    anchor = 'w'\n                    relx = 1\n                    padx2 = padx\n                self.label.place(in_=self.scale, bordermode='outside',\n                                 relx=relx, rely=0.5, anchor=anchor)\n\n                h = self.label.winfo_reqheight()\n                hs = self.scale.winfo_reqheight()\n                if h > hs:\n                    pady = (h - hs) // 2\n                    if self._tickinterval:\n                        if self._tickpos == 'n':\n                            pady1 = pady\n                        else:   # self._tickpos == 's'\n                            pady2 = pady\n                    else:\n                        pady1, pady2 = pady, pady\n\n        # ticks\n        pady1_2, pady2_2 = 0, 0\n        if self._tickinterval:\n            nb_interv = int(self._extent / self._tickinterval)\n            h = self.scale.winfo_reqheight()\n            if self._tickpos == 's':\n                for i in range(nb_interv + 1):\n                    tick = self._start + i * self._tickinterval\n                    self.ticks.append(tick)\n                    self.ticklabels.append(ttk.Label(self,\n                                                     style=self._style_name + \".TLabel\",\n                                                     text=self._formatter.format(tick)))\n                    self.ticklabels[i].place(in_=self.scale, bordermode='outside',\n                                             x=0, y=h + pady2 + 1, anchor='n')\n                pady2_2 = self.ticklabels[-1].winfo_reqheight()\n            else:  # self._tickpos == 'n':\n                for i in range(nb_interv + 1):\n                    tick = self._start + i * self._tickinterval\n                    self.ticks.append(tick)\n                    self.ticklabels.append(ttk.Label(self,\n                                                     style=self._style_name + \".TLabel\",\n                                                     text=self._formatter.format(tick)))\n                    self.ticklabels[i].place(in_=self.scale, bordermode='outside',\n                                             x=0, y=-1 - pady1, anchor='s')\n                pady1_2 = self.ticklabels[-1].winfo_reqheight()\n            self.update_idletasks()\n        self.scale.grid_configure(pady=(pady1 + pady1_2, pady2 + pady2_2),\n                                  padx=(padx1, padx2))", "response": "Create and grid the widgets for a horizontal orientation."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisplays the current value and update the label position.", "response": "def _display_value_horizontal(self, value):\n        \"\"\"Display the current value and update the label position.\"\"\"\n        if self._showvalue:\n            self.label.configure(text=self._formatter.format(float(value)))\n            self.update_idletasks()\n            if self._labelpos in ['n', 's']:\n                # position (in pixel) of the center of the slider\n                x = self.convert_to_pixels(float(value))\n                # pay attention to the borders\n                half_width = self.label.winfo_width() / 2\n                if x + half_width > self.scale.winfo_width():\n                    x = self.scale.winfo_width() - half_width\n                elif x - half_width < 0:\n                    x = half_width\n                self.label.place_configure(x=x)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _display_value_vertical(self, value):\n        if self._showvalue:\n            self.label.configure(text=self._formatter.format(float(value)))\n            if self._labelpos in ['e', 'w']:\n                y = self.convert_to_pixels(float(value))\n                self.label.place_configure(y=y)", "response": "Display the current value and update the label position."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndisplaying the ticks for a horizontal scale.", "response": "def _place_ticks_horizontal(self):\n        \"\"\"Display the ticks for a horizontal scale.\"\"\"\n        # first tick\n        tick = self.ticks[0]\n        label = self.ticklabels[0]\n        x = self.convert_to_pixels(tick)\n        half_width = label.winfo_reqwidth() / 2\n        if x - half_width < 0:\n            x = half_width\n        label.place_configure(x=x)\n        # ticks in the middle\n        for tick, label in zip(self.ticks[1:-1], self.ticklabels[1:-1]):\n            x = self.convert_to_pixels(tick)\n            label.place_configure(x=x)\n        # last tick\n        tick = self.ticks[-1]\n        label = self.ticklabels[-1]\n        x = self.convert_to_pixels(tick)\n        half_width = label.winfo_reqwidth() / 2\n        if x + half_width > self.scale.winfo_reqwidth():\n            x = self.scale.winfo_width() - half_width\n        label.place_configure(x=x)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndisplays the ticks for a vertical slider.", "response": "def _place_ticks_vertical(self):\n        \"\"\"Display the ticks for a vertical slider.\"\"\"\n        for tick, label in zip(self.ticks, self.ticklabels):\n            y = self.convert_to_pixels(tick)\n            label.place_configure(y=y)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _increment(self, *args):\n        value = self._var.get()\n        if self._resolution:\n            value = self._start + int(round((value - self._start) / self._resolution)) * self._resolution\n            self._var.set(value)\n        self.display_value(value)", "response": "Move the slider only by increment given by resolution."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _update_display(self, event=None):\n        try:\n            if self._showvalue:\n                self.display_value(self.scale.get())\n            if self._tickinterval:\n                self.place_ticks()\n        except IndexError:\n            # happens when configure is called during a orientation change\n            # because self.ticks is empty\n            pass", "response": "Update the display value and ticks."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _draw(self, color):\n        width = self.winfo_width()\n        height = self.winfo_height()\n        self.delete(\"bg\")\n        self.delete(\"cross_h\")\n        self.delete(\"cross_v\")\n        del self.bg\n        self.bg = tk.PhotoImage(width=width, height=height, master=self)\n        self._fill()\n        self.create_image(0, 0, image=self.bg, anchor=\"nw\", tags=\"bg\")\n        self.tag_lower(\"bg\")\n        h, s, v = color\n        x = v / 100.\n        y = (1 - s / 100.)\n        self.create_line(0, y * height, width, y * height, tags=\"cross_h\",\n                         fill=\"#C2C2C2\")\n        self.create_line(x * width, 0, x * width, height, tags=\"cross_v\",\n                         fill=\"#C2C2C2\")", "response": "Draw the gradient and the selection cross on the canvas."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchanging hue. :param value: new hue value (between 0 and 360) :type value: int", "response": "def set_hue(self, value):\n        \"\"\"\n        Change hue.\n\n        :param value: new hue value (between 0 and 360)\n        :type value: int\n        \"\"\"\n        old = self._hue\n        self._hue = value\n        if value != old:\n            self._fill()\n            self.event_generate(\"<<ColorChanged>>\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _on_click(self, event):\n        x = event.x\n        y = event.y\n        self.coords('cross_h', 0, y, self.winfo_width(), y)\n        self.coords('cross_v', x, 0, x, self.winfo_height())\n        self.event_generate(\"<<ColorChanged>>\")", "response": "Move cross on click."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake the cross follow the cursor.", "response": "def _on_move(self, event):\n        \"\"\"Make the cross follow the cursor.\"\"\"\n        w = self.winfo_width()\n        h = self.winfo_height()\n        x = min(max(event.x, 0), w)\n        y = min(max(event.y, 0), h)\n        self.coords('cross_h', 0, y, w, y)\n        self.coords('cross_v', x, 0, x, h)\n        self.event_generate(\"<<ColorChanged>>\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets selected color. :return: color under cursor as a (RGB, HSV, HEX) tuple", "response": "def get(self):\n        \"\"\"\n        Get selected color.\n\n        :return: color under cursor as a (RGB, HSV, HEX) tuple\n        \"\"\"\n        x = self.coords('cross_v')[0]\n        y = self.coords('cross_h')[1]\n        xp = min(x, self.bg.width() - 1)\n        yp = min(y, self.bg.height() - 1)\n        try:\n            r, g, b = self.bg.get(round2(xp), round2(yp))\n        except ValueError:\n            r, g, b = self.bg.get(round2(xp), round2(yp)).split()\n            r, g, b = int(r), int(g), int(b)\n        hexa = rgb_to_hexa(r, g, b)\n        h = self.get_hue()\n        s = round2((1 - float(y) / self.winfo_height()) * 100)\n        v = round2(100 * float(x) / self.winfo_width())\n        return (r, g, b), (h, s, v), hexa"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_hsv(self, sel_color):\n        width = self.winfo_width()\n        height = self.winfo_height()\n        h, s, v = sel_color\n        self.set_hue(h)\n        x = v / 100.\n        y = (1 - s / 100.)\n        self.coords('cross_h', 0, y * height, width, y * height)\n        self.coords('cross_v', x * width, 0, x * width, height)", "response": "Set the color of the current color in HSV format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _determine_hpp_url(self, platform, action):\n        base_uri = settings.BASE_HPP_URL.format(platform)\n        service = action + '.shtml'\n        result = '/'.join([base_uri, service])\n        return result", "response": "This function returns the Adyen HPP endpoint based on the provided platform and action."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _determine_checkout_url(self, platform, action):\n        api_version = settings.API_CHECKOUT_VERSION\n        if platform == \"test\":\n            base_uri = settings.ENDPOINT_CHECKOUT_TEST\n        elif self.live_endpoint_prefix is not None and platform == \"live\":\n            base_uri = settings.ENDPOINT_CHECKOUT_LIVE_SUFFIX.format(\n                self.live_endpoint_prefix)\n        elif self.live_endpoint_prefix is None and platform == \"live\":\n            errorstring = \"\"\"Please set your live suffix. You can set it\n                   by running 'settings.\n                   ENDPOINT_CHECKOUT_LIVE_SUFFIX = 'Your live suffix'\"\"\"\n            raise AdyenEndpointInvalidFormat(errorstring)\n        if action == \"paymentsDetails\":\n            action = \"payments/details\"\n        if action == \"paymentsResult\":\n            action = \"payments/result\"\n        if action == \"originKeys\":\n            api_version = settings.API_CHECKOUT_UTILITY_VERSION\n\n        return '/'.join([base_uri, api_version, action])", "response": "This function returns the Adyen API endpoint based on the provided platform service and action."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef request(self, url,\n                json=\"\",\n                data=\"\",\n                username=\"\",\n                password=\"\",\n                headers=None,\n                timout=30):\n        \"\"\"This is overridden on module initialization. This function will make\n        an HTTP POST to a given url. Either json/data will be what is posted to\n        the end point. he HTTP request needs to be basicAuth when username and\n        password are provided. a headers dict maybe provided,\n        whatever the values are should be applied.\n\n        Args:\n            url (str):                  url to send the POST\n            json (dict, optional):      Dict of the JSON to POST\n            data (dict, optional):      Dict, presumed flat structure of\n                                        key/value of request to place as\n                                        www-form\n            username (str, optional):    Username for basic auth. Must be\n                                        uncluded as part of password.\n            password (str, optional):   Password for basic auth. Must be\n                                        included as part of username.\n            headers (dict, optional):   Key/Value pairs of headers to include\n        Returns:\n            str:    Raw request placed\n            str:    Raw response received\n            int:    HTTP status code, eg 200,404,401\n            dict:   Key/Value pairs of the headers received.\n            :param timout:\n        \"\"\"\n        raise NotImplementedError('request of HTTPClient should have been '\n                                  'overridden on initialization. '\n                                  'Otherwise, can be overridden to '\n                                  'supply your own post method')", "response": "This function is called by the HTTPClient class to make a HTTP POST request to the specified url."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_in(request, action):\n    if not request:\n        req_str = \"\"\n        for idx, val in enumerate(actions[action]):\n            req_str += \"\\n\" + val\n        erstr = \"Provide a request dict with the following properties:\" \\\n                \" %s\" % req_str\n        raise ValueError(erstr)\n\n    required_fields = actions[action]\n    missing = []\n    for field in required_fields:\n        if not is_key_present(request, field):\n            missing.append(field)\n    if missing:\n        missing_string = \"\"\n        for idx, val in enumerate(missing):\n            missing_string += \"\\n\" + val\n        erstr = \"Provide the required request parameters to\" \\\n                \" complete this request: %s\" % missing_string\n        raise ValueError(erstr)\n\n    return True", "response": "This function checks for missing properties in the request dict for the corresponding action."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nresolve CLI option type name", "response": "def resolve_type_name(ctx, param, value):  # pylint: disable=unused-argument\n    \"\"\"Resolve CLI option type name\"\"\"\n\n    def _resolve(value):\n        \"\"\"Resolve single type name\"\"\"\n        value = [\n            type_id\n            for type_id, type_name in W1ThermSensor.TYPE_NAMES.items()\n            if type_name == value\n        ][0]\n        return value\n\n    if not value:\n        return value\n\n    if isinstance(value, tuple):\n        return [_resolve(v) for v in value]\n    else:\n        return _resolve(value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist all available sensors", "response": "def ls(types, as_json):  # pylint: disable=invalid-name\n    \"\"\"List all available sensors\"\"\"\n    sensors = W1ThermSensor.get_available_sensors(types)\n\n    if as_json:\n        data = [\n            {\"id\": i, \"hwid\": s.id, \"type\": s.type_name}\n            for i, s in enumerate(sensors, 1)\n        ]\n        click.echo(json.dumps(data, indent=4, sort_keys=True))\n    else:\n        click.echo(\n            \"Found {0} sensors:\".format(click.style(str(len(sensors)), bold=True))\n        )\n        for i, sensor in enumerate(sensors, 1):\n            click.echo(\n                \"  {0}. HWID: {1} Type: {2}\".format(\n                    click.style(str(i), bold=True),\n                    click.style(sensor.id, bold=True),\n                    click.style(sensor.type_name, bold=True),\n                )\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets temperatures of all available sensors", "response": "def all(types, unit, precision, as_json):  # pylint: disable=redefined-builtin\n    \"\"\"Get temperatures of all available sensors\"\"\"\n    sensors = W1ThermSensor.get_available_sensors(types)\n    temperatures = []\n    for sensor in sensors:\n        if precision:\n            sensor.set_precision(precision, persist=False)\n\n        temperatures.append(sensor.get_temperature(unit))\n\n    if as_json:\n        data = [\n            {\"id\": i, \"hwid\": s.id, \"type\": s.type_name, \"temperature\": t, \"unit\": unit}\n            for i, s, t in zip(count(start=1), sensors, temperatures)\n        ]\n        click.echo(json.dumps(data, indent=4, sort_keys=True))\n    else:\n        click.echo(\n            \"Got temperatures of {0} sensors:\".format(\n                click.style(str(len(sensors)), bold=True)\n            )\n        )\n        for i, sensor, temperature in zip(count(start=1), sensors, temperatures):\n            click.echo(\n                \"  Sensor {0} ({1}) measured temperature: {2} {3}\".format(\n                    click.style(str(i), bold=True),\n                    click.style(sensor.id, bold=True),\n                    click.style(str(round(temperature, 2)), bold=True),\n                    click.style(unit, bold=True),\n                )\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(id_, hwid, type_, unit, precision, as_json):\n    if id_ and (hwid or type_):\n        raise click.BadOptionUsage(\n            \"If --id is given --hwid and --type are not allowed.\"\n        )\n\n    if id_:\n        try:\n            sensor = W1ThermSensor.get_available_sensors()[id_ - 1]\n        except IndexError:\n            raise click.BadOptionUsage(\n                \"No sensor with id {0} available. \"\n                \"Use the ls command to show all available sensors.\".format(id_)\n            )\n    else:\n        sensor = W1ThermSensor(type_, hwid)\n\n    if precision:\n        sensor.set_precision(precision, persist=False)\n\n    temperature = sensor.get_temperature(unit)\n\n    if as_json:\n        data = {\n            \"hwid\": sensor.id,\n            \"type\": sensor.type_name,\n            \"temperature\": temperature,\n            \"unit\": unit,\n        }\n        click.echo(json.dumps(data, indent=4, sort_keys=True))\n    else:\n        click.echo(\n            \"Sensor {0} measured temperature: {1} {2}\".format(\n                click.style(sensor.id, bold=True),\n                click.style(str(temperature), bold=True),\n                click.style(unit, bold=True),\n            )\n        )", "response": "Get temperature of a specific sensor"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef precision(precision, id_, hwid, type_):\n    if id_ and (hwid or type_):\n        raise click.BadOptionUsage(\n            \"If --id is given --hwid and --type are not allowed.\"\n        )\n\n    if id_:\n        try:\n            sensor = W1ThermSensor.get_available_sensors()[id_ - 1]\n        except IndexError:\n            raise click.BadOptionUsage(\n                \"No sensor with id {0} available. \"\n                \"Use the ls command to show all available sensors.\".format(id_)\n            )\n    else:\n        sensor = W1ThermSensor(type_, hwid)\n\n    sensor.set_precision(precision, persist=True)", "response": "Change the precision for the sensor and persist it in the EEPROM."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_kernel_modules():\n    if not os.path.isdir(W1ThermSensor.BASE_DIRECTORY):\n        os.system(\"modprobe w1-gpio >/dev/null 2>&1\")\n        os.system(\"modprobe w1-therm >/dev/null 2>&1\")\n\n    for _ in range(W1ThermSensor.RETRY_ATTEMPTS):\n        if os.path.isdir(\n            W1ThermSensor.BASE_DIRECTORY\n        ):  # w1 therm modules loaded correctly\n            break\n        time.sleep(W1ThermSensor.RETRY_DELAY_SECONDS)\n    else:\n        raise KernelModuleLoadError()", "response": "Load kernel modules needed by the temperature sensor\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of all available sensors.", "response": "def get_available_sensors(cls, types=None):\n        \"\"\"\n            Return all available sensors.\n\n            :param list types: the type of the sensor to look for.\n                               If types is None it will search for all available types.\n\n            :returns: a list of sensor instances.\n            :rtype: list\n\n        \"\"\"\n        if not types:\n            types = cls.TYPE_NAMES.keys()\n        is_sensor = lambda s: any(s.startswith(hex(x)[2:]) for x in types)  # noqa\n        return [\n            cls(cls.RESOLVE_TYPE_STR[s[:2]], s[3:])\n            for s in os.listdir(cls.BASE_DIRECTORY)\n            if is_sensor(s)\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the raw sensor strings from the kernel module sysfs interface and returns the raw strings containing all bytes from the sensor memory", "response": "def raw_sensor_strings(self):\n        \"\"\"\n            Reads the raw strings from the kernel module sysfs interface\n\n            :returns: raw strings containing all bytes from the sensor memory\n            :rtype: str\n\n            :raises NoSensorFoundError: if the sensor could not be found\n            :raises SensorNotReadyError: if the sensor is not ready yet\n        \"\"\"\n        try:\n            with open(self.sensorpath, \"r\") as f:\n                data = f.readlines()\n        except IOError:\n            raise NoSensorFoundError(self.type_name, self.id)\n\n        if data[0].strip()[-3:] != \"YES\":\n            raise SensorNotReadyError(self)\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the raw integer ADC count from the sensor ADC", "response": "def raw_sensor_count(self):\n        \"\"\"\n            Returns the raw integer ADC count from the sensor\n\n            Note: Must be divided depending on the max. sensor resolution\n            to get floating point celsius\n\n            :returns: the raw value from the sensor ADC\n            :rtype: int\n\n            :raises NoSensorFoundError: if the sensor could not be found\n            :raises SensorNotReadyError: if the sensor is not ready yet\n        \"\"\"\n\n        # two complement bytes, MSB comes after LSB!\n        bytes = self.raw_sensor_strings[1].split()\n\n        # Convert from 16 bit hex string into int\n        int16 = int(bytes[1] + bytes[0], 16)\n\n        # check first signing bit\n        if int16 >> 15 == 0:\n            return int16  # positive values need no processing\n        else:\n            return int16 - (1 << 16)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the unit factor depending on the unit constant", "response": "def _get_unit_factor(cls, unit):\n        \"\"\"\n            Returns the unit factor depending on the unit constant\n\n            :param int unit: the unit of the factor requested\n\n            :returns: a function to convert the raw sensor value to the given unit\n            :rtype: lambda function\n\n            :raises UnsupportedUnitError: if the unit is not supported\n        \"\"\"\n        try:\n            if isinstance(unit, str):\n                unit = cls.UNIT_FACTOR_NAMES[unit]\n            return cls.UNIT_FACTORS[unit]\n        except KeyError:\n            raise UnsupportedUnitError()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the temperature in the specified unit", "response": "def get_temperature(self, unit=DEGREES_C):\n        \"\"\"\n            Returns the temperature in the specified unit\n\n            :param int unit: the unit of the temperature requested\n\n            :returns: the temperature in the given unit\n            :rtype: float\n\n            :raises UnsupportedUnitError: if the unit is not supported\n            :raises NoSensorFoundError: if the sensor could not be found\n            :raises SensorNotReadyError: if the sensor is not ready yet\n            :raises ResetValueError: if the sensor has still the initial value and no measurment\n        \"\"\"\n        if self.type in self.TYPES_12BIT_STANDARD:\n            value = self.raw_sensor_count\n            # the int part is 8 bit wide, 4 bit are left on 12 bit\n            # so divide with 2^4 = 16 to get the celsius fractions\n            value /= 16.0\n\n            # check if the sensor value is the reset value\n            if value == 85.0:\n                raise ResetValueError(self)\n\n            factor = self._get_unit_factor(unit)\n            return factor(value)\n\n        # Fallback to precalculated value for other sensor types\n        factor = self._get_unit_factor(unit)\n        return factor(self.raw_sensor_temp * 0.001)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the temperatures in the given units", "response": "def get_temperatures(self, units):\n        \"\"\"\n            Returns the temperatures in the specified units\n\n            :param list units: the units for the sensor temperature\n\n            :returns: the sensor temperature in the given units. The order of\n            the temperatures matches the order of the given units.\n            :rtype: list\n\n            :raises UnsupportedUnitError: if the unit is not supported\n            :raises NoSensorFoundError: if the sensor could not be found\n            :raises SensorNotReadyError: if the sensor is not ready yet\n        \"\"\"\n        sensor_value = self.get_temperature(self.DEGREES_C)\n        return [self._get_unit_factor(unit)(sensor_value) for unit in units]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_precision(self):\n        config_str = self.raw_sensor_strings[1].split()[4]  # Byte 5 is the config register\n        bit_base = int(config_str, 16) >> 5  # Bit 5-6 contains the resolution, cut off the rest\n        return bit_base + 9", "response": "Get the current precision from the sensor."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the precision of the sensor for the next readings. If the ``persist`` argument is set to ``False`` this value is \"only\" stored in the volatile SRAM, so it is reset when the sensor gets power-cycled. If the ``persist`` argument is set to ``True`` the current set precision is stored into the EEPROM. Since the EEPROM has a limited amount of writes (>50k), this command should be used wisely. Note: root permissions are required to change the sensors precision. Note: This function is supported since kernel 4.7. :param int precision: the sensor precision in bits. Valid values are between 9 and 12 :param bool persist: if the sensor precision should be written to the EEPROM. :returns: if the sensor precision could be set or not. :rtype: bool", "response": "def set_precision(self, precision, persist=False):\n        \"\"\"\n            Set the precision of the sensor for the next readings.\n\n            If the ``persist`` argument is set to ``False`` this value\n            is \"only\" stored in the volatile SRAM, so it is reset when\n            the sensor gets power-cycled.\n\n            If the ``persist`` argument is set to ``True`` the current set\n            precision is stored into the EEPROM. Since the EEPROM has a limited\n            amount of writes (>50k), this command should be used wisely.\n\n            Note: root permissions are required to change the sensors precision.\n\n            Note: This function is supported since kernel 4.7.\n\n            :param int precision: the sensor precision in bits.\n                                  Valid values are between 9 and 12\n            :param bool persist: if the sensor precision should be written\n                                 to the EEPROM.\n\n            :returns: if the sensor precision could be set or not.\n            :rtype: bool\n        \"\"\"\n        if not 9 <= precision <= 12:\n            raise ValueError(\n                \"The given sensor precision '{0}' is out of range (9-12)\".format(\n                    precision\n                )\n            )\n\n        exitcode = subprocess.call(\n            \"echo {0} > {1}\".format(precision, self.sensorpath), shell=True\n        )\n        if exitcode != 0:\n            raise W1ThermSensorError(\n                \"Failed to change resolution to {0} bit. \"\n                \"You might have to be root to change the precision\".format(precision)\n            )\n\n        if persist:\n            exitcode = subprocess.call(\n                \"echo 0 > {0}\".format(self.sensorpath), shell=True\n            )\n            if exitcode != 0:\n                raise W1ThermSensorError(\n                    \"Failed to write precision configuration to sensor EEPROM\"\n                )\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nround to n significant figs", "response": "def round_to_n(x, n):\n    \"\"\"\n    Round to sig figs\n    \"\"\"\n    return round(x, -int(np.floor(np.log10(x))) + (n - 1))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nshares axes limits without shared locators ticks and ticks", "response": "def sharey(axes):\n    \"\"\"\n    Shared axes limits without shared locators, ticks, etc.\n\n    By Joe Kington\n    \"\"\"\n    linker = Linker(axes)\n    for ax in axes:\n        ax._linker = linker"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fix_ticks(ax):\n    plt.setp(ax.get_yticklabels(), ha='center', x=0.5,\n             transform=ax._yaxis_transform)\n\n    # We'll still wind up with some tick labels beyond axes limits for reasons\n    # I don't fully understand...\n    limits = ax.get_ylim()\n    for label, loc in zip(ax.yaxis.get_ticklabels(), ax.yaxis.get_ticklocs()):\n        if loc < min(limits) or loc > max(limits):\n            label.set(visible=False)\n        else:\n            label.set(visible=True)", "response": "Fixes the ticklabels of the base class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef flatten_list(l):\n    if (l == []) or (l is None):\n        return l\n    if isinstance(l[0], list):\n        return flatten_list(l[0]) + flatten_list(l[1:])\n    return l[:1] + flatten_list(l[1:])", "response": "Unpacks lists in a list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconcatenating anything into a list.", "response": "def list_and_add(a, b):\n    \"\"\"\n    Concatenate anything into a list.\n\n    Args:\n        a: the first thing\n        b: the second thing\n\n    Returns:\n        list. All the things in a list.\n    \"\"\"\n    if not isinstance(b, list):\n        b = [b]\n    if not isinstance(a, list):\n        a = [a]\n    return a + b"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngrabs renames and transforms stuff from a lasio object.", "response": "def lasio_get(l,\n              section,\n              item,\n              attrib='value',\n              default=None,\n              remap=None,\n              funcs=None):\n    \"\"\"\n    Grabs, renames and transforms stuff from a lasio object.\n\n    Args:\n        l (lasio): a lasio instance.\n        section (str): The LAS section to grab from, eg ``well``\n        item (str): The item in the LAS section to grab from, eg ``name``\n        attrib (str): The attribute of the item to grab, eg ``value``\n        default (str): What to return instead.\n        remap (dict): Optional. A dict of 'old': 'new' LAS field names.\n        funcs (dict): Optional. A dict of 'las field': function() for\n            implementing a transform before loading. Can be a lambda.\n\n    Returns:\n        The transformed item.\n    \"\"\"\n    remap = remap or {}\n    item_to_fetch = remap.get(item, item)\n    if item_to_fetch is None:\n        return None\n\n    try:\n        obj = getattr(l, section)\n        result = getattr(obj, item_to_fetch)[attrib]\n    except:\n        return default\n\n    if funcs is not None:\n        f = funcs.get(item, null)\n        result = f(result)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninterpolates a parabolic variable.", "response": "def parabolic(f, x):\n    \"\"\"\n    Interpolation. From ageobot, from somewhere else.\n    \"\"\"\n    xv = 1/2. * (f[x-1] - f[x+1]) / (f[x-1] - 2 * f[x] + f[x+1]) + x\n    yv = f[x] - 1/4. * (f[x-1] - f[x+1]) * (xv - x)\n    return (xv, yv)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_nearest(a, value, index=False):\n    i = np.abs(a - value).argmin()\n    if index:\n        return i\n    else:\n        return a[i]", "response": "Find the array value or index closest to some given value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the nearest array value or index of the array value before some given value. Optionally return the fractional distance from that previous value.", "response": "def find_previous(a, value, index=False, return_distance=False):\n    \"\"\"\n    Find the nearest array value, or index of the array value, before some\n    given value. Optionally also return the fractional distance of the given\n    value from that previous value.\n\n    Args:\n        a (ndarray)\n        value (float)\n        index (bool): whether to return the index instead of the array value.\n            Default: False.\n        return_distance(bool): whether to return the fractional distance from\n            the nearest value to the specified value. Default: False.\n\n    Returns:\n        float. The array value (or index, as int) before the specified value.\n            If ``return_distance==True`` then a tuple is returned, where the\n            second value is the distance.\n    \"\"\"\n    b = a - value\n    i = np.where(b > 0)[0][0]\n    d = (value - a[i-1]) / (a[i] - a[i-1])\n    if index:\n        if return_distance:\n            return i - 1, d\n        else:\n            return i - 1\n    else:\n        if return_distance:\n            return a[i - 1], d\n        else:\n            return a[i - 1]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_edges(a):\n    edges = a[1:] == a[:-1]\n    tops = np.where(~edges)[0] + 1\n    tops = np.append(0, tops)\n    values = a[tops]\n\n    return tops, values", "response": "Find the edges of the last node in the tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef normalize(a, new_min=0.0, new_max=1.0):\n\n    n = (a - np.amin(a)) / np.amax(a - np.amin(a))\n    return n * (new_max - new_min) + new_min", "response": "From bruges normalize an array to [ 0 1 ) or to arbitrary new min and max."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef moving_avg_conv(a, length):\n    boxcar = np.ones(length)/length\n    return np.convolve(a, boxcar, mode=\"same\")", "response": "Moving average via convolution. Seems slower than naive."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extrapolate(a):\n    nans = np.where(~np.isnan(a))[0]\n    first, last = nans[0], nans[-1]\n    a[:first] = a[first]\n    a[last + 1:] = a[last]\n    return a", "response": "Extrapolate up and down an array from the first and last non - NaN samples."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove NaNs from the top and tail of a well log.", "response": "def top_and_tail(a):\n    \"\"\"\n    Remove the NaNs from the top and tail (only) of a well log.\n\n    Args:\n        a (ndarray): An array.\n    Returns:\n        ndarray: The top and tailed array.\n    \"\"\"\n    if np.all(np.isnan(a)):\n        return np.array([])\n    nans = np.where(~np.isnan(a))[0]\n    last = None if nans[-1]+1 == a.size else nans[-1]+1\n    return a[nans[0]:last]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dd2dms(dd):\n    m, s = divmod(dd * 3600, 60)\n    d, m = divmod(m, 60)\n    return int(d), int(m), s", "response": "Converts decimal degrees to DMS."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ricker(f, length, dt):\n    t = np.linspace(-int(length/2), int((length-dt)/2), int(length/dt))\n    y = (1. - 2.*(np.pi**2)*(f**2)*(t**2))*np.exp(-(np.pi**2)*(f**2)*(t**2))\n    return t, y", "response": "A function to get the time basis and amplitude values for a ricker wavelet."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hex_to_rgb(hexx):\n    h = hexx.strip('#')\n    l = len(h)\n\n    return tuple(int(h[i:i+l//3], 16) for i in range(0, l, l//3))", "response": "Utility function to convert a hex colour to rgb triples."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hex_is_dark(hexx, percent=50):\n    r, g, b = hex_to_rgb(hexx)\n    luma = (0.2126 * r + 0.7152 * g + 0.0722 * b) / 2.55  # per ITU-R BT.709\n\n    return (luma < percent)", "response": "Function to decide if a hex colour s brightness is dark."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef text_colour_for_hex(hexx, percent=50, dark='#000000', light='#ffffff'):\n    return light if hex_is_dark(hexx, percent=percent) else dark", "response": "Function to decide what colour to use for a given hex colour."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_lines(handle, line):\n    for i, l in enumerate(handle):\n        if i == line:\n            return l", "response": "Get zero - indexed line from an open file - like."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_lasio(cls, l, remap=None, funcs=None):\n        params = {}\n        funcs = funcs or {}\n        funcs['location'] = str\n        for field, (sect, code) in las_fields['location'].items():\n            params[field] = utils.lasio_get(l,\n                                            sect,\n                                            code,\n                                            remap=remap,\n                                            funcs=funcs)\n        return cls(params)", "response": "Make a Location object from a lasio object. Assumes we re starting a location object with a lasio object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a deviation survey to this instance and try to compute a position log from it.", "response": "def add_deviation(self, dev, td=None):\n        \"\"\"\n        Add a deviation survey to this instance, and try to compute a position\n        log from it.\n        \"\"\"\n        self.deviation = dev\n        try:\n            self.compute_position_log(td=td)\n        except:\n            self.position = None\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a function that converts the current object s MD to TVD.", "response": "def md2tvd(self, kind='linear'):\n        \"\"\"\n        Provides an transformation and interpolation function that converts\n        MD to TVD.\n\n        Args:\n            kind (str): The kind of interpolation to do, e.g. 'linear',\n                'cubic', 'nearest'.\n\n        Returns:\n            function.\n        \"\"\"\n        if self.position is None:\n            return lambda x: x\n        return interp1d(self.md, self.tvd,\n                        kind=kind,\n                        assume_sorted=True,\n                        fill_value=\"extrapolate\",\n                        bounds_error=False)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the position of the current object in the log file.", "response": "def compute_position_log(self,\n                             td=None,\n                             method='mc',\n                             update_deviation=True):\n        \"\"\"\n        Args:\n            deviation (ndarray): A deviation survey with rows like MD, INC, AZI\n            td (Number): The TD of the well, if not the end of the deviation\n                survey you're passing.\n            method (str):\n                'aa': average angle\n                'bt': balanced tangential\n                'mc': minimum curvature\n            update_deviation: This function makes some adjustments to the dev-\n                iation survey, to account for the surface and TD. If you do not\n                want to change the stored deviation survey, set to False.\n\n        Returns:\n            ndarray. A position log with rows like X-offset, Y-offset, Z-offset\n        \"\"\"\n        deviation = np.copy(self.deviation)\n\n        # Adjust to TD.\n        if td is not None:\n            last_row = np.copy(deviation[-1, :])\n            last_row[0] = td\n            deviation = np.vstack([deviation, last_row])\n\n        # Adjust to surface if necessary.\n        if deviation[0, 0] > 0:\n            deviation = np.vstack([np.array([0, 0, 0]), deviation])\n\n        last = deviation[:-1]\n        this = deviation[1:]\n\n        diff = this[:, 0] - last[:, 0]\n\n        Ia, Aa = np.radians(last[:, 1]), np.radians(last[:, 2])\n        Ib, Ab = np.radians(this[:, 1]), np.radians(this[:, 2])\n\n        if method == 'aa':\n            Iavg = (Ia + Ib) / 2\n            Aavg = (Aa + Ab) / 2\n            delta_N = diff * np.sin(Iavg) * np.cos(Aavg)\n            delta_E = diff * np.sin(Iavg) * np.sin(Aavg)\n            delta_V = diff * np.cos(Iavg)\n        elif method in ('bt', 'mc'):\n            delta_N = 0.5 * diff * np.sin(Ia) * np.cos(Aa)\n            delta_N += 0.5 * diff * np.sin(Ib) * np.cos(Ab)\n            delta_E = 0.5 * diff * np.sin(Ia) * np.sin(Aa)\n            delta_E += 0.5 * diff * np.sin(Ib) * np.sin(Ab)\n            delta_V = 0.5 * diff * np.cos(Ia)\n            delta_V += 0.5 * diff * np.cos(Ib)\n        else:\n            raise Exception(\"Method must be one of 'aa', 'bt', 'mc'\")\n\n        if method == 'mc':\n            _x = np.sin(Ib) * (1 - np.cos(Ab - Aa))\n            dogleg = np.arccos(np.cos(Ib - Ia) - np.sin(Ia) * _x)\n            dogleg[dogleg == 0] = 1e-9\n            rf = 2 / dogleg * np.tan(dogleg / 2)  # ratio factor\n            rf[np.isnan(rf)] = 1  # Adjust for NaN.\n            delta_N *= rf\n            delta_E *= rf\n            delta_V *= rf\n\n        # Prepare the output array.\n        result = np.zeros_like(deviation, dtype=np.float)\n\n        # Stack the results, add the surface.\n        _offsets = np.squeeze(np.dstack([delta_N, delta_E, delta_V]))\n        _offsets = np.vstack([np.array([0, 0, 0]), _offsets])\n        result += _offsets.cumsum(axis=0)\n\n        if update_deviation:\n            self.deviation = deviation\n\n        self.position = result\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a Jupyter Notebook magic repr function.", "response": "def _repr_html_(self):\n        \"\"\"\n        Jupyter Notebook magic repr function.\n        \"\"\"\n        # Make header.\n        r = '</th><th>'.join(['Index', 'UWI', 'Data', 'Curves'])\n        rows = '<tr><th>{}</th></tr>'.format(r)\n\n        # Make rows.\n        for i, w in enumerate(self.__list):\n            rows += '<tr><td>{}</td>'.format(i)\n            rows += '<td><strong>{}</strong></td>'.format(w.uwi)\n            rows += '<td>{}&nbsp;curves</td>'.format(len(w.data))\n            rows += '<td>{}</td></tr>'.format(', '.join(w.data.keys()))\n        html = '<table>{}</table>'.format(rows)\n\n        return html"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_canstrat_striplogs(self,\n                               path, uwi_transform=None, name='canstrat'):\n        \"\"\"\n        This may be too specific a method... just move it to the workflow.\n\n        Requires striplog.\n        \"\"\"\n        from striplog import Striplog\n\n        uwi_transform = uwi_transform or utils.null\n\n        for w in self.__list:\n            try:\n                dat_file = utils.find_file(str(uwi_transform(w.uwi)), path)\n            except:\n                print(\"- Skipping {}: something went wrong\".format(w.uwi))\n                continue\n\n            if dat_file is None:\n                print(\"- Omitting {}: no data\".format(w.uwi))\n                continue\n\n            # If we got here, we're using it.\n            print(\"+ Adding {} from {}\".format(w.uwi, dat_file))\n\n            w.data[name] = Striplog.from_canstrat(dat_file)\n\n        return", "response": "Add a striplog file to the list of related canstrats."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of all the mnemonics in turn and returns the highest thing in turn in the alias table.", "response": "def get_mnemonics(self, mnemonics, uwis=None, alias=None):\n        \"\"\"\n        Looks at all the wells in turn and returns the highest thing\n        in the alias table.\n\n        Args:\n            mnemonics (list)\n            alias (dict)\n\n        Returns:\n            list. A list of lists.\n        \"\"\"\n        # Let's not do the nested comprehension...\n        uwis = uwis or self.uwis\n        wells = [w for w in self.__list if w.uwi in uwis]\n        all_wells = []\n        for w in wells:\n            this_well = [w.get_mnemonic(m, alias=alias) for m in mnemonics]\n            all_wells.append(this_well)\n        return all_wells"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef count_mnemonic(self, mnemonic, uwis=uwis, alias=None):\n        all_mnemonics = self.get_mnemonics([mnemonic], uwis=uwis, alias=alias)\n        return len(list(filter(None, utils.flatten_list(all_mnemonics))))", "response": "Count the number of wells that have a given mnemonic given the mnemonic and an alias dict."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef curve_table_html(self,\n                         uwis=None,\n                         keys=None,\n                         alias=None,\n                         tests=None,\n                         exclude=None,\n                         limit=0):\n        \"\"\"\n        Another version of the curve table.\n\n        Args:\n            uwis (list): Only these UWIs. List of ``str``.\n            keys (list): Only these names. List of ``str``.\n            alias (dict): Alias table, maps names to mnemomnics in order of\n                preference.\n            tests (dict): Test table, maps names to lists of functions.\n            exclude (list): Except these names. List of ``str``. Ignored if\n                you pass ``keys``.\n            limit (int): Curve must be present in at least this many wells.\n\n        Returns:\n            str. HTML representation of the table.\n        \"\"\"\n        uwis = uwis or self.uwis\n        wells = [w for w in self.__list if w.uwi in uwis]\n\n        # This is hacky. See remark in well.get_mnemonics_from_regex().\n        if exclude is not None:\n            exclude = utils.flatten_list([w.get_mnemonics_from_regex(e) for e in exclude for w in wells])\n            if alias is not None:\n                exclude = [alias.get(e, e) for e in exclude]\n        else:\n            exclude = []\n\n        counter = self.__all_curve_names(uwis=uwis, count=True)\n        all_keys = [i[0] for i in counter\n                    if (i[0] not in exclude) and (i[1] >= limit)]\n        keys = utils.flatten_list(keys) or all_keys\n\n        tests = tests or {}\n        alias = alias or self.alias\n\n        # Make header.\n        keys_ = [k+'*' if k in alias else k for k in keys]\n        r = '</th><th>'.join(['Idx', 'UWI', 'Data', 'Passing'] + keys_)\n        rows = '<tr><th>{}</th></tr>'.format(r)\n\n        # Make summary row.\n        well_counts = [self.count_mnemonic(m, uwis=uwis, alias=alias)\n                       for m in keys]\n        w = len(wells)\n        well_count_strs = ['{}/{}&nbsp;wells'.format(c, w) for c in well_counts]\n        r = '</td><td>'.join(['', '', '', '%'] + well_count_strs)\n        rows += '<tr><td>{}</td></tr>'.format(r)\n\n        q_colours = {\n            0: '#FF3333',\n            1: '#33EE33',\n            -1: '#AACCAA'\n            # default: '#FFFFCC'  # Done with get when we use this dict.\n        }\n\n        # Make rows.\n        for i, w in enumerate(wells):\n\n            this_well = [w.get_curve(m, alias=alias)\n                         for m in keys]\n\n            q_well = w.qc_data(tests, alias)\n\n            curves = []\n            q_total, q_count = 0, 0\n\n            for c in this_well:\n                q = -1\n                num_tests, num_passes = 0, 0\n                if c is None:\n                    curves.append(('#CCCCCC', '', '', '#CCCCCC', '', ''))\n                else:\n                    if tests:\n                        q_this = q_well.get(c.mnemonic)\n                        if q_this:\n                            results = q_this.values()\n                            if results:\n                                num_tests = len(results)\n                                num_passes = sum(results)\n                                q = num_passes / num_tests\n                    q_colour = q_colours.get(q, '#FFCC33')\n                    c_mean = '{:.2f}'.format(float(np.nanmean(c))) if np.any(c[~np.isnan(c)]) else np.nan\n                    curves.append(('#CCEECC', c.mnemonic, f\"{num_passes}/{num_tests}\", q_colour, c_mean, c.units))\n                q_total += num_passes\n                q_count += num_tests\n\n            # Make general columns.\n            count = w.count_curves(keys, alias)\n            if count == 0:\n                score = '\u2013'\n            else:\n                score = '{:.0f}'.format(100*(q_total/q_count)) if (q_total >= 0) and (q_count > 0) else '\u2013'\n            s = '<td>{}</td><td><span style=\"font-weight:bold;\">{}</span></td><td>{}/{}&nbsp;curves</td><td>{}</td>'\n            rows += s.format(i, w.uwi, count, len(w.data), score)\n\n            # Make curve data columns.\n            for curve in curves:\n                s = '<td style=\"background-color:{}; line-height:80%; padding:5px 4px 2px 4px;\">{}'\n                t = '<div title=\"{{}}\" style=\"font-size:80%; float:right; cursor: default; padding:4px 0px 4px 6px; color:{{}};\">{0}</div>'\n                if tests:\n                    t = t.format('&#x2b24;')\n                else:\n                    t = t.format('')\n                s += t\n                s += '<br /><span style=\"font-size:70%; color:#33AA33\">{} {}</span></td>'\n                rows += s.format(*curve)\n            rows += '</tr>'\n\n        # Make the table and get out of here.\n        html = '<table>{}</table>'.format(rows)\n        return html", "response": "A method that returns an HTML representation of the curve table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot_kdes(self, mnemonic, alias=None, uwi_regex=None, return_fig=False):\n        wells = self.find_wells_with_curve(mnemonic, alias=alias)\n        fig, axs = plt.subplots(len(self), 1, figsize=(10, 1.5*len(self)))\n\n        curves = [w.get_curve(mnemonic, alias=alias) for w in wells]\n        all_data = np.hstack(curves)\n        all_data = all_data[~np.isnan(all_data)]\n\n        # Find values for common axis to exclude outliers.\n        amax = np.percentile(all_data, 99)\n        amin = np.percentile(all_data,  1)\n\n        for i, w in enumerate(self):\n            c = w.get_curve(mnemonic, alias=alias)\n\n            if uwi_regex is not None:\n                label = re.sub(uwi_regex, r'\\1', w.uwi)\n            else:\n                label = w.uwi\n\n            if c is not None:\n                axs[i] = c.plot_kde(ax=axs[i], amax=amax, amin=amin, label=label+'-'+str(c.mnemonic))\n            else:\n                continue\n\n        if return_fig:\n            return fig\n        else:\n            return", "response": "Plots all KDEs for all curves with the given mnemonic."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new Project with only the wells which have the named curve.", "response": "def find_wells_with_curve(self, mnemonic, alias=None):\n        \"\"\"\n        Returns a new Project with only the wells which have the named curve.\n\n        Args:\n            menmonic (str): the name of the curve to look for.\n            alias (dict): a welly alias dictionary.\n        \n        Returns:\n            project.\n        \"\"\"\n        return Project([w for w in self if w.get_curve(mnemonic, alias=alias) is not None])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new Project with only the wells which DO NOT have the named curve.", "response": "def find_wells_without_curve(self, mnemonic, alias=None):\n        \"\"\"\n        Returns a new Project with only the wells which DO NOT have the named curve.\n\n        Args:\n            menmonic (str): the name of the curve to look for.\n            alias (dict): a welly alias dictionary.\n        \n        Returns:\n            project.\n        \"\"\"\n        return Project([w for w in self if w.get_curve(mnemonic, alias=alias) is None])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new Project with only the wells named by UWI.", "response": "def get_wells(self, uwis=None):\n        \"\"\"\n        Returns a new Project with only the wells named by UWI.\n\n        Args:\n            uwis (list): list or tuple of UWI strings.\n        \n        Returns:\n            project.\n        \"\"\"\n        if uwis is None:\n            return Project(self.__list)\n        return Project([w for w in self if w.uwi in uwis])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef omit_wells(self, uwis=None):\n        if uwis is None:\n            raise ValueError('Must specify at least one uwi')\n        return Project([w for w in self if w.uwi not in uwis])", "response": "Returns a new project where wells with specified UWI are omitted from the project."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_well(self, uwi):\n        if uwi is None:\n            raise ValueError('a UWI must be provided')\n        matching_wells = [w for w in self if w.uwi == uwi]\n        return matching_wells[0] if len(matching_wells) >= 1 else None", "response": "Returns a Well object identified by UWI."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new Project object containing wells from self and right where the keys are set to the keys of the wells on the right.", "response": "def merge_wells(self, right, keys=None):\n        \"\"\"\n        Returns a new Project object containing wells from self where\n        curves from the wells on the right have been added. Matching between\n        wells in self and right is based on uwi match and ony wells in self\n        are considered\n\n        Args:\n            uwi (string): the UWI string for the well.\n        \n        Returns:\n            project\n        \"\"\"\n        wells = []\n        for w in self:\n            rw = right.get_well(w.uwi)\n            if rw is not None:\n                if keys is None:\n                    keys = list(rw.data.keys())\n                for k in keys:\n                    try:\n                        w.data[k] = rw.data[k]\n                    except:\n                        pass\n            wells.append(w)\n        return Project(wells)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef df(self):\n        import pandas as pd\n        return pd.concat([w.df(uwi=True) for w in self])", "response": "Returns a pandas DataFrame containing all the wells\n        curve data for all the wells in the Project."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a matrix of data for the current object.", "response": "def _data_as_matrix(self, X_keys, y_key=None,\n                        alias=None,\n                        legend=None,\n                        match_only=None,\n                        field=None,\n                        field_function=None,\n                        legend_field=None,\n                        table=None,\n                        basis=None,\n                        step=None,\n                        window_length=None,\n                        window_step=1,\n                        uwis=None,\n                        include_basis=False,\n                        include_index=False,\n                        include=None,\n                        complete_only=False,\n                        ):\n        \"\"\"\n        Make X.\n\n        \"\"\"\n        alias = alias or self.alias\n        if include is not None:\n            include = np.array(include)\n\n        if window_length is None:\n            window_length = 1\n\n        # Seed with known size.\n        cols = window_length * len(X_keys)\n        cols += sum([include_basis, include_index])\n\n        def get_cols(q):\n            if q is None: return 0\n            a = np.array(q)\n            try:\n                s = a.shape[0]\n            except IndexError:\n                s = 1\n            return s\n        cols += get_cols(include)\n\n        X = np.zeros(cols)\n        y = np.zeros(1)\n\n        # Build up the data.\n        for i, w in enumerate(self.get_wells(uwis)):\n\n            print(w.uwi, end=' ')\n\n            if not w.is_complete(X_keys, alias):\n                continue\n\n            _X, z = w.data_as_matrix(X_keys,\n                                     basis=basis,\n                                     step=step,\n                                     window_length=window_length,\n                                     window_step=window_step,\n                                     return_basis=True,\n                                     alias=alias)\n            if include is not None:\n                try:\n                    if np.ndim(include) == 0:\n                        x = include * np.ones_like(z)\n                        _X = np.hstack([np.expand_dims(x, 1), _X])\n                    elif np.ndim(include) == 1:\n                        for c in include:\n                            x = c * np.ones_like(z)\n                            _X = np.hstack([np.expand_dims(x, 1), _X])\n                    elif np.ndim(include) == 2:\n                        for c in include:\n                            x = c[i] * np.ones_like(z)\n                            _X = np.hstack([np.expand_dims(x, 1), _X])\n                    else:\n                        raise IndexError('Too many dimensions in include.')\n                except:\n                    raise WellError('Problem braodcasting include into X matrix.')\n\n            if include_basis:\n                _X = np.hstack([np.expand_dims(z, 1), _X])\n\n            if include_index:\n                index = i * np.ones_like(z)\n                _X = np.hstack([np.expand_dims(index, 1), _X])\n\n            X = np.vstack([X, _X])\n            print(_X.shape[0])\n\n            if y_key is None:\n                continue\n\n            y_key = w.get_mnemonic(y_key, alias=alias)\n\n            if y_key is None:\n                continue\n\n            try:\n                _y = w.data[y_key].to_basis(basis=z)\n            except:\n                _y = w.data[y_key].to_log(basis=z,\n                                          legend=legend,\n                                          match_only=match_only,\n                                          field=field,\n                                          field_function=field_function,\n                                          table=table,\n                                          legend_field=legend_field,\n                                          )\n\n            y = np.hstack([y, _y])\n\n        # Get rid of the 'seed'.\n        X = X[1:]\n        if y_key is None:\n            y = None\n        else:\n            y = y[1:]\n\n        return X, y"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef basis(self):\n        return np.linspace(self.start, self.stop, self.shape[0], endpoint=True)", "response": "Returns the depth or time basis of the curve."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn basic statistics about the current object.", "response": "def describe(self):\n        \"\"\"\n        Return basic statistics about the curve.\n        \"\"\"\n        stats = {}\n        stats['samples'] = self.shape[0]\n        stats['nulls'] = self[np.isnan(self)].shape[0]\n        stats['mean'] = float(np.nanmean(self.real))\n        stats['min'] = float(np.nanmin(self.real))\n        stats['max'] = float(np.nanmax(self.real))\n        return stats"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new instance of the class from a lasio curve object.", "response": "def from_lasio_curve(cls, curve,\n                         depth=None,\n                         basis=None,\n                         start=None,\n                         stop=None,\n                         step=0.1524,\n                         run=-1,\n                         null=-999.25,\n                         service_company=None,\n                         date=None):\n        \"\"\"\n        Makes a curve object from a lasio curve object and either a depth\n        basis or start and step information.\n\n        Args:\n            curve (ndarray)\n            depth (ndarray)\n            basis (ndarray)\n            start (float)\n            stop (float)\n            step (float): default: 0.1524\n            run (int): default: -1\n            null (float): default: -999.25\n            service_company (str): Optional.\n            data (str): Optional.\n\n        Returns:\n            Curve. An instance of the class.\n        \"\"\"\n        data = curve.data\n        unit = curve.unit\n\n        # See if we have uneven sampling.\n        if depth is not None:\n            d = np.diff(depth)\n            if not np.allclose(d - np.mean(d), np.zeros_like(d)):\n                # Sampling is uneven.\n                m = \"Irregular sampling in depth is not supported. \"\n                m += \"Interpolating to regular basis.\"\n                warnings.warn(m)\n                step = np.nanmedian(d)\n                start, stop = depth[0], depth[-1]+0.00001  # adjustment\n                basis = np.arange(start, stop, step)\n                data = np.interp(basis, depth, data)\n            else:\n                step = np.nanmedian(d)\n                start = depth[0]\n\n        # Carry on with easier situations.\n        if start is None:\n            if basis is not None:\n                start = basis[0]\n                step = basis[1] - basis[0]\n            else:\n                raise CurveError(\"You must provide a basis or a start depth.\")\n\n        if step == 0:\n            if stop is None:\n                raise CurveError(\"You must provide a step or a stop depth.\")\n            else:\n                step = (stop - start) / (curve.data.shape[0] - 1)\n\n        # Interpolate into this.\n\n        params = {}\n        params['mnemonic'] = curve.mnemonic\n        params['description'] = curve.descr\n        params['start'] = start\n        params['step'] = step\n        params['units'] = unit\n        params['run'] = run\n        params['null'] = null\n        params['service_company'] = service_company\n        params['date'] = date\n        params['code'] = curve.API_code\n\n        return cls(data, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a mnemonic get the alias name that it falls under.", "response": "def get_alias(self, alias):\n        \"\"\"\n        Given a mnemonic, get the alias name(s) it falls under. If there aren't\n        any, you get an empty list.\n        \"\"\"\n        alias = alias or {}\n        return [k for k, v in alias.items() if self.mnemonic in v]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot a 2D curve of the image.", "response": "def plot_2d(self, ax=None,\n                width=None,\n                aspect=60,\n                cmap=None,\n                curve=False,\n                ticks=(1, 10),\n                return_fig=False,\n                **kwargs,\n                ):\n        \"\"\"\n        Plot a 2D curve.\n\n        Args:\n            ax (ax): A matplotlib axis.\n            width (int): The width of the image.\n            aspect (int): The aspect ratio (not quantitative at all).\n            cmap (str): The colourmap to use.\n            curve (bool): Whether to plot the curve as well.\n            ticks (tuple): The tick interval on the y-axis.\n            return_fig (bool): whether to return the matplotlib figure.\n                Default False.\n\n        Returns:\n            ax. If you passed in an ax, otherwise None.\n        \"\"\"\n        # Set up the figure.\n        if ax is None:\n            fig = plt.figure(figsize=(2, 10))\n            ax = fig.add_subplot(111)\n            return_ax = False\n        else:\n            return_ax = True\n\n        # Set up the data.\n        cmap = cmap or 'viridis'\n        default = int(self.shape[0] / aspect)\n        if self.ndim == 1:\n            a = np.expand_dims(self, axis=1)\n            a = np.repeat(a, width or default, axis=1)\n        elif self.ndim == 2:\n            a = self[:, :width] if width < self.shape[1] else self\n        elif self.ndim == 3:\n            if 2 < self.shape[-1] < 5:\n                # Interpret as RGB or RGBA.\n                a = utils.normalize(np.copy(self))\n                cmap = None  # Actually doesn't matter.\n            else:\n                # Take first slice.\n                a = self[:, :width, 0] if width < self.shape[1] else self[..., 0]\n        else:\n            raise NotImplementedError(\"Can only handle up to 3 dimensions.\")\n\n        # At this point, a is either a 2D array, or a 2D (rgb) array.\n        extent = [0, width or default, self.stop, self.start]\n        im = ax.imshow(a, cmap=cmap, extent=extent)\n\n        if curve:\n            # Draw the path.\n            # TODO: add default kwargs?\n            paths = ax.fill_betweenx(self.basis, self, self.min(),\n                                     facecolor='none',\n                                     **kwargs,\n                                     )\n\n            # Make the 'fill' mask and clip the background image with it.\n            patch = PathPatch(paths._paths[0], visible=False)\n            ax.add_artist(patch)\n            im.set_clip_path(patch)\n\n        ax.set_xticks([])\n\n        # Rely on interval order.\n        lower, upper = self.stop, self.start\n        rng = abs(upper - lower)\n\n        ax.set_ylim([lower, upper])\n\n        # Make sure ticks is a tuple.\n        try:\n            ticks = tuple(ticks)\n        except TypeError:\n            ticks = (1, ticks)\n\n        # Avoid MAXTICKS error.\n        while rng/ticks[0] > 250:\n            mi, ma = 10*ticks[0], ticks[1]\n            if ma <= mi:\n                ma = 10 * mi\n            ticks = (mi, ma)\n\n        # Carry on plotting...\n        minorLocator = mpl.ticker.MultipleLocator(ticks[0])\n        ax.yaxis.set_minor_locator(minorLocator)\n\n        majorLocator = mpl.ticker.MultipleLocator(ticks[1])\n        majorFormatter = mpl.ticker.FormatStrFormatter('%d')\n        ax.yaxis.set_major_locator(majorLocator)\n        ax.yaxis.set_major_formatter(majorFormatter)\n\n        ax.yaxis.set_ticks_position('left')\n        ax.get_yaxis().set_tick_params(which='both', direction='out')\n\n        if return_ax:\n            return ax\n        elif return_fig:\n            return fig\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplotting a single object in a matplotlib figure.", "response": "def plot(self, ax=None, legend=None, return_fig=False, **kwargs):\n        \"\"\"\n        Plot a curve.\n\n        Args:\n            ax (ax): A matplotlib axis.\n            legend (striplog.legend): A legend. Optional.\n            return_fig (bool): whether to return the matplotlib figure.\n                Default False.\n            kwargs: Arguments for ``ax.set()``\n\n        Returns:\n            ax. If you passed in an ax, otherwise None.\n        \"\"\"\n        if ax is None:\n            fig = plt.figure(figsize=(2, 10))\n            ax = fig.add_subplot(111)\n            return_ax = False\n        else:\n            return_ax = True\n\n        d = None\n        if legend is not None:\n            try:\n                d = legend.get_decor(self)\n            except:\n                pass\n\n        if d is not None:\n            kwargs['color'] = d.colour\n            kwargs['lw'] = getattr(d, 'lineweight', None) or getattr(d, 'lw', 1)\n            kwargs['ls'] = getattr(d, 'linestyle', None) or getattr(d, 'ls', '-')\n\n            # Attempt to get axis parameters from decor.\n            axkwargs = {}\n\n            xlim = getattr(d, 'xlim', None)\n            if xlim is not None:\n                axkwargs['xlim'] = list(map(float, xlim.split(',')))\n\n            xticks = getattr(d, 'xticks', None)\n            if xticks is not None:\n                axkwargs['xticks'] = list(map(float, xticks.split(',')))\n\n            xscale = getattr(d, 'xscale', None)\n            if xscale is not None:\n                axkwargs['xscale'] = xscale\n\n            ax.set(**axkwargs)\n\n        ax.plot(self, self.basis, **kwargs)\n        ax.set_title(self.mnemonic)  # no longer needed\n        ax.set_xlabel(self.units)\n\n        if False:  # labeltop of axes?\n            ax.xaxis.tick_top()\n\n        if True:  # rotate x-tick labels\n            labels = ax.get_xticklabels()\n            for label in labels:\n                label.set_rotation(90)\n\n        ax.set_ylim([self.stop, self.start])\n        ax.grid('on', color='k', alpha=0.33, lw=0.33, linestyle='-')\n\n        if return_ax:\n            return ax\n        elif return_fig:\n            return fig\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interpolate(self):\n        nans, x = utils.nan_idx(self)\n        self[nans] = np.interp(x(nans), x(~nans), self[~nans])\n        return self", "response": "Interpolate across any missing zones."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninterpolate every entry in the set where condition is True.", "response": "def interpolate_where(self, condition):\n        \"\"\"\n        Remove then interpolate across\n        \"\"\"\n        raise NotImplementedError()\n        self[self < 0] = np.nan\n        return self.interpolate()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps to_basis to convert a new curve instance to a new basis.", "response": "def to_basis_like(self, basis):\n        \"\"\"\n        Make a new curve in a new basis, given an existing one. Wraps\n        ``to_basis()``.\n\n        Pass in a curve or the basis of a curve.\n\n        Args:\n            basis (ndarray): A basis, but can also be a Curve instance.\n\n        Returns:\n            Curve. The current instance in the new basis.\n        \"\"\"\n        try:  # To treat as a curve.\n            curve = basis\n            basis = curve.basis\n            undefined = curve.null\n        except:\n            undefined = None\n\n        return self.to_basis(basis=basis,\n                             undefined=undefined)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a new curve in a new basis, given a basis, or a new start, step, and/or stop. You only need to set the parameters you want to change. If the new extents go beyond the current extents, the curve is padded with the ``undefined`` parameter. Args: basis (ndarray) start (float) stop (float) step (float) undefined (float) Returns: Curve. The current instance in the new basis.", "response": "def to_basis(self, basis=None,\n                 start=None,\n                 stop=None,\n                 step=None,\n                 undefined=None):\n        \"\"\"\n        Make a new curve in a new basis, given a basis, or a new start, step,\n        and/or stop. You only need to set the parameters you want to change.\n        If the new extents go beyond the current extents, the curve is padded\n        with the ``undefined`` parameter.\n\n        Args:\n            basis (ndarray)\n            start (float)\n            stop (float)\n            step (float)\n            undefined (float)\n\n        Returns:\n            Curve. The current instance in the new basis.\n        \"\"\"\n        if basis is None:\n            if start is None:\n                new_start = self.start\n            else:\n                new_start = start\n            new_step = step or self.step\n            new_stop = stop or self.stop\n            # new_adj_stop = new_stop + new_step/100  # To guarantee inclusion.\n            # basis = np.arange(new_start, new_adj_stop, new_step)\n            steps = 1 + (new_stop - new_start) / new_step\n            basis = np.linspace(new_start, new_stop, int(steps), endpoint=True)\n        else:\n            new_start = basis[0]\n            new_step = basis[1] - basis[0]\n\n        if undefined is None:\n            undefined = np.nan\n        else:\n            undefined = undefined\n\n        interp = interp1d(self.basis, self,\n                          bounds_error=False,\n                          fill_value=undefined)\n\n        data = interp(basis)\n\n        params = self.__dict__.copy()\n        params['step'] = float(new_step)\n        params['start'] = float(new_start)\n\n        return Curve(data, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads the log at a specific depth or an array of depths.", "response": "def read_at(self, d, **kwargs):\n        \"\"\"\n        Read the log at a specific depth or an array of depths.\n\n        Args:\n            d (float or array-like)\n            interpolation (str)\n            index(bool)\n            return_basis (bool)\n\n        Returns:\n            float or ndarray.\n        \"\"\"\n        try:\n            return np.array([self._read_at(depth, **kwargs) for depth in d])\n        except:\n            return self._read_at(d, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef quality(self, tests, alias=None):\n        # Gather the test s.\n        # First, anything called 'all', 'All', or 'ALL'.\n        # Second, anything with the name of the curve we're in now.\n        # Third, anything that the alias list has for this curve.\n        # (This requires a reverse look-up so it's a bit messy.)\n        this_tests =\\\n            tests.get('each', [])+tests.get('Each', [])+tests.get('EACH', [])\\\n            + tests.get(self.mnemonic, [])\\\n            + utils.flatten_list([tests.get(a) for a in self.get_alias(alias=alias)])\n        this_tests = filter(None, this_tests)\n\n        # If we explicitly set zero tests for a particular key, then this\n        # overrides the 'all' and 'alias' tests.\n        if not tests.get(self.mnemonic, 1):\n            this_tests = []\n\n        return {test.__name__: test(self) for test in this_tests}", "response": "Run a series of tests and return the corresponding results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef quality_score(self, tests, alias=None):\n        results = self.quality(tests, alias=alias).values()\n        if results:\n            return sum(results) / len(results)\n        return -1", "response": "Run a series of tests and return the normalized score."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef block(self,\n              cutoffs=None,\n              values=None,\n              n_bins=0,\n              right=False,\n              function=None):\n        \"\"\"\n        Block a log based on number of bins, or on cutoffs.\n\n        Args:\n            cutoffs (array)\n            values (array): the values to map to. Defaults to [0, 1, 2,...]\n            n_bins (int)\n            right (bool)\n            function (function): transform the log if you want.\n\n        Returns:\n            Curve.\n        \"\"\"\n        # We'll return a copy.\n        params = self.__dict__.copy()\n\n        if (values is not None) and (cutoffs is None):\n            cutoffs = values[1:]\n\n        if (cutoffs is None) and (n_bins == 0):\n            cutoffs = np.mean(self)\n\n        if (n_bins != 0) and (cutoffs is None):\n            mi, ma = np.amin(self), np.amax(self)\n            cutoffs = np.linspace(mi, ma, n_bins+1)\n            cutoffs = cutoffs[:-1]\n\n        try:  # To use cutoff as a list.\n            data = np.digitize(self, cutoffs, right)\n        except ValueError:  # It's just a number.\n            data = np.digitize(self, [cutoffs], right)\n\n        if (function is None) and (values is None):\n            return Curve(data, params=params)\n\n        data = data.astype(float)\n\n        # Set the function for reducing.\n        f = function or utils.null\n\n        # Find the tops of the 'zones'.\n        tops, vals = utils.find_edges(data)\n\n        # End of array trick... adding this should remove the\n        # need for the marked lines below. But it doesn't.\n        # np.append(tops, None)\n        # np.append(vals, None)\n\n        if values is None:\n            # Transform each segment in turn, then deal with the last segment.\n            for top, base in zip(tops[:-1], tops[1:]):\n                data[top:base] = f(np.copy(self[top:base]))\n            data[base:] = f(np.copy(self[base:]))  # See above\n        else:\n            for top, base, val in zip(tops[:-1], tops[1:], vals[:-1]):\n                data[top:base] = values[int(val)]\n            data[base:] = values[int(vals[-1])]  # See above\n\n        return Curve(data, params=params)", "response": "Block a log based on number of bins or on values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef despike(self, window_length=33, samples=True, z=2):\n        window_length //= 1 if samples else self.step\n        z *= np.nanstd(self)  # Transform to curve's units\n        curve_sm = self._rolling_window(window_length, np.median)\n        spikes = np.where(np.nan_to_num(self - curve_sm) > z)[0]\n        spukes = np.where(np.nan_to_num(curve_sm - self) > z)[0]\n        out = np.copy(self)\n        params = self.__dict__.copy()\n        out[spikes] = curve_sm[spikes] + z\n        out[spukes] = curve_sm[spukes] - z\n        return Curve(out, params=params)", "response": "Returns a new curve with the despike of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\napplying a function over a window.", "response": "def apply(self, window_length, samples=True, func1d=None):\n        \"\"\"\n        Runs any kind of function over a window.\n\n        Args:\n            window_length (int): the window length. Required.\n            samples (bool): window length is in samples. Use False for a window\n                length given in metres.\n            func1d (function): a function that takes a 1D array and returns a\n                scalar. Default: ``np.mean()``.\n\n        Returns:\n            Curve.\n        \"\"\"\n        window_length /= 1 if samples else self.step\n        if func1d is None:\n            func1d = np.mean\n        params = self.__dict__.copy()\n        out = self._rolling_window(int(window_length), func1d)\n        return Curve(out, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_kde(self,\n                 ax=None,\n                 amax=None,\n                 amin=None,\n                 label=None,\n                 return_fig=False):\n        \"\"\"\n        Plot a KDE for the curve. Very nice summary of KDEs:\n        https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/\n\n        Args:\n            ax (axis): Optional matplotlib (MPL) axis to plot into. Returned.\n            amax (float): Optional max value to permit.\n            amin (float): Optional min value to permit.\n            label (string): What to put on the y-axis. Defaults to curve name.\n            return_fig (bool): If you want to return the MPL figure object.\n\n        Returns:\n            None, axis, figure: depending on what you ask for.\n        \"\"\"\n        from scipy.stats import gaussian_kde\n\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n            return_ax = False\n        else:\n            return_ax = True\n\n        a = self[~np.isnan(self)]\n\n        # Find values for common axis to exclude outliers.\n        if amax is None:\n            amax = np.percentile(a, 99)\n        if amin is None:\n            amin = np.percentile(a,  1)\n\n        x = a[np.abs(a - 0.5 * (amax + amin)) < 0.5 * (amax - amin)]\n        x_grid = np.linspace(amin, amax, 100)\n\n        kde = gaussian_kde(x)\n        std_a = kde.evaluate(x_grid)\n\n        img = np.array([std_a]) / np.max([std_a])\n        extent = [amin, amax, 0, 1]\n        ax.imshow(img, aspect='auto', cmap='viridis', extent=extent)\n        ax.set_yticklabels([])\n        ax.set_ylabel(label or self.mnemonic)\n\n        if return_ax:\n            return ax\n        elif return_fig:\n            return fig\n        else:\n            return None", "response": "Plot a KDE for the curve. Very nice summary of KDEs."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new object from a CSV file.", "response": "def from_csv(cls, csv_file):\n        \"\"\"\n        Not implemented. Will provide a route from CSV file.\n        \"\"\"\n        try:\n            param_dict = csv.DictReader(csv_file)\n            return cls(param_dict)\n        except:\n            raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a Well to a dictionary of card names.", "response": "def well_to_card_2(well, key):\n    \"\"\"\n    Args:\n        well (Well)\n        key (str): The key of the predicted Striplog in `well.data`.\n\n    Returns:\n        dict.\n    \"\"\"\n    dictionary = {}\n    dictionary['comp'] = ''\n    dictionary['spud'] = ''\n    dictionary['start'] = well.data[key].start.z\n    dictionary['stop'] = well.data[key].stop.z\n    dictionary['status'] = ''\n    dictionary['uwi'] = well.header.uwi\n    return dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting a single row from the file.", "response": "def write_row(dictionary, card, log):\n    \"\"\"\n    Processes a single row from the file.\n    \"\"\"\n    rowhdr = {'card': card, 'log': log}\n\n    # Do this as a list of 1-char strings.\n    # Can't use a string b/c strings are immutable.\n    row = [' '] * 80\n\n    # Make the row header.\n    for e in ['log', 'card']:\n        strt, stop, item = _put_field(cols(0), e, rowhdr[e])\n        if item is not None:\n            row[strt:stop] = list(item)\n\n    # Now make the rest of the row.\n    for field in cols(card):\n        strt, stop, item = _put_field(cols(card), field, dictionary.get(field))\n        if item is not None:\n            row[strt:stop] = list(item)\n\n    return ''.join(row) + '\\n'"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef basis(self):\n        precision_adj = self.dt / 100\n        return np.arange(self.start, self.stop - precision_adj, self.dt)", "response": "Compute the basis array for this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the synthetic as a Curve in depth. Facilitates plotting along - handed - side other curve data.", "response": "def as_curve(self, start=None, stop=None):\n        \"\"\"\n        Get the synthetic as a Curve, in depth. Facilitates plotting along-\n        side other curve data.\n        \"\"\"\n        params = {'start': start or getattr(self, 'z start', None),\n                  'mnemonic': 'SYN',\n                  'step': 0.1524\n                  }\n\n        return Curve(data, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots a synthetic. Args: ax (ax): A matplotlib axis. legend (Legend): For now, only here to match API for other plot methods. return_fig (bool): whether to return the matplotlib figure. Default False. Returns: ax. If you passed in an ax, otherwise None.", "response": "def plot(self, ax=None, return_fig=False, **kwargs):\n        \"\"\"\n        Plot a synthetic.\n\n        Args:\n            ax (ax): A matplotlib axis.\n            legend (Legend): For now, only here to match API for other plot\n                methods.\n            return_fig (bool): whether to return the matplotlib figure.\n                Default False.\n\n        Returns:\n            ax. If you passed in an ax, otherwise None.\n        \"\"\"\n        if ax is None:\n            fig = plt.figure(figsize=(2, 10))\n            ax = fig.add_subplot(111)\n            return_ax = False\n        else:\n            return_ax = True\n\n        hypertime = np.linspace(self.start, self.stop, (10 * self.size - 1) + 1)\n        hyperamp = np.interp(hypertime, self.basis, self)\n\n        ax.plot(hyperamp, hypertime, 'k')\n        ax.fill_betweenx(hypertime, hyperamp, 0, hyperamp > 0.0, facecolor='k', lw=0)\n        ax.invert_yaxis()\n        ax.set_title(self.name)\n\n        if return_ax:\n            return ax\n        elif return_fig:\n            return fig\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef no_gaps(curve):\n    tnt = utils.top_and_tail(curve)\n    return not any(np.isnan(tnt))", "response": "Check for gaps at the top and bottom of a sequence."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef no_spikes(tolerance):\n    def no_spikes(curve):\n        diff = np.abs(curve - curve.despike())\n        return np.count_nonzero(diff) < tolerance\n    return no_spikes", "response": "Returns a function that returns True if there are no spiky samples in the current hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the fraction of the extents that are good ( non - nan data.", "response": "def fraction_not_nans(curve):\n    \"\"\"\n    Returns the fraction of the curve extents that are good (non-nan data).\n    \"\"\"\n    return 1 - (len(np.extract(np.isnan(curve), curve)) / len(curve))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _repr_html_(self):\n        row1 = '<tr><th style=\"text-align:center;\" '\n        row1 += 'colspan=\"2\">{}<br><small>{{}}</small></th></tr>'\n        rows = row1.format(getattr(self.header, 'name', ''))\n        rows = rows.format(getattr(self.header, 'uwi', ''))\n        s = '<tr><td><strong>{k}</strong></td><td>{v}</td></tr>'\n\n        if getattr(self, 'location', None) is not None:\n            for k, v in self.location.__dict__.items():\n                if k in ['deviation', 'position']:\n                    continue\n                if k == 'crs':\n                    v = v.__repr__()\n                rows += s.format(k=k, v=v)\n\n        if getattr(self, 'data', None) is not None:\n            v = ', '.join(sorted(list(self.data.keys())))\n            rows += s.format(k=\"data\", v=v)\n\n        html = '<table>{}</table>'.format(rows)\n        return html", "response": "Jupyter Notebook magic repr function."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new a object from a lasio object.", "response": "def from_lasio(cls, l, remap=None, funcs=None, data=True, req=None, alias=None, fname=None):\n        \"\"\"\n        Constructor. If you already have the lasio object, then this makes a\n        well object from it.\n\n        Args:\n            l (lasio object): a lasio object.\n            remap (dict): Optional. A dict of 'old': 'new' LAS field names.\n            funcs (dict): Optional. A dict of 'las field': function() for\n                implementing a transform before loading. Can be a lambda.\n            data (bool): Whether to load curves or not.\n            req (dict): An alias list, giving all required curves. If not\n                all of the aliases are present, the well is empty.\n\n        Returns:\n            well. The well object.\n        \"\"\"\n        # Build a dict of curves.\n        curve_params = {}\n        for field, (sect, code) in LAS_FIELDS['data'].items():\n            curve_params[field] = utils.lasio_get(l,\n                                                  sect,\n                                                  code,\n                                                  remap=remap,\n                                                  funcs=funcs)\n\n        # This is annoying, but I need the whole depth array to\n        # deal with edge cases, eg non-uniform sampling.\n\n        # Add all required curves together.\n        if req:\n            reqs = utils.flatten_list([v for k, v in alias.items() if k in req])\n\n        # Using lasio's idea of depth in metres:\n        if l.depth_m[0] < l.depth_m[1]:\n            curve_params['depth'] = l.depth_m\n        else:\n            curve_params['depth'] = np.flipud(l.depth_m)\n\n        # Make the curve dictionary.\n        depth_curves = ['DEPT', 'TIME']\n        if data and req:\n            curves = {c.mnemonic: Curve.from_lasio_curve(c, **curve_params)\n                      for c in l.curves\n                      if (c.mnemonic[:4] not in depth_curves)\n                      and (c.mnemonic in reqs)}\n        elif data and not req:\n            curves = {c.mnemonic: Curve.from_lasio_curve(c, **curve_params)\n                      for c in l.curves\n                      if (c.mnemonic[:4] not in depth_curves)}\n        elif (not data) and req:\n            curves = {c.mnemonic: True\n                      for c in l.curves\n                      if (c.mnemonic[:4] not in depth_curves)\n                      and (c.mnemonic in reqs)}\n        else:\n            curves = {c.mnemonic: True\n                      for c in l.curves\n                      if (c.mnemonic[:4] not in depth_curves)}\n\n        if req:\n            aliases = utils.flatten_list([c.get_alias(alias)\n                                          for m, c\n                                          in curves.items()]\n                                         )\n            if len(set(aliases)) < len(req):\n                return cls(params={})\n\n        # Build a dict of the other well data.\n        params = {'las': l,\n                  'header': Header.from_lasio(l, remap=remap, funcs=funcs),\n                  'location': Location.from_lasio(l, remap=remap, funcs=funcs),\n                  'data': curves,\n                  'fname': fname}\n\n        for field, (sect, code) in LAS_FIELDS['well'].items():\n            params[field] = utils.lasio_get(l,\n                                            sect,\n                                            code,\n                                            remap=remap,\n                                            funcs=funcs)\n        return cls(params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_las(cls,\n                 fname,\n                 remap=None,\n                 funcs=None,\n                 data=True,\n                 req=None,\n                 alias=None,\n                 encoding=None,\n                 printfname=False\n                 ):\n        \"\"\"\n        Constructor. Essentially just wraps ``from_lasio()``, but is more\n        convenient for most purposes.\n\n        Args:\n            fname (str): The path of the LAS file, or a URL to one.\n            remap (dict): Optional. A dict of 'old': 'new' LAS field names.\n            funcs (dict): Optional. A dict of 'las field': function() for\n                implementing a transform before loading. Can be a lambda.\n            printfname (bool): prints filename before trying to load it, for \n                debugging\n\n        Returns:\n            well. The well object.\n        \"\"\"\n        if printfname:\n            print(fname)\n            \n        if re.match(r'https?://.+\\..+/.+?', fname) is not None:\n            try:\n                data = urllib.request.urlopen(fname).read().decode()\n            except urllib.HTTPError as e:\n                raise WellError('Could not retrieve url: ', e)\n            fname = (StringIO(data))\n\n        las = lasio.read(fname, encoding=encoding)\n\n        # Pass to other constructor.\n        return cls.from_lasio(las,\n                              remap=remap,\n                              funcs=funcs,\n                              data=data,\n                              req=req,\n                              alias=alias,\n                              fname=fname)", "response": "A more convenient constructor for creating a new object from a LAS file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the current curve data as a pandas. DataFrame object.", "response": "def df(self, keys=None, basis=None, uwi=False):\n        \"\"\"\n        Return current curve data as a ``pandas.DataFrame`` object.\n        \n        Everything has to have the same basis, because the depth\n        is going to become the index of the DataFrame. If you don't\n        provide one, ``welly`` will make one using ``survey_basis()``.\n\n        Args:\n            keys (list): List of strings: the keys of the data items to\n                survey, if not all of them.\n            basis (array): A basis, if you want to enforce one, otherwise\n                you'll get the result of ``survey_basis()``.\n            uwi (bool): Whether to add a 'UWI' column.\n    \n        Returns:\n            pandas.DataFrame.\n\n        \"\"\"\n        try:\n            import pandas as pd\n        except:\n            m = \"You must install pandas to use dataframes.\"\n            raise WellError(m)\n\n        from pandas.api.types import is_object_dtype\n\n        if keys is None:\n            keys = [k for k, v in self.data.items() if isinstance(v, Curve)]\n        else:\n            keys = utils.flatten_list(keys)\n\n        if basis is None:\n            basis = self.survey_basis(keys=keys)\n        if basis is None:\n            m = \"No basis was provided and welly could not retrieve common basis.\"\n            raise WellError(m)\n\n        data = {k: v.to_basis(basis) for k, v in self.data.items() if isinstance(v, Curve)}\n\n        df = pd.DataFrame(data, columns=list(self.data.keys()))\n        df['Depth'] = basis\n        df = df.set_index('Depth')\n\n        if uwi:\n            df['UWI'] = [self.uwi for _ in basis]\n            df = df.reset_index()\n            df = df.set_index(['UWI', 'Depth'])\n\n        for column in df.columns:\n            if is_object_dtype(df[column].dtype):\n                try:\n                    df[column] = df[column].astype(np.float64)\n                except ValueError:\n                    pass\n    \n        return df"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_lasio(self, keys=None, basis=None):\n\n        # Create an empty lasio object.\n        l = lasio.LASFile()\n        l.well.DATE = str(datetime.datetime.today())\n\n        # Deal with header.\n        for obj, dic in LAS_FIELDS.items():\n            if obj == 'data':\n                continue\n            for attr, (sect, item) in dic.items():\n                value = getattr(getattr(self, obj), attr, None)\n                try:\n                    getattr(l, sect)[item].value = value\n                except:\n                    h = lasio.HeaderItem(item, \"\", value, \"\")\n                    getattr(l, sect)[item] = h\n\n        # Clear curves from header portion.\n        l.header['Curves'] = []\n\n        # Add a depth basis.\n        if basis is None:\n            basis = self.survey_basis(keys=keys)\n        try:\n            l.add_curve('DEPT', basis)\n        except:\n            raise Exception(\"Please provide a depth basis.\")\n\n        # Add meta from basis.\n        setattr(l.well, 'STRT', basis[0])\n        setattr(l.well, 'STOP', basis[-1])\n        setattr(l.well, 'STEP', basis[1]-basis[0])\n\n        # Add data entities.\n        other = ''\n\n        if keys is None:\n            keys = [k for k, v in self.data.items() if isinstance(v, Curve)]\n        else:\n            keys = utils.flatten_list(keys)\n\n        for k in keys:\n            d = self.data[k]\n            if getattr(d, 'null', None) is not None:\n                d[np.isnan(d)] = d.null\n            try:\n                new_data = np.copy(d.to_basis_like(basis))\n            except:\n                # Basis shift failed; is probably not a curve\n                pass\n            try:\n                descr = getattr(d, 'description', '')\n                l.add_curve(k.upper(), new_data, unit=d.units, descr=descr)\n            except:\n                try:\n                    # Treat as OTHER\n                    other += \"{}\\n\".format(k.upper()) + d.to_csv()\n                except:\n                    pass\n\n        # Write OTHER, if any.\n        if other:\n            l.other = other\n\n        return l", "response": "Creates a lasio object from the current well."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_las(self, fname, keys=None, basis=None, **kwargs):\n        with open(fname, 'w') as f:\n            self.to_lasio(keys=keys, basis=basis).write(f, **kwargs)\n\n        return", "response": "Writes the current well instance as a LAS file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds curves from a LAS file to the current well instance.", "response": "def add_curves_from_las(self, fname, remap=None, funcs=None):\n        \"\"\"\n        Given a LAS file, add curves from it to the current well instance.\n        Essentially just wraps ``add_curves_from_lasio()``.\n\n        Args:\n            fname (str): The path of the LAS file to read curves from.\n            remap (dict): Optional. A dict of 'old': 'new' LAS field names.\n            funcs (dict): Optional. A dict of 'las field': function() for\n                implementing a transform before loading. Can be a lambda.\n\n        Returns:\n            None. Works in place.\n        \"\"\"\n        try:  # To treat as a single file\n            self.add_curves_from_lasio(lasio.read(fname),\n                                       remap=remap,\n                                       funcs=funcs\n                                       )\n        except:  # It's a list!\n            for f in fname:\n                self.add_curves_from_lasio(lasio.read(f),\n                                           remap=remap,\n                                           funcs=funcs\n                                           )\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding curves from a LAS file to the current well instance.", "response": "def add_curves_from_lasio(self, l, remap=None, funcs=None):\n        \"\"\"\n        Given a LAS file, add curves from it to the current well instance.\n        Essentially just wraps ``add_curves_from_lasio()``.\n\n        Args:\n            fname (str): The path of the LAS file to read curves from.\n            remap (dict): Optional. A dict of 'old': 'new' LAS field names.\n            funcs (dict): Optional. A dict of 'las field': function() for\n                implementing a transform before loading. Can be a lambda.\n\n        Returns:\n            None. Works in place.\n        \"\"\"\n        params = {}\n        for field, (sect, code) in LAS_FIELDS['data'].items():\n            params[field] = utils.lasio_get(l,\n                                            sect,\n                                            code,\n                                            remap=remap,\n                                            funcs=funcs)\n\n        curves = {c.mnemonic: Curve.from_lasio_curve(c, **params)\n                  for c in l.curves}\n\n        # This will clobber anything with the same key!\n        self.data.update(curves)\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplotting multiple tracks. Args: legend (striplog.legend): A legend instance. tracks (list): A list of strings and/or lists of strings. The tracks you want to plot from ``data``. Optional, but you will usually want to give it. track_titles (list): Optional. A list of strings and/or lists of strings. The names to give the tracks, if you don't want welly to guess. alias (dict): a dictionary mapping mnemonics to lists of mnemonics. basis (ndarray): Optional. The basis of the plot, if you don't want welly to guess (probably the best idea). return_fig (bool): Whether to return the matplotlig figure. Default False. extents (str): What to use for the y limits: 'td' \u2014 plot 0 to TD. 'curves' \u2014 use a basis that accommodates all the curves. 'all' \u2014 use a basis that accommodates everything. (tuple) \u2014 give the upper and lower explictly. Returns: None. The plot is a side-effect.", "response": "def plot(self,\n             legend=None,\n             tracks=None,\n             track_titles=None,\n             alias=None,\n             basis=None,\n             return_fig=False,\n             extents='td',\n             **kwargs):\n        \"\"\"\n        Plot multiple tracks.\n\n        Args:\n            legend (striplog.legend): A legend instance.\n            tracks (list): A list of strings and/or lists of strings. The\n                tracks you want to plot from ``data``. Optional, but you will\n                usually want to give it.\n            track_titles (list): Optional. A list of strings and/or lists of\n                strings. The names to give the tracks, if you don't want welly\n                to guess.\n            alias (dict): a dictionary mapping mnemonics to lists of mnemonics.\n            basis (ndarray): Optional. The basis of the plot, if you don't\n                want welly to guess (probably the best idea).\n            return_fig (bool): Whether to return the matplotlig figure. Default\n                False.\n            extents (str): What to use for the y limits:\n                'td' \u2014\u00a0plot 0 to TD.\n                'curves' \u2014\u00a0use a basis that accommodates all the curves.\n                'all' \u2014\u00a0use a basis that accommodates everything.\n                (tuple) \u2014\u00a0give the upper and lower explictly.\n\n        Returns:\n            None. The plot is a side-effect.\n        \"\"\"\n        # These will be treated differently.\n        depth_tracks = ['MD', 'TVD']\n\n        # Set tracks to 'all' if it's None.\n        tracks = tracks or list(self.data.keys())\n        track_titles = track_titles or tracks\n\n        # Figure out limits\n        if basis is None:\n            basis = self.survey_basis(keys=tracks)\n\n        if extents == 'curves':\n            upper, lower = basis[0], basis[-1]\n        elif extents == 'td':\n            try:\n                upper, lower = 0, self.location.td\n            except:\n                m = \"Could not read self.location.td, try extents='curves'\"\n                raise WellError(m)\n            if not lower:\n                lower = basis[-1]\n        elif extents == 'all':\n            raise NotImplementedError(\"You cannot do that yet.\")\n        else:\n            try:\n                upper, lower = extents\n            except:\n                upper, lower = basis[0], basis[-1]\n\n        # Figure out widths because we can't us gs.update() for that.\n        widths = [0.4 if t in depth_tracks else 1.0 for t in tracks]\n\n        # Set up the figure.\n        ntracks = len(tracks)\n        fig = plt.figure(figsize=(2*ntracks, 12), facecolor='w')\n        fig.suptitle(self.header.name, size=16, zorder=100,\n                     bbox=dict(facecolor='w', alpha=1.0, ec='none'))\n        gs = mpl.gridspec.GridSpec(1, ntracks, width_ratios=widths)\n\n        # Plot first axis.\n        # kwargs = {}\n        ax0 = fig.add_subplot(gs[0, 0])\n        ax0.depth_track = False\n        track = tracks[0]\n        if '.' in track:\n            track, kwargs['field'] = track.split('.')\n        if track in depth_tracks:\n            ax0 = self._plot_depth_track(ax=ax0, md=basis, kind=track)\n        else:\n            try:  # ...treating as a plottable object.\n                ax0 = self.get_curve(track, alias=alias).plot(ax=ax0, legend=legend, **kwargs)\n            except AttributeError:  # ...it's not there.\n                pass\n            except TypeError:  # ...it's a list.\n                for t in track:\n                    try:\n                        ax0 = self.get_curve(t, alias=alias).plot(ax=ax0, legend=legend, **kwargs)\n                    except AttributeError:  # ...it's not there.\n                        pass\n        tx = ax0.get_xticks()\n        ax0.set_xticks(tx[1:-1])\n        ax0.set_title(track_titles[0])\n\n        # Plot remaining axes.\n        for i, track in enumerate(tracks[1:]):\n            # kwargs = {}\n            ax = fig.add_subplot(gs[0, i+1])\n            ax.depth_track = False\n            if track in depth_tracks:\n                ax = self._plot_depth_track(ax=ax, md=basis, kind=track)\n                continue\n            if '.' in track:\n                track, kwargs['field'] = track.split('.')\n            plt.setp(ax.get_yticklabels(), visible=False)\n            try:  # ...treating as a plottable object.\n                ax = self.get_curve(track, alias=alias).plot(ax=ax, legend=legend, **kwargs)\n            except AttributeError:  # ...it's not there.\n                continue\n            except TypeError:  # ...it's a list.\n                for j, t in enumerate(track):\n                    if '.' in t:\n                        track, kwargs['field'] = track.split('.')\n                    try:\n                        ax = self.get_curve(t, alias=alias).plot(ax=ax, legend=legend, **kwargs)\n                    except AttributeError:\n                        continue\n                    except KeyError:\n                        continue\n\n            tx = ax.get_xticks()\n            ax.set_xticks(tx[1:-1])\n            ax.set_title(track_titles[i+1])\n\n        # Set sharing.\n        axes = fig.get_axes()\n        utils.sharey(axes)\n        axes[0].set_ylim([lower, upper])\n\n        # Adjust the grid.\n        gs.update(wspace=0)\n\n        # Adjust spines and ticks for non-depth tracks.\n        for ax in axes:\n            if ax.depth_track:\n                pass\n            if not ax.depth_track:\n                ax.set(yticks=[])\n                ax.autoscale(False)\n                ax.yaxis.set_ticks_position('none')\n                ax.spines['top'].set_visible(True)\n                ax.spines['bottom'].set_visible(True)\n                for sp in ax.spines.values():\n                    sp.set_color('gray')\n\n        if return_fig:\n            return fig\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlook at the basis of all the curves in the well. data and return the most complete common basis.", "response": "def survey_basis(self, keys=None, alias=None, step=None):\n        \"\"\"\n        Look at the basis of all the curves in ``well.data`` and return a\n        basis with the minimum start, maximum depth, and minimum step.\n\n        Args:\n            keys (list): List of strings: the keys of the data items to\n                survey, if not all of them.\n            alias (dict): a dictionary mapping mnemonics to lists of mnemonics.\n            step (float): a new step, if you want to change it.            \n\n        Returns:\n            ndarray. The most complete common basis.\n        \"\"\"\n        if keys is None:\n            keys = [k for k, v in self.data.items() if isinstance(v, Curve)]\n        else:\n            keys = utils.flatten_list(keys)\n\n        starts, stops, steps = [], [], []\n        for k in keys:\n            d = self.get_curve(k, alias=alias)\n            if keys and (d is None):\n                continue\n            try:\n                starts.append(d.basis[0])\n                stops.append(d.basis[-1])\n                steps.append(d.basis[1] - d.basis[0])\n            except Exception as e:\n                pass\n        if starts and stops and steps:\n            step = step or min(steps)\n            return np.arange(min(starts), max(stops)+1e-9, step)\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unify_basis(self, keys=None, basis=None):\n        if keys is None:\n            keys = [k for k, v in self.data.items() if isinstance(v, Curve)]\n        else:\n            keys = utils.flatten_list(keys)\n\n        if basis is None:\n            basis = self.survey_basis(keys=keys)\n        if basis is None:\n            m = \"No basis was provided and welly could not retrieve common basis.\"\n            raise WellError(m)\n\n        for k in keys:\n            if keys and (k not in keys):\n                continue\n            try:  # To treat as a curve.\n                self.data[k] = self.data[k].to_basis(basis)\n            except:  # It's probably a striplog.\n                continue\n\n        return", "response": "Unify the basis of the items in the data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mnemonics_from_regex(self, pattern):\n        regex = re.compile(pattern)\n        keys = list(self.data.keys())\n        return [m.group(0) for k in keys for m in [regex.search(k)] if m]", "response": "Get the mnemonics from a regex pattern."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_mnemonic(self, mnemonic, alias=None):\n        alias = alias or {}\n        aliases = alias.get(mnemonic, [mnemonic])\n        for a in aliases:\n            if a in self.data:\n                return a\n        return None", "response": "This method returns the mnemonic from the data dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_curve(self, mnemonic, alias=None):\n        return self.data.get(self.get_mnemonic(mnemonic, alias=alias), None)", "response": "Gets the mnemonic s entry in the alias dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncount the number of curves in the well that will be selected with the given key list and the given alias dict. Used by Project s curve table.", "response": "def count_curves(self, keys=None, alias=None):\n        \"\"\"\n        Counts the number of curves in the well that will be selected with the\n        given key list and the given alias dict. Used by Project's curve table.\n        \"\"\"\n        if keys is None:\n            keys = [k for k, v in self.data.items() if isinstance(v, Curve)]\n        else:\n            keys = utils.flatten_list(keys)\n\n        return len(list(filter(None, [self.get_mnemonic(k, alias=alias) for k in keys])))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if the well has at least one or more of the keys in its data dictionary.", "response": "def is_complete(self, keys=None, alias=None):\n        \"\"\"\n        Returns False if the well does not have one or more of the keys in its\n        data dictionary. Used by ``project.data_to_matrix()``.\n        \"\"\"\n        return any(k not in list(self.data.keys()) for k in keys)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_synthetic(self,\n                       srd=0,\n                       v_repl_seismic=2000,\n                       v_repl_log=2000,\n                       f=50,\n                       dt=0.001):\n        \"\"\"\n        Early hack. Use with extreme caution.\n\n        Hands-free. There'll be a more granualr version in synthetic.py.\n\n        Assumes DT is in \u00b5s/m and RHOB is kg/m3.\n\n        There is no handling yet for TVD.\n\n        The datum handling is probably sketchy.\n\n        TODO:\n            A lot.\n        \"\"\"\n        kb = getattr(self.location, 'kb', None) or 0\n        data0 = self.data['DT'].start\n        log_start_time = ((srd - kb) / v_repl_seismic) + (data0 / v_repl_log)\n\n        # Basic log values.\n        dt_log = self.data['DT'].despike()  # assume \u00b5s/m\n        rho_log = self.data['RHOB'].despike()  # assume kg/m3\n        if not np.allclose(dt_log.basis, rho_log.basis):\n            rho_log = rho_log.to_basis_like(dt_log)\n        Z = (1e6 / dt_log) * rho_log\n\n        # Two-way-time.\n        scaled_dt = dt_log.step * np.nan_to_num(dt_log) / 1e6\n        twt = 2 * np.cumsum(scaled_dt)\n        t = twt + log_start_time\n\n        # Move to time.\n        t_max = t[-1] + 10*dt\n        t_reg = np.arange(0, t_max+1e-9, dt)\n        Z_t = np.interp(x=t_reg, xp=t, fp=Z)\n\n        # Make RC series.\n        rc_t = (Z_t[1:] - Z_t[:-1]) / (Z_t[1:] + Z_t[:-1])\n        rc_t = np.nan_to_num(rc_t)\n\n        # Convolve.\n        _, ricker = utils.ricker(f=f, length=0.128, dt=dt)\n        synth = np.convolve(ricker, rc_t, mode='same')\n\n        params = {'dt': dt,\n                  'z start': dt_log.start,\n                  'z stop': dt_log.stop\n                  }\n\n        self.data['Synthetic'] = Synthetic(synth, basis=t_reg, params=params)\n\n        return None", "response": "Make a synthetic version of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning tests on a cohort of curves.", "response": "def qc_curve_group(self, tests, alias=None):\n        \"\"\"\n        Run tests on a cohort of curves.\n\n        Args:\n            alias (dict): an alias dictionary, mapping mnemonics to lists of\n                mnemonics.\n\n        Returns:\n            dict.\n        \"\"\"\n        keys = [k for k, v in self.data.items() if isinstance(v, Curve)]\n        if not keys:\n            return {}\n\n        all_tests = tests.get('all', tests.get('All', tests.get('ALL', [])))\n        data = {test.__name__: test(self, keys, alias) for test in all_tests}\n\n        results = {}\n        for i, key in enumerate(keys):\n            this = {}\n            for test, result in data.items():\n                this[test] = result[i]\n            results[key] = this\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef qc_data(self, tests, alias=None):\n        # We'll get a result for each curve here.\n        r = {m: c.quality(tests, alias) for m, c in self.data.items()}\n\n        s = self.qc_curve_group(tests, alias=alias)\n\n        for m, results in r.items():\n            if m in s:\n                results.update(s[m])\n\n        return r", "response": "Run a series of tests against the data and return the corresponding\n        results."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking a nice table out of the QC data.", "response": "def qc_table_html(self, tests, alias=None):\n        \"\"\"\n        Makes a nice table out of ``qc_data()``\n\n        Returns:\n            str. An HTML string.\n        \"\"\"\n        data = self.qc_data(tests, alias=alias)\n        all_tests = [list(d.keys()) for d in data.values()]\n        tests = list(set(utils.flatten_list(all_tests)))\n\n        # Header row.\n        r = '</th><th>'.join(['Curve', 'Passed', 'Score'] + tests)\n        rows = '<tr><th>{}</th></tr>'.format(r)\n\n        styles = {\n            True: \"#CCEECC\",   # Green\n            False: \"#FFCCCC\",  # Red\n        }\n\n        # Quality results.\n        for curve, results in data.items():\n\n            if results:\n                norm_score = sum(results.values()) / len(results)\n            else:\n                norm_score = -1\n\n            rows += '<tr><th>{}</th>'.format(curve)\n            rows += '<td>{} / {}</td>'.format(sum(results.values()), len(results))\n            rows += '<td>{:.3f}</td>'.format(norm_score)\n\n            for test in tests:\n                result = results.get(test, '')\n                style = styles.get(result, \"#EEEEEE\")\n                rows += '<td style=\"background-color:{};\">'.format(style)\n                rows += '{}</td>'.format(result)\n            rows += '</tr>'\n\n        html = '<table>{}</table>'.format(rows)\n        return html"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a Canstrat DAT (aka ASCII) file. TODO: The data part should probably belong to striplog, and only the header should be written by the well. Args: filename (str) key (str) log (str): the log name, should be 6 characters. lith_field (str) the name of the lithology field in the striplog's Primary component. Must match the Canstrat definitions. filename (str) as_text (bool): if you don't want to write a file.", "response": "def to_canstrat(self, key, log, lith_field, filename=None, as_text=False):\n        \"\"\"\n        Make a Canstrat DAT (aka ASCII) file.\n\n        TODO:\n            The data part should probably belong to striplog, and only the\n            header should be written by the well.\n\n        Args:\n           filename (str)\n           key (str)\n           log (str): the log name, should be 6 characters.\n           lith_field (str) the name of the lithology field in the striplog's\n               Primary component. Must match the Canstrat definitions.\n           filename (str)\n           as_text (bool): if you don't want to write a file.\n        \"\"\"\n        if (filename is None):\n            if (not as_text):\n                m = \"You must provide a filename or set as_text to True.\"\n                raise WellError(m)\n\n        strip = self.data[key]\n        strip = strip.fill()  # Default is to fill with 'null' intervals.\n\n        record = {1: [well_to_card_1(self)],\n                  2: [well_to_card_2(self, key)],\n                  8: [],\n                  7: [interval_to_card_7(iv, lith_field) for iv in strip]\n                  }\n\n        result = ''\n        for c in [1, 2, 8, 7]:\n            for d in record[c]:\n                result += write_row(d, card=c, log=log)\n\n        if as_text:\n            return result\n        else:\n            with open(filename, 'w') as f:\n                f.write(result)\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef data_as_matrix(self,\n                       keys=None,\n                       return_basis=False,\n                       basis=None,\n                       alias=None,\n                       start=None,\n                       stop=None,\n                       step=None,\n                       window_length=None,\n                       window_step=1,\n                       ):\n        \"\"\"\n        Provide a feature matrix, given a list of data items.\n\n        I think this will probably fail if there are striplogs in the data\n        dictionary for this well.\n\n        TODO:\n            Deal with striplogs and other data, if present.\n\n        Args:\n            keys (list): List of the logs to export from the data dictionary.\n            return_basis (bool): Whether or not to return the basis that was\n                used.\n            basis (ndarray): The basis to use. Default is to survey all curves\n                to find a common basis.\n            alias (dict): A mapping of alias names to lists of mnemonics.\n            start (float): Optionally override the start of whatever basis\n                you find or (more likely) is surveyed.\n            stop (float): Optionally override the stop of whatever basis\n                you find or (more likely) is surveyed.\n            step (float): Override the step in the basis from survey_basis.\n            window_length (int): The number of samples to return around each sample.\n                This will provide one or more shifted versions of the features.\n            window_step (int): How much to step the offset versions.\n\n        Returns:\n            ndarray.\n            or\n            ndarray, ndarray if return_basis=True\n        \"\"\"\n        if keys is None:\n            keys = [k for k, v in self.data.items() if isinstance(v, Curve)]\n        else:\n            # Only look at the alias list if keys were passed.\n            if alias is not None:\n                _keys = []\n                for k in keys:\n                    if k in alias:\n                        added = False\n                        for a in alias[k]:\n                            if a in self.data:\n                                _keys.append(a)\n                                added = True\n                                break\n                        if not added:\n                            _keys.append(k)\n                    else:\n                        _keys.append(k)\n                keys = _keys\n\n        if basis is None:\n            basis = self.survey_basis(keys=keys, step=step)\n\n        # Get the data, or None is curve is missing.\n        data = [self.data.get(k) for k in keys]\n\n        # Now cast to the correct basis, and replace any missing curves with\n        # an empty Curve. The sklearn imputer will deal with it. We will change\n        # the elements in place.\n        for i, d in enumerate(data):\n            if d is not None:\n                data[i] = d.to_basis(basis=basis)\n                # Allow user to override the start and stop from the survey.\n                if (start is not None) or (stop is not None):\n                    data[i] = data[i].to_basis(start=start, stop=stop, step=step)\n                    basis = data[i].basis\n            else:\n                # Empty_like gives unpredictable results\n                data[i] = Curve(np.full(basis.shape, np.nan), basis=basis)\n\n        if window_length is not None:\n            d_new = []\n            for d in data:\n                r = d._rolling_window(window_length,\n                                      func1d=utils.null,\n                                      step=window_step,\n                                      return_rolled=False,\n                                      )\n                d_new.append(r.T)\n            data = d_new\n\n        if return_basis:\n            return np.vstack(data).T, basis\n        else:\n            return np.vstack(data).T", "response": "Provide a feature matrix given a list of data items."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_string(cls, prjs):\n        def parse(v):\n            try:\n                return int(v)\n            except ValueError:\n                pass\n            try:\n                return float(v)\n            except ValueError:\n                return v\n\n        parts = [o.lstrip('+') for o in prjs.strip().split()]\n\n        items = map(\n            lambda kv: len(kv) == 2 and (kv[0], parse(kv[1])) or (kv[0], True),\n            (p.split('=') for p in parts))\n\n        return cls({k: v for k, v in items if '+'+k in PROJ4_PARAMS.keys()})", "response": "Turn a PROJ. 4 string into a mapping of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_epsg(cls, code):\n        if int(code) <= 0:\n            raise ValueError(\"EPSG codes are positive integers\")\n        return cls({'init': \"epsg:{}\".format(code), 'no_defs': True})", "response": "Returns an EPSG - like mapping object given an integer code."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_string(self):\n        def filt(x):\n            return '+'+x[0] in PROJ4_PARAMS.keys() and x[1] is not False\n\n        items = []\n        for k, v in sorted(filter(filt, self.items())):\n            items.append(\n                \"+\" + \"=\".join(\n                    map(str, filter(\n                        lambda y: (y or y == 0) and y is not True, (k, v)))))\n        return \" \".join(items)", "response": "Converts a CRS dict into a PROJ. 4 string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls CK and return the result of CK.", "response": "def call_ck(i):\n\n    \"\"\"\n    Input:  {\n              Input for CK\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              (stdout)     - stdout, if available\n              (stderr)     - stderr, if available\n              (std)        - stdout+stderr\n            }\n    \"\"\"\n\n    import subprocess\n    import re\n\n    # Check action\n    action=i.get('action','')\n    if action=='':\n       return {'return':1, 'error':'action is not defined'}\n\n    # Check that no special characters, otherwise can run any command from CMD\n    if not re.match('^[A-Za-z0-9-_]*$', action):\n       return {'return':1, 'error':'action contains illegal characters'}\n\n    # Generate tmp file\n    fd, fn=tempfile.mkstemp(suffix='.tmp', prefix='ck-') # suffix is important - CK will delete such file!\n    os.close(fd)\n\n    dc=i.get('detach_console','')\n    if dc=='yes': i['out']='con' # If detach, output as console\n\n    # Prepare dummy output\n    rr={'return':0}\n    rr['stdout']=''\n    rr['stderr']=''\n\n    # Save json to temporay file\n    rx=ck.save_json_to_file({'json_file':fn, 'dict':i})\n    if rx['return']>0: return rx\n\n    # Prepare command line\n    cmd='ck '+action+' @'+fn\n    if dc=='yes':\n       # Check platform\n       rx=ck.get_os_ck({})\n       if rx['return']>0: return rx\n\n       plat=rx['platform']\n\n       dci=ck.cfg.get('detached_console',{}).get(plat,{})\n\n       dcmd=dci.get('cmd','')\n       if dcmd=='':\n          return {'return':1, 'error':'detached console is requested but cmd is not defined in kernel configuration'}\n\n       dcmd=dcmd.replace('$#cmd#$', cmd)\n\n       if dci.get('use_create_new_console_flag','')=='yes':\n          process=subprocess.Popen(dcmd, stdin=None, stdout=None, stderr=None, shell=True, close_fds=True, creationflags=subprocess.CREATE_NEW_CONSOLE)\n       else:\n          # Will need to do the forking\n          try:\n             pid=os.fork()\n          except OSError as e:\n             return {'return':1, 'error':'forking detached console failed ('+format(e)+')'}\n\n          if pid==0:\n             os.setsid()\n\n             pid=os.fork()\n             if pid!=0: os._exit(0)\n\n             try:\n                 maxfd=os.sysconf(\"SC_OPEN_MAX\")\n             except (AttributeError, ValueError):\n                 maxfd=1024\n\n             for fd in range(maxfd):\n                 try:\n                    os.close(fd)\n                 except OSError:\n                    pass\n\n             os.open('/dev/null', os.O_RDWR)\n             os.dup2(0, 1)\n             os.dup2(0, 2)\n\n             # Normally child process\n             process=os.system(dcmd)\n             os._exit(0)\n\n       stdout=ck.cfg.get('detached_console_html', 'Console was detached ...')\n       stderr=''\n    else:\n       process=subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n       stdout,stderr=process.communicate()\n\n    try: stdout=stdout.decode('utf8')\n    except Exception as e: pass\n    try: stderr=stderr.decode('utf8')\n    except Exception as e: pass\n\n    rr['std']=stdout+stderr\n    rr['stdout']=stdout\n    rr['stderr']=stderr\n\n    return rr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef web_err(i):\n\n    \"\"\"\n    Input:  {\n              http - http object\n              type - content type\n              bin  - bytes to output\n            }\n\n    Output: {\n              return - 0\n            }\n    \"\"\"\n\n    http=i['http']\n    tp=i['type']\n    bin=i['bin']\n\n    try: bin=bin.decode('utf-8')\n    except Exception as e: pass\n\n    if tp=='json':\n       rx=ck.dumps_json({'dict':{'return':1, 'error':bin}})\n       if rx['return']>0:\n          bin2=rx['error'].encode('utf8')\n       else:\n          bin2=rx['string'].encode('utf-8')\n    elif tp=='con':\n       bin2=bin.encode('utf8')\n    else:\n       bin2=b'<html><body><pre>'+bin.encode('utf8')+b'</pre></body></html>'\n\n    i['bin']=bin2\n    return web_out(i)", "response": "This function is used to output a single object in a web server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add(i):\n    # Check if global writing is allowed\n    r=ck.check_writing({})\n    if r['return']>0: return r\n\n    o=i.get('out','')\n\n    a=i.get('repo_uoa','')\n    d=i.get('data_uoa','')\n    di=i.get('data_uid','')\n    dn=i.get('data_name','')\n\n    stable=i.get('stable','')\n    version=i.get('version','')\n    if stable=='yes': version='stable'\n    branch=i.get('branch','')\n    checkout=i.get('checkout','')\n\n    cr=i.get('current_repos',[])\n\n    rdeps=i.get('repo_deps',[])\n\n    quiet=i.get('quiet','')\n\n    overwrite=i.get('overwrite','')\n\n    remote=i.get('remote','')\n    rruoa=i.get('remote_repo_uoa','')\n    shared=i.get('shared','')\n    if shared=='yes': shared='git'\n\n    share=i.get('share','')\n    if share=='yes' and shared=='': shared='git'\n\n    ptr=''\n    zp=i.get('zip','')\n    gz=i.get('gitzip','')\n\n    if zp!='':\n       if zp.startswith('~'):\n          from os.path import expanduser\n          home = expanduser(\"~\")\n\n          zp=os.path.abspath(home+os.sep+zp[1:])\n\n       quiet='yes'\n\n       if d=='':\n          # Try to get data UOA\n          if not os.path.isfile(zp):\n             return {'return':1, 'error':'zip file not found'}\n\n          # Try to get .ckr.json\n          import zipfile\n\n          try:\n             with zipfile.ZipFile(zp) as z:\n                zip_has_git=False\n                if '.git/HEAD' in z.namelist():\n                   zip_has_git=True\n\n                if ck.cfg['repo_file'] in z.namelist():\n                   x=z.open(ck.cfg['repo_file'])\n                   y=x.read()\n\n                   r=ck.convert_json_str_to_dict({'str':y, 'skip_quote_replacement':'yes'})\n                   if r['return']>0: return r\n                   yd=r['dict']\n\n                   d=yd.get('data_uoa','')\n                   di=yd.get('data_uid','')\n                   dn=yd.get('data_name','')\n\n                   zip_shared=yd.get('dict',{}).get('shared','')\n                   if zip_shared!='':\n                      if zip_shared!='git' or zip_has_git:\n                         shared=yd['dict']['shared']\n                         share='yes'\n\n                   x.close()\n             z.close()\n          except Exception as e:\n             return {'return':1, 'error':'problem reading zip file ('+format(e)+')'}\n\n          if d=='':\n             x1=os.path.basename(zp)\n             d=os.path.splitext(x1)[0]\n             if d.startswith('ckr-'):\n                d=d[4:]\n\n             if o=='con':\n                ck.out('Auto-detected repo name from zip filename: '+d)\n\n    if gz=='yes':\n       zp=ck.cfg['default_shared_repo_url']+'/'+d+'/archive/master.zip'\n       ptr=d+'-master/'\n       quiet='yes'\n\n    rx=form_url(i)\n    if rx['return']>0: return rx\n    url=rx['url']\n\n    sync=i.get('sync','')\n    df=i.get('default','')\n\n    eaw=i.get('allow_writing','')\n\n    udp=i.get('use_default_path','yes')\n    ucp=i.get('use_current_path','')\n    if ucp=='yes': udp=''\n\n    # Get repo path (unless 'here' later)\n    px=i.get('path','')\n\n    # Check if import\n    imp=i.get('import','')\n    if imp=='yes': \n       if px=='': i['here']='yes'\n\n    # Get 'here' path\n    if i.get('here','')=='yes': px=os.getcwd()\n    p=px\n\n    if imp=='yes': \n       py=os.path.join(p,ck.cfg['repo_file'])\n       if os.path.isfile(py):\n          r=ck.load_json_file({'json_file':py})\n          if r['return']>0: return r\n          dc=r['dict']\n\n          d=dc.get('data_uoa','')\n          di=dc.get('data_uid','')\n          dn=dc.get('data_name','')\n\n    if p=='' and udp=='yes': p=os.path.join(ck.work['dir_repos'], d)\n\n    # Normalize path\n    p=os.path.normpath(p)\n\n    # If console mode, first, check if shared (GIT, etc)\n    if o=='con':\n       # Asking for alias\n       if df!='yes' and (d=='' or ck.is_uid(d)):\n          r=ck.inp({'text':'Enter an alias for this repository (or Enter to skip it): '})\n          d=r['string']\n          if d=='': d=di\n          if d=='':\n             r=ck.gen_uid({})\n             if r['return']>0: return r\n             di=r['data_uid']\n             d=di\n\n       # Asking for a user-friendly name\n       if df!='yes' and dn=='':\n          if quiet!='yes':\n             r=ck.inp({'text':'Enter a user-friendly name of this repository (or Enter to reuse alias): '})\n             dn=r['string']\n          if dn=='': dn=d\n\n       # Asking if remote\n       if df!='yes' and remote=='':\n          if quiet!='yes':\n             r=ck.inp({'text':'Is this repository a remote CK web service (y/N)? '})\n             remote=r['string'].lower()\n          if remote=='yes' or remote=='y': remote='yes'\n          else: remote=''\n\n       # Asking for a user-friendly name\n       if px=='' and df!='yes' and remote!='yes' and udp=='':\n          if quiet!='yes':\n             r=ck.inp({'text':'Would you like to create repo in the directory from CK_REPOS variable (Y/n): '})\n             x=r['string'].lower()\n             if x=='' or x=='yes' or x=='y':\n                p=os.path.join(ck.work['dir_repos'], d)\n\n       # Asking for remote url\n       if df!='yes' and remote=='yes' and url=='':\n          if quiet!='yes':\n             r=ck.inp({'text':'Enter URL of remote CK repo (example: http://localhost:3344/ck?): '})\n             url=r['string'].lower()\n          if url=='':\n             return {'return':1, 'error':'URL is empty'}\n\n       # Asking for remote repo UOA\n       if df!='yes' and remote=='yes' and rruoa=='':\n          if quiet!='yes':\n             r=ck.inp({'text':'Enter remote repo UOA or Enter to skip: '})\n             rruoa=r['string'].lower()\n\n       # Asking for shared\n       if remote=='' and shared=='' and share=='':\n          if quiet!='yes':\n             r=ck.inp({'text':'Is this repository shared via GIT (y/N)? '})\n             x=r['string'].lower()\n             if x=='yes' or x=='y':\n                share='yes'\n\n       if share=='yes' and shared=='': shared='git'\n\n       # Check additional parameters if git\n       ghu=i.get('githubuser','')\n       if shared=='git' and url=='':\n\n          if ghu!='': \n             durl=ck.cfg.get('github_repo_url','')\n             if not durl.endswith('/'): durl+='/'\n             durl+=ghu\n          else: durl=ck.cfg.get('default_shared_repo_url','')\n\n          if not durl.endswith('/'): durl+='/'\n          durl+=d\n#          if durl.startswith('http://') or durl.startswith('https://'):\n#             durl+='.git'\n\n          if quiet!='yes':\n             s='Enter URL of GIT repo '\n             if d=='': s+='(for example, https://github.com/ctuning/ck-analytics.git)'\n             else:     s+='(or Enter for '+durl+')'\n             r=ck.inp({'text': s+': '})\n             url=r['string'].lower()\n          if url=='': url=durl\n\n       # Check additional parameters if git\n       if shared=='git' and sync=='':\n          if quiet!='yes':\n             r=ck.inp({'text': 'Would you like to sync repo each time after writing to it (y/N)?: '})\n             x=r['string'].lower()\n             if x=='yes' or x=='y':\n                sync='yes'\n\n       # Asking for a user-friendly name\n       if df!='yes' and remote!='yes' and eaw=='':\n          if quiet!='yes':\n             r=ck.inp({'text':'Would you like to explicitly allow writing to this repository in case kernel disables all writing (y/N): '})\n             x=r['string'].lower()\n             if x=='yes' or x=='y':\n                eaw='yes'\n\n       # Check if add more deps\n       if quiet!='yes':\n          r=add_more_deps({})\n          if r['return']>0: return r\n\n          rdeps1=r['repo_deps']\n          for q in rdeps1:\n              rdeps.append(q)\n\n    # Check if already registered (if not remote)\n    if remote!='yes':\n       r=ck.find_repo_by_path({'path':p})\n       if r['return']>0 and r['return']!=16: \n          return r\n\n    # Check if repository is already registered with this path\n    r=ck.find_repo_by_path({'path':p})\n    if r['return']==0:\n       return {'return':16, 'error':'repository with a given path is already registered in CK'}\n    elif r['return']!=16: \n       return r\n\n    # Prepare local description file\n    py=os.path.join(p,ck.cfg['repo_file'])\n\n    # Create dummy if doesn't exist\n    if remote!='yes' and not os.path.isdir(p):\n       os.makedirs(p)\n\n    # If zip, get (download) and unzip file ...\n    if zp!='':\n       rz=get_and_unzip_archive({'zip':zp, 'path':p, 'path_to_remove':ptr, 'overwrite':overwrite, 'out':o})\n       if rz['return']>0: return rz\n\n       py = os.path.join(p, ck.cfg['repo_file'])\n       if os.path.isfile(py):\n           r = ck.load_json_file({'json_file': py})\n           if r['return'] > 0: return r\n           dc = r['dict']\n\n           d = dc.get('data_uoa', '')\n           di = dc.get('data_uid', '')\n           dn = dc.get('data_name', '')\n\n    # If git, clone repo\n    repo_had_local=True\n    dd={}\n    if remote!='yes' and shared=='git' and zp=='':\n       r=pull({'path':p, 'type':shared, 'url':url, 'clone':'yes', 'git':i.get('git',''), 'out':o})\n       if r['return']>0: return r\n\n       # Check if there is a local repo description\n       if os.path.isfile(py):\n          r=ck.load_json_file({'json_file':py})\n          if r['return']>0: return r\n          dc=r['dict']\n          ddc=dc.get('dict',{})\n          dd.update(ddc)\n\n          xd=dc.get('data_uoa','')\n          xdi=dc.get('data_uid','')\n          xdn=dc.get('data_name','')\n\n          if o=='con':\n             ck.out('Cloned repository has the following info:')\n             ck.out(' UID                = '+xdi)\n             ck.out(' UOA                = '+xd)\n             ck.out(' User friendly name = '+xdn)\n             ck.out('')\n          if i.get('skip_reusing_remote_info','')!='yes':\n             d=xd\n             di=xdi\n             dn=xdn\n       else:\n          repo_had_local=False\n\n    # Prepare meta description\n    if df=='yes': \n       dd['default']='yes'\n    if remote=='yes': \n       dd['remote']='yes'\n       if rruoa!='': \n          dd['remote_repo_uoa']=rruoa\n    if shared!='':\n       dd['shared']=shared\n       if sync!='': \n          dd['sync']=sync\n    if url!='': \n       dd['url']=url\n    if remote!='yes': \n       dd['path']=p\n    if eaw=='yes':\n       dd['allow_writing']='yes'\n    if len(rdeps)>0:\n       dd['repo_deps']=rdeps\n\n    # Check if need to describe for Artifact Evaluation\n    if i.get('describe','')=='yes':\n       r=describe({'dict':dd})\n       if r['return']>0: return r\n\n    # If not default, go to common core function to create entry\n    if df!='yes':\n       ii={'module_uoa':work['self_module_uoa'],\n           'action':'add',\n           'data_uoa':d,\n           'dict':dd,\n           'common_func':'yes'}\n       if a!='': ii['repo_uoa']=a\n       if di!='': ii['data_uid']=di\n       if dn!='': ii['data_name']=dn\n       rx=ck.access(ii)\n       if rx['return']>0: return rx\n    else:\n       # Load default repo and prepare return\n       ii={'module_uoa':work['self_module_uoa'],\n           'action':'load',\n           'data_uoa':'default',\n           'common_func':'yes'}\n       rx=ck.access(ii)\n    px=rx['path']\n    dx=rx['data_uid']\n    alias=rx['data_alias']\n\n    # Update repo cache if not default local\n    dz={'data_uoa':d, 'data_uid':dx, 'data_alias':alias, 'path_to_repo_desc':px, 'data_name':dn, 'dict':dd}\n\n    if df!='yes':\n       r=ck.reload_repo_cache({}) # Ignore errors\n       ck.cache_repo_uoa[d]=dx\n       ck.cache_repo_info[dx]=dz\n       r=ck.save_repo_cache({})\n       if r['return']>0: return r\n\n    # Record local info of the repo (just in case)\n    if remote!='yes':\n       if 'path_to_repo_desc' in dz: del (dz['path_to_repo_desc'])        # Avoid recording some local info\n       if dz.get('dict',{}).get('path','')!='': del (dz['dict']['path'])  # Avoid recording some local info\n\n       if not os.path.isfile(py):\n          ry=ck.save_json_to_file({'json_file':py, 'dict':dz})\n          if ry['return']>0: return ry\n\n       # If sync (or pulled repo did not have local description), add it ...\n       if sync=='yes' or (shared=='git' and not repo_had_local):\n          ppp=os.getcwd()\n\n          os.chdir(p)\n          ss=ck.cfg['repo_types'][shared]['add'].replace('$#path#$', px).replace('$#files#$', ck.cfg['repo_file'])\n          os.system(ss)\n\n          os.chdir(ppp)\n\n    # If console mode, print various info\n    if o=='con':\n       ck.out('')\n       ck.out('CK repository successfully registered!')\n       ck.out('')\n\n       if df!='yes':\n          ck.out('CK repo description path = '+px)\n          ck.out('CK repo UID              = '+dx)\n\n    # Recache repos otherwise may be problems with deps\n    if o=='con':\n       ck.out('')\n       ck.out('Recaching repos to speed up access ...')\n       ck.out('')\n    r=recache({'out':o})\n    if r['return']>0: return r\n\n    # Check deps\n    if o=='con':\n       ck.out('  ========================================')\n       ck.out('  Checking dependencies on other repos ...')\n       ck.out('')\n\n    how='pull'\n    if gz=='yes': how='add'\n    r=deps({'path':p,\n            'current_path':cr,\n            'how':how,\n            'version':version,\n            'branch':branch,\n            'checkout':checkout,\n            'out':o})\n    if r['return']>0: return r\n\n    # Print if default\n    if df=='yes' and o=='con':\n       ck.out('')\n       ck.out('Please, do not forget to add path to this repository to CK_LOCAL_REPO environment variable:')\n       ck.out('')\n       ck.out('  Linux: export CK_LOCAL_REPO='+p)\n       ck.out('  Windows: set CK_LOCAL_REPO='+p)\n\n    return rx", "response": "Add a new entry to the CK cache."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the info of a repository", "response": "def update(i):\n    \"\"\"\n    Update repository info\n\n    Input:  {\n              data_uoa                   - data UOA of the repo\n\n              (shared)                   - if not remote and =='git', shared through GIT\n\n              (url)                      - if type=='git', URL of remote repository or git repository\n              (hostname)                 - if !='', automatically form url above (add http:// + /ck?)\n              (port)                     - if !='', automatically add to url above\n              (hostext)                  - if !='', add to the end of above URL instead of '/ck?' -\n                                           useful when CK server is accessed via Apache2, IIS, Nginx or other web servers\n\n              (sync)                     - if 'yes' and type=='git', sync repo after each write operation\n              (allow_writing)            - if 'yes', allow writing \n                                           (useful when kernel is set to allow writing only to such repositories)\n\n              (repo_deps)                - dict with dependencies on other shared repositories with following keys:\n                                             \"repo_uoa\"\n                                             (\"repo_uid\") - specific UID (version) of a repo\n                                             (\"repo_url\") - URL of the shared repository (if not from github.com/ctuning)\n\n              (update)                   - if 'yes', force updating\n\n              (describe)                 - describe repository for Artifact Evaluation (see http://cTuning.org/ae)\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n            }\n\n    \"\"\"\n\n    # Check if global writing is allowed\n    r=ck.check_writing({})\n    if r['return']>0: return r\n\n    o=i.get('out','')\n\n    duoa=i.get('data_uoa','')\n\n    remote=i.get('remote','')\n    rruoa=i.get('remote_repo_uoa','')\n    shared=i.get('shared','')\n    sync=i.get('sync','')\n\n    rx=form_url(i)\n    if rx['return']>0: return rx\n    url=rx['url']\n\n    rdeps=i.get('repo_deps',[])\n\n    eaw=i.get('allow_writing','')\n\n    # Get configuration (not from Cache - can be outdated info!)\n#    r=ck.load_repo_info_from_cache({'repo_uoa':duoa})\n    r=ck.access({'action':'load',\n                 'module_uoa':work['self_module_uoa'],\n                 'data_uoa':duoa})\n    if r['return']>0: return r\n\n    p=r.get('dict',{}).get('path','')\n\n    dn=r.get('data_name','')\n    d=r['dict']\n\n    remote=d.get('remote','')\n\n    changed=False\n\n    # Check user-friendly name\n    if dn!='':\n       ck.out('Current user-friendly name of this repository: '+dn)\n       ck.out('')\n\n    r=ck.inp({'text':'Enter a user-friendly name of this repository (or Enter to keep old value): '})\n    x=r['string']\n    if x!='': \n       dn=x\n       changed=True\n\n    # If remote, update URL\n    shared=d.get('shared','')\n    if remote=='yes':\n       url=d.get('url','')\n       ck.out('Repository is remote ...')\n       ck.out('')\n       ck.out('Current URL: '+url)\n       ck.out('')\n       rx=ck.inp({'text':'Enter new URL (or Enter to leave old one): '})\n       x=rx['string']\n       if x!='': \n          d['url']=x\n          changed=True\n    elif shared!='':\n       url=d.get('url','')\n       ck.out('Repository is shared ...')\n       ck.out('')\n       ck.out('Current URL: '+url)\n\n       if shared=='git':\n          sync=d.get('sync','')\n          ck.out('')\n          if sync!='':\n             ck.out('Current sync setting: '+sync)\n          r=ck.inp({'text': 'Would you like to sync repo each time after writing to it (y/N)?: '})\n          x=r['string'].lower()\n          if x=='yes' or x=='y':\n             d['sync']='yes'\n             changed=True\n\n    # Asking about forbidding explicit writing to this repository\n    if remote!='yes' and eaw=='':\n       if eaw=='': eaw=d.get('allow_writing','')\n       ck.out('')\n       if eaw!='':\n          ck.out('Current \"allow writing\" setting: '+eaw)\n\n       r=ck.inp({'text':'Would you like to allow explicit writing to this repository when kernel disables all writing (y/N): '})\n       x=r['string'].lower()\n       if x=='yes' or x=='y':\n          d['allow_writing']='yes'\n          changed=True\n\n    # Check if explicit deps\n    if len(rdeps)>0:\n       if 'repo_deps' not in d: d['repo_deps']=rdeps\n       else: \n          for q in rdeps:\n              d['repo_deps'].append(q)\n       changed=True\n\n    # Print deps\n    rdeps=d.get('repo_deps',[])\n    if len(rdeps)>0:\n       ck.out('')\n       ck.out('Current dependencies on other repositories:')\n       r=print_deps({'repo_deps':rdeps, 'out':o, 'out_prefix':'  '})\n       if r['return']>0: return r\n       ck.out('')\n\n    # Check if add more deps\n    r=add_more_deps({})\n    if r['return']>0: return r\n\n    rdeps1=r['repo_deps']\n    if len(rdeps1)>0:\n       if 'repo_deps' not in d: d['repo_deps']=rdeps1\n       else: \n          for q in rdeps1:\n              d['repo_deps'].append(q)\n       changed=True\n\n    # Check if need to describe for Artifact Evaluation\n    if i.get('describe','')=='yes':\n       r=describe({'dict':d})\n       if r['return']>0: return r\n       changed=True\n\n    # Write if changed\n    if changed or i.get('update','')=='yes':\n       if o=='con':\n          ck.out('')\n          ck.out('Updating repo info ...')\n\n       rx=ck.access({'action':'update',\n                     'module_uoa':ck.cfg['repo_name'],\n                     'data_uoa':duoa,\n                     'data_name':dn,\n                     'dict':d,\n                     'common_func':'yes',\n                     'overwrite':'yes'})\n       if rx['return']>0: return rx\n\n       # Recaching\n       if o=='con':\n          ck.out('')\n          ck.out('Recaching repos to speed up access ...')\n          ck.out('')\n       r=recache({'out':o})\n       if r['return']>0: return r\n\n       # Updating local repository description\n       if remote!='yes':\n          r=ck.load_repo_info_from_cache({'repo_uoa':duoa})\n          if r['return']>0: return r\n\n          del(r['return'])\n          if 'path_to_repo_desc' in r: del (r['path_to_repo_desc'])        # Avoid recording some local info\n          if r.get('dict',{}).get('path','')!='': del (r['dict']['path'])  # Avoid recording some local info\n\n          py=os.path.join(p, ck.cfg['repo_file'])\n\n          ry=ck.save_json_to_file({'json_file':py, 'dict':r})\n          if ry['return']>0: return ry\n\n    return {'return':0}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pull(i):\n\n    o=i.get('out','')\n\n    xrecache=False\n\n    pp=[]\n    px=i.get('path','')\n    t=i.get('type','')\n    url=i.get('url','')\n\n    stable=i.get('stable','')\n    version=i.get('version','')\n    if stable=='yes': version='stable'\n    branch=i.get('branch','')\n    checkout=i.get('checkout','')\n\n    ip=i.get('ignore_pull','')\n\n    cr=i.get('current_repos',[])\n\n    tt='pull'\n    if i.get('clone','')=='yes': tt='clone'\n\n    if px!='': \n       pp.append({'path':px, 'type':t, 'url':url})\n\n    uoa=i.get('data_uoa','')\n\n    cids=i.get('cids',[])\n    if len(cids)>0 and uoa=='': \n       uoa=cids[0]\n\n    # If url is not empty and uoa is empty, get name from URL:\n    if url!='' and uoa=='' and px=='':\n       ix=url.rfind('/')\n       if ix>0:\n          uoa=url[ix+1:]\n          if uoa.endswith('.git'):\n             uoa=uoa[:-4]\n\n          i['data_uoa']=uoa\n\n    if uoa=='' and len(pp)==0 and url=='': \n       uoa='*'\n\n    if uoa!='':\n       if uoa.find('*')>=0 or uoa.find('?')>=0:\n          r=ck.list_data({'module_uoa':work['self_module_uoa'], 'data_uoa':uoa})\n          if r['return']>0: return r\n\n          lst=r['lst']\n          for q in lst:\n              # Loading repo\n              r=ck.access({'action':'load',\n                           'module_uoa':work['self_module_uoa'],\n                           'data_uoa':q['data_uoa'],\n                           'common':'yes'})\n              if r['return']>0: return r\n              d=r['dict']\n              t=d.get('shared','')\n              duoa=r['data_uoa']\n\n              if d.get('recache','')=='yes':\n                 xrecache=True\n\n              if t!='':\n                 p=d.get('path','')\n                 url=d.get('url','')\n                 checkouts=d.get('checkouts',{})\n                 pp.append({'path':p, 'type':t, 'url':url, 'data_uoa':duoa, 'checkouts':checkouts})\n       else:\n          # Loading repo\n          r=ck.access({'action':'load',\n                       'module_uoa':work['self_module_uoa'],\n                       'data_uoa':uoa,\n                       'common':'yes'})\n          if r['return']>0: \n             if r['return']==16:\n                # If not found, try to add from GIT\n\n                i['action']='add'\n                i['shared']='yes'\n                x=i.get('quiet','')\n                if x=='': x='yes'\n                i['quiet']=x\n                i['current_repos']=cr\n\n                return add(i)\n             else:\n                return r\n\n          d=r['dict']\n          duoa=r['data_uoa']\n\n          if d.get('recache','')=='yes':\n             xrecache=True\n\n          p=d['path']\n          t=d.get('shared','')\n          url=d.get('url','')\n          checkouts=d.get('checkouts',{})\n\n          pp.append({'path':p, 'type':t, 'url':url, 'data_uoa':duoa, 'checkouts':checkouts})\n\n    # Updating ...\n    for q in pp:\n        p=q.get('path','')\n        duoa=q.get('data_uoa','')\n        t=q.get('type','')\n        url=q.get('url','')\n\n        # Semi hack (useful for Anton)\n        if i.get('git','')=='yes':\n           url=url.replace('https://','git@')\n\n           j=url.find('/')\n           if j>0:\n              url=url[:j]+':'+url[j+1:]\n\n           url+='.git'\n\n        if o=='con' and tt!='clone':\n           ck.out('******************************************************************')\n           ck.out('Updating repo \"'+duoa+'\" ...')\n           ck.out('')\n           ck.out('  Local path: '+p)\n           ck.out('  URL:        '+url)\n\n        if t=='git':\n           # Check if git is installed\n           rq=ck.gen_tmp_file({})\n           if rq['return']>0: return rq\n           xfn=rq['file_name']\n\n           os.system('git --version > '+xfn)\n\n           rq=ck.load_text_file({'text_file':xfn, \n                                 'delete_after_read':'yes'})\n           xs=''\n           if rq['return']==0:\n              xs=rq['string'].strip()\n\n           if xs.find(' version ')<0:\n              return{'return':1, 'error':'git command line client is not found - please, install it or download repo as zip'}\n\n           # Continue\n           try:\n              px=os.getcwd()\n           except OSError:\n              from os.path import expanduser\n              px=expanduser(\"~\")\n\n           if not os.path.isdir(p):\n              os.makedirs(p)\n\n           if o=='con':\n              ck.out('')\n              ck.out('  cd '+p)\n           os.chdir(p)\n\n           r=0\n           if ip!='yes':\n              s=ck.cfg['repo_types'][t][tt].replace('$#url#$', url).replace('$#path#$', p)\n\n              if o=='con':\n                 ck.out('  '+s)\n                 ck.out('')\n\n              r=os.system(s)\n\n              if o=='con': \n                 ck.out('')\n\n           os.chdir(px) # Restore path\n\n           if r>0:\n              if o=='con':\n                 ck.out('')\n                 ck.out(' WARNING: repository update likely failed OR IN A DIFFERENT BRANCH/CHECKOUT (git exit code: '+str(r)+')')\n                 ck.out('')\n                 rx=ck.inp({'text': 'Would you like to continue (Y/n)?: '})\n                 x=rx['string'].lower()\n                 if x=='n' or x=='no':\n                    return {'return':1, 'error':'repository update likely failed - exit code '+str(r)}\n              else:\n                 return {'return':1, 'error':'repository update likely failed - exit code '+str(r)}\n        else:\n           if o=='con':\n              ck.out('CK warning: this repository is not shared!')\n\n        # Check deps\n        if tt!='clone': # clone is done in add ...\n           if o=='con':\n              ck.out('  ========================================')\n              ck.out('  Checking dependencies on other repos ...')\n              ck.out('')\n\n           r=deps({'path':p,\n                   'current_path':cr,\n                   'how':'pull',\n                   'version':version,\n                   'branch':branch,\n                   'checkout':checkout,\n                   'out':o})\n           if r['return']>0: return r\n\n    # Re-caching ...\n    if xrecache:\n       if o=='con':\n          ck.out('  ==============================================')\n          ck.out('  At least one repository requires recaching ...')\n          ck.out('')\n\n       r=recache({'out':o})\n       if r['return']>0: return r\n\n    return {'return':0}", "response": "Pull a branch from another branch"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npushes an entry into the order of the entries in the order of the entries in the order they were found.", "response": "def push(i):\n    \"\"\"\n    Input:  {\n              (path)  - repo UOA (where to create entry)\n              (type)  - type\n              (url)   - URL\n\n                or\n\n              (data_uoa)  - repo UOA\n\n              (clone) - if 'yes', clone repo instead of update\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n            }\n\n    \"\"\"\n\n    o=i.get('out','')\n\n    pp=[]\n    px=i.get('path','')\n    t=i.get('type','')\n    url=i.get('url','')\n\n    if px!='': \n       pp.append({'path':px, 'type':t, 'url':url})\n\n    uoa=i.get('data_uoa','')\n    cids=i.get('cids',[])\n    if len(cids)>0: uoa=cids[0]\n\n    if uoa=='' and len(pp)==0: uoa='*'\n\n    if uoa!='':\n       if uoa.find('*')>=0 or uoa.find('?')>=0:\n          r=ck.list_data({'module_uoa':work['self_module_uoa'], 'data_uoa':uoa})\n          if r['return']>0: return r\n\n          lst=r['lst']\n          for q in lst:\n              # Loading repo\n              r=ck.access({'action':'load',\n                           'module_uoa':work['self_module_uoa'],\n                           'data_uoa':q['data_uoa'],\n                           'common':'yes'})\n              if r['return']>0: return r\n              d=r['dict']\n              t=d.get('shared','')\n              if t!='':\n                 p=d.get('path','')\n                 url=d.get('url','')\n                 pp.append({'path':p, 'type':t, 'url':url})\n       else:\n          # Loading repo\n          r=ck.access({'action':'load',\n                       'module_uoa':work['self_module_uoa'],\n                       'data_uoa':uoa,\n                       'common':'yes'})\n          if r['return']>0: return r\n          d=r['dict']\n\n          p=d['path']\n          t=d.get('shared','')\n          url=d.get('url','')\n\n          pp.append({'path':p, 'type':t, 'url':url})\n\n    # Pushing ...\n    for q in pp:\n        p=q.get('path','')\n        t=q.get('type','')\n        url=q.get('url','')\n\n        if o=='con':\n           ck.out('')\n           ck.out('Trying to commit and push '+p+' ...')\n\n        if t=='git':\n           px=os.getcwd()\n\n           if not os.path.isdir(p):\n              return {'return':1, 'error':'local path to repository is not found'}\n\n           if o=='con':\n              ck.out('')\n              ck.out('cd '+p+' ...')\n\n           os.chdir(p)\n\n           s=ck.cfg['repo_types'][t]['commit'].replace('$#url#$', url).replace('$#path#$', p)\n           if o=='con':\n              ck.out('')\n              ck.out('Executing command: '+s)\n              ck.out('')\n           r=os.system(s)\n\n           if o=='con': \n              ck.out('')\n\n           s=ck.cfg['repo_types'][t]['push'].replace('$#url#$', url).replace('$#path#$', p)\n           if o=='con':\n              ck.out('')\n              ck.out('Executing command: '+s)\n              ck.out('')\n           r=os.system(s)\n\n           if o=='con': \n              ck.out('')\n\n           os.chdir(px) # Restore path\n\n           if r>0:\n              if o=='con':\n                 ck.out('')\n                 ck.out(' WARNING: repository update likely failed - exit code '+str(r))\n                 ck.out('')\n                 rx=ck.inp({'text': 'Would you like to continue (Y/n)?: '})\n                 x=rx['string'].lower()\n                 if x=='n' or x=='no':\n                    return {'return':1, 'error':'repository update likely failed - exit code '+str(r)}\n              else:\n                 return {'return':1, 'error':'repository update likely failed - exit code '+str(r)}\n        else:\n           if o=='con':\n              ck.out('CK warning: this repository is not shared!')\n\n    return {'return':0}"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninput {} Output: { return - return code = 0, if successful > 0, if error (error) - error text if return > 0 }", "response": "def recache(i):\n    \"\"\"\n    Input:  {}\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n            }\n    \"\"\"\n\n    o=i.get('out','')\n\n    # Listing all repos\n    r=ck.access({'action':'list',\n                 'module_uoa':ck.cfg['repo_name']})\n    if r['return']>0: return r\n    l=r['lst']\n\n    cru={}\n    cri={}\n\n    # Processing repos\n    # We need 2 passes (if some repos such as remote ones are referenced inside new repos)\n\n    for ps in [0,1]:\n        for q in l:\n            if ps==0 or (ps==1 and q.get('processed','')!='yes'):\n               ruoa=q['repo_uoa']\n               muoa=q['module_uoa']\n               duoa=q['data_uoa']\n               duid=q['data_uid']\n\n               # First try to load from cache to check that not remote ...\n               remote=False\n               rx=ck.load_repo_info_from_cache({'repo_uoa':duoa})\n               if rx['return']==0: \n                  rd=rx.get('dict',{})\n                  if rd.get('remote','')=='yes':\n                     remote=True\n\n               if not remote:\n                  if duid==ck.cfg['repo_uid_default'] or duid==ck.cfg['repo_uid_local']:\n                     if o=='con':\n                        ck.out('Skipping repo '+duoa+' ...')\n                  else:\n                     if o=='con':\n                        ck.out('Processing repo '+duoa+' ...')\n\n                     # Repo dictionary (may be changing in .ckr.json)\n                     dt={}\n\n                     # Find real repo and get .ckr.json\n                     rx=ck.access({'action':'where',\n                                   'module_uoa':muoa,\n                                   'data_uoa':duoa})\n                     if rx['return']==0: \n                        pckr=os.path.join(rx['path'], ck.cfg['repo_file'])\n                        if os.path.isfile(pckr):\n                           rx=ck.load_json_file({'json_file':pckr})\n                           if rx['return']>0: return rx\n\n                           dt=rx['dict']['dict']\n\n                     # Load extra info repo (do not use repo, since may not exist in cache)\n                     rx=ck.access({'action':'load',\n                                   'module_uoa':muoa,\n                                   'data_uoa':duoa})\n                     if rx['return']>0: \n                        if ps==0:\n                           continue\n                        else:\n                           return rx\n\n                     if len(dt)==0:\n                        dt=rx['dict']\n                     else:\n                        if rx['dict'].get('path','')!='':\n                           dt['path']=rx['dict']['path']\n\n                     dname=rx['data_name']\n                     dalias=rx['data_alias']\n                     dp=rx['path']\n\n                     if duoa!=duid:\n                        cru[duoa]=duid\n\n                     dd={'dict':dt}\n\n                     dd['data_uid']=duid\n                     dd['data_uoa']=duoa\n                     dd['data_alias']=dalias\n                     dd['data_name']=dname\n                     dd['path_to_repo_desc']=dp\n\n                     cri[duid]=dd\n\n                  q['processed']='yes'\n\n        # Recording \n        ck.cache_repo_uoa=cru\n        ck.cache_repo_info=cri\n\n        rx=ck.save_repo_cache({})\n        if rx['return']>0: return rx\n\n        rx=ck.reload_repo_cache({'force':'yes'})\n        if rx['return']>0: return rx\n\n    if o=='con':\n       ck.out('')\n       ck.out('Repositories were successfully recached!')\n\n    return {'return':0}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rm(i):\n\n    # Check if global writing is allowed\n    r=ck.check_writing({})\n    if r['return']>0: return r\n\n    global cache_repo_uoa, cache_repo_info\n\n    ruoa=i.get('repo_uoa','')\n    uoa=i.get('data_uoa','')\n\n    o=i.get('out','')\n\n    if uoa=='': \n       return {'return':1, 'error':'UOA of the repository is not defined'}\n\n    wf=i.get('with_files','')\n    if wf=='': wf=i.get('all','')\n\n    force=i.get('force','')\n\n    r=ck.access({'action':'load',\n                 'repo_uoa':ruoa,\n                 'module_uoa':work['self_module_uoa'],\n                 'data_uoa':uoa,\n                 'common_func':'yes'})\n    if r['return']>0: return r\n    duid=r['data_uid']\n    duoa=r['data_uoa']\n\n    d=r['dict']\n    p=d.get('path','')\n\n    to_delete=True\n    if o=='con' and force!='yes':\n       r=ck.inp({'text':'Are you sure to delete information about repository '+duoa+' (y/N): '})\n       c=r['string'].lower()\n       if c!='yes' and c!='y': to_delete=False\n\n    if to_delete and o=='con' and force!='yes' and wf=='yes':\n       r=ck.inp({'text':'You indicated that you want to DELETE ALL ENTRIES IN THE REPOSITORY! Are you sure (y/N): '})\n       x=r['string'].lower()\n       if x!='yes' and x!='y': wf=''\n\n    if to_delete:\n       if o=='con': \n          ck.out('')\n          ck.out('Reloading repo cache ...')\n       r=ck.reload_repo_cache({}) # Ignore errors\n       if r['return']>0: return r\n\n       if o=='con': ck.out('Removing from cache ...')\n       if duoa in ck.cache_repo_uoa: del (ck.cache_repo_uoa[duoa])\n       if duid in ck.cache_repo_info: del (ck.cache_repo_info[duid])\n\n       if o=='con': ck.out('Rewriting repo cache ...')\n       r=ck.save_repo_cache({})\n       if r['return']>0: return r\n\n       if o=='con': ck.out('Removing entry ...')\n       r=ck.access({'action':'remove',\n                    'repo_uoa':ruoa,\n                    'module_uoa':work['self_module_uoa'],\n                    'data_uoa':uoa,\n                    'common_func':'yes'})\n       if r['return']>0: return r\n\n       if wf=='yes' and p!='':\n          if o=='con': ck.out('Removing entries from the repository ...')\n          import shutil\n          if os.path.isdir(p): \n             shutil.rmtree(p, onerror=ck.rm_read_only)\n\n       if o=='con': \n          ck.out('')\n          ck.out('Information about repository was removed successfully!')\n          if wf!='yes':\n             ck.out('Note: repository itself was not removed!')\n\n    return {'return':0}", "response": "Delete a single entry in a single repository"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef where(i):\n\n    o=i.get('out','')\n\n    duoa=i.get('data_uoa','')\n    r=ck.find_path_to_repo({'repo_uoa':duoa})\n    if r['return']>0: return r\n\n    d=r.get('dict',{})\n    url=d.get('url','')\n\n    p=r['path']\n    if p=='':\n       p=url\n\n    if o=='con':\n       ck.out(p)\n\n    return r", "response": "This function returns the path to the branch of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nzips the current file with the specified data_uoa into the archive_name.", "response": "def zip(i):\n    \"\"\"\n    Input:  {\n              data_uoa       - repo UOA\n\n              (archive_path) - if '' create inside repo path\n\n              (archive_name) - if !='' use it for zip name\n              (auto_name)    - if 'yes', generate name name from data_uoa: ckr-<repo_uoa>.zip\n              (bittorent)    - if 'yes', generate zip name for BitTorrent: ckr-<repo_uid>-YYYYMMDD.zip\n\n              (overwrite)    - if 'yes', overwrite zip file\n              (store)        - if 'yes', store files instead of packing\n\n\n              (data)         - CID allowing to add only these entries with pattern (can be from another archive)\n\n              (all)          - archive all files\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n            }\n\n    \"\"\"\n\n    o=i.get('out','')\n\n    duoa1=i.get('data_uoa','')\n    duid=''\n    path=''\n\n    if duoa1!='':\n       # Find path to repo\n       r=ck.find_path_to_repo({'repo_uoa':duoa1})\n       if r['return']>0: return r\n\n       duoa=r['repo_uoa']\n       duid=r['repo_uid']\n       path=r['path']\n\n    an=i.get('archive_name','')\n#    if an=='': an='ckr.zip'\n\n    if i.get('auto_name','')=='yes':\n       an='ckr'\n       if duoa!='': an+='-'+duoa\n       an+='.zip'\n    elif i.get('bittorrent','')=='yes':\n       import time\n       an='ckr-'\n       if duid!='': an+=duid\n       an+='-'+time.strftime('%Y%m%d')+'.zip'\n    elif an=='': \n       if duoa1=='': an='ckr.zip'\n       else: an='ckr-'+duoa1+'.zip'\n\n    ap=i.get('archive_path','')\n#    if ap=='': ap=path\n\n    pfn=os.path.join(ap, an)\n\n    if pfn.startswith('~'):\n       from os.path import expanduser\n       home = expanduser(\"~\")\n\n       pfn=os.path.abspath(home+os.sep+pfn[1:])\n\n    if os.path.isfile(pfn):\n       if i.get('overwrite','')=='yes':\n          os.remove(pfn)\n       else:\n          return {'return':1, 'error':'archive '+pfn+' already exists'}\n\n    if o=='con':\n       ck.out('Creating archive '+pfn+' - please wait, it may take some time ...')\n\n    # Check all files\n    ignore=[]\n    if i.get('all','')!='yes':\n       ignore=ck.cfg.get('ignore_directories_when_archive_repo',[])\n\n    # Prepare archive\n    import zipfile\n\n    zip_method=zipfile.ZIP_DEFLATED\n    if i.get('store','')=='yes':\n       zip_method=zipfile.ZIP_STORED\n\n    # Prepare list of files\n    fl={}\n\n    data=i.get('data','')\n    if data!='':\n       xpm={}\n\n       rx=ck.access({'action':'search',\n                     'cid':data})\n       if rx['return']>0: return rx\n       lst=rx['lst']\n       for q in lst:\n           pp=q['path']\n\n           pm1,pd=os.path.split(pp)\n           pr,pm=os.path.split(pm1)\n\n           if pr not in fl:\n              fl[pr]=[]\n\n           ry=ck.find_path_to_entry({'path':pr, 'data_uoa':pm})\n           if ry['return']>0: return ry\n           pm_uid=ry['data_uid']\n           pm_alias=ry['data_alias']\n\n           if pm_alias!='':\n              if pm_alias not in xpm:\n                 xpm[pm_alias]=pm_uid\n                 fl[pr].append(os.path.join(ck.cfg['subdir_ck_ext'], ck.cfg['file_alias_a'] + pm_alias))\n                 fl[pr].append(os.path.join(ck.cfg['subdir_ck_ext'], ck.cfg['file_alias_u'] + pm_uid))\n\n           ry=ck.find_path_to_entry({'path':pm1, 'data_uoa':pd})\n           if ry['return']>0: return ry\n           pd_uid=ry['data_uid']\n           pd_alias=ry['data_alias']\n\n           if pd_alias!='':\n              fl[pr].append(os.path.join(pm, ck.cfg['subdir_ck_ext'], ck.cfg['file_alias_a'] + pd_alias))\n              fl[pr].append(os.path.join(pm, ck.cfg['subdir_ck_ext'], ck.cfg['file_alias_u'] + pd_uid))\n\n           r=ck.list_all_files({'path':pp, 'all':'yes', 'ignore_names':ignore})\n           if r['return']>0: return r\n           for q in r['list']:\n               fx=os.path.join(pm,pd,q)\n               fl[pr].append(fx)\n\n    else:\n       r=ck.list_all_files({'path':path, 'all':'yes', 'ignore_names':ignore})\n       if r['return']>0: return r\n       fl[path]=r['list']\n\n    # Write archive\n    try:\n       f=open(pfn, 'wb')\n       z=zipfile.ZipFile(f, 'w', zip_method)\n       for path in fl:\n           fl1=fl[path]\n           for fn in fl1:\n               p1=os.path.join(path, fn)\n               z.write(p1, fn, zip_method)\n       z.close()\n       f.close()\n\n    except Exception as e:\n       return {'return':1, 'error':'failed to prepare archive ('+format(e)+')'}\n\n    return {'return':0}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unzip(i):\n\n    o=i.get('out','')\n\n    duoa=i.get('data_uoa','')\n    if duoa=='': duoa='local'\n\n    overwrite=i.get('overwrite','')\n\n    zip=i.get('zip','')\n    if zip=='': zip='ckr.zip'\n\n    # Find path to repo\n    r=ck.find_path_to_repo({'repo_uoa':duoa})\n    if r['return']>0: return r\n\n    path=r['path']\n\n    # Unzipping archive\n    rz=get_and_unzip_archive({'zip':zip, 'path':path, 'overwrite':overwrite, 'out':o})\n    if rz['return']>0: return rz\n\n    return {'return':0}", "response": "Unzips a single archive of a single object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_and_unzip_archive(i):\n\n    o=i.get('out','')\n\n    zp=i['zip']\n    p=i['path']\n    pr=i.get('path_to_remove','')\n\n    overwrite=i.get('overwrite','')\n\n    # If zip, get (download) and unzip file ...\n    rm_zip=False\n    if zp.find('://')>=0:\n       if o=='con':\n          ck.out('Downloading CK archive ('+zp+') - it may take some time ...')\n\n       rm_zip=True\n\n       # Generate tmp file\n       import tempfile\n       fd, fn=tempfile.mkstemp(suffix='.tmp', prefix='ck-') # suffix is important - CK will delete such file!\n       os.close(fd)\n       os.remove(fn)\n\n       # Import modules compatible with Python 2.x and 3.x\n       import urllib\n\n       try:\n          import urllib.request as urllib2\n       except:\n          import urllib2\n\n       # Prepare request\n       request = urllib2.Request(zp)\n\n       # Connect\n       try:\n          f=urllib2.urlopen(request)\n       except Exception as e:\n          return {'return':1, 'error':'Failed downloading CK archive ('+format(e)+')'}\n\n       import time\n       t = time.time()\n       t0 = t\n\n       chunk=32767\n       size=0\n\n       try:\n          fo=open(fn, 'wb')\n       except Exception as e:\n          return {'return':1, 'error':'problem opening file='+fn+' ('+format(e)+')'}\n\n       # Read from Internet\n       try:\n          while True:\n             s=f.read(chunk)\n             if not s: break\n             fo.write(s)\n\n             size+=len(s)\n\n             if o=='con' and (time.time()-t)>3:\n                speed='%.1d' % (size/(1000*(time.time()-t0)))\n                ck.out('  Downloaded '+str(int(size/1000))+' KB ('+speed+' KB/sec.) ...')\n                t=time.time()\n\n          f.close()\n       except Exception as e:\n          return {'return':1, 'error':'Failed downlading CK archive ('+format(e)+')'}\n\n       fo.close()\n\n       zp=fn\n\n    # Unzip if zip\n    if zp!='':\n       if o=='con':\n          ck.out('  Extracting to '+p+' ...')\n\n       import zipfile\n       f=open(zp,'rb')\n       z=zipfile.ZipFile(f)\n\n       # First, try to find .ckr.json\n       xprefix=''\n       for dx in z.namelist():\n           if pr!='' and dx.startswith(pr): dx=dx[len(pr):]\n           if dx.endswith(ck.cfg['repo_file']):\n              xprefix=dx[:-len(ck.cfg['repo_file'])]\n              break\n\n       # Second, extract files\n       for dx in z.namelist():\n           dx1=dx\n           if pr!=''and dx1.startswith(pr): dx1=dx1[len(pr):]\n           if xprefix!='' and dx1.startswith(xprefix): dx1=dx1[len(xprefix):]\n\n           if dx1!='':\n              pp=os.path.join(p,dx1)\n              if dx.endswith('/'): \n                 # create directory \n                 if not os.path.exists(pp): os.makedirs(pp)\n              else:\n                 # extract file\n                 ppd=os.path.dirname(pp)\n                 if not os.path.exists(ppd): os.makedirs(ppd)\n\n                 if os.path.isfile(pp) and overwrite!='yes':\n                    if o=='con':\n                       ck.out('File '+dx+' already exists in the entry - skipping ...')\n                 else:\n                    fo=open(pp, 'wb')\n                    fo.write(z.read(dx))\n                    fo.close()\n       f.close()\n\n       if rm_zip:\n          os.remove(zp)\n\n    return {'return':0}", "response": "Get and unzip CK archive."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a list of dependencies for a given repo UOA", "response": "def deps(i):\n    \"\"\"\n    Input:  {\n              (data_uoa)      - repo UOA\n                  or\n              (path)          - path to .cmr.json\n\n              (current_repos) - list of repos being updated (to avoid infinite recursion)\n\n              (how)           - 'pull' (default) or 'add'\n\n              (version)       - checkout version (default - stable)\n              (branch)        - git branch\n              (checkout)      - git checkout\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n            }\n\n    \"\"\"\n\n    import os\n\n    o=i.get('out','')\n\n    duoa=i.get('data_uoa','')\n\n    cr=i.get('current_repos',[])  # Added repos to avoid duplication/recursion\n\n    how=i.get('how','')\n    if how=='': how='pull'\n\n    p=i.get('path','')\n    if p=='':\n       r=ck.access({'action':'load',\n                    'module_uoa':work['self_module_uoa'],\n                    'data_uoa':duoa})\n       if r['return']>0: return r\n       dr=r['dict']\n       p=dr.get('path','')\n\n    if p!='':\n       # path to repo description\n       pp=os.path.join(p, ck.cfg['repo_file'])\n       if os.path.isfile(pp):\n          r=ck.load_json_file({'json_file':pp})\n          if r['return']>0: return r\n\n          d=r['dict']\n\n          # Check checkouts\n          version=i.get('version','')\n          branch=i.get('branch','')\n          checkout=i.get('checkout','')\n\n          if version!='':\n             cx=d.get('dict',{}).get('checkouts',{}).get(version,{})\n             branch=cx.get('branch','')\n             checkout=cx.get('checkout','')\n\n          ppp=os.getcwd()\n\n          os.chdir(p)\n\n          if branch!='':\n             if o=='con':\n                ck.out('  ====================================')\n                ck.out('  git checkout '+branch)\n                ck.out('')\n\n             r=ck.run_and_get_stdout({'cmd':['git','checkout',branch]})\n             ck.out(r.get('stdout',''))\n             ck.out(r.get('stderr',''))\n\n          if checkout!='':\n             if o=='con':\n                ck.out('  ====================================')\n                ck.out('  git checkout '+checkout)\n                ck.out('')\n\n             r=ck.run_and_get_stdout({'cmd':['git','checkout',checkout]})\n             ck.out(r.get('stdout',''))\n             ck.out(r.get('stderr',''))\n\n          os.chdir(ppp)\n\n          rp1=d.get('dict',{}).get('repo_deps',[])\n          if len(rp1)==0:\n             rp1=d.get('repo_deps',[]) # for backwards compatibility ...\n\n          rp2=[]\n          rp=[]\n\n          if len(rp1)>0:\n             for xruoa in rp1:\n                 if type(xruoa)!=list: # for backwards compatibility\n                    ruoa=xruoa.get('repo_uoa','')\n                    if xruoa.get('repo_uid','')!='': ruoa=xruoa['repo_uid']\n                    if ruoa!='' and ruoa not in cr:\n                       rp2.append(xruoa)\n\n          # Add dependencies on other repositories (but avoid duplication)\n          if len(rp2)==0:\n             if o=='con':\n                ck.out('  No dependencies on other repositories found!')\n          else:\n             for xruoa in rp2:\n                 ruoa=xruoa.get('repo_uoa','')\n                 if xruoa.get('repo_uid','')!='': ruoa=xruoa['repo_uid']\n                 rurl=xruoa.get('repo_url','')\n                 if ruoa!='':\n                    x='  Dependency on repository '+ruoa+' '\n\n                    # Check if this repo exists\n                    r=ck.access({'action':'load',\n                                 'module_uoa':work['self_module_uoa'],\n                                 'data_uoa':ruoa})\n                    if r['return']>0: \n                       if r['return']!=16: return r\n                       rp.append(xruoa)\n                       x+=': should be resolved ...'\n                    else:\n                       # If explicit branch, still add !\n                       branch=xruoa.get('branch','')\n                       checkout=xruoa.get('checkout','')\n                       stable=xruoa.get('stable','')\n                       version=xruoa.get('version','')\n\n                       if branch!='' or checkout!='' or stable!='' or version!='':\n                          xruoa['ignore_pull']='yes'\n                          rp.append(xruoa)\n                          x+=': should be switched to explicit branch ...'\n                       else:\n                          x+=': Ok'\n\n                    if o=='con':\n                       ck.out(x)\n\n          if len(rp)>0:\n             for xruoa in rp:\n                 ruoa=xruoa.get('repo_uoa','')\n                 ruid=xruoa.get('repo_uid','')\n                 rurl=xruoa.get('repo_url','')\n\n                 branch=xruoa.get('branch','')\n                 checkout=xruoa.get('checkout','')\n                 stable=xruoa.get('stable','')\n                 version=xruoa.get('version','')\n\n                 ignore_pull=xruoa.get('ignore_pull','')\n\n                 if o=='con':\n                    ck.out('')\n                    x=''\n                    if ruid!='': x=' ('+ruid+')'\n                    ck.out('  Resolving dependency on repo: '+ruoa+x)\n                    ck.out('')\n\n                 if ruid!='': cr.append(ruid)\n                 else: cr.append(ruoa)\n\n                 ii={'action':how,\n                     'module_uoa':work['self_module_uoa'],\n                     'data_uoa':ruoa,\n                     'current_repos':cr,\n                     'url':rurl,\n                     'ignore_pull':ignore_pull,\n                     'branch':branch,\n                     'checkout':checkout,\n                     'stable':stable,\n                     'version':version,\n                     'out':o}\n                 if ruid!='': ii['data_uid']=ruid\n                 if how=='add': ii['gitzip']='yes'\n                 r=ck.access(ii)\n                 if r['return']>0: return r\n\n    return {'return':0, 'current_repos':cr}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_deps(i):\n\n    o=i.get('out','')\n\n    op=i.get('out_prefix','')\n\n    duoa=i.get('data_uoa','')\n    if duoa!='':\n       # Get configuration\n       r=ck.load_repo_info_from_cache({'repo_uoa':duoa})\n       if r['return']>0: return r\n\n       d=r['dict']\n       rp1=d.get('dict',{}).get('repo_deps',[])\n    else:\n       rp1=i['repo_deps']\n\n    if len(rp1)==0:\n       rp1=d.get('repo_deps',[]) # for compatibility ...\n\n    if o=='con' and len(rp1)>0:\n       for q in rp1:\n           ruoa=q.get('repo_uoa','')\n           ruid=q.get('repo_uid','')\n           rurl=q.get('repo_url','')\n\n           x=op+ruoa\n           if ruid!='': x+='; '+ruid\n           elif rurl!='': x+='; '\n           if rurl!='': x+='; '+rurl\n\n           ck.out(x)\n\n    return {'return':0, 'repo_deps':rp1}", "response": "Print the dependencies of the repo_uoa and the repo_deps of the other shared repos."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninputting { } Output: { return - return code = 0, if successful > 0, if error (error) - error text if return > 0 repo_deps - list with dependencies on other repositories ... }", "response": "def add_more_deps(i):\n    \"\"\"\n    Input:  {\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              repo_deps    - list with dependencies on other repositories ...\n            }\n\n    \"\"\"\n\n    rp=[]\n\n    r=ck.inp({'text': 'Would you like to add extra dependencies on other shared repositories (y/N)?: '})\n    x=r['string'].lower()\n    if x=='yes' or x=='y':\n       ck.out('')\n       ck.out('Use the following format: repo UOA; (repo UID) (; repo URL)')\n       ck.out('For example:')\n       ck.out('  ck-autotuning')\n       ck.out('  ck-dissemination-modules;;https://github.com/gfursin/ck-dissemination-modules.git')\n       ck.out('')\n       ck.out('Press Enter to stop adding repositories!')\n       ck.out('')\n\n       while True:\n          r=ck.inp({'text': ''})\n          x=r['string'].strip()\n          if x=='': break\n             \n          z={}\n\n          y=x.split(';')\n          if len(y)>0:\n             z['repo_uoa']=y[0].strip()\n             if len(y)>1:\n                z['repo_uid']=y[1].strip()\n                if len(y)>2:\n                   z['repo_url']=y[2].strip()\n         \n          if len(z)>0:\n             rp.append(z)\n\n    return {'return':0, 'repo_deps':rp}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef renew(i):\n\n    # Check if global writing is allowed\n    r=ck.check_writing({})\n    if r['return']>0: return r\n\n    o=i.get('out','')\n\n    duoa=i.get('data_uoa','')\n    if duoa=='':\n       return {'return':1, 'error':'repository UOA is not specified'}\n\n    # Get configuration (not from Cache - can be outdated info!)\n#    r=ck.load_repo_info_from_cache({'repo_uoa':duoa})\n    r=ck.access({'action':'load',\n                 'module_uoa':work['self_module_uoa'],\n                 'data_uoa':duoa})\n    if r['return']>0: return r\n\n    p=r.get('dict',{}).get('path','')\n    d=r['dict']\n    dn=r.get('data_name','')\n\n    shared=d.get('shared','')\n    url=d.get('url','')\n\n    if shared!='git' and url=='':\n       return {'return':1, 'error':'this repository is not shared and can not be renewed!'}\n\n    # first delete\n    ii={'action':'rm',\n        'module_uoa':work['self_module_uoa'],\n        'data_uoa':duoa,\n        'all':'yes'}\n    if o=='con':\n       ii['out']='con'\n    r=ck.access(ii)\n    if r['return']>0: return r\n\n    # pull again\n    ii={'action':'pull',\n        'module_uoa':work['self_module_uoa'],\n        'data_uoa':duoa,\n        'data_name':dn,\n        'url':url,\n        'stable':i.get('stable',''),\n        'checkout':i.get('checkout','')}\n    if o=='con':\n       ii['out']='con'\n    return ck.access(ii)", "response": "renew a node in the order that it is in the cache"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninputs { data_uoa - data UOA of the repo } Output: { return - return code = 0, if successful > 0, if error (error) - error text if return > 0 }", "response": "def browse(i):\n    \"\"\"\n    Input:  {\n              data_uoa                   - data UOA of the repo\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n            }\n\n    \"\"\"\n\n    o=i.get('out','')\n\n    duoa=i.get('data_uoa','')\n    if duoa=='':\n       return {'return':1, 'error':'repository UOA is not specified'}\n\n    # Get configuration (not from Cache - can be outdated info!)\n#    r=ck.load_repo_info_from_cache({'repo_uoa':duoa})\n    r=ck.access({'action':'load',\n                 'module_uoa':work['self_module_uoa'],\n                 'data_uoa':duoa})\n    if r['return']>0: return r\n\n    p=r.get('dict',{}).get('path','')\n    d=r['dict']\n    dn=r.get('data_name','')\n\n    shared=d.get('shared','')\n    url=d.get('url','')\n\n    if shared!='git' and url=='':\n       return {'return':1, 'error':'this repository is not shared!'}\n\n    import webbrowser\n    webbrowser.open(url)\n\n    return {'return':0}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef describe(i):\n\n    d=i.get('dict',{})\n\n    a=d.get('artifact_pack_description',{})\n\n    ck.out('')\n    ck.out('Please, provide description of your artifact pack (see http://cTuning.org/ae for more details):')\n    ck.out('')\n\n    r=ck.inp({'text':'Enter title of your artifact pack (or associated paper): '})\n    x=r['string']\n    if x!='': a['title']=x\n\n    r=ck.inp({'text':'List contributors: '})\n    x=r['string']\n    if x!='': a['contributors']=x\n\n    r=ck.inp({'text':'Enter artifact abstract: '})\n    x=r['string']\n    if x!='': a['description']=x\n\n    r=ck.inp({'text':'Enter license of the whole artifact pack: '})\n    x=r['string']\n    if x!='': a['license']=x\n\n    r=ck.inp({'text':'Enter version/revision of the whole artifact pack: '})\n    x=r['string']\n    if x!='': a['version']=x\n\n    r=ck.inp({'text':'How delivered (URL, DOI, OCRID, etc): '})\n    x=r['string']\n    if x!='': a['how_delivered']=x\n\n    r=ck.inp({'text':'Describe software dependencies: '})\n    x=r['string']\n    if x!='': a['software_dependencies']=x\n\n    r=ck.inp({'text':'Describe hardware dependencies: '})\n    x=r['string']\n    if x!='': a['hardware_dependencies']=x\n\n    r=ck.inp({'text':'Describe data sets: '})\n    x=r['string']\n    if x!='': a['datasets']=x\n\n    r=ck.inp({'text':'Describe installation procedure: '})\n    x=r['string']\n    if x!='': a['installation']=x\n\n    r=ck.inp({'text':'Describe possible experiment parameterization: '})\n    x=r['string']\n    if x!='': a['parameterization']=x\n\n    r=ck.inp({'text':'Describe experiment workflow: '})\n    x=r['string']\n    if x!='': a['experiment_workflow']=x\n\n    r=ck.inp({'text':'Describe evaluation procedure and expected output: '})\n    x=r['string']\n    if x!='': a['evaluation']=x\n\n    r=ck.inp({'text':'Acknowledgments: '})\n    x=r['string']\n    if x!='': a['acknowledgments']=x\n\n    r=ck.inp({'text':'Misc notes: '})\n    x=r['string']\n    if x!='': a['notes']=x\n\n    d['artifact_pack_description']=a\n\n    return {'return':0, 'dict':d}", "response": "Describe the artifact pack"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef form_url(i):\n\n    url=i.get('url','')\n\n    if i.get('hostname','')!='':\n       url='http://'+i['hostname']\n\n       if i.get('port','')!='':\n          url+=':'+i['port']\n\n       if i.get('hostext','')!='':\n          url+='/'+i['hostext']\n       else:\n          url+='/ck?'\n\n    return {'return':0, 'url':url}", "response": "Form URL of a CK server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef show(i):\n\n    import os\n\n    o=i.get('out','')\n\n    curdir=os.getcwd()\n\n    duoa=i.get('data_uoa','')\n\n    reset=i.get('reset','')\n\n    stable=i.get('stable','')\n    version=i.get('version','')\n    if stable=='yes': version='stable'\n\n    r=ck.list_data({'module_uoa':work['self_module_uoa'], \n                    'data_uoa':duoa})\n    if r['return']>0: return r\n\n    if o=='con':\n       ck.out('Please wait - it may take some time ...')\n       ck.out('')\n\n    r=ck.reload_repo_cache({}) # Ignore errors\n\n    # Init header.\n    pp=[({'branch':'branch', 'origin':'origin', 'checkout':'local', 'path':'path', 'type':'type', 'url':'url', 'data_uoa':'data_uoa'})]\n    il=0\n    for q in ck.cache_repo_info:\n        # Get repo info\n        qq=ck.cache_repo_info[q]\n\n        d=qq['dict']\n\n        t=d.get('shared','')\n\n        if t!='':\n           duoa=qq['data_uoa']\n\n           if len(duoa)>il: il=len(duoa)\n\n           p=d.get('path','')\n           url=d.get('url','')\n\n           branch=''\n           origin=''\n           checkout=''\n\n           if os.path.isdir(p):\n              # Detect status\n              os.chdir(p)\n\n              if reset=='yes':\n                 r=ck.run_and_get_stdout({'cmd':['git','checkout','master']})\n\n              if version!='':\n                 cx=qq.get('dict',{}).get('checkouts',{}).get(version,{})\n                 branch=cx.get('branch','')\n                 checkout=cx.get('checkout','')\n\n                 if branch!='':\n                    r=ck.run_and_get_stdout({'cmd':['git','checkout',branch]})\n\n                 if checkout!='':\n                    r=ck.run_and_get_stdout({'cmd':['git','checkout',checkout]})\n\n              # FGG TBD: we may need to add explicit check for branch/checkout in repo_deps here?\n              # OR MAYBE NOT - need to think ...\n\n              # Get current branch\n              r=ck.run_and_get_stdout({'cmd':['git','rev-parse','--abbrev-ref','HEAD']})\n              if r['return']==0 and r['return_code']==0:\n                 branch=r['stdout'].strip()\n\n              # Get origin hash\n              r=ck.run_and_get_stdout({'cmd':['git','rev-parse','--short','origin/HEAD']})\n              if r['return']==0 and r['return_code']==0:\n                 origin=r['stdout'].strip()\n\n              # Get current hash (append '-dirty' on dirty working tree)\n              r=ck.run_and_get_stdout({'cmd':['git','describe','--match=NeVeRmAtCh','--always','--abbrev','--dirty']})\n              if r['return']==0 and r['return_code']==0:\n                 checkout=r['stdout'].strip()\n\n           pp.append({'branch':branch, 'origin':origin, 'checkout':checkout, 'path':p, 'type':t, 'url':url, 'data_uoa':duoa})\n\n    # Print\n    for q in pp:\n        name=q['data_uoa']\n\n        x=name+' '*(il-len(name))\n\n        branch=q.get('branch','')\n        origin=q.get('origin','')\n        checkout=q.get('checkout','')\n        url=q.get('url','')\n\n        if branch!='' or 'origin' or checkout!='' or url!='':\n           x+=' ( '+branch+' ; '+origin+' ; '+checkout+' ; '+url+' )'\n\n        ck.out(x)\n\n    os.chdir(curdir)\n\n    return {'return':0}", "response": "Show the contents of a single object in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ren(i):\n\n    o=i.get('out','')\n\n    duoa=i.get('data_uoa','')\n\n    if duoa=='':\n       return {'return':1, 'error':'repo is not defined'}\n\n    r=ck.access({'action':'load',\n                 'module_uoa':work['self_module_uid'],\n                 'data_uoa':duoa})\n    if r['return']>0: return r\n\n    dd=r['dict']\n    dp=r['path']\n    duoa_real=r['data_uoa']\n    dname=r['data_name']\n\n    nduoa=i.get('new_data_uoa','')\n\n    if nduoa=='':\n       xcids=i.get('xcids',[])\n       if len(xcids)>0: \n          xcid=xcids[0]\n          nduoa=xcid.get('data_uoa','')\n\n    if nduoa=='':\n       xcids=i.get('cids',[])\n       if len(xcids)>0: \n          nduoa=xcids[0]\n\n    if nduoa=='': \n       return {'return':1, 'error':'new repo name is not defined'}\n\n    if nduoa=='': \n       return {'return':1, 'error':'new repo name is not defined'}\n\n    if nduoa=='local' or nduoa=='default': \n       return {'return':1, 'error':'new repo name already exists'}\n\n    # Check if such repo doesn't exist\n    r=ck.access({'action':'load',\n                 'module_uoa':work['self_module_uid'],\n                 'data_uoa':nduoa})\n    if r['return']==0:\n       return {'return':1, 'error':'repo already exists'}\n\n    # Update .ckr.json\n    dpp=dd.get('path','')\n    if dpp!='':\n       pckr=os.path.join(dpp,ck.cfg['repo_file'])\n\n       r=ck.load_json_file({'json_file':pckr})\n       if r['return']>0: return r\n\n       dckr=r['dict']\n\n       x=dckr.get('data_uoa','')\n       if x!='' and x==duoa_real: dckr['data_uoa']=nduoa\n\n       x=dckr.get('data_alias','')\n       if x!='' and x==duoa_real: dckr['data_alias']=nduoa\n\n       x=dckr.get('data_name','')\n       if x!='' and x==duoa_real: dckr['data_name']=nduoa\n\n       r=ck.save_json_to_file({'json_file':pckr, 'dict':dckr})\n       if r['return']>0: return r\n\n    # Rename repo entry using internal command\n    r=ck.access({'action':'ren',\n                 'module_uoa':work['self_module_uid'],\n                 'data_uoa':duoa,\n                 'new_data_uoa':nduoa,\n                 'common_func':'yes'})\n    if r['return']>0: return r\n\n    # Recache repos\n    r1=recache({'out':o})\n    if r1['return']>0: return r1\n\n    return r", "response": "ren function for a single node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pull(i):\n\n    import os\n\n    o=i.get('out','')\n\n    kernel=i.get('kernel','')\n    install=i.get('install','')\n\n    if kernel=='yes' or install=='yes':\n       ck_root=ck.work['env_root']\n\n       if not os.path.isdir(ck_root):\n          return {'return':1, 'error':'Can\\'t find CK in '+ck_root+' - please check CK_ROOT env'}\n\n       os.chdir(ck_root)\n\n    if kernel=='yes':\n       cont=True\n       if not os.path.isdir(os.path.join(ck_root,'.git')) and i.get('force','')!='yes':\n          ck.out('WARNING: seems like your CK_ROOT installation is not from GitHub - skipping kernel update ...')\n          ck.out('')\n          cont=False\n\n       if cont:\n          if o=='con':\n             ck.out('Updating CK from GitHub ...')\n             ck.out('')\n             ck.out('  cd '+ck_root)\n             ck.out('  git pull')\n             ck.out('')\n\n          rx=os.system('git pull')\n          if rx>0: \n             return {'return':1, 'error':'CK update failed'}\n\n          if o=='con':\n             ck.out('')\n\n    if install=='yes':\n       c=ck.cfg['install_ck_as_lib']\n\n       py=os.getenv('CK_PYTHON','')\n       if py!='':\n          c=c.replace('python ',py+' ')\n\n       ck.out('')\n       ck.out('Installing CK kernel as python module ...')\n\n       ck.out('')\n       ck.out('  cd '+ck_root)\n\n       # Get OS\n       r=ck.get_os_ck({})\n       if r['return']>0: return r\n       plat=r['platform']\n       if plat!='win':\n          c=ck.cfg.get('linux_sudo','')+' '+c\n\n       ck.out('  '+c)\n       ck.out('')\n       os.system(c)\n\n    return ck.access({'action':'pull',\n                      'module_uoa':'repo',\n                      'out':o})", "response": "Pulls the CK kernel and returns a list of the CKs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef out(s):\n\n    if allow_print: \n       if con_encoding=='':\n          x=sys.stdin.encoding\n          if x==None: \n             b=s.encode()\n          else:\n             b=s.encode(x, 'ignore')\n       else:\n          b=s.encode(con_encoding, 'ignore')\n\n       if sys.version_info[0]>2:\n          try: # We encountered issues on ipython with Anaconda\n               # and hence made this work around\n             sys.stdout.buffer.write(b)\n             sys.stdout.buffer.write(b'\\n')\n          except Exception as e: \n             print(s)\n             pass\n       else:\n          print(b)\n\n    sys.stdout.flush()\n\n    return None", "response": "Print a unicode string to stdout."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef debug_out(i):\n\n    import copy\n    import json\n\n    ii={}\n\n    # Check main unprintable keys\n    for k in i:\n        try:\n           s=json.dumps(i[k])\n        except Exception as e: \n           pass\n        else:\n           ii[k]=i[k]\n\n    # Dump\n    out(json.dumps(ii, indent=2))\n\n    return {'return':0}", "response": "Dump the dictionary to a file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint unicode string to stderr", "response": "def eout(s):\n    \"\"\"\n    Input:  s - unicode string to print\n\n    Output: Nothing\n    \"\"\"\n\n    if allow_print: \n       if con_encoding=='':\n          x=sys.stdin.encoding\n          if x==None: \n             b=s.encode()\n          else:\n             b=s.encode(x, 'ignore')\n       else:\n          b=s.encode(con_encoding, 'ignore')\n\n       if sys.version_info[0]>2:\n          try: # We encountered issues on ipython with Anaconda\n               # and hence made this work around\n             sys.stderr.buffer.write(b)\n             sys.stderr.buffer.write(b'\\n')\n          except Exception as e: \n             sys.stderr.write(s)\n             pass\n       else:\n          sys.stderr.write(b)\n\n    sys.stderr.flush()\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninputting { return - return code error - error text } Output: Nothing; quits program", "response": "def err(r):\n    \"\"\"\n    Input:  {\n              return - return code\n              error - error text\n            }\n\n    Output: Nothing; quits program\n    \"\"\"\n\n    import sys\n\n    rc=r['return']\n    re=r['error']\n\n    out('Error: '+re)\n    sys.exit(rc)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninput { cmd - command line (timeout) - timeout in seconds (granularity 0.01 sec) - may cause overheads ... } Output: { return - return code = 0, if successful > 0, if error = 8, if timeout (error) - error text if return > 0 return_code - return code from app }", "response": "def system_with_timeout(i):\n\n    \"\"\"\n    Input:  {\n              cmd       - command line\n              (timeout) - timeout in seconds (granularity 0.01 sec) - may cause overheads ...\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n                                         =  8, if timeout\n              (error)      - error text if return > 0\n\n              return_code  - return code from app\n            }\n    \"\"\"\n\n    import subprocess\n    import time\n\n    cmd=i['cmd']\n\n    rc=0\n\n    to=i.get('timeout','')\n\n    p=subprocess.Popen(cmd, shell=True)\n\n    if to != '':\n       xto=float(to)\n\n       t0=time.time()\n       t=0\n       tx=float(i['timeout'])\n\n       while p.poll() == None and t<xto:\n          time.sleep(0.1)\n          t=time.time()-t0\n\n       if t>=xto and p.poll()==None:\n          system_with_timeout_kill(p)\n          return {'return':8, 'error':'process timed out and had been terminated'}\n    else:\n       p.wait()\n\n    rc=p.returncode\n    return {'return':0, 'return_code':rc}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun a command and returns the stdout of the command.", "response": "def run_and_get_stdout(i):\n  \"\"\"\n  Input:  {\n            cmd       - list of command line arguments, starting with the command itself\n            (shell)   - if 'yes', reuse shell environment\n          }\n\n  Output: {\n            return       - return code =  0, if successful\n                                       >  0, if error\n                                       =  8, if timeout\n            (error)      - error text if return > 0\n\n            return_code  - return code from app\n\n            stdout       - string, standard output of the command\n          }\n  \"\"\"\n\n  import subprocess\n  import shlex\n  import platform\n\n  cmd=i['cmd']\n  if type(cmd)!=list:\n      # Split only on non-Windows platforms (since Windows takes a string in Popen)\n      if not platform.system().lower().startswith('win'):\n          cmd=shlex.split(cmd)\n\n  xshell=False\n  if i.get('shell','')=='yes':\n      xshell=True\n\n  p1 = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=xshell)\n  output, error = p1.communicate()\n\n  if sys.version_info[0]>2:\n      output = output.decode(encoding='UTF-8')\n      error = error.decode(encoding='UTF-8')\n\n  return {'return':0, 'return_code':p1.returncode, 'stdout':output, 'stderr':error}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget value from dict1 and dict2", "response": "def get_from_dicts(dict1, key, default_value, dict2, extra=''):\n    \"\"\"\n    Input:  dict1         - first check in this dict (and remove if there)\n            key           - key in dict1\n            default_value - default value if not found\n            dict2         - then check from here\n\n    Output: value\n    \"\"\"\n\n    value=default_value\n\n    if key not in dict1:\n       if dict2!=None:\n          value=dict2.get(extra+key, default_value)\n    else:\n       value=dict1[key]\n       del(dict1[key])\n\n       if dict2!=None:\n          dict2[extra+key]=value\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts ISO date time into a series of objects.", "response": "def convert_iso_time(i):\n    \"\"\"\n    Input:  {\n              iso_datetime - iso date time\n            }\n\n    Output: { \n              return         - return code =  0, if successful\n                                           >  0, if error\n              (error)        - error text if return > 0\n              (datetime_obj) - datetime object\n            }\n    \"\"\"\n\n    t=i['iso_datetime']\n\n    import datetime\n    import time\n\n    dto=None\n\n    ok=True\n\n    try: dto=datetime.datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S.%f\")\n    except Exception as e: \n       ok=False\n       pass\n\n    if not ok:\n       ok=True\n       try: dto=datetime.datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S\")\n       except Exception as e: \n          ok=False\n          pass\n\n    if not ok:\n       ok=True\n       try: dto=datetime.datetime.strptime(t, \"%Y-%m-%dT%H:%M\")\n       except Exception as e: \n          ok=False\n          pass\n\n    if not ok:\n       ok=True\n       try: dto=datetime.datetime.strptime(t, \"%Y-%m-%dT%H\")\n       except Exception as e: \n          ok=False\n          pass\n\n    if not ok:\n       ok=True\n       try: dto=datetime.datetime.strptime(t, \"%Y-%m-%d\")\n       except Exception as e: \n          ok=False\n          pass\n\n    if not ok:\n       ok=True\n       try: dto=datetime.datetime.strptime(t, \"%Y-%m\")\n       except Exception as e: \n          ok=False\n          pass\n\n    if not ok:\n       ok=True\n       try: dto=datetime.datetime.strptime(t, \"%Y\")\n       except Exception as e: \n          return {'return':1, 'error':'can\\'t parse ISO date time: '+t}\n\n    return {'return':0, 'datetime_obj':dto}"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninputs { text - text to print } Output: { return - return code = 0 string - input string }", "response": "def inp(i):\n    \"\"\"\n    Input:  {\n              text - text to print\n            }\n\n    Output: {\n              return       - return code =  0\n\n              string       - input string\n            }\n    \"\"\"\n\n    t=i['text']\n\n    if con_encoding=='':\n       x=sys.stdin.encoding\n       if x==None: \n          b=t.encode()\n       else:\n          b=t.encode(x, 'ignore')\n    else:\n       b=t.encode(con_encoding, 'ignore') # pragma: no cover \n\n    if sys.version_info[0]>2:\n       try: b=b.decode(sys.stdin.encoding)\n       except Exception as e: \n         try: b=b.decode('utf8')\n         except Exception as e: pass\n\n    if sys.version_info[0]>2:\n       s=input(b)\n    else:\n       x=sys.stdin.encoding\n       if x==None:\n          x='utf8'\n       s=raw_input(b).decode(x).encode('utf8')\n\n    return {'return':0, 'string':s}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nselect a key in the array", "response": "def select(i):\n    \"\"\"\n    Input:  {\n              dict             - dict with values being dicts with 'name' as string to display and 'sort' as int (for ordering)\n              (title)          - print title\n              (error_if_empty) - if 'yes' and Enter, make error\n              (skip_sort)      - if 'yes', do not sort array\n            }\n\n    Output: {\n              return       - return code =  0\n\n              string       - selected dictionary key\n            }\n    \"\"\"\n\n    s=''\n\n    title=i.get('title','')\n    if title!='':\n       out(title)\n       out('')\n\n    d=i['dict']\n    if i.get('skip_sort','')!='yes':\n       kd=sorted(d, key=lambda v: d[v].get('sort',0))\n    else:\n       kd=d\n\n    j=0\n    ks={}\n    for k in kd:\n        q=d[k]\n\n        sj=str(j)\n        ks[sj]=k\n\n        qn=q.get('name','')\n\n        out(sj+') '+qn)\n\n        j+=1\n\n    out('')\n    rx=inp({'text':'Make your selection (or press Enter for 0): '})\n    if rx['return']>0: return rx\n    sx=rx['string'].strip()\n\n    if sx=='':\n       if i.get('error_if_empty','')=='yes':\n          return {'return':1, 'error':'selection is empty'}\n\n       s=kd[0]\n    else:\n       if sx not in ks:\n          return {'return':1, 'error':'selection is not recognized'}\n       s=ks[sx]\n\n    return {'return':0, 'string':s}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef select_uoa(i):\n\n    se=i.get('skip_enter','')\n\n    lst=i.get('choices',[])\n\n    if i.get('skip_sort','')!='yes':\n       klst=sorted(lst, key=lambda v: v['data_uoa'])\n    else:\n       klst=lst\n\n    zz={}\n    iz=0\n\n    for z1 in klst:\n        z=z1['data_uid']\n        zu=z1['data_uoa']\n\n        zs=str(iz)\n        zz[zs]=z\n\n        out(zs+') '+zu+' ('+z+')')\n\n        iz+=1\n\n    out('')\n    y='Select UOA'\n    if se!='yes': y+=' (or press Enter for 0)'\n    y+=': '\n\n    rx=inp({'text':y})\n    x=rx['string'].strip()\n    if x=='' and se!='yes': x='0' \n\n    if x not in zz:\n       return {'return':1, 'error':'number is not recognized'}\n\n    dduoa=zz[x]\n\n    return {'return':0, 'choice':dduoa}", "response": "Select UOA from a list of available unicorn objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a string of comma - separated tags to a list of tags.", "response": "def convert_str_tags_to_list(i):\n    \"\"\"\n    Input:  either a list, or a string of comma-separated tags.\n\n    Output: If i is a list, it's returned.\n            If i is a string, the list of tags it represents is returned \n            (each tag is stripped of leading and trailing whitespace).\n\n    \"\"\"\n\n    r=[]\n\n    if type(i)==list:\n       r=i\n    else:\n       ii=i.split(',')\n       for q in ii:\n           q=q.strip()\n           if q!='':\n              r.append(q)\n\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if global writing is allowed for this module", "response": "def check_writing(i):\n    \"\"\"\n    Input:  {\n              (module_uoa)\n              (module_uid)\n\n              (repo_uoa)\n              (repo_uid)\n              (repo_dict)\n\n              (delete)     - if 'yes', check if global delete operation is allowed\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n              (repo_dict)  - repo cfg if available\n            }\n\n    \"\"\"\n\n    dl=i.get('delete','')\n\n    if dl=='yes' and cfg.get('forbid_global_delete','')=='yes':\n       return {'return':1, 'error':'delete/rename operations are forbidden'}\n\n    if cfg.get('forbid_global_writing','')=='yes':\n       return {'return':1, 'error':'global writing is forbidden'}\n\n    if len(i)==0: \n       return {'return':0} # Check only global writing\n\n    if cfg.get('forbid_writing_modules','')=='yes':\n       muoa=i.get('module_uoa','')\n       muid=i.get('module_uid','')\n       if muoa==cfg['module_name'] or (muid!='' and muid in cfg['module_uids']):\n          return {'return':1, 'error':'writing/changing modules is forbidden'}\n\n    ruoa=i.get('repo_uoa','')\n    ruid=i.get('repo_uid','')\n\n    if cfg.get('forbid_writing_to_default_repo','')=='yes':\n       if ruoa==cfg['repo_name_default'] or ruid==cfg['repo_uid_default']:\n          return {'return':1, 'error':'writing to default repo is forbidden'}\n\n    if cfg.get('forbid_writing_to_local_repo','')=='yes':\n       if ruoa==cfg['repo_name_local'] or ruid==cfg['repo_uid_local']:\n          return {'return':1, 'error':'writing to local repo is forbidden'}\n\n    rr={'return':0}\n\n    # Load info about repo\n    rd={}\n    if ruoa!='':\n       if 'repo_dict' in i:\n          rd=i['repo_dict']\n       else:\n          rx=load_repo_info_from_cache({'repo_uoa':ruoa})\n          if rx['return']>0: return rx\n          rd=rx.get('dict',{})\n       rr['repo_dict']=rd\n\n    if cfg.get('allow_writing_only_to_allowed','')=='yes':\n       if rd.get('allow_writing','')!='yes':\n          return {'return':1, 'error':'writing to this repo is forbidden'}\n\n    if rd.get('forbid_deleting','')=='yes' and dl=='yes':\n       return {'return':1, 'error':'deleting in this repo is forbidden'}\n\n    return rr"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the version of the current language", "response": "def get_version(i):\n    \"\"\"\n    Input:  {}\n\n    Output: {\n              return       - return code =  0\n\n              version      - list starting from major version number\n              version_str  - version string\n            }\n\n    \"\"\"\n\n    import copy\n\n    s=''\n\n    x=copy.deepcopy(cfg['version'])\n\n    for q in x:\n        if s!='': s+='.'\n        s+=str(q)\n\n    return {'return':0, 'version':x, 'version_str':s}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gen_tmp_file(i):\n\n    xs=i.get('suffix','')\n    xp=i.get('prefix','')\n    s=i.get('string','')\n\n    import tempfile\n    fd, fn=tempfile.mkstemp(suffix=xs, prefix=xp)\n    os.close(fd)\n    os.remove(fn)\n\n    if i.get('remove_dir','')=='yes':\n       fn=os.path.basename(fn)\n\n    return {'return':0, 'file_name':fn}", "response": "Generate a temporary file for the next iteration of the next iteration."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the OS CK file.", "response": "def get_os_ck(i):\n    \"\"\"\n    Input:  {\n              (bits)      - force OS bits\n            }\n\n    Output: {\n              return      - return code =  0\n              platform    - 'win' or 'linux'\n              bits        - OS bits in string: 32 or 64\n              python_bits - Python installation bits in string: 32 or 64\n\n            }\n    \"\"\"\n\n    import os\n    import platform\n    import struct\n\n    pbits=str(8 * struct.calcsize(\"P\"))\n\n    plat='linux'\n    if platform.system().lower().startswith('win'): # pragma: no cover\n       plat='win'\n\n    obits=i.get('bits','')\n    if obits=='':\n       obits='32'\n       if plat=='win':\n          # Trying to get fast way to detect bits\n          if os.environ.get('ProgramW6432','')!='' or os.environ.get('ProgramFiles(x86)','')!='': # pragma: no cover\n             obits='64'\n       else:\n          # On Linux use first getconf LONG_BIT and if doesn't work use python bits\n\n          obits=pbits\n\n          r=gen_tmp_file({})\n          if r['return']>0: return r\n          fn=r['file_name']\n\n          cmd='getconf LONG_BIT > '+fn\n          rx=os.system(cmd)\n          if rx==0:\n             r=load_text_file({'text_file':fn, \n                               'delete_after_read':'yes'})\n             if r['return']==0:\n                s=r['string'].strip()\n                if len(s)>0 and len(s)<4:\n                   obits=s\n\n    return {'return':0, 'platform':plat, 'bits':obits, 'python_bits':pbits}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gen_uid(i):\n\n    import uuid\n    import random\n\n    uid=str(uuid.uuid4().hex)\n\n    if len(uid)!=32:\n       return {'return':1, 'error':'problem generating UID : len='+str(len(uid))+' !=32'} # pragma: no cover \n\n    random.seed\n\n    x=random.randrange(0,16)\n    return {'return':0, 'data_uid':uid[x:x+16]}", "response": "Generate a unique ID for the current object"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if a string is a valid UID", "response": "def is_uid(str):\n    \"\"\"\n    Input:  string to check\n\n    Output: True if UID, otherwise False\n    \"\"\"\n\n    import re\n\n    if len(str)!=16:\n       return False\n\n    pattern = r'[^\\.a-f0-9]'\n    if re.search(pattern, str.lower()):\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_uoa(str):\n\n    if str.find(cfg['detect_cur_cid'])>=0 or str.find(cfg['detect_cur_cid1'])>=0: return False\n    if str.find('*')>=0: return False\n    if str.find('?')>=0: return False\n\n    return True", "response": "Check if string is allowed UOA"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prepare_special_info_about_entry(i):\n\n    # Add control info\n    d={'engine':'CK',\n       'version':cfg['version']}\n\n    if cfg.get('default_developer','')!='':\n       d['author']=cfg['default_developer']\n\n    if cfg.get('default_developer_email','')!='':\n       d['author_email']=cfg['default_developer_email']\n\n    if cfg.get('default_developer_webpage','')!='':\n       d['author_webpage']=cfg['default_developer_webpage']\n\n    if cfg.get('default_license','')!='':\n       d['license']=cfg['default_license']\n\n    if cfg.get('default_copyright','')!='':\n       d['copyright']=cfg['default_copyright']\n\n    r=get_current_date_time({})\n    d['iso_datetime']=r['iso_datetime']\n\n    return {'return':0, 'dict': d}", "response": "Prepares special info about entry for special info about"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_json_str_to_dict(i):\n\n    s=i['str']\n\n    if i.get('skip_quote_replacement','')!='yes':\n       s=s.replace('\"', '\\\\\"')\n       s=s.replace('\\'', '\"')\n\n    try:\n       d=json.loads(s, encoding='utf8')\n    except Exception as e:\n       return {'return':1, 'error':'problem converting text to json ('+format(e)+')'}\n\n    return {'return':0, 'dict': d}", "response": "Convert json string to dict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_json_file(i):\n\n    fn=i['json_file']\n\n    try:\n      if sys.version_info[0]>2:\n         f=open(fn, 'r', encoding='utf8')\n      else:\n         f=open(fn, 'r')\n    except Exception as e:\n       return {'return':16, 'error':'problem opening json file='+fn+' ('+format(e)+')'}\n\n    try:\n      s=f.read()\n    except Exception as e:\n       f.close()\n       return {'return':1, 'error':'problem reading json file='+fn+' ('+format(e)+')'}\n\n    f.close()\n\n    try:\n      if sys.version_info[0]>2:\n         d=json.loads(s)\n      else:\n         d=json.loads(s, encoding='utf8')\n    except Exception as e:\n       return {'return':1, 'error':'problem parsing json from file='+fn+' ('+format(e)+')'}\n\n    return {'return':0, 'dict': d}", "response": "Load a single object from a json file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_yaml_file(i):\n\n    import yaml\n\n    fn=i['yaml_file']\n\n    try:\n      if sys.version_info[0]>2:\n         f=open(fn, 'r', encoding='utf8')\n      else:\n         f=open(fn, 'r')\n    except Exception as e:\n       return {'return':16, 'error':'problem opening YAML file='+fn+' ('+format(e)+')'}\n\n    try:\n      s=f.read()\n    except Exception as e:\n       f.close()\n       return {'return':1, 'error':'problem reading YAML file='+fn+' ('+format(e)+')'}\n\n    f.close()\n\n    try:\n      d=yaml.load(s)\n    except Exception as e:\n       return {'return':1, 'error':'problem parsing YAML from file='+fn+' ('+format(e)+')'}\n\n    return {'return':0, 'dict': d}", "response": "Load a single object from a YAML file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_text_file(i):\n\n    fn=i['text_file']\n\n    en=i.get('encoding','')\n    if en=='' or en==None: en='utf8'\n\n    try:\n       f=open(fn, 'rb')\n    except Exception as e:\n       return {'return':16, 'error':'problem opening text file='+fn+' ('+format(e)+')'}\n\n    try:\n       b=f.read()\n    except Exception as e:\n       f.close()\n       return {'return':1, 'error':'problem reading text file='+fn+' ('+format(e)+')'}\n\n    f.close()\n\n    r={'return':0, 'bin':b}\n\n    if i.get('delete_after_read','')=='yes':\n       import os\n       os.remove(fn)\n\n    if i.get('keep_as_bin','')!='yes':\n       s=b.decode(en).replace('\\r','') # decode into Python string (unicode in Python3)\n       r['string']=s\n\n       cl=i.get('split_to_list','')\n       cd=i.get('convert_to_dict','')\n\n       if cl=='yes' or cd=='yes':\n          lst=s.split('\\n')\n          r['lst']=lst\n\n          if cd=='yes':\n             dd={}\n\n             ss=i.get('str_split','')\n             rq=i.get('remove_quotes','')\n             if ss=='': ss=':'\n\n             for q in lst:\n                 qq=q.strip()\n                 ix=qq.find(ss)\n                 if ix>0:\n                    k=qq[0:ix].strip()\n                    v=''\n                    if ix+1<len(qq):\n                       v=qq[ix+1:].strip()\n                    if v!='' and rq=='yes':\n                       if v.startswith('\"'): v=v[1:]\n                       if v.endswith('\"'): v=v[:-1]\n                    dd[k]=v\n\n             r['dict']=dd\n\n    return r", "response": "Load a text file into a list of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef substitute_str_in_file(i):\n\n    fn=i['filename']\n    s1=i['string1']\n    s2=i['string2']\n\n    # Load text file (unicode)\n    r=load_text_file({'text_file':fn})\n    if r['return']>0: return r\n\n    # Replace\n    x=r['string']\n    x=x.replace(s1,s2)\n\n    # Save text file (unicode)\n    r=save_text_file({'text_file':fn, 'string':x})\n    if r['return']>0: return r\n\n    return {'return':0}", "response": "Substitute string in file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndumping the dictionary to json", "response": "def dump_json(i):\n    \"\"\"\n    Input:  {\n              dict          - dictionary\n              (skip_indent) - if 'yes', skip indent\n              (sort_keys)   - if 'yes', sort keys\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n\n              string       - json string (in utf8)\n            }\n    \"\"\"\n\n    d=i['dict']\n    si=i.get('skip_indent','')\n\n    sk=False\n    if i.get('sort_keys','')=='yes': sk=True\n\n    try:\n       if sys.version_info[0]>2:\n          if si=='yes': s=json.dumps(d, ensure_ascii=False, sort_keys=sk)\n          else:         s=json.dumps(d, indent=2, ensure_ascii=False, sort_keys=sk)\n       else:\n          if si=='yes': s=json.dumps(d, ensure_ascii=False, encoding='utf8', sort_keys=sk)\n          else:         s=json.dumps(d, indent=2, ensure_ascii=False, encoding='utf8', sort_keys=sk)\n    except Exception as e:\n       return {'return':1, 'error':'problem converting dict to json ('+format(e)+')'}\n\n    return {'return':0, 'string':s}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_json_to_file(i):\n\n    fn=i['json_file']\n\n    if i.get('safe','')=='yes':\n       d=i['dict']\n\n       sd={}\n\n       # Check main unprintable keys\n       for k in d:\n           try:\n              json.dumps(d[k])\n           except Exception as e: \n              pass\n           else:\n              sd[k]=d[k]\n\n       i['dict']=sd\n\n    r=dumps_json(i)\n    if r['return']>0: return r\n    s=r['string'].replace('\\r','')+'\\n'\n\n    return save_text_file({'text_file':fn, 'string':s})", "response": "Save a JSON object to a file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_yaml_to_file(i):\n\n    import yaml\n\n    fn=i['yaml_file']\n    d=i['dict']\n\n    try:\n       # If using just dump and keys are in unicode, \n       # pyyaml adds warning and makes produced yaml unparsable\n       s=yaml.safe_dump(d) \n    except Exception as e:\n       return {'return':1, 'error':'problem converting dict to YAML ('+format(e)+')'}\n\n    return save_text_file({'text_file':fn, 'string':s})", "response": "Save a dictionary to a YAML file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_text_file(i):\n\n    fn=i['text_file']\n\n    s=i['string']\n\n    try:\n      s=s.replace('\\r','')\n    except Exception as e:\n       pass\n\n    try:\n      s=s.replace(b'\\r',b'')\n    except Exception as e:\n       pass\n\n    m='w'\n    if i.get('append','')=='yes': m='a'\n\n    try:\n       s=s.encode('utf8')\n    except Exception as e:\n       pass\n\n    try:\n#      if sys.version_info[0]>2:\n#         f=open(fn, m+'b')\n#         f.write(s)\n#      else:\n      f=open(fn, m+'b')\n      f.write(s)\n    except Exception as e:\n       return {'return':1, 'error':'problem writing text file='+fn+' ('+format(e)+')'}\n\n    f.close()\n\n    return {'return':0}", "response": "Save the text file of the current object"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncopies string to clipboard", "response": "def copy_to_clipboard(i): # pragma: no cover \n    \"\"\"\n    Input:  {\n              string - string to copy\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n            }\n    \"\"\"\n\n    s=i['string']\n\n    failed=False\n    ee=''\n\n    # Try to load pyperclip (seems to work fine on Windows)\n    try:\n       import pyperclip\n    except Exception as e:\n       ee=format(e)\n       failed=True\n       pass\n\n    if not failed:\n       pyperclip.copy(s)\n    else:\n       failed=False\n\n       # Try to load Tkinter\n       try:\n          from Tkinter import Tk\n       except ImportError as e:\n          ee=format(e)\n          failed=True\n          pass\n\n       if failed:\n          failed=False\n          try:\n             from tkinter import Tk\n          except ImportError as e:\n             ee=format(e)\n             failed=True\n             pass\n\n       if failed:\n          return {'return':1, 'error':'none of pyperclip/Tkinter/tkinter packages is installed'}\n\n       # Copy to clipboard\n       try:\n          r = Tk()\n          r.withdraw()\n          r.clipboard_clear()\n          r.clipboard_append(s)\n          r.destroy()\n       except Exception as e:\n          return {'return':1, 'error':'problem copying string to clipboard ('+format(e)+')'}\n\n    return {'return':0}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge_dicts(i):\n\n    a=i['dict1']\n    b=i['dict2']\n\n    for k in b:\n        v=b[k]\n        if type(v) is dict:\n           if k not in a:\n              a.update({k:b[k]})\n           elif type(a[k])==dict:\n              merge_dicts({'dict1':a[k], 'dict2':b[k]})\n           else:\n              a[k]=b[k]\n        elif type(v) is list:\n           a[k]=[]\n           for y in v:\n               a[k].append(y)\n        else:\n           a[k]=b[k]\n\n    return {'return':0, 'dict1':a}", "response": "Merge the dict1 and dict2 into a single dict1."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting file to upload string", "response": "def convert_file_to_upload_string(i):\n    \"\"\"\n    Input:  {\n              filename - file name to convert\n            }\n\n    Output: {\n              return              - return code =  0, if successful\n                                                >  0, if error\n              (error)             - error text if return > 0\n              file_content_base64 - string that can be transmitted through Internet\n            }\n    \"\"\"\n\n    import base64\n\n    fn=i['filename']\n\n    if not os.path.isfile(fn):\n       return {'return':1, 'error':'file '+fn+' not found'}\n\n    s=b''\n    try:\n       f=open(fn, 'rb')\n       while True:\n          x = f.read(32768);\n          if not x: break\n          s+=x\n       f.close()\n    except Exception as e:\n       return {'return':1, 'error':'error reading file ('+format(e)+')'}\n\n    s=base64.urlsafe_b64encode(s).decode('utf8')\n\n    return {'return':0, 'file_content_base64': s}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert_upload_string_to_file(i):\n\n    import base64\n\n    x=i['file_content_base64']\n\n    fc=base64.urlsafe_b64decode(str(x)) # convert from unicode to str since base64 works on strings\n                                        # should be safe in Python 2.x and 3.x\n\n    fn=i.get('filename','')\n\n    if fn=='':\n       rx=gen_tmp_file({'prefix':'tmp-'})\n       if rx['return']>0: return rx\n       px=rx['file_name']\n    else:\n       px=fn\n\n    fn1, fne = os.path.splitext(px)\n\n    if os.path.isfile(px):\n       return {'return':1, 'error':'file already exists in the current directory'}\n    try:\n       fx=open(px, 'wb')\n       fx.write(fc)\n       fx.close()\n    except Exception as e:\n       return {'return':1, 'error':'problem writing file='+px+' ('+format(e)+')'}\n\n    return {'return':0, 'filename':px, 'filename_ext': fne}", "response": "Convert string to file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef input_json(i):\n\n\n    t=i['text']\n\n    out(t)\n\n    s=''\n\n    while True:\n       r=inp({'text':''})\n       if r['return']>0: return r\n       ss=r['string'].strip()\n       if ss=='': break\n       s+=ss\n\n    s=s.strip()\n\n    if s=='': s='{}' # empty json\n    else:\n       if not s.startswith('{'): s='{'+s\n       if not s.endswith('}'): s+='}'\n\n    r=convert_json_str_to_dict({'str':s, 'skip_quote_replacement':'yes'})\n    if r['return']>0: return r\n\n    d=r['dict']\n\n    return {'return':0, 'string': s, 'dict':d}", "response": "Input a JSON object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts CK list to dict.", "response": "def convert_ck_list_to_dict(i):\n    \"\"\"\n    Input:  [ \n              CK list: see 'action' function from this kernel\n            ]\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              ck_dict      - {\n                               \"action\":action\n                               \"cid\":module_uoa or CID (x means that it may not be really CID \n                                       and has to be processed specially\n                               \"cids\":[cid1, cid2, cid3, ...]\n                               \"key1\":value1\n                               \"key2\":value2\n                               ...\n                               \"key10\":\"\"\n                               \"key11\":value11\n                               keys/values from file_json; if file extension is .tmp, \n                                                           it will be deleted after read!\n                               keys/values from cmd_json\n                               \"unparsed\":unparsed_cmd\n                             }\n\n    \"\"\"\n    obj={}\n    obj['cids']=[]\n\n    l=len(i)\n\n    if l>0: obj['action']=i[0]\n\n    module_uoa_or_cid=''\n\n    # Parsing\n    cx=True # Start first processing CIDs and then turn it off when something else is encountered\n\n    if l>1:\n       for x in range(1, len(i)):\n           p=i[x].rstrip()\n\n           #####################################\n           if p=='--':\n              cx=False\n              p2=i[x+1:]\n              obj['unparsed']=p2\n              break\n\n           #####################################\n           elif p.startswith('--'):\n              cx=False\n\n              p=p[2:]\n              p1=p \n              p2='yes'\n              q=p.find(\"=\")\n              if q>0:\n                 p1=p[0:q]\n                 if len(p)>q:\n                   p2=p[q+1:]\n              obj[p1]=p2\n\n           #####################################\n           elif p.startswith('-'):\n              cx=False\n\n              p=p[1:]\n              p1=p \n              p2='yes'\n              q=p.find(\"=\")\n              if q>0:\n                 p1=p[0:q]\n                 if len(p)>q:\n                   p2=p[q+1:]\n              obj[p1]=p2\n\n           #####################################\n           elif p.startswith(\"@@@\"):\n              cx=False\n              jd=p[3:]\n              if len(jd)<3:\n                 return {'return':1, 'error':'can\\'t parse command line option '+p}\n\n              y=convert_json_str_to_dict({'str':jd})\n              if y['return']>0: return y\n\n              merge_dicts({'dict1':obj, 'dict2':y['dict']})\n\n           #####################################\n           elif p.startswith(\"@@\"):\n              cx=False\n              key=p[2:]\n\n              x='Add JSON to input'\n              if key!='': x+=' for key \"'+key+'\"'\n              x+=' (double Enter to stop):\\n'\n\n              rx=input_json({'text':x})\n              if rx['return']>0: return rx\n\n              dy=rx['dict']\n\n              dx=obj\n              if key!='':\n                 if key not in obj: obj[key]={}\n                 dx=obj[key]\n\n              merge_dicts({'dict1':dx, 'dict2':dy})\n\n           #####################################\n           elif p.startswith(\"@\"):\n              cx=False\n\n              name=p[1:]\n              if len(name)<2:\n                 return {'return':1, 'error':'can\\'t parse command line option '+p}\n\n              y=load_json_file({'json_file':name})\n              if y['return']>0: return y\n\n              if name.endswith('.tmp'):\n                 os.remove(name)   \n\n              merge_dicts({'dict1':obj, 'dict2':y['dict']})\n\n           #####################################\n           elif p.find('=')>=0:\n              cx=False\n\n              p1=p \n              p2=''\n              q=p.find(\"=\")\n              if q>0:\n                 p1=p[0:q]\n                 if len(p)>q:\n                   p2=p[q+1:]\n              obj[p1]=p2\n           #####################################\n           else:\n              # If no module_uoa_or_cid -> set it\n              if module_uoa_or_cid=='': \n                 module_uoa_or_cid=p\n              else:\n                 # Otherwise add to CIDs\n                 obj['cids'].append(p)\n\n    if module_uoa_or_cid!='': obj['cid']=module_uoa_or_cid\n\n    return {'return':0, 'ck_dict':obj}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes a new node in the cluster.", "response": "def init(i): # pragma: no cover\n    \"\"\"\n    Input:  {}\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n            }\n    \"\"\"\n\n    global cfg, work, initialized, paths_repos, type_long, string_io\n\n    if initialized:\n       return {'return':0}\n\n    # Split version\n    cfg['version']=__version__.split('.')\n\n    # Default URL. FIXME: should be formed from wfe_host and wfe_port when they are known.\n    # cfg['wfe_url_prefix'] = 'http://%s:%s/web?' % (cfg['default_host'], cfg['default_port'])\n\n    # Check long/int types\n    try:\n       x=long\n    except Exception as e:\n       type_long=int\n    else:\n       type_long=long\n\n    # Import StringIO\n    if sys.version_info[0]>2:\n       import io\n       string_io=io.StringIO\n    else:\n       from StringIO import StringIO\n       string_io=StringIO\n\n    # Check where are repos (to keep compatibility with past CK < V1.5)\n    p=''\n\n    import inspect\n    pxx=os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n    px=os.path.dirname(pxx)\n    py=os.path.join(pxx, cfg['subdir_default_repo'])\n    if os.path.isdir(py):\n       p=py\n\n    if p=='':\n       from distutils.sysconfig import get_python_lib\n       px=get_python_lib()\n       py=os.path.join(px, cfg['kernel_dir'], cfg['subdir_default_repo'])\n       if os.path.isdir(py):\n          p=py\n\n    if p=='':\n       import site\n       for px in site.getsitepackages():\n           py=os.path.join(px, cfg['kernel_dir'],cfg['subdir_default_repo'])\n           if os.path.isdir(py):\n              p=py\n              break\n\n    # Check CK_ROOT environment variable\n    s=os.environ.get(cfg['env_key_root'],'').strip()\n    if s!='':\n       work['env_root']=os.path.realpath(s)\n\n       for px in cfg['kernel_dirs']:\n           py=os.path.join(work['env_root'], px, cfg['subdir_default_repo'])\n           if os.path.isdir(py):\n              p=py\n              break\n    elif px!='':\n      work['env_root']=px\n\n    if p=='':\n       return {'return':1, 'error':'Internal CK error (can\\'t find default repo) - please report to authors'}\n\n    # Check default repo\n    work['dir_default_repo']=p\n    work['dir_default_repo_path']=os.path.join(work['dir_default_repo'], cfg['module_repo_name'], cfg['repo_name_default'])\n    work['dir_default_kernel']=os.path.join(work['dir_default_repo'], cfg['subdir_kernel'])\n    work['dir_default_cfg']=os.path.join(work['dir_default_kernel'], cfg['subdir_kernel_default'], cfg['subdir_ck_ext'], cfg['file_meta'])\n\n    work['dir_work_repo']=work['dir_default_repo']\n    work['dir_work_repo_path']=work['dir_default_repo_path']\n    work['dir_work_kernel']=work['dir_default_kernel']\n    work['dir_work_cfg']=work['dir_default_cfg']\n\n    work['repo_name_work']=cfg['repo_name_default']\n    work['repo_uid_work']=cfg['repo_uid_default']\n\n    # Check external repos\n    rps=os.environ.get(cfg['env_key_repos'],'').strip()\n    if rps=='': \n       # Get home user directory\n       from os.path import expanduser\n       home = expanduser(\"~\")\n\n       # In the original version, if path to repos was not defined, I was using CK path,\n       # however, when installed as root, it will fail\n       # rps=os.path.join(work['env_root'],cfg['subdir_default_repos'])\n       # hence I changed to <user home dir>/CK\n       rps=os.path.join(home, cfg['user_home_dir_ext'])\n       if not os.path.isdir(rps):\n          os.makedirs(rps)\n\n    work['dir_repos']=rps\n\n    # Check CK_LOCAL_REPO environment variable - if doesn't exist, create in user space\n    s=os.environ.get(cfg['env_key_local_repo'],'').strip()\n\n    if s=='':\n       # Set up local default repository\n       s=os.path.join(rps, cfg['repo_name_local'])\n       if not os.path.isdir(s):\n          os.makedirs(s)\n\n          # Create description\n          rq=save_json_to_file({'json_file':os.path.join(s,cfg['repo_file']),\n                                'dict':{'data_alias':cfg['repo_name_local'],\n                                        'data_uoa':cfg['repo_name_local'],\n                                        'data_name':cfg['repo_name_local'],\n                                        'data_uid':cfg['repo_uid_local']},\n                                'sort_keys':'yes'})\n          if rq['return']>0: return rq\n\n    if s!='':\n       work['local_kernel_uoa']=cfg['subdir_kernel_default']\n       x=os.environ.get(cfg['env_key_local_kernel_uoa'],'').strip()\n       if x!='': work['local_kernel_uoa']=x\n\n       work['dir_local_repo']=os.path.realpath(s)\n       work['dir_local_repo_path']=os.path.join(work['dir_local_repo'], cfg['module_repo_name'], cfg['repo_name_local'])\n       work['dir_local_kernel']=os.path.join(work['dir_local_repo'], cfg['subdir_kernel'])\n       work['dir_local_cfg']=os.path.join(work['dir_local_kernel'], work['local_kernel_uoa'], cfg['subdir_ck_ext'], cfg['file_meta'])\n\n       # Update work repo!\n       work['dir_work_repo']=work['dir_local_repo']\n       work['dir_work_repo_path']=work['dir_local_repo_path']\n       work['dir_work_kernel']=work['dir_local_kernel']\n       work['dir_work_cfg']=work['dir_local_cfg']\n\n       work['repo_name_work']=cfg['repo_name_local']\n       work['repo_uid_work']=cfg['repo_uid_local']\n\n       paths_repos.append({'path':work['dir_local_repo'],\n                           'repo_uoa':cfg['repo_name_local'],\n                           'repo_uid':cfg['repo_uid_local'],\n                           'repo_alias':cfg['repo_name_local']})\n\n    paths_repos.append({'path':work['dir_default_repo'],\n                        'repo_uoa':cfg['repo_name_default'],\n                        'repo_uid':cfg['repo_uid_default'],\n                        'repo_alias':cfg['repo_name_default']})\n\n    # Prepare repo cache\n    work['dir_cache_repo_uoa']=os.path.join(work['dir_work_repo'],cfg['file_cache_repo_uoa'])\n    work['dir_cache_repo_info']=os.path.join(work['dir_work_repo'],cfg['file_cache_repo_info'])\n\n    # Check if first time and then copy local cache files (with remote-ck)\n    if not os.path.isfile(work['dir_cache_repo_uoa']) and not os.path.isfile(work['dir_cache_repo_info']):\n       rx=load_text_file({'text_file':os.path.join(work['dir_default_repo'],cfg['file_cache_repo_uoa'])})\n       if rx['return']>0: return rx\n       x1=rx['string']\n\n       rx=load_text_file({'text_file':os.path.join(work['dir_default_repo'],cfg['file_cache_repo_info'])})\n       if rx['return']>0: return rx\n       x2=rx['string']\n\n       rx=save_text_file({'text_file':work['dir_cache_repo_info'], 'string':x2})\n       if rx['return']>0: return rx\n\n       rx=save_text_file({'text_file':work['dir_cache_repo_uoa'], 'string':x1})\n       if rx['return']>0: return rx\n\n    # Check if local configuration exists, and if not, create it\n    if not os.path.isfile(work['dir_local_cfg']):\n       # Create empty local configuration\n       rx=add({'repo_uoa':cfg['repo_name_local'],\n               'module_uoa':cfg['subdir_kernel'],\n               'data_uoa':work['local_kernel_uoa']})\n       if rx['return']>0:\n          return {'return':rx['return'],\n                  'error':'can\\'t create local configuration entry'}\n\n    # Read kernel configuration (if exists)\n    if os.path.isfile(work['dir_work_cfg']):\n       r=load_json_file({'json_file':work['dir_work_cfg']})\n       if r['return']>0: return r\n       cfg1=r['dict']\n\n       # Update cfg\n       r=merge_dicts({'dict1':cfg, 'dict2':cfg1})\n       if r['return']>0: return r\n\n    initialized=True\n\n    return {'return':0}"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists all files in the current directory.", "response": "def list_all_files(i):\n    \"\"\"\n    Input:  {\n              path               - top level path\n              (file_name)        - search for a specific file name\n              (pattern)          - return only files with this pattern\n              (path_ext)         - path extension (needed for recursion)\n              (limit)            - limit number of files (if directories with a large number of files)\n              (number)           - current number of files\n              (all)              - if 'yes' do not ignore special directories (like .cm)\n              (ignore_names)     - list of names to ignore\n              (ignore_symb_dirs) - if 'yes', ignore symbolically linked dirs \n                                   (to avoid recursion such as in LLVM)\n              (add_path)         - if 'yes', add path\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              list         - dictionary of all files:\n                               {\"file_with_full_path\":{\"size\":.., \"path\":..}\n              sizes        - sizes of files (the same order)\n              number       - number of files in a current directory (needed for recursion)\n            }\n    \"\"\"\n\n    number=0\n    if i.get('number','')!='': \n       number=int(i['number'])\n\n    inames=i.get('ignore_names',[])\n\n    fname=i.get('file_name','')\n\n    limit=-1\n    if i.get('limit','')!='': \n       limit=int(i['limit'])\n\n    a={} \n\n    iall=i.get('all','')\n\n    pe=''\n    if i.get('path_ext','')!='': \n       pe=i['path_ext']\n\n    po=i.get('path','')\n    if sys.version_info[0]<3: po=unicode(po)\n\n    pattern=i.get('pattern','')\n    if pattern!='':\n       import fnmatch\n\n    xisd=i.get('ignore_symb_dirs','')\n    isd=False\n    if xisd=='yes': isd=True\n\n    ap=i.get('add_path','')\n\n    try:\n       dirList=os.listdir(po)\n    except Exception as e:\n        None\n    else:\n        for fn in dirList:\n            p=os.path.join(po, fn)\n            if iall=='yes' or fn not in cfg['special_directories']:\n               if len(inames)==0 or fn not in inames:\n                  if os.path.isdir(p):\n                     if not isd or os.path.realpath(p)==p:\n                        r=list_all_files({'path':p, 'all':iall, 'path_ext':os.path.join(pe, fn),\n                                          'number':str(number), 'ignore_names':inames, 'pattern':pattern,\n                                          'file_name':fname, 'ignore_symb_dirs':xisd, 'add_path':ap, 'limit': limit})\n                        if r['return']>0: return r\n                        a.update(r['list'])\n                  else:\n                     add=True\n\n                     if fname!='' and fname!=fn:\n                        add=False\n\n                     if pattern!='' and not fnmatch.fnmatch(fn, pattern):\n                        add=False\n\n                     if add:\n                        pg=os.path.join(pe, fn)\n                        if os.path.isfile(p):\n                           a[pg]={'size':os.stat(p).st_size}\n\n                           if ap=='yes': a[pg]['path']=po\n\n                  number=len(a)\n                  if limit!=-1 and number>=limit:\n                     break\n\n    return {'return':0, 'list':a, 'number':str(number)}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreloading the cache for the current user.", "response": "def reload_repo_cache(i):\n    \"\"\"\n    Input:  {\n              (force)      - if 'yes', force recaching\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n            }\n    \"\"\"\n\n    global cache_repo_uoa, cache_repo_info, paths_repos_all, cache_repo_init\n\n    if i.get('force','')=='yes': # pragma: no cover\n       cache_repo_init=False\n       paths_repos_all=[]\n\n    if not cache_repo_init:\n       # Load repo UOA -> UID disambiguator\n       r=load_json_file({'json_file':work['dir_cache_repo_uoa']})\n       if r['return']!=16 and r['return']>0: return r\n       cache_repo_uoa=r.get('dict',{})\n\n       # Load cached repo info\n       r=load_json_file({'json_file':work['dir_cache_repo_info']})\n       if r['return']!=16 and r['return']>0: return r\n       cache_repo_info=r.get('dict',{})\n\n       # Prepare all paths\n       for q in cache_repo_info:\n           qq=cache_repo_info[q]\n           dd=qq['dict']\n           p=dd.get('path','')\n           if p!='':\n              paths_repos_all.append({'path':os.path.normpath(p),\n                                      'repo_uoa':qq['data_uoa'],\n                                      'repo_uid':qq['data_uid'],\n                                      'repo_alias':qq['data_alias']})\n\n    cache_repo_init=True\n\n    return {'return':0}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_repo_cache(i):\n\n    r=save_json_to_file({'json_file':work['dir_cache_repo_uoa'], 'dict':cache_repo_uoa})\n    if r['return']>0: return r\n\n    r=save_json_to_file({'json_file':work['dir_cache_repo_info'], 'dict':cache_repo_info})\n    if r['return']>0: return r\n\n    return {'return':0}", "response": "Save repo cache to json file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload repo info from cache", "response": "def load_repo_info_from_cache(i):\n    \"\"\"\n    Input:  {\n              repo_uoa - repo_uoa\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                           16, if repo not found (may be warning)\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              repo_uoa     - repo UOA\n              repo_uid     - repo UID\n              repo_alias   - repo alias\n\n              all other info from repo dict\n            }\n    \"\"\"\n\n    ruoa=i['repo_uoa']\n    ruid=ruoa\n\n    if ruoa==cfg['repo_name_default'] or ruoa==cfg['repo_uid_default']:\n       d={}\n       d[\"path_to_repo_desc\"]=work['dir_default_repo_path']\n       d[\"data_uid\"]=cfg['repo_uid_default']\n       d[\"data_alias\"]=cfg['repo_name_default']\n       d[\"data_uoa\"]=cfg['repo_name_default']\n       d[\"dict\"]={\"default\":\"yes\"}\n    elif ruoa==cfg['repo_name_local'] or ruoa==cfg['repo_uid_local']:\n       d={}\n       d[\"path_to_repo_desc\"]=work['dir_local_repo_path']\n       d[\"data_uid\"]=cfg['repo_uid_local']\n       d[\"data_alias\"]=cfg['repo_name_local']\n       d[\"data_uoa\"]=cfg['repo_name_local']\n       d[\"dict\"]={\"default\":\"yes\"}\n    else:\n       r=reload_repo_cache({}) # Ignore errors\n       if r['return']>0: return r\n\n       if not is_uid(ruoa): \n          ruid=cache_repo_uoa.get(ruoa,'')\n          if ruid=='':\n             return {'return':1, 'error':'repository \"'+ruoa+'\" is not found in the cache. Check if repository exists or try \"ck recache repo\"'}\n\n       d=cache_repo_info.get(ruid,{})\n       if len(d)==0:\n          return {'return':1, 'error':'repository is not found in the cache'}\n\n    r={'return':0}\n    r.update(d)\n\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding a repository by path.", "response": "def find_repo_by_path(i):\n    \"\"\"\n    Input:  {\n              path - path to repo\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                           16, if repo not found (may be warning)\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              repo_uoa     - repo UOA\n              repo_uid     - repo UID\n              repo_alias   - repo alias\n            }\n    \"\"\"\n\n    p=i['path']\n    if p!='': p=os.path.normpath(p)\n\n    found=False\n    if p==work['dir_default_repo']:\n       uoa=cfg['repo_name_default']\n       uid=cfg['repo_uid_default']\n       alias=uoa\n       found=True\n    elif p==work['dir_local_repo']:\n       uoa=cfg['repo_name_local']\n       uid=cfg['repo_uid_local']\n       alias=uoa\n       found=True\n    else:\n       r=reload_repo_cache({}) # Ignore errors\n       if r['return']>0: return r\n\n       for q in cache_repo_info:\n           qq=cache_repo_info[q]\n           if p==qq['dict'].get('path',''):\n              uoa=qq['data_uoa']\n              uid=qq['data_uid']\n              alias=uid\n              if not is_uid(uoa): alias=uoa\n              found=True\n              break\n\n    if not found:\n       return {'return':16, 'error': 'repository not found in this path'}\n\n    return {'return':0, 'repo_uoa': uoa, 'repo_uid': uid, 'repo_alias':alias}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the path to a repository in the cache.", "response": "def find_path_to_repo(i):\n    \"\"\"\n    Input:  {\n              (repo_uoa) - repo UOA; if empty, get the default repo\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                           16, if repo not found (may be warning)\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              dict         - dict from cache\n              path         - path to repo\n\n              repo_uoa     - repo UOA\n              repo_uid     - repo UID\n              repo_alias   - repo alias\n            }\n    \"\"\"\n\n    a=i.get('repo_uoa','')\n    ai=a\n\n    pr=''\n    if a!='':\n       if a==cfg['repo_name_default'] or a==cfg['repo_uid_default']:\n          pr=work['dir_default_repo']\n          uoa=cfg['repo_name_default']\n          uid=cfg['repo_uid_default']\n          alias=uoa\n          dt={}\n       elif a==cfg['repo_name_local'] or a==cfg['repo_uid_local']:\n          pr=work['dir_local_repo']\n          uoa=cfg['repo_name_local']\n          uid=cfg['repo_uid_local']\n          alias=uoa\n          dt={}\n       else:\n          # Reload cache if not initialized\n          r=reload_repo_cache({}) # Ignore errors\n          if r['return']>0: return r\n\n          if not is_uid(a):\n             ai=cache_repo_uoa.get(a,'')\n             if ai=='':\n                return {'return':1, 'error':'repository \"'+a+'\" was not found in cache'}\n\n          cri=cache_repo_info.get(ai, {})\n          if len(cri)==0:\n             return {'return':1, 'error':'repository \"'+ai+'\" was not found in cache'}\n\n          dt=cri.get('dict',{})\n          pr=dt.get('path','')\n\n          uoa=cri['data_uoa']\n          uid=cri['data_uid']\n          alias=cri['data_alias']\n\n    else:\n       # Get current repo path\n       pr=work['dir_work_repo']\n       uoa=work['repo_name_work']\n       uid=work['repo_uid_work']\n       alias=uoa\n       dt={}\n\n    return {'return':0, 'path':pr, 'repo_uoa':uoa, 'repo_uid':uid, 'repo_alias':alias, 'dict':dt}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_path_to_data(i):\n    muoa=i['module_uoa']\n    muid='?'\n    duoa=i['data_uoa']\n    duid='?'\n\n    ruoa=i.get('repo_uoa','')\n    ruid=''\n    ralias=''\n    if ruoa!='':\n       r=find_path_to_repo({'repo_uoa':ruoa})\n       if r['return']>0: return r\n       ps=[r]\n       qmax=1\n    else:\n       ps=paths_repos\n       qmax=2\n\n    # Search\n    found=False\n\n    pr=''\n    pm=''\n    pd=''\n\n    for q in range(0,qmax):\n        if found: break\n\n        if q==1:\n           # Check / reload all repos\n           r=reload_repo_cache({}) # Ignore errors\n           if r['return']>0: return r\n           ps=paths_repos_all\n\n        for prx in ps:\n            pr=prx['path']\n            ruoa=prx['repo_uoa']\n            ruid=prx['repo_uid']\n            ralias=prx['repo_alias']\n            r=find_path_to_entry({'path':pr, 'data_uoa':muoa})\n            if r['return']>0 and r['return']!=16: return r\n            elif r['return']==0:\n               muoa=r['data_uoa']\n               muid=r['data_uid']\n               malias=r['data_alias']\n               pm=r['path']\n               r1=find_path_to_entry({'path':pm, 'data_uoa':duoa})\n               if r1['return']>0 and r1['return']!=16: return r1\n               elif r1['return']==0:\n                  found=True\n                  pd=r1['path']\n                  duoa=r1['data_uoa']\n                  duid=r1['data_uid']\n                  dalias=r1['data_alias']\n                  break\n\n               if found: break\n\n    if not found:\n       s=''\n#       if ruoa!='': s+=ruoa+':'\n       s+=muoa+':'+duoa+'\" ('\n       if ruoa!='': \n#          if ruid!='':s+=ruid+':'\n#          else: s+='?:'\n          s+='?:'\n       s+=muid+':'+duid+')'\n\n       return {'return':16, 'error':'can\\'t find path to CK entry \"'+s}\n\n#    # Get info about repo\n#    if ruid=='':\n#       r=find_repo_by_path({'path':pr})\n#       if r['return']>0: return r\n#       ruoa=r['repo_uoa']\n#       ruid=r['repo_uid']\n#       ralias=r['repo_alias']\n#       qmax=1\n\n    return {'return':0, 'path':pd, 'path_module':pm, 'path_repo':pr,\n                        'repo_uoa':ruoa, 'repo_uid':ruid, 'repo_alias':ralias,\n                        'module_uoa':muoa, 'module_uid':muid, 'module_alias':malias,\n                        'data_uoa':duoa, 'data_uid':duid, 'data_alias':dalias}", "response": "Find the path to data UOA entry with the specified module UOA"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind path to a repository entry in the tree", "response": "def find_path_to_entry(i):\n    \"\"\"\n    Input:  {\n              path     - path to a repository\n              data_uoa - data UOA\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                           16, if data not found (may be warning)\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              path         - path to data entry\n              data_uid     - data uid (from UOA)\n              data_alias   - data alias (from UOA)\n              data_uoa     - data alias or data uid, if data alias==''\n            }\n    \"\"\"\n\n    p=i['path']\n    duoa=i['data_uoa']\n\n    if duoa=='': # pragma: no cover\n       raise Exception('data_uoa is empty')\n\n    # Disambiguate UOA\n    alias=''\n    if is_uid(duoa):\n       # If UID\n       uid=duoa\n\n       # Check if alias exists\n       p1=os.path.join(p, cfg['subdir_ck_ext'], cfg['file_alias_u'] + uid)\n       found_alias=False\n       if os.path.isfile(p1):\n          try:\n             f=open(p1)\n             alias=f.readline().strip()\n             f.close()\n             found_alias=True\n          except Exception as e:\n             None\n\n       # If alias exists, check directory with alias\n       if found_alias:\n          p2=os.path.join(p, alias)\n          return {'return':0, 'path':p2, 'data_uid':uid, 'data_alias':alias, 'data_uoa':alias}\n\n       p2=os.path.join(p, uid)\n       if os.path.isdir(p2):\n          return {'return':0, 'path':p2, 'data_uid':uid, 'data_alias':'', 'data_uoa':uid}\n\n       return {'return':-1}\n\n    # If alias\n    alias=duoa\n\n    p1=os.path.join(p, alias)\n    if sys.version_info[0]<3: \n       try: p1=p1.encode('utf8')\n       except Exception as e: pass\n    if os.path.isdir(p1):\n       # Check uid for this alias\n       p2=os.path.join(p, cfg['subdir_ck_ext'], cfg['file_alias_a'] + alias)\n       try:\n          f=open(p2)\n          uid=f.readline().strip()\n          f.close()\n       except Exception as e:\n          return {'return':10, 'error':'inconsistent entry: alias \"'+alias+'\" exists, but not the UID in file '+p2, \n                               'path':p1, 'data_alias':alias}\n\n       return {'return':0, 'path':p1, 'data_uid':uid, 'data_alias':alias, 'data_uoa':alias}\n\n    return {'return':16, 'error':'can\\'t find path to CK entry'}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads meta data from a path", "response": "def load_meta_from_path(i):\n    \"\"\"\n    Input:  {\n              path           - path to a data entry\n\n              (skip_updates) - if 'yes', do not load updates\n              (skip_desc)    - if 'yes', do not load descriptions\n            }\n\n    Output: {\n              return         - return code =  0, if successful\n                                           >  0, if error\n              (error)        - error text if return > 0\n\n              dict           - dict with meta description\n              path           - path to json file with meta description\n\n              (info)         - dict with info if exists\n              (path_info)    - path to json file with info\n\n              (updates)      - dict with updates if exists\n              (path_updates) - path to json file with updates\n              (path_desc)    - path to json file with API description\n            }\n    \"\"\"\n\n    p=i['path']\n\n    slu=i.get('skip_updates','')\n    sld=i.get('skip_desc','')\n\n    p1=os.path.join(p, cfg['subdir_ck_ext'], cfg['file_meta'])\n    if not os.path.isfile(p1):\n       p1=os.path.join(p, cfg['subdir_ck_ext'], cfg['file_meta_old']) # For compatibility with cM\n       if not os.path.isfile(p1):\n          p1=''\n\n    if p1!='':\n       rx={'return':0}\n\n       r=load_json_file({'json_file':p1})\n       if r['return']>0: return r\n       rx['path']=p1\n       rx['dict']=r['dict']\n\n       # Check info file\n       p2=os.path.join(p, cfg['subdir_ck_ext'], cfg['file_info'])\n       if os.path.isfile(p2):\n          r=load_json_file({'json_file':p2})\n          if r['return']>0: return r\n          rx['path_info']=p2\n          rx['info']=r['dict']\n\n       # Check updates file\n       if slu!='yes':\n          p3=os.path.join(p, cfg['subdir_ck_ext'], cfg['file_updates'])\n          if os.path.isfile(p3):\n             r=load_json_file({'json_file':p3})\n             if r['return']>0: return r\n             rx['path_updates']=p3\n             rx['updates']=r['dict']\n\n       # Check desc file\n       if sld!='yes':\n          p4=os.path.join(p, cfg['subdir_ck_ext'], cfg['file_desc'])\n          if os.path.isfile(p4):\n             r=load_json_file({'json_file':p4})\n             if r['return']>0: return r\n             rx['path_desc']=p4\n             rx['desc']=r['dict']\n\n       return rx\n    else:\n       return {'return':1, 'error':'meta description is not found in path '+p}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_module_from_path(i):\n\n    p=i['path']\n    n=i['module_code_name']\n\n    xcfg=i.get('cfg',None)\n\n    # Find module\n    try:\n       x=imp.find_module(n, [p])\n    except ImportError as e: # pragma: no cover\n       return {'return':1, 'error':'can\\'t find module code (path='+p+', name='+n+', err='+format(e)+')'}\n\n    ff=x[0]\n    full_path=x[1]\n\n    # Check if code has been already loaded\n    if full_path in work['cached_module_by_path'] and work['cached_module_by_path_last_modification'][full_path]==os.path.getmtime(full_path):\n       ff.close()\n       # Code already loaded \n       return work['cached_module_by_path'][full_path]\n\n    # Check if has dependency on specific CK kernel version\n    if xcfg!=None:\n       kd=xcfg.get('min_kernel_dep','')\n       if kd!='':\n          rx=check_version({'version':kd})\n          if rx['return']>0: return rx\n\n          ok=rx['ok']\n          version_str=rx['current_version']\n\n          if ok!='yes':\n             return {'return':1, 'error':'module \"'+i.get('data_uoa','')+'\" requires minimal CK kernel version '+kd+' while your version is '+version_str} \n\n    # Generate uid for the run-time extension of the loaded module \n    # otherwise modules with the same extension (key.py for example) \n    # will be reloaded ...\n\n    r=gen_uid({})\n    if r['return']>0: return r\n    ruid='rt-'+r['data_uid']\n\n    try:\n       c=imp.load_module(ruid, ff, full_path, x[2])\n    except ImportError as e: # pragma: no cover\n       return {'return':1, 'error':'can\\'t load module code (path='+p+', name='+n+', err='+format(e)+')'}\n\n    x[0].close()\n\n    # Initialize module with this CK instance \n    c.ck=sys.modules[__name__]\n    if xcfg!=None: c.cfg=xcfg\n\n    # Initialize module\n    if i.get('skip_init','')!='yes':\n       # Check if init function exists\n       if getattr(c, 'init')!=None:\n          r=c.init(i)\n          if r['return']>0: return r\n\n    r={'return':0, 'code':c, 'path':full_path, 'cuid':ruid}\n\n    # Cache code together with its time of change\n    work['cached_module_by_path'][full_path]=r\n    work['cached_module_by_path_last_modification'][full_path]=os.path.getmtime(full_path)\n\n    return r", "response": "Load a module from a path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef perform_remote_action(i):\n\n    # Import modules compatible with Python 2.x and 3.x\n    import urllib\n\n    try:    import urllib.request as urllib2\n    except: import urllib2 # pragma: no cover\n\n    try:    from urllib.parse import urlencode\n    except: from urllib import urlencode # pragma: no cover\n\n    rr={'return':0}\n\n    # Get action\n    act=i.get('action','')\n\n    # Check output\n    o=i.get('out','')\n\n    if o=='con':\n#       out('Initiating remote access ...')\n#       out('')\n       i['out']='con'\n       i['quiet']='yes'\n       if act=='pull':\n          i['out']='json'\n    else:\n       i['out']='json'\n\n#    # Clean up input\n#    if o!='json_file': \n#       rr['out']='json' # Decided to return json to show that it's remote ...\n\n    if 'cid' in i: \n       del(i['cid']) # already processed\n\n    # Get URL\n    url=i.get('remote_server_url','')\n\n    # Process i\n    if 'remote_server_url' in i: del(i['remote_server_url'])\n\n    # Pre process if push file ...\n    if act=='push':\n       # Check file\n       fn=i.get('filename','')\n       if fn=='':\n          x=i.get('cids',[])\n          if len(x)>0:\n             fn=x[0]\n\n       if fn=='':\n          return {'return':1, 'error':'filename is empty'}\n\n       if not os.path.isfile(fn):\n          return {'return':1, 'error':'file '+fn+' not found'}\n\n       rx=convert_file_to_upload_string({'filename':fn})\n       if rx['return']>0: return rx\n\n       i['file_content_base64']=rx['file_content_base64']\n\n       # Leave only filename without path\n       i['filename']=os.path.basename(fn)\n\n    # Prepare post variables\n    r=dumps_json({'dict':i, 'skip_indent':'yes'})\n    if r['return']>0: return r\n    s=r['string'].encode('utf8')\n\n    post=urlencode({'ck_json':s})\n    if sys.version_info[0]>2: post=post.encode('utf8')\n\n    # If auth\n    au=i.get('remote_server_user','')\n    if au!='': \n       del(i['remote_server_user'])\n\n       ap=i.get('remote_server_pass','')\n       if ap!='': \n          del(i['remote_server_pass'])\n\n       auth = urllib2.HTTPPasswordMgrWithDefaultRealm()\n       auth.add_password(None, url, au, ap)\n       urllib2.install_opener(urllib2.build_opener(urllib2.HTTPBasicAuthHandler(auth)))\n\n    # Prepare request\n    request = urllib2.Request(url, post)\n\n    # Connect\n    try:\n       f=urllib2.urlopen(request)\n    except Exception as e:\n       return {'return':1, 'error':'Access to remote CK repository failed ('+format(e)+')'}\n\n    # Read from Internet\n    try:\n       s=f.read()\n       f.close()\n    except Exception as e:\n       return {'return':1, 'error':'Failed reading stream from remote CK web service ('+format(e)+')'}\n\n    # Check output\n    try: s=s.decode('utf8')\n    except Exception as e: pass\n    if o=='con' and act!='pull':\n       out(s.rstrip())\n    else:\n       # Try to convert output to dictionary\n       r=convert_json_str_to_dict({'str':s, 'skip_quote_replacement':'yes'})\n       if r['return']>0: \n          return {'return':1, 'error':'can\\'t parse output from remote CK server ('+r['error']+'):\\n'+s[:256]+'\\n\\n...)'}\n\n       d=r['dict']\n\n       if 'return' in d: d['return']=int(d['return']) # Fix for some strange behavior when 'return' is not integer - should check why ...\n\n       if d.get('return',0)>0:\n          return d\n\n       # Post process if pull file ...\n       if act=='pull':\n          if o!='json' and o!='json_file':\n             # Convert encoded file to real file ...\n             x=d.get('file_content_base64','')\n\n             fn=d.get('filename','')\n             if fn=='': fn=cfg['default_archive_name']\n\n             r=convert_upload_string_to_file({'file_content_base64':x, 'filename':fn})\n             if r['return']>0: return r\n\n             if 'file_content_base64' in d: del(d['file_content_base64'])\n\n       rr.update(d)\n\n    # Restore original output\n    i['out']=o\n\n    return rr", "response": "Perform remote action on the ironic server"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform the action of the CK kernel", "response": "def perform_action(i):\n    \"\"\"\n    Input:  {\n              all parameters from function 'access'\n\n              (web)         - if 'yes', called from the web\n\n              (common_func) - if 'yes', ignore search for modules \n                                        and call common func from the CK kernel\n\n              (local)       - if 'yes', run locally even if remote repo ...\n            }\n\n    Output: {\n              return  - return code =  0, if successful\n                                         >  0, if error\n              (error) - error text if return > 0\n\n              (out)   - if action change output, return it\n              Output from the module/action\n            }\n    \"\"\"\n\n    # Check action\n    action=i.get('action','')\n    if action=='':\n       action='short_help'\n    elif action=='-?' or action=='-h' or action=='--help':\n       action='help'\n\n    # Check web\n    wb=i.get('web','')\n\n    # Substitute # in CIDs\n    cid=i.get('cid','')\n    cids=i.get('cids',[])\n\n    out=i.get('out','')\n\n    need_subst=False\n    rc={} # If CID from current directory\n\n    if cid.startswith(cfg['detect_cur_cid']) or cid.startswith(cfg['detect_cur_cid1']):\n       need_subst=True\n    else:\n       for c in cids:\n           if c.startswith(cfg['detect_cur_cid']) or c.startswith(cfg['detect_cur_cid1']): \n              need_subst=True\n              break\n\n    # If need to substitute #, attempt to detect current CID\n    if need_subst:\n       rc=detect_cid_in_current_path({})\n       if rc['return']>0: return rc\n\n    # Process cid (module or CID)\n    module_uoa=cid\n    if cid.find(':')>=0 or cid.startswith(cfg['detect_cur_cid']) or cid.startswith(cfg['detect_cur_cid1']):\n       # Means that CID\n       r=parse_cid({'cid':cid, 'cur_cid':rc})\n       if r['return']>0: return r\n       module_uoa=r.get('module_uoa','')\n\n       duoa=r.get('data_uoa','')\n       if duoa!='': i['data_uoa']=duoa\n\n       ruoa=r.get('repo_uoa','')\n       if ruoa!='': i['repo_uoa']=ruoa\n\n\n    # If module_uoa exists in input, set module_uoa\n    if i.get('module_uoa','')!='': module_uoa=i['module_uoa']\n    i['module_uoa']=module_uoa\n\n    # Check if repo exists and possibly remote!\n    remote=False\n\n    local=i.get('local','')\n\n    rs=i.get('remote_server_url','')\n    if rs=='': \n       ruoa=i.get('repo_uoa','')\n       if ruoa!='' and ruoa.find('*')<0 and ruoa.find('?')<0:\n          rq=load_repo_info_from_cache({'repo_uoa':ruoa})\n          if rq['return']>0: return rq\n\n          dd=rq.get('dict',{})\n          if dd.get('remote','')=='yes' and local!='yes':\n             rs=dd.get('url','')\n             if rs=='':\n                return {'return':1, 'error':'URL of remote repository is not defined'}\n\n             i['remote_server_url']=rs\n\n             if dd.get('remote_user','')!='':\n                i['remote_server_user']=dd['remote_user']\n\n             # It is completely unsave - just for proof of concept ...\n             if dd.get('remote_password','')!='':\n                i['remote_server_pass']=dd['remote_password']\n\n             if dd.get('remote_repo_uoa','')!='':\n                i['repo_uoa']=dd['remote_repo_uoa']\n             else:\n                del (i['repo_uoa'])\n\n             if i.get('remote_repo_uoa','')!='':\n                i['repo_uoa']=i['remote_repo_uoa']\n                del(i['remote_repo_uoa'])\n\n    if rs!='' and local!='yes':\n       return perform_remote_action(i)\n\n    # Process and parse cids -> xcids\n    xcids=[]\n\n    for c in cids:\n       r=parse_cid({'cid':c, 'cur_cid':rc, 'ignore_error':'yes'}) # here we ignore errors, since can be a file name, etc\n       if r['return']>0: return r\n       xcids.append(r)\n    i['xcids']=xcids\n\n    # Check if common function\n    cf=i.get('common_func','')\n\n    # Check if no module_uoa, not common function, then try to get module from current \n    module_detected_from_dir=False\n    if not need_subst and cf!='yes' and module_uoa=='' and action not in cfg['common_actions']:\n       rc=detect_cid_in_current_path({})\n       if rc['return']==0:\n          module_uoa=rc.get('module_uoa','')\n          module_detected_from_dir=True\n\n    display_module_uoa  = module_uoa\n    default_action_name = None\n    loaded_module       = None\n\n    ## If a specific module_uoa was given (not a wildcard) :\n    #\n    if cf!='yes' and module_uoa!='' and module_uoa.find('*')<0 and module_uoa.find('?')<0:\n       # Find module and load meta description\n       rx=load({'module_uoa':cfg['module_name'], \n                'data_uoa':module_uoa})\n       if rx['return']>0: return rx\n\n       xmodule_uoa=rx['data_uoa']\n       xmodule_uid=rx['data_uid']\n       display_module_uoa = '\"{}\"'.format(xmodule_uoa)\n       if xmodule_uoa!=xmodule_uid:\n          display_module_uoa += ' ({})'.format(xmodule_uid)\n\n       # Check if allowed to run only from specific repos\n       if cfg.get('allow_run_only_from_allowed_repos','')=='yes':\n          ruid=rx['repo_uid']\n\n          if ruid not in cfg.get('repo_uids_to_allow_run',[]):\n             return {'return':1, 'error':'executing modules from this repository is not allowed'}\n\n       u=rx['dict']\n       p=rx['path']\n\n       declared_action      = action in u.get('actions',{})\n       default_action_name  = u.get('default_action_name','')\n       intercept_kernel     = i.get('{}.intercept_kernel'.format(module_uoa),'')\n\n       if declared_action or default_action_name:\n          # Load module\n          mcn=u.get('module_name',cfg['module_code_name'])\n\n          r=load_module_from_path({'path':p, 'module_code_name':mcn, 'cfg':u, 'data_uoa':rx['data_uoa']})\n          if r['return']>0: return r\n\n          loaded_module=r['code']\n          loaded_module.work['self_module_uid']=rx['data_uid']\n          loaded_module.work['self_module_uoa']=rx['data_uoa']\n          loaded_module.work['self_module_alias']=rx['data_alias']\n          loaded_module.work['path']=p\n\n          action1=u.get('actions_redirect',{}).get(action,'')\n          if action1=='': action1=action\n\n          if i.get('help','')=='yes' or i.get('api','')=='yes':\n             return get_api({'path':p, 'func':action1, 'out':out})\n\n          if wb=='yes' and (out=='con' or out=='web') and u.get('actions',{}).get(action,{}).get('for_web','')!='yes':\n             return {'return':1, 'error':'this action is not supported in remote/web mode'}\n\n          if declared_action:\n              a=getattr(loaded_module, action1)\n              return a(i)\n          elif default_action_name and intercept_kernel:\n              a=getattr(loaded_module, default_action_name)\n              return a(i)\n          # otherwise fall through and try a \"special\" kernel method first\n\n\n    # Check if action == special keyword (add, delete, list, etc)\n    if (module_uoa!='' and action in cfg['common_actions']) or \\\n       ((module_uoa=='' or module_detected_from_dir) and action in cfg['actions']):\n       # Check function redirect - needed if action \n       #   is the same as internal python keywords such as list\n       action1=cfg['actions_redirect'].get(action,'')\n       if action1=='': action1=action\n\n       if i.get('help','')=='yes' or i.get('api','')=='yes':\n          return get_api({'path':'', 'func':action1, 'out':out})\n\n       if wb=='yes' and (out=='con' or out=='web') and cfg.get('actions',{}).get(action,{}).get('for_web','')!='yes':\n          return {'return':1, 'error':'this action is not supported in remote/web mode '}\n\n       a=getattr(sys.modules[__name__], action1)\n       return a(i)\n\n    if default_action_name:\n       a=getattr(loaded_module, default_action_name)\n       return a(i)\n\n    # Prepare error\n    if module_uoa=='':\n       er='in kernel'\n    else:\n       er='in module '+display_module_uoa\n\n    return {'return':1,'error':'action \"'+action+'\" not found '+er}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_api(i):\n\n    p=i.get('path','')\n    f=i.get('func','')\n    o=i.get('out','')\n\n    muoa=i.get('module_uoa','')\n\n    t=''      # last function description (if redirect to another API)\n    t_orig='' # original function description\n    l=0       # API line\n    a=''      # accumulated API\n\n    if p=='' and muoa!='':\n       rx=load({'module_uoa':cfg['module_name'], \n                'data_uoa':muoa})\n       if rx['return']>0: return rx\n       p=rx['path']\n\n    if p=='':\n       p1=os.path.dirname(os.path.dirname(work['dir_default_repo']))\n       p=os.path.join(p1, cfg['file_kernel_py'])\n\n       if not os.path.isfile(p):\n          return {'return':1, 'error':'kernel not found in '+p}\n    else:\n       p=os.path.join(p, 'module.py')\n\n    if os.path.isfile(p):\n       rx=load_text_file({'text_file':p, 'split_to_list':'yes'})\n       if rx['return']>0: return rx\n\n       lst=rx['lst']\n\n       k=-1\n       while k<len(lst)-1:\n             k+=1\n             q=lst[k]\n\n             if q.find('def '+f+'(')>=0 or q.find('def '+f+' (')>=0 or \\\n                q.find('def\\t'+f+'(')>=0 or q.find('def\\t'+f+' (')>=0:\n\n                j=k-1\n                if j>=0 and lst[j].strip()=='': j-=1\n\n                x='x'\n                while j>=0 and x!='' and not x.startswith('###'):\n                  x=lst[j].strip()\n                  if x!='' and not x.startswith('###'):\n                     if x=='#': x=' '\n                     elif x.startswith('# '): x=x[2:]\n                     t=x+'\\n'+t\n                  j-=1\n\n                if t!='':\n                   l=j+2\n                   if t_orig=='': t_orig=t \n\n                # Find starting point of an API\n                j=k+1\n                if j<len(lst) and lst[j].find('\"\"\"')>=0: \n                   j+=1\n\n                # Check if redirect to another function\n                restart=False\n                if j<len(lst):\n                   x=lst[j].strip()\n                   if x.lower().startswith(\"see\"):\n                      z1=x.find('\"')\n                      if z1>0:\n                         z2=x.find('\"',z1+1)\n                         if z2>0:\n                            f=x[z1+1:z2] # new function name\n                            k=-1\n                            restart=True # restart search for new function\n                   \n                if not restart:\n                   x=''\n                   while x.find('\"\"\"')<0 and j<len(lst):\n                       x=lst[j]\n                       if x.find('\"\"\"')<0:\n                          a+=x+'\\n'\n                       j+=1\n\n    if t=='' and a=='':\n       return {'return':1, 'error':'function not found'}\n\n    dd=t_orig.strip()\n    if o=='con':\n       out('Description: '+dd)\n       out('')\n       out('Module: '+p)\n       out('')\n       out('Line: '+str(l))\n       out('')\n       out('API:')\n       out(a)\n    elif o=='web':\n       out('<B>Function:</B> '+t+'<BR>')\n       out('<BR>')\n       out('<B>Module:</B> '+p+'<BR>')\n       out('<BR>')\n       out('<B>API:</B><BR>')\n       out('<pre>')\n       out(a)\n       out('</pre><BR>')\n\n    return {'return':0, 'title':t, 'desc':dd, 'module':p, 'api':a, 'line':l}", "response": "Get API from file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_cid(i):\n\n    r={'return':0}\n    c=i['cid'].strip()\n\n    ie=i.get('ignore_error','')\n\n    cc=i.get('cur_cid', {})\n\n    a0=cc.get('repo_uoa','')\n    m0=cc.get('module_uoa','')\n    d0=cc.get('data_uoa','')\n\n    if c.startswith(cfg['detect_cur_cid']) or c.startswith(cfg['detect_cur_cid1']):\n       c=c[1:]\n\n    x=c.split(':')\n    if len(x)<2 and m0=='':\n       if ie!='yes': \n          return {'return':1, 'error':'unknown CID format'}\n       else: \n          return r\n\n    if c=='':\n       r['repo_uoa']=a0\n       r['module_uoa']=m0\n       r['data_uoa']=d0\n    elif len(x)==1:\n       if a0!='': r['repo_uoa']=a0\n       r['module_uoa']=m0\n       r['data_uoa']=x[0]\n    elif len(x)==2:\n       if a0!='': r['repo_uoa']=a0\n       r['module_uoa']=x[0]\n       r['data_uoa']=x[1]\n    elif len(x)==3:\n       r['repo_uoa']=x[0]\n       r['module_uoa']=x[1]\n       r['data_uoa']=x[2]\n    else:\n       if ie!='yes': \n          return {'return':1, 'error':'unknown CID format'}\n\n    return r", "response": "Parse CID from input data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_entry(i):\n\n    p0=i.get('path','')\n    d=i.get('data_uoa','')\n    di=i.get('data_uid','')\n\n    xforce=i.get('force','')\n    if xforce=='yes':\n       force=True\n    else:\n       force=False\n\n    # If no uoa, generate UID\n    alias=''\n    uid=''\n    if d=='':\n       if di=='':\n          r=gen_uid({})\n          if r['return']>0: return r\n          uid=r['data_uid']\n       else:\n          uid=di\n\n          # Check if already exists\n          r=find_path_to_entry({'path':p0, 'data_uoa':uid})\n          if r['return']>0 and r['return']!=16: return r\n          elif r['return']==0:\n             r['return']=16\n             return r\n\n       alias=''\n    else:\n       # Check if already exists\n       if not force:\n          r=find_path_to_entry({'path':p0, 'data_uoa':d})\n          if r['return']>0 and r['return']!=16: return r\n          elif r['return']==0:\n             r['return']=16\n             return r\n\n       if is_uid(d):\n          uid=d\n          alias=''\n       else:\n          alias=d\n          if di!='':\n             uid=i['data_uid']\n          else: \n             r=gen_uid({})\n             if r['return']>0: return r\n             uid=r['data_uid']\n\n    if alias!='':\n       p=os.path.join(p0, alias)\n    else:\n       p=os.path.join(p0, uid)\n\n    # Check alias disambiguation\n    if alias!='':\n       p1=os.path.join(p0, cfg['subdir_ck_ext'])\n       if not os.path.isdir(p1):\n          # Create .cm directory\n          try:    # pragma: no cover\n             os.mkdir(p1)\n          except Exception as e:\n             return {'return':1, 'error':format(e)}\n\n       # Check if alias->uid exist\n       p3=os.path.join(p1, cfg['file_alias_a'] + alias)\n       if os.path.isfile(p3):     # pragma: no cover\n          try:\n             fx=open(p3)\n             uid1=fx.readline().strip()\n             fx.close()\n          except Exception as e:\n             None\n\n          if uid1!=uid:\n             return {'return':1, 'error':'different alias->uid disambiguator already exists in '+p3}\n\n       ru=save_text_file({'text_file':p3, 'string':uid+'\\n'})\n       if ru['return']>0: return ru\n\n       # Check if uid->alias exist\n       p2=os.path.join(p1, cfg['file_alias_u'] + uid)\n       if os.path.isfile(p2):     # pragma: no cover\n          try:\n             fx=open(p2)\n             alias1=fx.readline().strip()\n             fx.close()\n          except Exception as e:\n             None\n\n          if alias1!=alias:\n             return {'return':1, 'error':'different uid->alias disambiguator already exists in '+p2}\n\n       ru=save_text_file({'text_file':p2, 'string':alias+'\\n'})\n       if ru['return']>0: return ru\n\n    # Create directory\n    if not os.path.exists(p):\n       try:\n          os.mkdir(p)\n       except Exception as e:\n          return {'return':1, 'error':format(e)}\n\n    uoa=uid\n    if alias!='': uoa=alias\n\n    return {'return':0, 'path':p, 'data_uid':uid, 'data_alias':alias, 'data_uoa':uoa}", "response": "Create an entry in the nexus tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete an alias from the entry in the entry in the entry in the entry in the entry in the entry.", "response": "def delete_alias(i):\n    \"\"\"\n    Input:  {\n              path         - path to the entry\n              data_uid     - data UID\n              (data_alias) - data alias\n              (repo_dict)  - repo cfg if available to check sync\n              (share)      - if 'yes', try to rm via GIT\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n            }\n    \"\"\"\n\n    rd=i.get('repo_dict',{})\n    rshared=rd.get('shared','')\n    rsync=rd.get('sync','')\n\n    if i.get('share','')=='yes': rshared='git'\n\n    p=i['path']\n    alias=i.get('data_alias','')\n    uid=''\n\n    if alias!='' and os.path.isdir(p):\n       p0=os.path.join(p, cfg['subdir_ck_ext'])\n\n       p9=cfg['file_alias_a'] + alias\n       p1=os.path.join(p0, p9)\n\n       if rshared!='':\n          ppp=os.getcwd()\n          os.chdir(p0)\n\n       if os.path.isfile(p1):\n          try:\n             f=open(p1)\n             uid=f.readline().strip()\n             f.close()\n          except Exception as e:\n             None\n\n          if rshared!='':\n             ss=cfg['repo_types'][rshared]['rm'].replace('$#files#$', p9)\n             rx=os.system(ss)\n\n          if os.path.isfile(p1): os.remove(p1)\n\n       if uid=='': uid=i['data_uid']\n\n       if uid!='':\n          p9=cfg['file_alias_u'] + uid\n          p1=os.path.join(p0, p9)\n          if os.path.isfile(p1):\n             if rshared!='':\n                ss=cfg['repo_types'][rshared]['rm'].replace('$#files#$', p9)\n                rx=os.system(ss)\n\n             if os.path.isfile(p1): os.remove(p1)\n\n       if rshared!='':\n          os.chdir(ppp)\n\n    return {'return':0}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_directory(i):\n\n    import shutil\n\n    p=i['path']\n\n    if os.path.isdir(p):\n       shutil.rmtree(p, onerror=rm_read_only)\n\n    return {'return':0}", "response": "Input:  {\n              path - path to delete\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n            }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef flatten_dict(i):\n\n    prefix='#'\n    if i.get('prefix','')!='': prefix=str(i['prefix'])\n\n    a=i['dict']\n    aa={}\n\n    pk=i.get('prune_keys','')\n    if pk=='': pk=[]\n\n    flatten_dict_internal(a, aa, prefix, pk)\n\n    return {'return':0, 'dict': aa}", "response": "Flatten a dictionary into a single node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_by_flat_key(i):\n    # Check vars\n    v=None\n\n    a=i['dict']\n    k=i['key']\n\n    # Remove leading # if exists\n    if len(k)>0 and k[0:1]=='#': k=k[1:]\n\n    k1=''\n    kt='' # type '#' or '@'\n    x=0\n    finish=False\n\n    while not finish:\n        y=k[x]\n        x+=1\n\n        if y=='#' or y=='@':\n           if kt=='#':\n              if k1 not in a: break\n              a=a[k1]\n           elif kt=='@':\n              if len(a)<=type_long(k1): break\n              a=a[type_long(k1)]\n           k1=''\n           kt=y\n        else:\n           k1+=y\n\n        if x>=len(k): break\n\n    if k1!='' and kt!='':\n       if kt=='#':   \n          if k1 in a: v=a[k1]\n       else:         \n          if len(a)>type_long(k1): v=a[type_long(k1)]\n\n    return {'return':0, 'value': v}", "response": "Get the value of a flat key in the dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef restore_flattened_dict(i):\n    # Check vars\n    a={} # default\n    b=i['dict']\n    first=True\n    for x in b:\n        if first: \n           first=False\n           y=x[1:2]\n           if y=='@': a=[]\n           else: a={}\n\n        set_by_flat_key({'dict':a, 'key':x, 'value':b[x]})\n\n    return {'return':0, 'dict': a}", "response": "Restore the flattened dict to the original state"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets lock for the current entry in the order they were acquired.", "response": "def set_lock(i):\n    \"\"\"\n    Input:  {\n              path               - path to be locked\n\n              (get_lock)         - if 'yes', lock this entry\n              (lock_retries)     - number of retries to aquire lock (default=11)\n              (lock_retry_delay) - delay in seconds before trying to aquire lock again (default=3)\n              (lock_expire_time) - number of seconds before lock expires (default=30)\n\n              (unlock_uid)       - UID of the lock to release it\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         = 32, couldn't acquire lock (still locked after all retries)\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              (lock_uid)   - lock UID, if locked successfully\n            }\n    \"\"\"\n    p=i['path']\n\n    gl=i.get('get_lock','')\n    uuid=i.get('unlock_uid','')\n    exp=float(i.get('lock_expire_time','30'))\n\n    rr={'return':0}\n\n    if gl=='yes' or uuid!='':\n       pl=os.path.join(p, cfg['subdir_ck_ext'], cfg['file_for_lock'])\n\n       luid=''\n       if os.path.isfile(pl):\n          import time\n\n          # Read lock file\n          try:\n             f=open(pl)\n             luid=f.readline().strip()\n             exp=float(f.readline().strip())\n             if exp<0: exp=1\n             f.close()\n          except Exception as e:\n             return {'return':1, 'error':'problem reading lock file'}\n\n          # Check if lock has expired\n          if gl=='yes' and uuid=='':\n             # Retry if locked\n             retry=int(i.get('lock_retries','11'))\n             retryd=float(i.get('lock_retry_delay','3'))\n\n             dt=os.path.getmtime(pl)+exp-time.time()\n             if dt>0: \n                while retry>0 and os.path.isfile(pl) and dt>0:\n                   retry-=1\n                   time.sleep(retryd)\n                   if os.path.isfile(pl): \n                      dt=os.path.getmtime(pl)+exp-time.time()\n\n                if retry==0 and dt>0 and os.path.isfile(pl):\n                   return {'return':32, 'error':'entry is still locked'}\n\n             luid=''\n             if os.path.isfile(pl): os.remove(pl)\n\n       # Release lock if requested (and if not locked by another UID)\n       if luid!='' and uuid!='':\n          if luid!=uuid:\n             return {'return':32, 'error': 'entry is locked with another UID'}\n          luid=''\n          os.remove(pl)\n\n       # Finish acquiring lock\n       if gl=='yes':\n          # (Re)acquire lock\n          if uuid=='':\n             r=gen_uid({})\n             if r['return']>0: return r\n             luid=r['data_uid']\n          else:\n             luid=uuid\n\n          # Write lock file\n          try:\n             f=open(pl,'w')\n             f.write(luid+'\\n')\n             f.write(str(exp)+'\\n')\n             f.close()\n          except Exception as e:\n             return {'return':1, 'error':'problem writing lock file'}\n\n          rr['lock_uid']=luid\n\n    return rr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_lock(i):\n    p=i['path']\n    uuid=i.get('unlock_uid','')\n\n    pl=os.path.join(p, cfg['subdir_ck_ext'], cfg['file_for_lock'])\n\n    luid=''\n    if os.path.isfile(pl):\n       import time\n\n       # Read lock file\n       try:\n          f=open(pl)\n          luid=f.readline().strip()\n          exp=float(f.readline().strip())\n          if exp<0: exp=1\n          f.close()\n       except Exception as e:\n          return {'return':1, 'error':'problem reading lock file'}\n\n       # Check if lock has expired\n       dt=os.path.getmtime(pl)+exp-time.time()\n       if dt<0: \n          # Expired\n          if uuid=='' or uuid==luid:\n             os.remove(pl)\n          else:\n             return {'return':32, 'error':'entry lock UID is not matching'}\n       else:\n          if uuid=='':\n             return {'return':32, 'error':'entry is locked'}\n          elif uuid!=luid:\n             return {'return':32, 'error':'entry is locked with different UID'}\n\n    elif uuid!='':\n       return {'return':32, 'error':'lock was removed or expired'}\n\n    return {'return':0}", "response": "Check if the lock is valid and return the result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_current_date_time(i):\n\n    import datetime\n\n    a={}\n\n    now1=datetime.datetime.now()\n    now=now1.timetuple()\n\n    a['date_year']=now[0]\n    a['date_month']=now[1]\n    a['date_day']=now[2]\n    a['time_hour']=now[3]\n    a['time_minute']=now[4]\n    a['time_second']=now[5]\n\n    return {'return':0, 'array':a, 'iso_datetime':now1.isoformat()}", "response": "Get the current date time for the current iteration"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetects if a path is in the current path and returns the cid of the repository.", "response": "def detect_cid_in_current_path(i):\n    \"\"\"\n    Input:  {\n              (path)     - path, otherwise current directory\n            }\n\n    Output: {\n              return         - return code =  0, if successful\n                                           >  0, if error\n              (error)        - error text if return > 0\n\n              repo_uoa       - repo UOA\n              repo_uid       - repo UID\n              repo_alias     - repo alias\n              (module_uoa)   - module UOA\n              (module_uid)   - module UID\n              (module_alias) - module alias\n              (data_uoa)     - data UOA\n              (data_uid)     - data UID\n              (data_alias)   - data alias\n            }\n    \"\"\"\n\n    p=i.get('path','')\n    if p=='': p=os.getcwd()\n    p=os.path.normpath(p)\n\n    dirs=[]\n    p1=''\n    pr='*'\n    found=False\n\n    while pr!='':\n       p1=os.path.join(p, cfg['repo_file'])\n\n       if os.path.isfile(p1): \n          found=True\n          break\n\n       p2=os.path.split(p)\n       p=p2[0]\n       pr=p2[1]\n       dirs.append(pr)\n\n    if not found:\n       return {'return':16, 'error':'repository is not detected in the current path'}\n\n    # Find info about repo (prepared as return dict)\n    r=find_repo_by_path({'path':p})\n    if r['return']>0: return r\n\n    # Check info about module\n    ld=len(dirs)\n    if ld>0:\n       m=dirs[ld-1]\n\n       rx=find_path_to_entry({'path':p, 'data_uoa':m})\n       if rx['return']>0 and rx['return']!=16: return rx\n       elif rx['return']==0:\n          r['module_uoa']=rx['data_uoa']\n          r['module_uid']=rx['data_uid']\n          r['module_alias']=rx['data_alias']\n\n       # Check info about data\n       if ld>1:\n          d=dirs[ld-2]\n\n          rx=find_path_to_entry({'path':os.path.join(p,m), 'data_uoa':d})\n          if rx['return']>0 and rx['return']!=16: return rx\n          elif rx['return']==0:\n             r['data_uoa']=rx['data_uoa']\n             r['data_uid']=rx['data_uid']\n             r['data_alias']=rx['data_alias']\n\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the uid of the aurant", "response": "def uid(i):\n    \"\"\"\n    Input:  {}\n\n    Output: {\n              Output from 'gen_uid' function\n            }\n\n    \"\"\"\n\n    o=i.get('out','')\n\n    r=gen_uid({})\n    if r['return']>0: return r\n\n    if o=='con':\n       out(r['data_uid'])\n\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef version(i):\n\n    o=i.get('out','')\n\n    r=get_version({})\n    if r['return']>0: return r\n    version_str=r['version_str']\n\n    if o=='con':\n       out('V'+version_str)\n\n    return r", "response": "Function to get the version of the\naurant"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the python version of the node.", "response": "def python_version(i):\n    \"\"\"\n    Input:  {}\n\n    Output: {\n               version - sys.version\n               version_info - sys.version_info\n            }\n\n    \"\"\"\n\n    import sys\n\n    o=i.get('out','')\n\n    v1=sys.version\n    v2=sys.version_info\n\n    if o=='con':\n       out(v1)\n\n    return {'return':0, 'version':v1, 'version_info':v2}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef status(i):\n\n    outdated=''\n\n    o=i.get('out','')\n\n    try:    import urllib.request as urllib2\n    except: import urllib2\n\n    try:    from urllib.parse import urlencode\n    except: from urllib import urlencode\n\n    page=''\n    try:\n       res=urllib2.urlopen(cfg['status_url'])\n       page=res.read()\n    except urllib2.HTTPError as e:\n       return {'return':1, 'error':'Problem accessing server ('+format(e)+')'}\n    except urllib2.URLError as e:\n       return {'return':1, 'error':'Problem accessing server ('+format(e)+')'}\n\n    # Support for Python 3\n    if sys.version_info[0]>2:\n       try: \n          page=page.decode('utf-8')\n       except Exception as e: \n          pass\n\n    if page!='':\n       s1='version=\\''\n       i1=page.find(s1)\n       if i1>0:\n          i2=page.find('\\'',i1+9)\n          if i2>0:\n             lversion_str=page[i1+len(s1):i2].strip()\n\n             rx=check_version({'version':lversion_str})\n             if rx['return']>0: return rx\n\n             ok=rx['ok']\n             version_str=rx['current_version']\n             if ok!='yes':\n                outdated='yes'\n\n                if o=='con':\n                   out('Your version is outdated: V'+version_str)\n                   out('New available version   : V'+lversion_str)\n                   u=cfg.get('ck_web','')\n                   if u!='':\n                      out('')\n                      out('If you install CK via pip, upgrade it as follows (prefix with \"sudo\" on Linux):')\n                      out(' $ pip install ck --upgrade')\n                      out('')\n                      out('If you use GitHub version, update CK kernel (and all other repositories) as follows:')\n                      out(' $ ck pull all --kernel')\n                      out('')\n                      out('Visit '+u+' for more details!')\n\n    if o=='con':\n       if outdated!='yes':\n          out('Your version is up-to-date: V'+version_str)\n       elif outdated=='':\n          out('Problem checking version ...')\n\n    return {'return':0, 'outdated':outdated}", "response": "Check if the current version of the current version is outdated and return the status of the current version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking CK kernel version", "response": "def check_version(i):\n    \"\"\"\n    Input:  {\n              version      - your version (string)\n            }\n\n    Output: {\n              return          - return code =  0\n\n              ok              - if 'yes', your CK kernel version is outdated\n              current_version - your CK kernel version\n            }\n\n    \"\"\"\n\n    ok='yes'\n\n    r=get_version({})\n    if r['return']>0: return r\n    version=r['version']\n    version_str=r['version_str']\n\n    lversion_str=i['version'].replace('dev','.1') # for compatibility with older versions\n    lversion=lversion_str.split('.')\n\n    # Comparing\n    for q in range(0, len(version)):\n        if len(lversion)<=q:\n            break\n\n        v=version[q]\n        lv=lversion[q]\n\n        # try int first, then try string\n        try:\n           if int(lv)>int(v):\n               ok='no'\n               break\n        except Exception as e:\n            pass\n\n        if lv>v:\n            ok='no'\n            break\n\n        if lv<v:\n            break\n\n    return {'return':0, 'ok':ok, 'current_version':version_str}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_entry_to_cid(i):\n\n    xcuoa=''\n    xcid=''\n\n    if i.get('module_uoa','')!='': cuoa=i['module_uoa']\n    else: cuoa='?'\n    if i.get('module_uid','')!='': cid=i['module_uid']\n    else: cid='?'\n\n    cuoa+=':'\n    cid+=':'\n\n    if i.get('data_uoa','')!='': cuoa+=i['data_uoa']\n    else: cuoa+='?'\n    if i.get('data_uid','')!='': cid+=i['data_uid']\n    else: cid+='?'\n\n    if i.get('repo_uoa','')!='': xcuoa=i['repo_uoa']+':'+cuoa\n    else: xcuoa='?:'+cuoa\n    if i.get('repo_uid','')!='': xcid=i['repo_uid']+':'+cid\n    else: xcid='?:'+cid\n\n    r={'return':0}\n    r['cuoa']=cuoa\n    r['cid']=cid\n    r['xcuoa']=xcuoa\n    r['xcid']=xcid\n\n    return r", "response": "Convert entry to cid"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef webhelp(i):\n    a=i.get('repo_uoa','')\n    m=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    url=cfg['wiki_data_web']\n\n    if m!='':\n       if duoa=='': \n          duoa=m\n          m=cfg['module_name']\n\n       r=find_path_to_data({'repo_uoa':a, 'module_uoa':m, 'data_uoa':duoa})\n       if r['return']>0: return r\n       p=r['path']\n\n       rx=convert_entry_to_cid(r)\n       if rx['return']>0: return rx\n\n       cuoa=rx['cuoa']\n       cid=rx['cid']\n       xcuoa=rx['xcuoa']\n       xcid=rx['xcid']\n\n       # Prepare URL\n       url+=cid.replace(':','_')\n\n    out('Opening web page '+url+' ...')\n\n    import webbrowser\n    webbrowser.open(url)\n\n    return {'return':0}", "response": "Input:  { from access function }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n            }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wiki(i):\n    ruoa=i.get('repo_uoa','')\n    muoa=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    url=cfg['wiki_data_web']\n\n    if muoa=='' or duoa=='':\n       # Try to detect CID in current path\n       rx=detect_cid_in_current_path({})\n       if rx['return']==0:\n          muoa=rx.get('module_uoa','')\n          duoa=rx.get('data_uoa','')\n\n    if muoa=='' or duoa=='':\n       return guide({}) #{'return':1, 'error':'entry is not defined'}\n\n    r=find_path_to_data({'repo_uoa':ruoa, 'module_uoa':muoa, 'data_uoa':duoa})\n    if r['return']>0: return r\n\n    rx=convert_entry_to_cid(r)\n    if rx['return']>0: return rx\n\n    cuoa=rx['cuoa']\n    cid=rx['cid']\n    xcuoa=rx['xcuoa']\n    xcid=rx['xcid']\n\n    # Prepare URL\n    url+=cid.replace(':','_')\n\n    out('Opening web page '+url+' ...')\n\n    import webbrowser\n    webbrowser.open(url)\n\n    return {'return':0}", "response": "Return a wiki page for the specified entry"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef webapi(i):\n    ruoa=i.get('repo_uoa','')\n    muoa=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    url=cfg['api_web']\n\n    if muoa=='':\n       muoa=duoa\n\n    if muoa=='':\n       url+='ck_'+cfg['subdir_kernel']+'_api/html/kernel_8py.html'\n    else:\n       duoa=muoa\n       muoa=cfg['module_name']\n\n       r=load({'repo_uoa':ruoa, 'module_uoa':muoa, 'data_uoa':duoa})\n       if r['return']>0: return r\n       muoa=r['data_uoa']\n\n       url+='ck_modules_api/html/'+muoa+'_2module_8py.html'\n\n    out('Opening web page '+url+' ...')\n    out('  Note: if discussion wiki page doesn\\'t exist yet, root Wiki will open!')\n    out('        You should then log in to Github, and restart ck wiki agian to edit it ...')\n\n    import webbrowser\n    webbrowser.open(url)\n\n    return {'return':0}", "response": "webapi - webapi - webapi"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninput { (template) - use this web template (repo_uoa) - (module_uoa) - (data_uoa) - view a given entry (extra_url) - extra URL } Output: { return - return code = 0, if successful > 0, if error (error) - error text if return > 0 }", "response": "def browser(i):\n    \"\"\"\n    Input:  {\n              (template)   - use this web template\n              (repo_uoa)   -\n              (module_uoa) - \n              (data_uoa)   - view a given entry\n              (extra_url)  - extra URL\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n            }\n\n    \"\"\"\n\n    # Check if ck-web is installed\n    r=find({'module_uoa':'module',\n            'data_uoa':'wfe'})\n    if r['return']>0:\n       if r['return']!=16: return r\n \n       out('Seems like ck-web repository is not installed (can\\'t find wfe module)!')\n       out('Please, install it via \"ck pull repo:ck-web\" and try again!')\n\n       return {'return':0}\n\n    t=i.get('template','')\n\n    ruoa=i.get('repo_uoa','')\n    muoa=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    cid=''\n    if duoa!='' or muoa!='' or ruoa!='':\n       if ruoa!='': cid=ruoa+':'\n       if muoa!='': cid+=muoa+':'\n       if duoa!='': cid+=duoa\n\n    # Starting web service and asking to open page\n    return access({'action':'start', 'module_uoa':'web', 'browser':'yes', \n                   'template':t, 'cid':cid, 'extra_url':i.get('extra_url','')})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninput {} Output: { return - return code = 0, if successful > 0, if error (error) - error text if return > 0 }", "response": "def guide(i):\n    \"\"\"\n    Input:  {}\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n            }\n\n    \"\"\"\n    url=cfg['ck_web_wiki']\n\n    out('Opening web page '+url+' ...')\n\n    import webbrowser\n    webbrowser.open(url)\n\n    return {'return':0}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef help(i):\n\n    o=i.get('out','')\n\n    m=i.get('module_uoa','')\n    if m=='':\n       m='<module_uoa>'\n\n    h= 'Usage: '+cfg['cmd'].replace('$#module_uoa#$', m)+'\\n'\n\n    h+='\\n'\n    h+='  Common actions:\\n'\n\n    for q in sorted(cfg['common_actions']):\n        s=q\n        desc=cfg['actions'][q].get('desc','')\n        if desc!='': s+=' - '+desc\n        h+='    '+s+'\\n'\n\n    h+='\\n'\n    h+='  Module actions:\\n'\n\n    if m=='<module_uoa>':\n       # Internal actions (for this kernel)\n       for q in sorted(cfg['actions']):\n           if q not in cfg['common_actions']:\n              s=q\n              desc=cfg['actions'][q].get('desc','')\n              if desc!='': s+=' - '+desc\n              h+='    '+s+'\\n'\n    else:\n       # Attempt to load \n       r=list_actions({'module_uoa':m}) \n       if r['return']>0: return r\n       actions=r['actions']\n\n       if len(actions)==0:\n          h+='    Not described yet ...\\n'\n       else:\n          for q in sorted(actions.keys()):\n              s=q\n              desc=actions[q].get('desc','')\n              if desc!='': s+=' - '+desc\n              h+='    '+s+'\\n'\n\n    if m=='<module_uoa>':\n       h+='\\n'\n       h+=cfg['help_examples']\n\n    h+='\\n'\n    h+=cfg['help_web']\n\n    if o=='con': out(h)\n\n    return {'return':0, 'help':h}", "response": "Help text for the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nshorts help text for CK", "response": "def short_help(i):\n    \"\"\"\n    Input:  {\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              help         - help text\n            }\n\n\n\n    \"\"\"\n\n    o=i.get('out','')\n\n    r=version({})\n    if r['return']>0: return r\n\n    h='CK version: '+r['version_str']+'\\n'\n\n    r=python_version({})\n    if r['return']>0: return r\n\n    h+='\\nPython version used by CK: '+r['version']+'\\n'\n\n    h+='\\nAll internal CK commands: ck help\\n'\n\n    h+='\\n'+cfg['help_web'].replace('\\n','').strip().replace('   ','')\n\n    if o=='con': \n       out(h)\n\n    return {'return':0, 'help':h}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef print_input(i):\n\n    o=i.get('out','')\n\n    rx=dumps_json({'dict':i, 'sort_keys':'yes'})\n    if rx['return']>0: return rx\n\n    h=rx['string']\n\n    if o=='con': out(h)\n\n    return {'return':0, 'html':h}", "response": "Print the input as a JSON object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets info about a single module", "response": "def info(i):\n    \"\"\"\n    Input:  {\n              (repo_uoa)\n              module_uoa\n              (data_uoa)\n            }\n\n    Output: {\n              Output of 'load' function\n            }\n\n    \"\"\"\n\n    o=i.get('out','')\n\n    ruoa=i.get('repo_uoa','')\n    muoa=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    if muoa=='':\n       return {'return':1, 'error':'module UOA is not defined'}\n\n    module_info=False\n    if duoa=='':\n       module_info=True\n       duoa=muoa\n       muoa=cfg['module_name']\n\n    ii={'module_uoa':muoa, 'data_uoa':duoa}\n    if ruoa!='': ii['repo_uoa']=ruoa\n    r=load(ii)\n    if r['return']>0: return r\n\n    if o=='con':\n       if module_info:\n          p=r['path']\n          dd=r['dict']\n\n          developer=dd.get('developer','')\n          license=dd.get('license','')\n          desc=dd.get('desc','')\n\n          # Get user-friendly CID\n          rx=convert_entry_to_cid(r)\n          if rx['return']>0: return rx\n\n          cuoa=rx['cuoa']\n          cid=rx['cid']\n          xcuoa=rx['xcuoa']\n          xcid=rx['xcid']\n\n          out('*** CID ***')\n          out(cuoa+' ('+cid+')')\n\n          out('')\n          out('*** Path ***')\n          out(p)\n\n          if desc!='':\n             out('')\n             out('*** Description ***')\n             out(desc)\n\n          if developer!='':\n             out('')\n             out('*** Developer ***')\n             out(developer)\n\n          if license!='':\n             out('')\n             out('*** License ***')\n             out(license)\n\n       else:\n          p=r['path']\n          duid=r['data_uid']\n          dalias=r['data_alias']\n          muid=r['module_uid']\n          malias=r['module_alias']\n\n          out('Path  =        '+p)\n          out('')\n          out('Data alias =   '+dalias)\n          out('Data UID   =   '+duid)\n          out('')\n          out('Module alias = '+malias)\n          out('Module UID   = '+muid)\n\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction to detect CIDs in current path", "response": "def path(i):\n    \"\"\"\n    Input:  {}\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              Output from from 'detect_cid_in_current_path' function\n            }\n\n    \"\"\"\n\n    o=i.get('out','')\n\n    r=detect_cid_in_current_path(i)\n    if r['return']>0: return r\n\n    rx=convert_entry_to_cid(r)\n    if rx['return']>0: return rx\n\n    cuoa=rx['cuoa']\n    cid=rx['cid']\n    xcuoa=rx['xcuoa']\n    xcid=rx['xcid']\n\n    # If console, print CIDs\n    if o=='con':\n       out(cuoa)\n       out(cid)\n       out(xcuoa)\n       out(xcid)\n\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cid(i):\n\n    o=i.get('out','')\n\n    # Check which CID to detect\n    ruoa=i.get('repo_uoa','')\n    muoa=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    if ruoa=='' and muoa=='' and duoa=='':\n       r=detect_cid_in_current_path(i)\n    else:\n       r=find({'repo_uoa':ruoa, 'module_uoa':muoa, 'data_uoa':duoa})\n    if r['return']>0: return r\n\n    rx=convert_entry_to_cid(r)\n    if rx['return']>0: return rx\n\n    cid=rx['cid']\n\n    # If console, print CIDs\n    if o=='con':\n       out(cid)\n       # Try to copy to Clipboard if supported by OS\n       rx=copy_to_clipboard({'string':cid})\n       # Ignore error\n\n    return r", "response": "detect CID in current path"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef copy_path_to_clipboard(i):\n\n    import os\n    p=os.getcwd()\n\n    if i.get('add_quotes','')=='yes':\n       p='\"'+p+'\"'\n\n    rx=copy_to_clipboard({'string':p})\n    # Ignore error\n\n    return {'return':0}", "response": "Copy path to the clipboard"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the entry with the specified information from the specified repository.", "response": "def load(i):\n    \"\"\"\n    Input:  {\n              (repo_uoa)              - repo UOA\n              module_uoa              - module UOA\n              data_uoa                - data UOA\n\n              (get_lock)              - if 'yes', lock this entry\n              (lock_retries)          - number of retries to aquire lock (default=5)\n              (lock_retry_delay)      - delay in seconds before trying to aquire lock again (default=10)\n              (lock_expire_time)      - number of seconds before lock expires (default=30)\n\n              (skip_updates)          - if 'yes', do not load updates\n              (skip_desc)             - if 'yes', do not load descriptions\n\n              (load_extra_json_files) - list of files to load from the entry\n\n              (unlock_uid)            - UID of the lock to release it\n\n              (min)                   - show minimum when output to console (i.e. meta and desc)\n\n              (create_if_not_found)   - if 'yes', create, if entry is not found - useful to create and lock entries\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              dict         - entry meta description\n              (info)       - entry info\n              (updates)    - entry updates\n              (desc)       - entry description\n\n              path         - path to data entry\n              path_module  - path to module entry with this entry\n              path_repo    - path to the repository of this entry\n              repo_uoa     - repo UOA \n              repo_uid     - repo UID\n              repo_alias   - repo alias\n              module_uoa   - module UOA \n              module_uid   - module UID\n              module_alias - module alias\n              data_uoa     - data UOA\n              data_uid     - data UID\n              data_alias   - data alias\n              data_name    - user friendly name\n\n              (extra_json_files) - dict with extra json files (key is the filename from 'load_extra_json_files')\n\n              (lock_uid)   - unlock UID, if locked successfully\n            }\n    \"\"\"\n\n    o=i.get('out','')\n\n    a=i.get('repo_uoa','')\n    m=i.get('module_uoa','')\n    d=i.get('data_uoa','')\n\n    if d=='':\n       return {'return':1, 'error':'data UOA is not defined'}\n\n    r=find_path_to_data({'repo_uoa':a, 'module_uoa':m, 'data_uoa':d})\n    if r['return']>0: \n       if r['return']==16 and i.get('create_if_not_found','')=='yes':\n          r=add({'repo_uoa':a, 'module_uoa':m, 'data_uoa':d})\n          if r['return']>0:\n             return r\n          r=find_path_to_data({'repo_uoa':a, 'module_uoa':m, 'data_uoa':d})\n          if r['return']>0: return r\n       else:\n          return r\n\n    p=r['path']\n\n    slu=i.get('skip_updates','')\n    sld=i.get('skip_desc','')\n\n    # Set/check lock\n    i['path']=p\n    rx=set_lock(i)\n    if rx['return']>0: return rx\n\n    luid=rx.get('lock_uid','')\n\n    # Load meta description\n    r1=load_meta_from_path({'path':p, 'skip_updates':slu, 'skip_desc':sld})\n    if r1['return']>0: return r1\n\n    r.update(r1)\n    r['path']=p\n\n    r['data_name']=r1.get('info',{}).get('data_name','')\n\n    if luid!='': r['lock_uid']=luid\n\n    # If load extra files\n    lejf=i.get('load_extra_json_files',[])\n    if len(lejf)>0:\n       ejf={}\n       for ff in lejf:\n           rx=load_json_file({'json_file':os.path.join(p,ff)})\n           if rx['return']>0: return rx\n           ejf[ff]=rx['dict']\n       r['extra_json_files']=ejf\n\n    # If console mode, print json\n    if o=='con':\n       dd=r\n       if i.get('min','')=='yes':\n          dd={\n              'desc':r.get('desc',{}),\n              'dict':r.get('dict',{})\n             }\n\n       rr=dumps_json({'dict':dd, 'sort_keys':'yes'})\n       if rr['return']==0:\n          out(rr['string'])\n\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find(i):\n\n    o=i.get('out','')\n    i['out']=''\n\n    # Check wildcards\n    lst=[]\n\n    a=i.get('repo_uoa','')\n    m=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    if m=='':\n       return {'return':1, 'error':'module UOA is not defined'}\n    if duoa=='':\n       return {'return':1, 'error':'data UOA is not defined'}\n\n    if a.find('*')>=0 or a.find('?')>=0 or m.find('*')>=0 or m.find('?')>=0 or duoa.find('*')>=0 or duoa.find('?')>=0: \n       r=list_data({'repo_uoa':a, 'module_uoa':m, 'data_uoa':duoa})\n       if r['return']>0: return r\n\n       lst=r['lst']\n\n       r={'return':0}\n\n       if len(lst)>0:\n          r.update(lst[0])\n       else:\n          return {'return':1, 'error':'entry was not found'}\n\n    else:\n       # Find path to data\n       r=find_path_to_data(i)\n       if r['return']>0: return r\n\n       p=r['path']\n       ruoa=r.get('repo_uoa','')\n       ruid=r.get('repo_uid','')\n       muoa=r.get('module_uoa','')\n       muid=r.get('module_uid','')\n       duid=r.get('data_uid','')\n       duoa=r.get('data_alias','')\n       if duoa=='': duoa=duid\n\n       lst.append({'path':p, 'repo_uoa':ruoa, 'repo_uid':ruid, \n                             'module_uoa':muoa, 'module_uid':muid, \n                             'data_uoa':duoa, 'data_uid': duid})\n\n    if o=='con':\n       pf='' \n       for q in lst:\n           p=q['path']\n           out(p)\n           if pf=='': pf=p\n\n    i['out']=o\n\n    r['number_of_entries']=len(lst)\n\n    return r", "response": "Find the next entry in the list of modules and data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninput { (repo_uoa) - repo UOA module_uoa - module UOA data_uoa - data UOA or cid } Output: { Output of the 'load' function string - prepared string 'cd {path to entry}' }", "response": "def cd(i):\n    \"\"\"\n    Input:  {\n              (repo_uoa)  - repo UOA\n              module_uoa  - module UOA\n              data_uoa    - data UOA\n                 or\n              cid\n\n            }\n\n    Output: { \n              Output of the 'load' function \n\n              string - prepared string 'cd {path to entry}'\n            }\n    \"\"\"\n\n    o=i.get('out','')\n\n    i['out']=''\n    r=find(i)\n    i['out']=o\n\n    if r['return']>0: return r\n\n    noe=r.get('number_of_entries','')\n    if noe=='': noe=0\n\n    if noe>1 and o=='con':\n       out('CK warning: '+str(noe)+' entries found! Selecting the first one ...')\n       out('')\n\n    p=r.get('path','')\n    if p!='':\n       rx=get_os_ck({})\n       if rx['return']>0: return rx\n\n       plat=rx['platform']\n\n       s='cd '\n       if plat=='win':\n          s+='/D '\n\n       if p.find(' ')>0:\n          p='\"'+p+'\"'\n       s+=p\n\n       out(s)\n\n       r['string']=s\n\n       import platform\n       import subprocess\n\n       out('')\n       out('Warning: you are in a new shell with a reused environment. Enter \"exit\" to return to the original one!')\n\n       if platform.system().lower().startswith('win'): # pragma: no cover\n          p = subprocess.Popen([\"cmd\", \"/k\", s], shell = True, env=os.environ)\n          p.wait()\n\n       else:\n          rx=gen_tmp_file({})\n          if rx['return']>0: return rx\n          fn=rx['file_name']\n\n          rx=save_text_file({'text_file':fn, 'string':s})\n          if rx['return']>0: return rx\n\n          os.system(\"bash --rcfile \"+fn)\n\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the CDC of a single object", "response": "def cdc(i): # pragma: no cover\n    \"\"\"\n    Input:  {\n              (repo_uoa)  - repo UOA\n              module_uoa  - module UOA\n              data_uoa    - data UOA\n                 or\n              cid\n\n            }\n\n    Output: { \n              Output of the 'load' function \n            }\n    \"\"\"\n\n    r=cd(i)\n    if r['return']>0: return r\n\n    s=r.get('string','')\n    if s!='':\n       rx=copy_to_clipboard({'string':s})\n       if rx['return']>0: return rx\n\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd an entry to the order of the entries in the order they were created.", "response": "def add(i):\n    \"\"\"\n    Input:  {\n              (repo_uoa)             - repo UOA\n              module_uoa             - module UOA\n              data_uoa               - data UOA\n              (data_uid)             - data UID (if uoa is an alias)\n              (data_name)            - user friendly data name\n\n              (dict_from_cid)        -\n              (dict_from_repo_uoa)   - \n              (dict_from_module_uoa) - \n              (dict_from_data_uoa)   - if present, pre-load dict \n                                       from this (module_uoa):data_uoa (analog of copy)\n\n              (update)               - if == 'yes' and entry exists, update it\n\n              (dict)                 - meta description to record\n              (substitute)           - if 'yes' and update=='yes' substitute dictionaries, otherwise merge!\n\n              (desc)                 - description of an entry (gradually adding API description in flat format)\n\n              (extra_json_files)     - dict with extra json files to save to entry (key is a filename)\n\n              (tags)                 - list or comma separated list of tags to add to entry\n\n              (info)                 - entry info to record - normally, should not use it!\n              (extra_info)           - enforce extra info such as\n                                          author\n                                          author_email\n                                          author_webpage\n                                          license\n                                          copyright\n                                       If not specified then taken from kernel (prefix 'default_')\n\n              (updates)              - entry updates info to record - normally, should not use it!\n              (ignore_update)        - if 'yes', do not add info about update\n\n              (ask)                  - if 'yes', ask questions, otherwise silent\n\n              (unlock_uid)           - unlock UID if was previously locked\n\n              (sort_keys)            - by default, 'yes'\n\n              (share)                - if 'yes', try to add via GIT\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                           16, if entry already exists\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              Output from the 'create_entry' function\n            }\n\n    \"\"\"\n\n    o=i.get('out','')\n\n    t='added'\n\n    ra=i.get('repo_uoa','')\n    m=i.get('module_uoa','')\n    d=i.get('data_uoa','')\n    di=i.get('data_uid','')\n    dn=i.get('data_name','')\n\n    uuid=i.get('unlock_uid','')\n\n    up=i.get('update','')\n\n    ask=i.get('ask','')\n\n    # Get repo path\n    r=find_path_to_repo({'repo_uoa':ra})\n    if r['return']>0: return r\n    pr=r['path']\n\n    ruoa=r['repo_uoa']\n    ruid=r['repo_uid']\n    ralias=r['repo_alias']\n\n    rd=r['dict']\n    rshared=rd.get('shared','')\n    rsync=rd.get('sync','')\n\n    if i.get('share','')=='yes': rsync='yes'\n\n    # Check if writing is allowed\n    ii={'module_uoa':m, 'repo_uoa':r['repo_uoa'], 'repo_uid':r['repo_uid'], 'repo_dict':rd}\n    r=check_writing(ii)\n    if r['return']>0: return r\n\n    # Load info about module\n    r=load({'module_uoa':cfg['module_name'],\n            'data_uoa':m})\n    if r['return']>0: return r\n    elif r['return']==16: \n       return {'return':8, 'error':'can\\'t find path to module \"'+m+'\"'}\n    muoa=r['data_uoa']\n    muid=r['data_uid']\n    malias=r['data_alias']\n    pm=r['path']\n\n    uid=r['data_uid']\n    alias=r['data_alias']\n    if alias=='': alias=uid\n    module_desc=r['dict']\n\n    # Ask additional questions\n    if o=='con' and ask=='yes':\n       # Asking for alias\n       if d=='' or is_uid(d):\n          r=inp({'text':'Enter an alias (or Enter to skip it): '})\n          d=r['string']\n\n       # Asking for user-friendly name\n       if dn=='' and up!='yes':\n          r=inp({'text':'Enter a user-friendly name of this entry (or Enter to reuse alias): '})\n          dn=r['string']\n\n    # Load dictionary from other entry if needed\n    dfcid=i.get('dict_from_cid','')\n    dfruoa=i.get('dict_from_repo_uoa','')\n    dfmuoa=i.get('dict_from_module_uoa','')\n    dfduoa=i.get('dict_from_data_uoa','')\n\n    if dfcid!='':\n       r=parse_cid({'cid':dfcid})\n       if r['return']>0: return r\n       dfruoa=r.get('repo_uoa','')\n       dfmuoa=r.get('module_uoa','')\n       dfduoa=r.get('data_uoa','')\n\n    if d!='' and not is_uoa(d):\n       return {'return':1, 'error':'alias has disallowed characters'}\n\n    if dfduoa!='':\n       if dfmuoa=='': dfmuoa=m\n\n       ii={'module_uoa':dfmuoa, 'data_uoa':dfduoa}\n       if dfruoa!='': ii['repo_uoa']=dfruoa\n\n       r=load(ii)\n       if r['return']>0: return r\n\n       df=r.get('dict',{})\n\n    # Create first level entry (module) \n    r=create_entry({'path':pr, 'data_uoa':alias, 'data_uid':uid})\n    if r['return']>0 and r['return']!=16: return r\n    p1=r['path']\n\n    # Create second level entry (data)\n    i1={'path':p1}\n    pdd=''\n    if di!='': \n       i1['data_uid']=di\n    if d!='': \n       i1['data_uoa']=d\n    rr=create_entry(i1)\n    if rr['return']>0 and rr['return']!=16: return rr\n\n    duid=rr['data_uid']\n    pdd=rr['data_uoa']\n    dalias=rr['data_alias']\n\n    # Preparing meta-description\n    a={}\n    info={}\n    updates={}\n    desc={}\n\n    p2=rr['path']\n    p3=os.path.join(p2, cfg['subdir_ck_ext'])\n    p4=os.path.join(p3, cfg['file_meta'])\n    p4i=os.path.join(p3, cfg['file_info'])\n    p4u=os.path.join(p3, cfg['file_updates'])\n    p4d=os.path.join(p3, cfg['file_desc'])\n\n    # If last entry exists\n    if rr['return']==16:\n       if up=='yes':\n          t='updated'\n\n          # Check if locked\n          rl=check_lock({'path':p2, 'unlock_uid':uuid})\n          if rl['return']>0: \n             if rl['return']==32:\n                rl['data_uoa']=pdd\n                rl['data_uid']=duid\n             return rl\n\n          # Entry exists, load configuration if update\n          r2=load_meta_from_path({'path':p2})\n          if r2['return']>0: return r2\n          a=r2['dict']\n          info=r2.get('info',{})\n          updates=r2.get('updates',{})\n          desc=r2.get('desc',{})\n\n          if dn=='': dn=info.get('data_name','')\n       else:\n          return {'return':16,'error':'entry already exists in path ('+p2+')'}\n    else:\n       # Create configuration directory\n       if not os.path.isdir(p3):\n          try:\n             os.mkdir(p3)\n          except Exception as e:\n             return {'return':1, 'error':format(e)}\n\n    if dn=='' and not is_uid(d):\n       dn=d\n\n    if dfduoa!='':\n       r=merge_dicts({'dict1':a, 'dict2':df})\n       if r['return']>0: return r\n\n    # If dict, info and updates are in input, try to merge ...\n    cma=i.get('dict',{})\n    cmad=i.get('desc',{})\n    if i.get('substitute','')=='yes':\n       a=cma\n       desc=cmad\n    else:\n       r=merge_dicts({'dict1':a, 'dict2':cma})\n       if r['return']>0: return r\n       r=merge_dicts({'dict1':desc, 'dict2':cmad})\n       if r['return']>0: return r\n\n    # Check tags \n    xtags=a.get('tags',[])\n\n    tags=i.get('tags','')\n    if tags=='': tags=[]\n    elif type(tags)!=list:\n       tags=tags.split(',')\n\n    for l in range(0,len(tags)):\n        ll=tags[l].strip()\n        if ll not in xtags:\n           xtags.append(ll)\n\n    if len(xtags)>0:\n       a['tags']=xtags\n\n    # Process info\n    cminfo=i.get('info',{})\n    if len(cminfo)!=0:\n       info=cminfo\n#       r=merge_dicts({'dict1':info, 'dict2':cminfo})\n#       if r['return']>0: return r\n\n    cmupdates=i.get('updates',{})\n    if len(cmupdates)!=0:\n       updates=cmupdates\n#       r=merge_dicts({'dict1':updates, 'dict2':cmupdates})\n#       if r['return']>0: return r\n\n    # If name exists, add\n    info['backup_module_uoa']=muoa\n    info['backup_module_uid']=muid\n    info['backup_data_uid']=duid\n    if dn!='': info['data_name']=dn\n\n    # Add control info\n    ri=prepare_special_info_about_entry({})\n    if ri['return']>0: return ri\n    x=ri['dict']\n\n    # Check if pre-set control params such as author, copyright, license\n    ei=i.get('extra_info',{})\n    if len(ei)!=0: x.update(ei)\n\n    y=info.get('control',{})\n\n    if i.get('ignore_update','')!='yes':\n       if len(y)==0:\n          info['control']=x\n       else:\n          y=updates.get('control',[])\n          y.append(x)\n          updates['control']=y\n\n    sk=i.get('sort_keys','')\n    if sk=='': sk='yes'\n\n    if len(updates)>0:\n       # Record updates\n       rx=save_json_to_file({'json_file':p4u, 'dict':updates, 'sort_keys':sk})\n       if rx['return']>0: return rx\n\n    # Record meta description\n    rx=save_json_to_file({'json_file':p4, 'dict':a, 'sort_keys':sk})\n    if rx['return']>0: return rx\n\n    # Record info\n    rx=save_json_to_file({'json_file':p4i, 'dict':info, 'sort_keys':sk})\n    if rx['return']>0: return rx\n\n    # Record desc\n    rx=save_json_to_file({'json_file':p4d, 'dict':desc, 'sort_keys':sk})\n    if rx['return']>0: return rx\n\n    # Record extra files if there\n    ejf=i.get('extra_json_files',{})\n    if len(ejf)>0:\n       for ff in ejf:\n           dff=ejf[ff]\n           rz=save_json_to_file({'json_file':os.path.join(p2,ff), 'dict':dff, 'sort_keys':sk})\n           if rz['return']>0: return rz\n\n    if o=='con':\n       out('Entry '+d+' ('+duid+', '+p2+') '+t+' successfully!')\n\n    # Check if needs to be synced\n    if rshared!='' and rsync=='yes':\n       ppp=os.getcwd()\n\n       os.chdir(pr)\n       if os.path.isdir(cfg['subdir_ck_ext']):\n          ss=cfg['repo_types'][rshared]['add'].replace('$#path#$', pr).replace('$#files#$', cfg['subdir_ck_ext'])\n          rx=os.system(ss)\n\n       os.chdir(p1)\n       if os.path.isdir(cfg['subdir_ck_ext']):\n          ss=cfg['repo_types'][rshared]['add'].replace('$#path#$', pr).replace('$#files#$', cfg['subdir_ck_ext'])\n          rx=os.system(ss)\n\n       ss=cfg['repo_types'][rshared]['add'].replace('$#path#$', pr).replace('$#files#$', pdd)\n       rx=os.system(ss)\n\n       os.chdir(ppp)\n\n    # Prepare output\n    rr={'return':0,\n        'dict': a,\n        'info': info,\n        'updates': updates, \n        'path':p2,\n        'path_module': pm,\n        'path_repo': pr,\n        'repo_uoa':ruoa,\n        'repo_uid':ruid,\n        'repo_alias':ralias,\n        'module_uoa':muoa,\n        'module_uid':muid,\n        'module_alias':malias,\n        'data_uoa':pdd,\n        'data_uid':duid,\n        'data_alias':dalias,\n        'data_name':dn}\n\n    # Check if need to add index\n    if cfg.get('use_indexing','')=='yes':\n       muid=rr['module_uid']\n       duid=rr['data_uid']\n       path='/'+muid+'/'+duid+'/1'\n       ri=access_index_server({'request':'DELETE', 'path':path})\n       if ri['return']>0: return ri\n       ri=access_index_server({'request':'PUT', 'path':path, 'dict':rr})\n       if ri['return']>0: return ri\n\n    # Remove lock after update if needed\n    if uuid!='':\n       pl=os.path.join(p2, cfg['subdir_ck_ext'], cfg['file_for_lock'])\n       if os.path.isfile(pl): os.remove(pl)\n\n    rr['return']=0\n\n    return rr"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the entry in the order they were copied.", "response": "def update(i):\n    \"\"\"\n    Input:  {\n              (repo_uoa)             - repo UOA\n              module_uoa             - module UOA\n              data_uoa               - data UOA\n              (data_uid)             - data UID (if uoa is an alias)\n              (data_name)            - user friendly data name\n\n              (dict_from_cid)        -\n              (dict_from_repo_uoa)   - \n              (dict_from_module_uoa) - \n              (dict_from_data_uoa)   - if present, pre-load dict \n                                       from this (module_uoa):data_uoa (analog of copy)\n\n              (dict)                 - meta description to record\n              (substitute)           - if 'yes', substitute dictionaries, otherwise merge!\n\n              (tags)                 - list or comma separated list of tags to add to entry\n\n              (info)                 - entry info to record - normally, should not use it!\n              (updates)              - entry updates info to record - normally, should not use it!\n              (ignore_update)        - if 'yes', do not add info about update\n\n              (ask)                  - if 'yes', ask questions, otherwise silent\n\n              (unlock_uid)           - unlock UID if was previously locked\n\n              (sort_keys)            - if 'yes', sort keys\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              Output from the 'add' function (the last one in case of wildcards)\n            }\n\n    \"\"\"\n\n    # Check if global writing is allowed\n    r=check_writing({})\n    if r['return']>0: return r\n\n    # Try to load entry, if doesn't exist, add entry\n    dd={}\n\n    o=i.get('out','')\n    i['out']=''\n\n    # Check wildcards\n    lst=[]\n\n    a=i.get('repo_uoa','')\n    m=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    if duoa=='': duoa='*'\n\n    single_not_found=False # If no wild cards and entry not found, then add\n\n    if a.find('*')>=0 or a.find('?')>=0 or m.find('*')>=0 or m.find('?')>=0 or duoa.find('*')>=0 or duoa.find('?')>=0: \n       r=list_data({'repo_uoa':a, 'module_uoa':m, 'data_uoa':duoa})\n       if r['return']>0: return r\n\n       lst=r['lst']\n    else:\n       # Find path to data\n       r=find_path_to_data(i)\n       if r['return']>0: \n          single_not_found=True\n       else:\n          p=r['path']\n          ruoa=r.get('repo_uoa','')\n          ruid=r.get('repo_uid','')\n          muoa=r.get('module_uoa','')\n          muid=r.get('module_uid','')\n          duid=r.get('data_uid','')\n          duoa=r.get('data_alias','')\n          if duoa=='': duoa=duid\n\n          lst.append({'path':p, 'repo_uoa':ruoa, 'repo_uid':ruid, \n                                'module_uoa':muoa, 'module_uid':muid, \n                                'data_uoa':duoa, 'data_uid': duid})\n\n    # Update entries\n    i['out']=o\n\n    r={'return':0}\n    if single_not_found:\n       r=add(i)\n    else:\n       i['update']='yes'\n\n       for q in lst:\n           ii={}\n           ii.update(i)\n           ii.update(q)\n           r=add(ii)\n           if r['return']>0: return r\n\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nedit an entry in the backup file.", "response": "def edit(i): # pragma: no cover\n    \"\"\"\n    Input:  {\n              (repo_uoa)             - repo UOA\n              module_uoa             - module UOA\n              data_uoa               - data UOA\n\n              (ignore_update)        - (default==yes) if 'yes', do not add info about update\n              (sort_keys)            - (default==yes) if 'yes', sort keys\n\n              (edit_desc)            - if 'yes', edit description rather than meta \n                                       (useful for compiler descriptions)\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n            }\n\n    \"\"\"\n\n    o=i.get('out','')\n\n    ruoa=i.get('repo_uoa','')\n    muoa=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    iu=i.get('ignore_update','')\n    if iu=='': iu='yes'\n\n    ed=i.get('edit_desc','')\n\n    sk=i.get('sort_keys','')\n    if sk=='': sk='yes'\n\n    ii={'action':'load',\n        'repo_uoa':ruoa,\n        'module_uoa':muoa,\n        'data_uoa':duoa,\n        'common_func':'yes'}\n    r=access(ii)\n    if r['return']>0: return r\n\n    desc=r.get('desc',{})\n    meta=r['dict']\n\n    # Record to tmp file\n    import tempfile\n    fd, fn=tempfile.mkstemp(suffix='.tmp', prefix='ck-') # suffix is important - CK will delete such file!\n    os.close(fd)\n    os.remove(fn)\n\n    if ed=='yes': dd=desc\n    else:         dd=meta\n\n    r=save_json_to_file({'json_file':fn, 'dict':dd, 'sort_keys':sk})\n    if r['return']>0: return r\n\n    # Get OS\n    r=get_os_ck({})\n    if r['return']>0: return r\n    plat=r['platform']\n\n    x=cfg['external_editor'][plat].replace('$#filename#$', fn)\n\n    os.system(x)\n\n    # Load file\n    r=load_json_file({'json_file':fn})\n    if r['return']>0: return r\n\n    if ed=='yes': desc=r['dict']\n    else:         meta=r['dict']\n\n    # Update entry to finish sync/indexing\n    ii={'action':'update',\n        'repo_uoa':ruoa,\n        'module_uoa':muoa,\n        'data_uoa':duoa,\n        'common_func':'yes',\n        'ignore_update':iu,\n        'dict':meta,\n        'desc':desc,\n        'substitute':'yes',\n        'sort_keys':sk,\n        'out':o}\n    r=access(ii)\n\n    # Delete tmp file\n    if os.path.isfile(fn): \n       os.remove(fn)\n\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving all entries in the order of the specified module.", "response": "def rm(i):\n    \"\"\"\n    Input:  {\n              (repo_uoa)      - repo UOA    ; can be wild cards\n              module_uoa      - module UOA  ; can be wild cards\n              data_uoa        - data UOA    ; can be wild cards\n\n              (force)         - if 'yes', force deleting without questions\n                 or\n              (f)             - to be compatible with rm -f \n\n              (share)         - if 'yes', try to remove via GIT\n\n              (tags)          - use these tags in format tags=x,y,z to prune rm\n                   or\n              (search_string) - prune entries with expression *?\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n            }\n\n    \"\"\"\n\n    a=i.get('repo_uoa','')\n\n    # Check if global writing is allowed\n    r=check_writing({'repo_uoa':a, 'delete':'yes'})\n    if r['return']>0: return r\n\n    o=i.get('out','')\n\n    m=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    if duoa=='':\n       return {'return':1, 'error':'data UOA is not defined'}\n\n    lst=[]\n\n    tags=i.get('tags','')\n    ss=i.get('search_string','')\n\n    # Check wildcards\n    if a.find('*')>=0 or a.find('?')>=0 or m.find('*')>=0 or m.find('?')>=0 or duoa.find('*')>=0 or duoa.find('?')>=0: \n       if tags=='' and ss=='':\n          r=list_data({'repo_uoa':a, 'module_uoa':m, 'data_uoa':duoa})\n          if r['return']>0: return r\n       else:\n          r=search({'repo_uoa':a, 'module_uoa':m, 'data_uoa':duoa, 'tags':tags, 'search_string':ss})\n          if r['return']>0: return r\n\n       lst=r['lst']\n    else:\n       # Find path to data\n       r=find_path_to_data({'repo_uoa':a, 'module_uoa':m, 'data_uoa':duoa})\n       if r['return']>0: return r\n       p=r['path']\n       ruoa=r.get('repo_uoa','')\n       ruid=r.get('repo_uid','')\n       muoa=r.get('module_uoa','')\n       muid=r.get('module_uid','')\n       duid=r.get('data_uid','')\n       duoa=r.get('data_alias','')\n       if duoa=='': duoa=duid\n\n       lst.append({'path':p, 'repo_uoa':ruoa, 'repo_uid':ruid, \n                             'module_uoa':muoa, 'module_uid':muid, \n                             'data_uoa':duoa, 'data_uid': duid})\n\n    force=i.get('force','')\n    if force=='':\n       force=i.get('f','')\n\n    first=True\n    for ll in lst:\n        p=ll['path']\n        pm=os.path.split(p)[0]\n\n        muid=ll['module_uid']\n        muoa=ll['module_uoa']\n        duid=ll['data_uid']\n        duoa=ll['data_uoa']\n\n        if duoa!=duid: dalias=duoa\n        else: dalias=''\n\n        # Get user-friendly CID\n        x=muoa+':'+duoa\n        if o=='con':\n           # Try to check if has data name (useful for env)\n           p2=os.path.join(p, cfg['subdir_ck_ext'], cfg['file_info'])\n           if os.path.isfile(p2):\n              r2=load_json_file({'json_file':p2})\n              if r2['return']==0:\n                 x2=r2['dict'].get('data_name','')\n                 if x2!='':\n                    x='\"'+x2+'\"\\n    '+x\n\n        xcuoa=x+' ('+muid+':'+duid+')'\n\n        # Check repo/module writing\n        ii={'module_uoa':m, 'repo_uoa':ll['repo_uoa'], 'repo_uid':ll['repo_uid']}\n        r=check_writing(ii)\n        if r['return']>0: return r\n\n        rd=r.get('repo_dict',{})\n        rshared=rd.get('shared','')\n        rsync=rd.get('sync','')\n\n        shr=i.get('share','')\n        if shr=='yes': \n           rshared='git'\n           rsync='yes'\n\n        # If interactive\n        to_delete=True\n        if o=='con' and force!='yes':\n           r=inp({'text':'Are you sure to delete CK entry '+xcuoa+' ? (y/N): '})\n           c=r['string'].lower()\n           if c!='y' and c!='yes': to_delete=False\n\n        # If deleting\n        if to_delete:\n           # First remove alias if exists\n           if dalias!='':\n              # Delete alias\n              r=delete_alias({'path':pm, 'data_alias':dalias, 'data_uid':duid, 'repo_dict':rd, 'share':shr})\n              if r['return']>0: return r\n\n           if rshared!='':\n              pp=os.path.split(p)\n              pp0=pp[0]\n              pp1=pp[1]\n\n              ppp=os.getcwd()\n              os.chdir(pp0)\n\n              ss=cfg['repo_types'][rshared]['rm'].replace('$#files#$', pp1)\n              rx=os.system(ss)\n\n           # Delete directory\n           r={'return':0}\n           if os.path.isdir(p):\n              r=delete_directory({'path':p})\n\n           if rshared!='':\n              os.chdir(ppp)\n\n           if r['return']>0: return r\n\n           # Check if need to delete index\n           if cfg.get('use_indexing','')=='yes':\n              path='/'+muid+'/'+duid+'/1'\n              ri=access_index_server({'request':'DELETE', 'path':path})\n              if ri['return']>0: return ri\n\n           if o=='con':\n              out('   Entry '+xcuoa+' was successfully deleted!')\n\n    return {'return':0}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ren(i):\n\n    # Check if global writing is allowed\n    r=check_writing({'delete':'yes'})\n    if r['return']>0: return r\n\n    o=i.get('out','')\n\n    ruoa=i.get('repo_uoa','')\n    muoa=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    if muoa=='': return {'return':1, 'error':'module UOA is not defined'}\n    if duoa=='': return {'return':1, 'error':'data UOA is not defined'}\n\n    # Attempt to load\n    ii={'module_uoa':muoa, 'data_uoa':duoa}\n    if ruoa!='': ii['repo_uoa']=ruoa\n    r=load(ii)\n    if r['return']>0: return r\n    rdd=r\n    muid=r['module_uid']\n    pr=r['path_repo']\n    ddi=r['info']\n\n    duoa=r['data_uoa']\n    duid=r['data_uid']\n    dalias=r['data_alias']\n\n    change_data_name=(ddi.get('data_name','')==dalias)\n\n    p=r['path']\n    pm=r['path_module'] \n\n    p1=os.path.join(pm, cfg['subdir_ck_ext'])\n    pn=p\n\n    # Check if writing is allowed\n    ii={'module_uoa':muoa, 'module_uid':muid, 'repo_uoa':ruoa, 'repo_uid':r['repo_uid']}\n    r=check_writing(ii)\n    if r['return']>0: return r\n\n    rd=r.get('repo_dict',{})\n    rshared=rd.get('shared','')\n    rsync=rd.get('sync','')\n\n    shr=i.get('share','')\n    if shr=='yes': \n       rshared='git'\n       rsync='yes'\n\n    # Check if index -> delete old index\n    if cfg.get('use_indexing','')=='yes':\n       path='/'+muid+'/'+duid+'/1'\n       ri=access_index_server({'request':'DELETE', 'path':path})\n       if ri['return']>0: return ri\n\n    # Check new data UOA\n    nduoa=i.get('new_data_uoa','')\n    nduid=i.get('new_data_uid','')\n\n    if nduid=='' and i.get('new_uid','')=='yes':\n       rx=gen_uid({})\n       if rx['return']>0: return rx\n       nduid=rx['data_uid']\n\n    xcids=i.get('xcids',[])\n    if len(xcids)>0: \n       xcid=xcids[0]\n       nduoa=xcid.get('data_uoa','')\n\n    if i.get('remove_alias','')=='yes':\n       nduoa=duid\n\n    if nduoa=='': nduoa=duoa\n\n    if nduid!=duid:\n       # Check that new UID doesn't exist\n       p2=os.path.join(p1, cfg['file_alias_u'] + nduid)\n       if os.path.isfile(p2):\n          return {'return':1, 'error':'new UID already exists'}\n\n    # Check if adding UID to alias\n    if i.get('add_uid_to_alias','')=='yes':\n       x=nduid\n       if x=='': x=duid\n       nduoa+='-'+x\n\n    if nduoa!=duoa:\n       if not is_uoa(nduoa):\n          return {'return':1, 'error':'alias has disallowed characters'}\n\n       # Need to rename directory\n       if os.path.isdir(nduoa):\n          return {'return':1, 'error': 'new alias already exists'}\n\n       pn=os.path.join(pm, nduoa)\n\n       if rshared!='' and rsync=='yes':\n          import shutil\n\n          shutil.copytree(p,pn)\n\n          ppp=os.getcwd()\n\n          pp=os.path.split(pn)\n          pp0=pp[0]\n          pp1=pp[1]\n\n          os.chdir(pp0)\n          ss=cfg['repo_types'][rshared]['add'].replace('$#files#$', pp1)\n          rx=os.system(ss)\n\n          pp=os.path.split(p)\n          pp0=pp[0]\n          pp1=pp[1]\n\n          ss=cfg['repo_types'][rshared]['rm'].replace('$#files#$', pp1)\n          rx=os.system(ss)\n\n          os.chdir(ppp)\n\n          if os.path.isdir(p):\n             shutil.rmtree(p, onerror=rm_read_only)\n\n       else:\n          os.rename(p, pn)\n\n    if nduid!='' or change_data_name:\n       # Change backup_data_uid in info file\n       ppi=os.path.join(pn,cfg['subdir_ck_ext'],cfg['file_info'])\n\n       if nduid!='':\n          ddi['backup_data_uid']=nduid\n\n       if change_data_name:\n          ddi['data_name']=nduoa\n\n       rx=save_json_to_file({'json_file':ppi, 'dict':ddi, 'sort_keys':'yes'})\n       if rx['return']>0: return rx\n\n    if nduid=='': nduid=duid\n\n    # Remove old alias disambiguator\n    if not is_uid(duoa):\n       r=delete_alias({'path':pm, 'data_uid':duid, 'data_alias':duoa, 'share':shr})\n       if r['return']>0: return r\n\n    # Add new disambiguator, if needed\n    if not is_uid(nduoa):\n       if not os.path.isdir(p1):\n          # Create .cm directory\n          try:\n             os.mkdir(p1)\n          except Exception as e:\n             return {'return':1, 'error':format(e)}\n\n       # Write UOA disambiguator\n       p3=os.path.join(p1, cfg['file_alias_a'] + nduoa)\n\n       ru=save_text_file({'text_file':p3, 'string':nduid+'\\n'})\n       if ru['return']>0: return ru\n\n       # Write UID disambiguator\n       p2=os.path.join(p1, cfg['file_alias_u'] + nduid)\n\n       ru=save_text_file({'text_file':p2, 'string':nduoa+'\\n'})\n       if ru['return']>0: return ru\n\n       if rshared!='' and rsync=='yes':\n          ppp=os.getcwd()\n\n          pp=os.path.split(p1)\n          pp0=pp[0]\n          pp1=pp[1]\n\n          os.chdir(pp0)\n          ss=cfg['repo_types'][rshared]['add'].replace('$#files#$', pp1)\n          rx=os.system(ss)\n\n          os.chdir(ppp)\n\n    # Check if index and add new\n    if cfg.get('use_indexing','')=='yes':\n       if is_uid(nduoa): nduid=nduoa\n       path='/'+muid+'/'+nduid+'/1'\n       ri=access_index_server({'request':'DELETE', 'path':path})\n       if ri['return']>0: return ri\n       ri=access_index_server({'request':'PUT', 'path':path, 'dict':rdd})\n       if ri['return']>0: return ri\n\n    if o=='con':\n       out('Entry was successfully renamed!')\n\n    return {'return':0}", "response": "ren function for the get_next_entry function"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cp(i):\n\n    move=i.get('move','')\n\n    # Check if global writing is allowed\n    r=check_writing({})\n    if r['return']>0: return r\n\n    import shutil\n\n    o=i.get('out','')\n\n    ruoa=i.get('repo_uoa','')\n    muoa=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    if muoa=='': return {'return':1, 'error':'module UOA is not defined'}\n    if duoa=='': return {'return':1, 'error':'data UOA is not defined'}\n\n    # Attempt to load\n    ii={'module_uoa':muoa, 'data_uoa':duoa}\n    if ruoa!='': ii['repo_uoa']=ruoa\n    r=load(ii)\n    if r['return']>0: return r\n    rdd=r\n    muid=r['module_uid']\n\n    duoa=r['data_uoa']\n    duid=r['data_uid']\n\n    p=r['path']\n\n    dd=r.get('dict',{})\n    di=r.get('info',{})\n    du=r.get('updates',{})\n    dx=r.get('desc',{})\n\n    if move!='yes':\n       control=di.get('control',{})\n\n       control['version']=cfg['version']\n\n       rdt=get_current_date_time({})\n       control['iso_datetime']=rdt['iso_datetime']\n\n       di['control']=control\n\n    # Check if writing is allowed\n    ii={'module_uoa':muoa, 'module_uid':r['module_uid'], 'repo_uoa':ruoa, 'repo_uid':r['repo_uid']}\n    r=check_writing(ii)\n    if r['return']>0: return r\n\n    # Check new CID\n    nruoa=i.get('new_repo_uoa','')\n    nmuoa=i.get('new_module_uoa','')\n    nduoa=i.get('new_data_uoa','')\n    nduid=i.get('new_data_uid','')\n\n    xcids=i.get('xcids',[])\n    if len(xcids)>0: \n       xcid=xcids[0]\n       nduoa=xcid.get('data_uoa','')\n\n       if nduoa=='': nduoa=duoa\n\n       x=xcid.get('module_uoa','')\n       if x!='': nmuoa=x\n\n       x=xcid.get('repo_uoa','')\n       if x!='': nruoa=x\n\n    if i.get('keep_old_uid','')=='yes': nduid=duid\n\n    if nmuoa=='': nmuoa=muoa\n    if nruoa=='': nruoa=ruoa\n\n    # Adding new entry \n    if nruoa==ruoa and nmuoa==muoa and nduid==duid:\n       return {'return':1, 'error':'moving within the same directory - use \"rename\" instead'}\n\n    # Check if writing is allowed to the new repo\n    ii={'repo_uoa':nruoa}\n    r=check_writing(ii)\n    if r['return']>0: return r\n\n    rd=r.get('repo_dict',{})\n    rshared=rd.get('shared','')\n    rsync=rd.get('sync','')\n\n    ii={'module_uoa':nmuoa, 'data_uoa': nduoa, 'dict':dd, 'info':di, \n        'updates':du, 'desc':dx, 'ignore_update':'yes'}\n    if nduid!='': ii['data_uid']=nduid\n    if nruoa!='': ii['repo_uoa']=nruoa\n    r=add(ii)\n    if r['return']>0: return r\n    pn=r['path']\n    nmuid=r['module_uid']\n\n    # Recursively copying all files (except .cm)\n    if i.get('without_files','')!='yes':\n       rx=list_all_files({'path':p})\n       if rx['return']>0: return rx\n\n       for q in rx['list']:\n           p1=os.path.join(p,q)\n           pn1=os.path.join(pn,q)\n\n           # Create if dir\n           pn1d=os.path.dirname(pn1)\n           if not os.path.isdir(pn1d): os.makedirs(pn1d)\n\n           shutil.copy(p1,pn1)\n\n    if rshared!='' and rsync=='yes':\n       ppp=os.getcwd()\n\n       pp=os.path.split(pn)\n       pp0=pp[0]\n       pp1=pp[1]\n\n       os.chdir(pp0)\n       ss=cfg['repo_types'][rshared]['add'].replace('$#files#$', pp1)\n       rx=os.system(ss)\n\n       os.chdir(ppp)\n\n    tt='copied'\n    # If move, remove old one\n    if move=='yes':\n       tt='moved'\n\n       ii={'module_uoa':muoa, 'data_uoa': duoa}\n       if ruoa!='': ii['repo_uoa']=ruoa\n       rx=rm(ii)\n       if rx['return']>0: return rx\n\n    # Check if index and add new\n    if cfg.get('use_indexing','')=='yes':\n       if is_uid(nduoa): nduid=nduoa\n       path='/'+nmuid+'/'+nduid+'/1'\n       ri=access_index_server({'request':'DELETE', 'path':path})\n       if ri['return']>0: return ri\n       ri=access_index_server({'request':'PUT', 'path':path, 'dict':rdd})\n       if ri['return']>0: return ri\n\n    if o=='con':\n       out('Entry '+muoa+':'+duoa+' was successfully '+tt+'!')\n\n    return r", "response": "cp - function to copy a CID or duplicate"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mv(i):\n\n    # Check if global writing is allowed\n    r=check_writing({'delete':'yes'})\n    if r['return']>0: return r\n\n    # Check if wild cards\n    ruoa=i.get('repo_uoa','')\n    muoa=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n    nduoa=i.get('new_data_uoa','')\n    nduid=i.get('new_data_uid','')\n\n    xcids=i.get('xcids',[])\n    if len(xcids)>0: \n       xcid=xcids[0]\n       nduoa=xcid.get('data_uoa','')\n\n    if (duoa.find('*')>=0 or duoa.find('?')>=0) and nduoa=='' and nduid=='':\n       r=list_data({'repo_uoa':ruoa, 'module_uoa':muoa, 'data_uoa':duoa})\n       if r['return']>0: return r\n\n       lst=r['lst']\n    else:\n       lst=[{'repo_uoa':ruoa, 'module_uoa':muoa, 'data_uoa':duoa}]\n\n    i['move']='yes'\n    i['keep_old_uid']='yes'\n\n    r={'return':0}\n    for ll in lst:\n        i['repo_uoa']=ll['repo_uoa']\n        i['module_uoa']=ll['module_uoa']\n        i['data_uoa']=ll['data_uoa']\n        r=copy(i)\n        if r['return']>0: return r\n\n    return r", "response": "MV function for the CID nocompletion"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_file(i):\n\n    # Check if global writing is allowed\n    r=check_writing({'delete':'yes'})\n    if r['return']>0: return r\n\n    o=i.get('out','')\n\n    ruoa=i.get('repo_uoa','')\n    muoa=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    # Check file\n    fn=i.get('filename','')\n    if fn=='':\n       x=i.get('cids',[])\n       if len(x)>0:\n          fn=x[0]\n\n    if fn=='':\n       return {'return':1, 'error':'filename is empty'}\n\n    if duoa=='':\n       return {'return':1, 'error':'data UOA is not defined'}\n\n    if fn=='':\n       return {'return':1, 'error':'filename is not defined'}\n\n    # Get info about entry\n    r=load({'repo_uoa':ruoa, 'module_uoa':muoa, 'data_uoa':duoa})\n    if r['return']>0: return r\n\n    p=r['path']\n\n    ruoa=r['repo_uoa']\n    ruid=r['repo_uid']\n\n    # Check repo/module writing\n    ii={'module_uoa':muoa, 'repo_uoa':ruoa, 'repo_uid':ruid}\n    r=check_writing(ii)\n    if r['return']>0: return r\n\n    rd=r.get('repo_dict',{})\n    rshared=rd.get('shared','')\n    rsync=rd.get('sync','')\n\n    p1=os.path.normpath(os.path.join(p, fn))\n    px=os.path.normpath(os.path.join(p, cfg['subdir_ck_ext']))\n\n    if p1.startswith(px):\n       return {'return':1, 'error':'path points to the special directory with meta info'}\n\n    if not p1.startswith(p):\n       return {'return':1, 'error':'path is outside entry'}\n\n    if not os.path.isfile(p1) and not os.path.isdir(p1):\n       return {'return':1, 'error':'file or directory is not found'}\n\n    p2=os.path.split(p1)\n    px0=p2[0]\n    px1=p2[1]\n\n    if rshared!='':\n       ppp=os.getcwd()\n       os.chdir(px0)\n\n       ss=cfg['repo_types'][rshared]['rm'].replace('$#files#$', px1)\n       rx=os.system(ss)\n\n    if os.path.isfile(p1):\n       os.remove(p1)\n\n    if os.path.isdir(p1):\n       import shutil\n       shutil.rmtree(p1, onerror=rm_read_only)\n\n    if rshared!='':\n       os.chdir(ppp)\n\n    return {'return':0}", "response": "Delete a file in a single object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_data(i):\n\n    import time\n    start_time = time.time()\n\n    xls=i.get('limit_size','')\n    if xls=='': xls='0'\n    ls=int(xls)\n    ils=0\n\n    lst=[]\n\n    o=i.get('out','')\n\n    debug=(cfg.get('debug','').lower()=='yes' or cfg.get('debug','').lower()=='1')\n\n    iu=i.get('ignore_update', '')\n\n    prf=i.get('print_full','')\n    if prf=='': prf=i.get('all','')\n    iprf=(prf=='yes')\n\n    prn=i.get('print_name','')\n    if prn=='': prn=i.get('name','')\n    iprn=(prn=='yes')\n\n    ipru=(i.get('print_uid','')=='yes')\n\n    # Add info about entry to the final list \n    # (particularly when searching by special keywords, \n    # such as name or date of creation\n\n    iaf=(i.get('add_info','')=='yes')\n\n    iam=(i.get('add_meta','')=='yes')\n\n    aidb=i.get('add_if_date_before','')\n    aida=i.get('add_if_date_after','')\n    aid=i.get('add_if_date','')\n\n    # Support ISO and human readable time\n    aidb=aidb.strip().replace(' ','T')\n    aida=aida.strip().replace(' ','T')\n    aid=aid.strip().replace(' ','T')\n\n    oaidb=None\n    oaida=None\n    oaid=None\n\n    sn=i.get('search_by_name','')\n\n    if aidb!='' or aida!='' or aid!='':\n\n       import datetime\n       if aidb!='': \n          rx=convert_iso_time({'iso_datetime':aidb})\n          if rx['return']>0: return rx\n          oaidb=rx['datetime_obj']\n       if aida!='': \n          rx=convert_iso_time({'iso_datetime':aida})\n          if rx['return']>0: return rx\n          oaida=rx['datetime_obj']\n       if aid!='':  \n          rx=convert_iso_time({'iso_datetime':aid})\n          if rx['return']>0: return rx\n          oaid=rx['datetime_obj']\n\n    if oaidb!=None or oaida!=None or oaid!=None or sn!='': \n       iaf=True\n\n    dnatl=i.get('do_not_add_to_lst','')\n    idnatl=False\n    if dnatl=='yes': idnatl=True\n\n    ruoa=i.get('repo_uoa','')\n    muoa=i.get('module_uoa','')\n    muid=i.get('module_uid','')\n    duoa=i.get('data_uoa','')\n\n    lruoa=i.get('repo_uoa_list',[])\n    lmuoa=i.get('module_uoa_list',[])\n    lduoa=i.get('data_uoa_list',[])\n\n    to=float(i.get('time_out','30'))\n    elapsed_time=0\n\n    if duoa=='': duoa='*'\n    if muoa=='' and muid=='': muoa='*'\n    if ruoa=='': ruoa='*'\n\n    sff=i.get('filter_func','')\n    ff=i.get('filter_func_addr',None)\n    if sff!='':\n       ff=getattr(sys.modules[__name__], sff)\n    if ff!=None:\n       sd=i.get('search_dict',{})\n       ic=i.get('ignore_case','')\n       ss=i.get('search_string','')\n       if ic=='yes': ss=ss.lower()\n\n    # Check if wild cards present (only repo or data)\n    wr=''\n    wm=''\n    wd=''\n\n    if ruoa.find('*')>=0 or ruoa.find('?')>=0: wr=ruoa\n    if muoa.find('*')>=0 or muoa.find('?')>=0: wm=muoa\n    if duoa.find('*')>=0 or duoa.find('?')>=0: wd=duoa\n\n    if wr!='' or wm!='' or wd!='':\n       import fnmatch\n\n    zr={}\n\n    fixed_repo=False\n    if ruoa!='' and wr=='':\n       # Try to load a given repository\n       r=access({'action':'load',\n                 'module_uoa':cfg['repo_name'],\n                 'data_uoa':ruoa,\n                 'common_func':'yes'})\n       if r['return']>0: return r\n       duid=r['data_uid']\n\n       zr[duid]=r\n       fixed_repo=True\n    else:\n       # Prepare all repositories\n       r=reload_repo_cache({}) # Ignore errors\n       if r['return']>0: return r\n       zr=cache_repo_info \n\n    # Start iterating over repositories\n    ir=0\n    iir=True\n    zrk=list(zr.keys())\n    lr=len(zrk)\n    finish=False\n    while iir:\n       skip=False\n       if fixed_repo:\n          if ir>0:\n             skip=True\n             iir=False\n          else:\n             ruid=zrk[0]\n             d=zr[ruid]\n             dd=d.get('dict',{})\n             remote=dd.get('remote','')\n             if remote=='yes':\n                skip=True\n             else:\n                ruoa=d.get('data_uoa','')\n                p=dd.get('path','')\n                if ruid==cfg['repo_uid_default']: p=work.get('dir_default_repo','')\n                elif ruid==cfg['repo_uid_local']: p=work.get('dir_local_repo','')\n       elif ir==0:\n          ruoa=cfg['repo_name_default']\n          ruid=cfg['repo_uid_default']\n          p=work.get('dir_default_repo','')\n       elif ir==1:\n          ruoa=cfg['repo_name_local']\n          ruid=cfg['repo_uid_local']\n          p=work.get('dir_local_repo','')\n          if p=='': \n             skip=True\n       else:\n          if ir<lr+2:\n             ruid=zrk[ir-2]\n             d=zr[ruid]\n             dd=d.get('dict',{})\n             remote=dd.get('remote','')\n             if remote=='yes':\n                skip=True\n             else:\n                ruoa=d.get('data_uoa','')\n                p=dd.get('path','')\n          else:\n             skip=True\n             iir=False\n\n       # Check if wild cards\n       if not skip and p!='' and wr!='':\n          if len(lruoa)>0 and (ruoa not in lruoa and ruid not in lruoa):\n             skip=True\n          elif wr=='*':\n             pass\n          elif is_uid(ruoa): \n             skip=True # If have wildcards, but not alias\n          elif not fnmatch.fnmatch(ruoa, wr):\n             skip=True\n\n       # Check if got proper path\n       if not skip and p!='':\n          # Prepare modules in the current directory\n          xm=[]\n\n          if muoa!='' and wm=='': \n             xm.append(muoa)\n          else:   \n             # Now iterate over modules inside a given path\n             try:\n                lm=os.listdir(p)\n             except Exception as e:\n                None\n             else:\n                for fn in lm:\n                    if os.path.isdir(os.path.join(p,fn)) and fn not in cfg['special_directories']:\n                       xm.append(fn)\n\n          # Iterate over modules\n          for mu in xm:\n              r=find_path_to_entry({'path':p, 'data_uoa':mu})\n              if r['return']==0:\n                 mp=r['path']\n                 muid=r['data_uid']\n                 muoa=r['data_uoa']\n\n                 mskip=False\n\n                 if wm!='':\n                    if len(lmuoa)>0 and (muoa not in lmuoa and muid not in lmuoa):\n                       mskip=True\n                    elif wm=='*':\n                       pass\n                    elif is_uid(muoa): \n                       mskip=True # If have wildcards, but not alias\n                    elif not fnmatch.fnmatch(muoa, wm):\n                       mskip=True\n\n                 if not mskip:\n                    # Prepare data in the current directory\n                    xd=[]\n\n                    if duoa!='' and wd=='': \n                       r=find_path_to_entry({'path':mp, 'data_uoa':duoa})\n                       if r['return']==0:\n                          xd.append(duoa)\n                    else:   \n                       # Now iterate over data inside a given path\n                       try:\n                          ld=os.listdir(mp)\n                       except Exception as e:\n                          None\n                       else:\n                          for fn in ld:\n                              if os.path.isdir(os.path.join(mp,fn)) and fn not in cfg['special_directories']:\n                                 xd.append(fn)\n\n                    # Iterate over data\n                    if len(lduoa)>0:\n                       xd=lduoa\n\n                    for du in xd:\n                        r=find_path_to_entry({'path':mp, 'data_uoa':du})\n                        if r['return']!=0: continue\n\n                        dp=r['path']\n                        dpcfg=os.path.join(dp,cfg['subdir_ck_ext'])\n                        dpinfo=os.path.join(dp,cfg['subdir_ck_ext'],cfg['file_info'])\n                        dpmeta=os.path.join(dp,cfg['subdir_ck_ext'],cfg['file_meta'])\n                        tduid=r['data_uid']\n                        tduoa=r['data_uoa']\n\n                        if os.path.isdir(dpcfg): # Check if really CK data entry\n                           dskip=False\n\n                           if wd!='':\n                              if len(lduoa)>0 and (tduoa not in lduoa and tduid not in lduoa):\n                                 dskip=True\n                              elif wd=='*':\n                                 pass\n                              elif is_uid(tduoa): \n                                 dskip=True # If have wildcards, but not alias\n                              elif not fnmatch.fnmatch(tduoa, wd):\n                                 dskip=True\n\n                           if not dskip:\n                              # Iterate over data \n                              ll={'repo_uoa':ruoa, 'repo_uid':ruid,\n                                 'module_uoa':muoa, 'module_uid':muid,\n                                 'data_uoa':tduoa, 'data_uid':tduid,\n                                 'path':dp}\n\n                              # Need to load info?\n                              if iaf or iprn:\n                                 if os.path.isfile(dpinfo):\n                                    y=load_json_file({'json_file':dpinfo})\n                                    if y['return']>0: \n                                       if not debug: continue\n                                       return y\n                                    ll['info']=y['dict']\n\n                              # Need to load meta?\n                              if iam:\n                                 if os.path.isfile(dpmeta):\n                                    y=load_json_file({'json_file':dpmeta})\n                                    if y['return']>0: \n                                       if not debug: continue\n                                       return y\n                                    ll['meta']=y['dict']\n\n                              # Call filter\n                              fskip=False\n\n                              if ff!=None and ff!='':\n                                 ll['out']=o\n                                 ll['search_dict']=sd\n                                 ll['search_string']=ss\n                                 ll['ignore_case']=ic\n                                 ll['ignore_update']=iu\n\n                                 if oaidb!=None: ll['obj_date_before']=oaidb\n                                 if oaida!=None: ll['obj_date_after']=oaida\n                                 if oaid!=None: ll['obj_date']=oaid\n                                 if sn!=None: ll['search_by_name']=sn\n\n                                 rx=ff(ll)\n                                 if rx['return']>0: \n                                    if not debug: continue\n                                    return rx\n\n                                 if rx.get('skip','')=='yes':\n                                    fskip=True\n\n                              # Append\n                              if not fskip:\n                                 ils+=1\n \n                                 if not idnatl:\n                                    lst.append(ll)\n\n                                 if o=='con':\n                                    x=''\n                                    if iprf: x=ruoa+':'+muoa+':'\n                                    if sys.version_info[0]<3: \n                                       y=tduoa\n                                       try: y=y.decode(sys.stdin.encoding)\n                                       except Exception as e: \n                                         try: y=y.decode('utf8')\n                                         except Exception as e: pass\n                                       x+=y\n                                    else: x+=tduoa\n                                    if ipru: x+=' ('+tduid+')'\n                                    if iprn:\n                                       name=ll.get('info',{}).get('data_name','')\n                                       if name!='':\n                                          x=name+' ('+x+')'\n                                    out(x)\n\n                              # Check timeout\n                              elapsed_time = time.time() - start_time\n                              if to!=-1 and elapsed_time>to:\n                                 finish=True\n                                 break\n\n                              # Check size\n                              if ls>0 and ils==ls:\n                                 finish=True\n                                 break\n\n                    if finish: break\n          if finish: break\n\n       # Finish iteration over repositories\n       ir+=1\n\n    if o=='con' and i.get('print_time','')=='yes':\n       out('Elapsed time: '+str(elapsed_time)+' sec., number of entries: '+str(ils))\n\n    rr={'return':0, 'lst':lst, 'elapsed_time':str(elapsed_time)}\n    if finish: rr['timed_out']='yes'\n\n    return rr", "response": "List the data for a module"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef search(i):\n    o=i.get('out','')\n    ss=i.get('search_string','')\n    ls=i.get('limit_size','5000')\n\n    rr={'return':0}\n\n    sd=i.get('search_dict',{})\n\n    tags=i.get('tags','')\n    if tags!='':\n       xtags=tags.split(',')\n       xtags1=[]\n       for q in xtags:\n           xtags1.append(q.strip())\n       sd['tags']=xtags1\n\n    # Check if index\n    if cfg.get('use_indexing','')!='yes' or i.get('internal','')=='yes':\n       if ss!='':\n          i['filter_func']='search_string_filter'\n       else:\n          sfd=i.get('search_flat_dict',{})\n\n          if len(sfd)>0:\n             r=restore_flattened_dict({'dict':sfd})\n             if r['return']>0: return r\n\n             nd=r['dict']\n\n             sd.update(nd)\n\n             del (i['search_flat_dict'])\n\n          i['filter_func']='search_filter'\n\n       i['search_dict']=sd\n\n       pf=i.get('print_full','')\n       if pf=='': pf='yes'\n       i['print_full']=pf\n\n       rr=list_data(i)\n    else:\n       import time\n       start_time = time.time()\n\n       dss={}\n\n       ruoa=i.get('repo_uoa','')\n       muoa=i.get('module_uoa','')\n       duoa=i.get('data_uoa','')\n\n       lruoa=i.get('repo_uoa_list',[])\n       lmuoa=i.get('module_uoa_list',[])\n       lduoa=i.get('data_uoa_list',[])\n\n       if ruoa!='': lruoa.append(ruoa)\n       if muoa!='': lmuoa.append(muoa)\n       if duoa!='': lduoa.append(duoa)\n\n       if len(lruoa)>0:\n          if ss!='': ss+=' AND '\n          ss+=' ('\n          first=True\n          for x in lruoa:\n              if first: first=False\n              else: ss+=' OR '\n              ss+='(repo_uid:\"'+x+'\") OR (repo_uoa:\"'+x+'\")'\n          ss+=')'\n\n       if len(lmuoa)>0:\n          if ss!='': ss+=' AND '\n          ss+='('\n          first=True\n          for x in lmuoa:\n              if first: first=False\n              else: ss+=' OR '\n              ss+='(module_uid:\"'+x+'\") OR (module_uoa:\"'+x+'\")'\n          ss+=')'\n\n       if len(lduoa)>0:\n          if ss!='': ss+=' AND '\n          ss+='('\n          first=True\n          for x in lduoa:\n              if first: first=False\n              else: ss+=' OR '\n              ss+='(data_uid:\"'+x+'\") OR (data_uoa:\"'+x+'\")'\n          ss+=')'\n\n       # Check search keys\n       first=True\n       for u in sd:\n           v=sd[u]\n           if first: \n              first=False\n              if ss=='': ss+='('\n              else: ss+=' AND ('\n           else: \n              ss+=' AND '\n           if type(v)==list:\n              first1=True\n              for lk in v:\n                  if first1:\n                     first1=False\n                  else:\n                     ss+=' AND '\n                  ss+=u+':\"'+str(lk)+'\"'\n           else:\n              ss+=u+':\"'+v+'\"'\n\n       # Check special parameters\n       aidb=i.get('add_if_date_before','')\n       aida=i.get('add_if_date_after','')\n       aid=i.get('add_if_date','')\n\n       # Support ISO and human readable time\n       aidb=aidb.strip().replace(' ','T')\n       aida=aida.strip().replace(' ','T')\n       aid=aid.strip().replace(' ','T')\n\n       sn=i.get('search_by_name','')\n\n       if sn!='':\n          if first: \n             first=False\n             if ss=='': ss+='('\n             else: ss+=' AND ('\n          else: \n             ss+=' AND '\n\n          if sn.find('*')<0 and sn.find('?')<0:\n             ss+='data_name:\"'+sn+'\"'\n          else:\n             ss+='data_name:'+sn+''\n\n       if aidb!='' or aida!='' or aid!='':\n          if first: \n             first=False\n             if ss=='': ss+='('\n             else: ss+=' AND ('\n          else: \n             ss+=' AND '\n\n          ss+='iso_datetime:'\n          if aid!='': ss+='\"'+aid+'\"'\n          else:\n             ss+='['\n             if aida!='': \n                ss+='\"'+aida+'\"'\n             else:\n                ss+='*'\n             if aidb!='':\n                ss+=' TO \"'+aidb+'\"'\n             ss+='] '\n\n       # Finish query\n       if not first:\n          ss+=')'\n\n       # Prepare ElasticSearch query\n       import urllib\n\n       path='/_search?'\n       if ss!='': path+='q='+urllib.quote_plus(ss.encode('utf-8'))\n       if ls!='': path+='&size='+ls\n\n#       dss={'query':{'filtered':{'filter':{'terms':sd}}}}\n       dss={}\n\n       ri=access_index_server({'request':'GET', 'path':path, 'dict':dss})\n       if ri['return']>0: return ri\n\n       dd=ri['dict'].get('hits',{}).get('hits',[])\n\n       lst=[]\n       for qx in dd:\n           q=qx.get('_source',{})\n           ruoa=q.get('repo_uoa','')\n           ruid=q.get('repo_uid','')\n           muoa=q.get('module_uoa','')\n           muid=q.get('module_uid','')\n           duoa=q.get('data_uoa','')\n           duid=q.get('data_uid','')\n           path=q.get('path','')\n\n           lst.append({'repo_uoa':ruoa, 'repo_uid':ruid,\n              'module_uoa':muoa, 'module_uid':muid,\n              'data_uoa':duoa, 'data_uid':duid,\n              'path':path})\n\n           if o=='con':\n              x=ruoa+':'+muoa+':'\n              if sys.version_info[0]<3: \n                 y=duoa\n                 try: y=y.decode(sys.stdin.encoding)\n                 except Exception as e: \n                   try: y=y.decode('utf8')\n                   except Exception as e: pass\n                 x+=y\n              else: x+=duoa\n              out(x)\n\n       rr['lst']=lst\n       rr['elapsed_time']=str(time.time() - start_time)\n\n       if o=='con' and i.get('print_time','')=='yes':\n          out('Elapsed time: '+rr['elapsed_time']+' sec., number of entries: '+str(len(lst)))\n\n    return rr", "response": "Search for the entry in the order of the data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsearches filter for the entry in the order they appear in the entry.", "response": "def search_filter(i):\n    \"\"\"\n    Input:  {\n              repo_uoa             - repo UOA\n              module_uoa           - module UOA\n              data_uoa             - data UOA\n              path                 - path  \n\n              (search_dict)        - search if this dict is a part of the entry\n              (ignore_case)        - if 'yes', ignore case of letters\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              lst          - [{'repo_uoa', 'repo_uid',\n                               'module_uoa', 'module_uid', \n                               'data_uoa','data_uid',\n                               'path'}]\n            }\n\n    \"\"\"\n\n    ic=i.get('ignore_case','')\n\n    # Check special info\n    info=i.get('info',{})\n    if len(info)!='':\n       oaidb=i.get('obj_date_before', None)\n       oaida=i.get('obj_date_after', None)\n       oaid=i.get('obj_date', None)\n       sn=i.get('search_by_name','')\n\n       # Check dates\n       if oaidb!=None or oaida!=None or oaid!=None:\n          idt=info.get('control',{}).get('iso_datetime','')\n          if idt!='':\n             rx=convert_iso_time({'iso_datetime':idt})\n             if rx['return']>0: return rx\n             oidt=rx['datetime_obj']\n\n             if oaidb!=None and oidt>oaidb: return {'return':0, 'skip':'yes'}\n             if oaida!=None and oidt<oaida: return {'return':0, 'skip':'yes'}\n             if oaid!=None and oidt!=oaid: return {'return':0, 'skip':'yes'}\n\n       # Check if search by name\n       if sn!='':\n          ro=find_string_in_dict_or_list({'dict':{'string':info.get('data_name','')}, \n                                          'search_string':sn,\n                                          'ignore_case':ic})\n          if ro['return']>0: return ro\n          if ro['found']!='yes': return {'return':0, 'skip':'yes'}\n\n    # To be fast, load directly\n    p=i['path']\n\n    skip='yes'\n\n    sd=i.get('search_dict',{})\n\n    p1=os.path.join(p,cfg['subdir_ck_ext'],cfg['file_meta'])\n    if not os.path.isfile(p1):\n       p1=os.path.join(p,cfg['subdir_ck_ext'],cfg['file_meta_old'])\n       if not os.path.isfile(p1):\n          return {'return':0, 'skip':'yes'}\n\n    r=load_json_file({'json_file':p1})\n    if r['return']>0: return r\n    d=r['dict']\n\n    # Check directly\n    rx=compare_dicts({'dict1':d, 'dict2':sd, 'ignore_case':ic})\n    if rx['return']>0: return rx\n    equal=rx['equal']\n    if equal=='yes': skip='no'\n\n    return {'return':0, 'skip':skip}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninputs { dict1 - dictionary 1 dict2 - dictionary 2 (ignore_case) - ignore case of letters Note that if dict1 and dict2 has lists, the results will be as follows: * dict1={\"key\":['a','b','c']} dict2={\"key\":['a','b']} EQUAL * dict1={\"key\":['a','b']} dict2={\"key\":['a','b','c']} NOT EQUAL } Output: { return - return code = 0, if successful > 0, if error (error) - error text if return > 0 equal - if 'yes' dictionaries are equal }", "response": "def compare_dicts(i):\n    \"\"\"\n    Input:  {\n              dict1         - dictionary 1\n              dict2         - dictionary 2\n              (ignore_case) - ignore case of letters\n\n              Note that if dict1 and dict2 has lists, the results will be as follows:\n\n              * dict1={\"key\":['a','b','c']}\n                dict2={\"key\":['a','b']}\n                EQUAL\n\n              * dict1={\"key\":['a','b']}\n                dict2={\"key\":['a','b','c']}\n                NOT EQUAL\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              equal        - if 'yes' dictionaries are equal\n            }\n    \"\"\"\n\n    d1=i.get('dict1',{})\n    d2=i.get('dict2',{})\n\n    equal='yes'\n\n    bic=False\n    ic=i.get('ignore_case','')\n    if ic=='yes': bic=True\n\n    for q2 in d2:\n        v2=d2[q2]\n        if type(v2)==dict:\n           if q2 not in d1:\n              equal='no'\n              break\n\n           v1=d1[q2]\n\n           rx=compare_dicts({'dict1':v1,'dict2':v2, 'ignore_case':ic})\n           if rx['return']>0: return rx\n           equal=rx['equal']\n           if equal=='no':\n              break\n        elif type(v2)==list:\n           # For now can check only values in list\n           if q2 not in d1:\n              equal='no'\n              break\n\n           v1=d1[q2]\n\n           if type(v1)!=list:\n              equal='no'\n              break\n\n           for m in v2:\n               if m not in v1:\n                  equal='no'\n                  break\n\n           if equal=='no':\n              break\n        else:\n           if q2 not in d1:\n              equal='no'\n              break\n\n           if equal=='no':\n              break\n\n           v1=d1[q2]\n\n           if bic and type(v1)!=int and type(v1)!=float and type(v1)!=bool: \n              v1=v1.lower()\n              v2=v2.lower()\n\n           if v2!=v1:\n              equal='no'\n              break\n\n    return {'return':0, 'equal':equal}"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninputs { dict1 - dictionary 1 dict2 - dictionary 2 (ignore_case) - ignore case of letters (space_as_none) - if 'yes', consider \"\" as None (keys_to_ignore) - list of keys to ignore (can be wildcards) } Output: { return - return code = 0, if successful > 0, if error (error) - error text if return > 0 equal - if 'yes' dictionaries are equal }", "response": "def compare_flat_dicts(i):\n    \"\"\"\n    Input:  {\n              dict1            - dictionary 1\n              dict2            - dictionary 2\n              (ignore_case)    - ignore case of letters\n              (space_as_none)  - if 'yes', consider \"\" as None\n              (keys_to_ignore) - list of keys to ignore (can be wildcards)\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              equal        - if 'yes' dictionaries are equal\n            }\n    \"\"\"\n\n    d1=i.get('dict1',{})\n    d2=i.get('dict2',{})\n\n    equal='yes'\n\n    ic=False\n    x=i.get('ignore_case','')\n    if x=='yes': ic=True\n\n    san=None\n    x=i.get('space_as_none','')\n    if x=='yes': san=''\n\n    # Create common set of keys\n    keys=list(d1.keys())\n    for q in d2:\n        if q not in keys:\n           keys.append(q)\n\n    # If keys to ignore\n    kti=i.get('keys_to_ignore',[])\n    if len(kti)>0:\n       import fnmatch\n\n       x=[]\n       for q in keys:\n           skip=False\n           for k in kti:\n               if fnmatch.fnmatch(q,k):\n                  skip=True\n           if not skip:\n              x.append(q)\n       keys=x\n\n    # Compare all keys\n    for q in keys:\n        v1=d1.get(q, san)\n        v2=d2.get(q, san)\n\n        if ic and type(v1)!=int and type(v1)!=float and type(v1)!=bool: \n           v1=v1.lower()\n           v2=v2.lower()\n\n        if v1!=v2:\n           equal='no'\n           break\n\n    return {'return':0, 'equal':equal}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_string_in_dict_or_list(i):\n\n    d=i.get('dict',{})\n\n    found='no'\n\n    wc=False\n    ss=i.get('search_string','')\n    if ss.find('*')>=0 or ss.find('?')>=0:\n       wc=True\n       import fnmatch\n\n    bic=False\n    ic=i.get('ignore_case','')\n    if ic=='yes': \n       bic=True\n       ss=ss.lower()\n\n    for q in d:\n        if type(d)==dict:   v=d[q]\n        elif type(d)==list: v=q\n        else:               v=str(q)\n\n        if type(v)==dict or type(v)==list:\n           rx=find_string_in_dict_or_list({'dict':v, 'search_string':ss, 'ignore_case':ic})\n           if rx['return']>0: return rx\n           found=rx['found']\n           if found=='yes':\n              break\n        else:\n           try: v=str(v)\n           except Exception as e: pass\n\n           if bic: \n              v=v.lower()\n\n           if (wc and fnmatch.fnmatch(v, ss)) or v==ss:\n              found='yes'\n              break\n\n    return {'return':0, 'found':found}", "response": "This function will find the string in the dictionary or list of strings in the dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef search_string_filter(i):\n\n    # To be fast, load directly\n    p=i['path']\n\n    skip='yes'\n\n    ss=i.get('search_string','')\n    if ss=='':\n       skip='no'\n    else:\n       ic=i.get('ignore_case','')\n\n       p1=os.path.join(p,cfg['subdir_ck_ext'],cfg['file_meta'])\n       if not os.path.isfile(p1):\n          p1=os.path.join(p,cfg['subdir_ck_ext'],cfg['file_meta_old'])\n          if not os.path.isfile(p1):\n             return {'return':0, 'skip':'yes'}\n\n       r=load_json_file({'json_file':p1})\n       if r['return']>0: return r\n       d=r['dict']\n\n       # Check directly\n       rx=find_string_in_dict_or_list({'dict':d, 'search_string':ss, 'ignore_case':ic})\n       if rx['return']>0: return rx\n       found=rx['found']\n       if found=='yes': skip='no'\n\n    return {'return':0, 'skip':skip}", "response": "Search string in a single object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninputting { request - request type ('PUT', 'DELETE', 'TEST') (path) - path (dict) - query as dict to send } Output: { return - return code = 0, if successful > 0, if error (error) - error text if return > 0 dict - returned dict }", "response": "def access_index_server(i):\n    \"\"\"\n    Input:  {\n              request        - request type ('PUT', 'DELETE', 'TEST')\n              (path)         - path  \n              (dict)         - query as dict to send\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              dict         - returned dict\n            }\n\n    \"\"\"\n\n    request=i['request']\n\n    # Prepare URL\n    host=cfg.get('index_host','')\n    if host=='':\n       return {'return':1, 'error':'index host is not defined in configuration'}\n\n    url=host\n    port=cfg.get('index_port','')\n    if port!='':\n       url+=':'+port\n\n    path=i.get('path','')\n    url+=path\n\n    dd=i.get('dict',{})\n    ddo={}\n\n    if cfg.get('index_use_curl','')=='yes':\n       import tempfile\n\n       fd1, fn1=tempfile.mkstemp(suffix='.tmp', prefix='ck-')\n       os.close(fd1)\n       os.remove(fn1)\n\n       fd2, fn2=tempfile.mkstemp(suffix='.tmp', prefix='ck-')\n       os.close(fd2)\n       os.remove(fn2)\n\n       r=save_json_to_file({'json_file':fn1, 'dict':dd})\n       if r['return']>0: return r\n\n       cmd='curl -X'+request+' '+url+' -d @'+fn1+' -s -o '+fn2\n       os.system(cmd)\n\n       # Read output\n       if not os.path.isfile(fn2):\n          return {'return':1, 'error':'problem accessing indexing server - maybe indexing server is down?'}\n\n       r=load_json_file({'json_file':fn2})\n\n       if os.path.isfile(fn1): os.remove(fn1)\n       if os.path.isfile(fn2): os.remove(fn2)\n\n       if r['return']>0: return r\n       ddo=r['dict']\n    else:\n       try:\n          import urllib.request as urllib2\n       except:\n          import urllib2\n\n       try:\n          from urllib.parse import urlencode\n       except:\n          from urllib import urlencode\n\n       # Prepare post variables\n       r=dumps_json({'dict':dd, 'skip_indent':'yes'})\n       if r['return']>0: return r\n       s=r['string'].encode('utf8')\n\n       rq = urllib2.Request(url, s)\n       if request=='DELETE':\n          rq.get_method = lambda: request\n\n       not_found=False\n       try:\n          f=urllib2.urlopen(rq)\n       except urllib2.URLError as e:\n          se=format(e)\n          if request=='DELETE' and se.find('404')>0:\n             not_found=True\n          else:\n             return {'return':1, 'error':'problem accessing indexing server ('+se+')'}\n\n       if not not_found:\n          try:\n             s=f.read()\n             f.close()\n          except Exception as e:\n             return {'return':1, 'error':'can\\'t parse output during indexing ('+format(e)+')'}\n\n          if sys.version_info[0]>2:\n             s=s.decode('utf8')\n\n          r=convert_json_str_to_dict({'str':s, 'skip_quote_replacement':'yes'})\n          if r['return']>0: \n             return {'return':1, 'error':'can\\'t parse output from index server ('+r['error']+')'}\n          ddo=r['dict']\n\n    return {'return':0, 'dict':ddo}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_action(i):\n\n    # Check if global writing is allowed\n    r=check_writing({})\n    if r['return']>0: return r\n\n    o=i.get('out','')\n\n    ruoa=i.get('repo_uoa','')\n    muoa=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    func=i.get('func','')\n    desc=i.get('desc','')\n\n    fweb=i.get('for_web','')\n\n    if muoa=='':\n       return {'return':1, 'error':'module UOA is not defined'}\n\n    if duoa!='':\n       muoa=duoa\n       duoa=''\n\n    # Find path to module\n    ii={'module_uoa':cfg['module_name'],\n        'data_uoa':muoa}\n    if ruoa!='': ii['repo_uoa']=ruoa\n    r=load(ii)\n    if r['return']>0: return r\n\n    pp=r['path']\n    dd=r['dict']\n\n    actions=dd.get('actions',{})\n\n    # Check func and desc\n    if o=='con':\n       if func=='':\n          r=inp({'text':'Add action function (or Enter to stop):    '})\n          func=r['string']\n\n       if func!='':\n          if fweb=='':\n             r1=inp({'text':'Support web (y/N):                         '})\n             fweb=r1['string'].lower()\n             if fweb=='y' or fweb=='yes': fweb='yes'\n             else: fweb=''\n\n          if desc=='':\n             r1=inp({'text':'Add action description:                    '})\n             desc=r1['string']\n\n    # Check if empty\n    if func=='':\n       return {'return':1, 'error':'action (function) is not defined'}\n\n    if func in actions:\n       return {'return':1, 'error':'action (function) already exists in the module'}\n\n    # Adding actions\n    actions[func]={}\n    if desc!='': actions[func]['desc']=desc\n    if fweb!='': actions[func]['for_web']=fweb\n    dd['actions']=actions\n\n    if i.get('skip_appending_dummy_code','')!='yes':\n       ii={'module_uoa':cfg['module_name'],\n           'data_uoa':cfg['module_name']}\n       r=load(ii)\n       if r['return']>0: return r\n\n       px=r['path']\n       pd=r['dict']\n\n       pma=os.path.join(px, pd['dummy_module_action'])\n\n       # Load module action dummy\n       r=load_text_file({'text_file':pma})\n       if r['return']>0: return r\n       spma=r['string']\n\n       # Load current module\n       pmx=os.path.join(pp, cfg['module_full_code_name'])\n       r=load_text_file({'text_file':pmx})\n       if r['return']>0: return r\n       spm=r['string']\n\n       # Update\n       spm+='\\n'+spma.replace('$#action#$', func).replace('$#desc#$',desc)\n\n       # Write current module\n       rx=save_text_file({'text_file':pmx, 'string':spm})\n       if rx['return']>0: return rx\n\n    # Update data entry\n    if o=='con': out('')\n    ii={'module_uoa':cfg['module_name'],\n        'data_uoa':muoa,\n        'dict':dd,\n        'out':o,\n        'sort_keys':'yes'}\n    if ruoa!='': ii['repo_uoa']=ruoa\n    r=update(ii)\n    if r['return']>0: return r\n\n    return r", "response": "Add action to the list of modules"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_action(i):\n    # Check if global writing is allowed\n    r=check_writing({})\n    if r['return']>0: return r\n\n    o=i.get('out','')\n\n    ruoa=i.get('repo_uoa','')\n    muoa=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    func=i.get('func','')\n\n    if muoa=='':\n       return {'return':1, 'error':'module UOA is not defined'}\n\n    if duoa!='':\n       muoa=duoa\n       duoa=''\n\n    # Find path to module\n    ii={'module_uoa':cfg['module_name'],\n        'data_uoa':muoa}\n    if ruoa!='': ii['repo_uoa']=ruoa\n    r=load(ii)\n    if r['return']>0: return r\n\n    pp=r['path']\n    dd=r['dict']\n\n    actions=dd.get('actions',{})\n\n    # Check func and desc\n    if o=='con':\n       if func=='':\n          r=inp({'text':'Enter function to be removed (or Enter to quit) - note that we remove only reference to this function from the module meta: '})\n          func=r['string']\n\n    # Check if empty\n    if func=='':\n       return {'return':1, 'error':'action (function) is not defined'}\n\n    if func not in actions:\n       return {'return':1, 'error':'action (function) is not found in the module'}\n\n    del (actions[func])\n\n    dd['actions']=actions\n\n    # Update data entry\n    if o=='con': out('')\n    ii={'module_uoa':cfg['module_name'],\n        'data_uoa':muoa,\n        'dict':dd,\n        'substitute':'yes',\n        'sort_keys':'yes',\n        'out':o}\n    if ruoa!='': ii['repo_uoa']=ruoa\n    r=update(ii)\n    if r['return']>0: return r\n\n    if o=='con':\n       out('')\n       out('Reference to the function \"'+func+'\" was removed from module meta. Function body was not removed from the python code')\n\n    return r", "response": "Remove action from the module"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_actions(i):\n\n    o=i.get('out','')\n\n    ruoa=i.get('repo_uoa','')\n    muoa=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    if muoa!='':\n       if duoa!='':\n          muoa=duoa\n          duoa=''\n\n       # Find path to module 'module' to get dummies\n       ii={'action':'load',\n           'module_uoa':cfg['module_name'],\n           'data_uoa':muoa,\n           'common_func':'yes'}\n       if ruoa!='': ii['repo_uoa']=ruoa\n\n       r=access(ii)\n       if r['return']>0: return r\n\n       dd=r['dict']\n   \n       actions=dd.get('actions',{})\n    else:\n       actions=cfg['actions']\n\n    # If console, print actions\n    if o=='con':\n       for q in sorted(actions.keys()):\n           s=q\n\n           desc=actions[q].get('desc','')\n           if desc!='': s+=' - '+desc\n\n           out(s)\n\n    return {'return':0, 'actions':actions}", "response": "List the actions of a kernel"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npull whole entry into a single archive", "response": "def pull(i):\n    \"\"\"\n    Input:  {\n              (repo_uoa)      - repo UOA, if needed\n              module_uoa      - module UOA \n              data_uoa        - data UOA\n\n              (filename)      - filename (with path) (if empty, set archive to 'yes')\n                  or\n              (cid[0])\n                                if empty, create an archive of the entry\n              (archive)       - if 'yes' pull whole entry as zip archive using filename or ck_archive.zip\n              (all)           - if 'yes' and archive, add even special directories (.cm, .svn, .git, etc)\n\n\n              (out)           - if 'json' or 'json_file', encode file and return in r\n              (skip_writing)  - if 'yes', do not write file (not archive) to current directory\n\n              (pattern)       - return only files with this pattern\n              (patterns)      - multiple patterns (useful to pack mutiple points in experiments)\n\n              (encode_file)   - if 'yes', encode file\n\n              (skip_tmp)      - if 'yes', skip tmp files and directories\n            }\n\n    Output: {\n              return                - return code =  0, if successful\n                                                  >  0, if error\n              (error)               - error text if return > 0\n              (file_content_base64) - if i['to_json']=='yes', encoded file\n              (filename)            - filename to record locally\n            }\n\n    \"\"\"\n\n    o=i.get('out','')\n\n    tj=False\n    if o=='json' or o=='json_file' or i.get('encode_file','')=='yes':\n       tj=True\n\n    st=i.get('skip_tmp','')\n\n    ruoa=i.get('repo_uoa','')\n    muoa=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    pat=i.get('pattern','')\n    pats=i.get('patterns',[])\n    if pat!='':\n       pats.append(pat)\n\n    fn=i.get('filename','')\n    if fn=='':\n       x=i.get('cids',[])\n       if len(x)>0:\n          fn=x[0]\n\n    # Attempt to load data (to find path, etc)\n    r=load({'repo_uoa':ruoa, 'module_uoa':muoa, 'data_uoa':duoa})\n    if r['return']>0: return r\n\n    p=r['path']\n    muoa=r['module_uoa']\n    duoa=r['data_uoa']\n    dd=r['dict']\n\n    # How output\n    sw=i.get('skip_writing','')\n\n    # Prepare output\n    rr={'return':0}\n\n    # Check what to pull\n    pfn=''\n\n    if fn=='': \n       i['archive']='yes'\n\n    delete_file=''\n\n    if i.get('archive','')!='yes':\n       # Get file\n       pfn=os.path.normpath(os.path.join(p,fn))\n\n       # Check that file is not getting outside paths ...\n       if not pfn.startswith(p):\n          return {'return':1, 'error':'path of file is outside entry'}\n\n       if not os.path.isfile(pfn):\n          return {'return':1, 'error':'file not found'}\n\n       if not tj and sw!='yes':\n          # Copy file to current directory\n          if os.path.isfile(fn):\n             return {'return':1, 'error':'file already exists in the current directory'}\n\n          # Copy file\n          import shutil\n          shutil.copyfile(pfn,fn)\n\n       py=os.path.split(fn)\n       rr['filename']=py[1]\n\n    else:\n       # Prepare archive name\n       if fn!='': \n          # Check that file is not getting outside paths ...\n          fn=os.path.normpath(os.path.join(os.getcwd(),fn))\n          if not pfn.startswith(os.getcwd()):\n             return {'return':1, 'error':'archive filename should not have path'}\n\n       else:\n          if tj:\n             # Generate tmp file\n             import tempfile\n             fd, fn=tempfile.mkstemp(suffix='.tmp', prefix='ck-') # suffix is important - CK will delete such file!\n             os.close(fd)\n             os.remove(fn)\n             delete_file=fn\n          else:\n             fn=cfg['default_archive_name']\n       pfn=fn\n\n       if os.path.isfile(pfn):\n          return {'return':1, 'error':'archive file already exists in the current directory'}\n\n       # Prepare archive\n       import zipfile\n\n       zip_method=zipfile.ZIP_DEFLATED\n\n       gaf=i.get('all','')\n\n       fl={}\n\n       if len(pats)>0:\n          for q in pats:\n              r=list_all_files({'path':p, 'all':gaf, 'pattern':q})\n              if r['return']>0: return r\n\n              flx=r['list']\n\n              for k in flx:\n                  fl[k]=flx[k]\n       else:\n          r=list_all_files({'path':p, 'all':gaf})\n          if r['return']>0: return r\n\n          fl=r['list']\n\n       # Write archive\n       try:\n          f=open(pfn, 'wb')\n          z=zipfile.ZipFile(f, 'w', zip_method)\n          for fn in fl:\n              if st!='yes' or not fn.startswith('tmp'):\n                 p1=os.path.join(p, fn)\n                 z.write(p1, fn, zip_method)\n          z.close()\n          f.close()\n\n       except Exception as e:\n          return {'return':1, 'error':'failed to prepare archive ('+format(e)+')'}\n\n    # If add to JSON\n    if tj:\n       r=convert_file_to_upload_string({'filename':pfn})\n       if r['return']>0: return r\n\n       rr['file_content_base64']=r['file_content_base64']\n\n       if delete_file!='': os.remove(delete_file)\n\n    return rr"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npush a new entry into the archive.", "response": "def push(i):\n    \"\"\"\n    Input:  {\n              (repo_uoa)            - repo UOA, if needed\n              module_uoa            - module UOA \n              data_uoa              - data UOA\n\n              (filename)            - local filename \n                  or\n              (cid[0])\n\n              (extra_path)          - extra path inside entry (create if doesn't exist)\n\n              (file_content_base64) - if !='', take its content and record into filename\n\n              (archive)             - if 'yes' push to entry and unzip ...\n\n              (overwrite)           - if 'yes', overwrite files\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n            }\n\n    \"\"\"\n\n    # Check if global writing is allowed\n    r=check_writing({})\n    if r['return']>0: return r\n\n    o=i.get('out','')\n\n    ruoa=i.get('repo_uoa','')\n    muoa=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    # Check file\n    fn=i.get('filename','')\n    if fn=='':\n       x=i.get('cids',[])\n       if len(x)>0:\n          fn=x[0]\n\n    if fn=='':\n       return {'return':1, 'error':'filename is empty'}\n\n    fcb=False\n    if 'file_content_base64' in i:\n       import base64\n\n       bin=base64.urlsafe_b64decode(i['file_content_base64'].encode('utf8')) # convert from unicode to str since base64 works on strings\n                                                                   # should be safe in Python 2.x and 3.x\n       fcb=True\n    else:\n       if not os.path.isfile(fn):\n          return {'return':1, 'error':'file '+fn+' not found'}\n\n    # Attempt to load data (to find path, etc)\n    rx=load({'repo_uoa':ruoa, 'module_uoa':muoa, 'data_uoa':duoa})\n    if rx['return']>0: return rx\n\n    p=rx['path']\n    muoa=rx['module_uoa']\n    duoa=rx['data_uoa']\n    dd=rx['dict']\n\n    px=os.path.normpath(os.path.join(p, cfg['subdir_ck_ext']))\n\n    ruoa=rx['repo_uoa']\n    ruid=rx['repo_uid']\n\n    # Check repo/module writing\n    ii={'module_uoa':muoa, 'repo_uoa':ruoa, 'repo_uid':ruid}\n    r=check_writing(ii)\n    if r['return']>0: return r\n    rd=r.get('repo_dict',{})\n\n    rshared=rd.get('shared','')\n    rsync=rd.get('sync','')\n\n    # Prepare path\n    p1=i.get('extra_path','')\n    if p1!='':\n       p2=os.path.normpath(os.path.join(p,p1))\n       if not p2.startswith(p):\n          return {'return':1,'error':'extra path is outside entry'}\n\n       p=p2\n\n    # Create missing dirs\n    if not os.path.isdir(p): os.makedirs(p)\n\n    overwrite=i.get('overwrite','')\n\n    # Copy or record file\n    p3=os.path.normpath(os.path.join(p, fn))\n    if not p3.startswith(p3):\n       return {'return':1,'error':'extra path is outside entry'}\n\n    if p3.startswith(px):\n       return {'return':1, 'error':'path points to the special directory with meta info'}\n\n    if os.path.isfile(p3) and overwrite!='yes':\n       return {'return':1,'error':'file already exists in the entry'}\n\n    if fcb:\n       try:\n         f=open(p3, 'wb')\n         f.write(bin)\n         f.close()\n       except Exception as e:\n          return {'return':1, 'error':'problem writing text file='+p3+' ('+format(e)+')'}\n    else:\n       import shutil\n       shutil.copyfile(fn, p3)\n\n    # Process if archive\n    y=''\n    if i.get('archive','')=='yes':\n       rx=unzip_file({'archive_file':p3,\n                      'path':p,\n                      'overwrite':overwrite,\n                      'delete_after_unzip':'yes'})\n       if rx['return']>0: return rx\n       y='and unziped '\n\n    if rshared!='':\n       ppp=os.getcwd()\n\n       pp=os.path.split(p)\n       pp0=pp[0]\n       pp1=pp[1]\n\n       os.chdir(pp0)\n\n       ss=cfg['repo_types'][rshared]['add'].replace('$#files#$', pp1)\n       rx=os.system(ss)\n\n       os.chdir(ppp)\n\n    if o=='con':\n       out('File was pushed '+y+'successfully!')\n\n    return {'return':0}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unzip_file(i):\n\n    import zipfile\n\n    p=i.get('path','')\n    if p=='':\n       p=os.getcwd()\n\n    p3=i['archive_file']\n\n    overwrite=i.get('overwrite','')\n\n    dau=i.get('delete_after_unzip','')\n\n    s=[]\n\n    f=open(p3,'rb')\n    z=zipfile.ZipFile(f)\n    for d in z.namelist():\n        if not d.startswith('.') and not d.startswith('/') and not d.startswith('\\\\'):\n           pp=os.path.join(p,d)\n           if d.endswith('/'): \n              # create directory \n              if not os.path.exists(pp): os.makedirs(pp)\n           else:\n              ppd=os.path.dirname(pp)\n              if not os.path.exists(ppd): os.makedirs(ppd)\n\n              # extract file\n              if os.path.isfile(pp) and overwrite!='yes':\n                 s.append(d)\n              else:\n                 fo=open(pp, 'wb')\n                 fo.write(z.read(d))\n                 fo.close()\n    f.close()\n\n    if dau=='yes':\n       os.remove(p3)\n\n    return {'return':0, 'skipped':s}", "response": "Unzips a file into a list of files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting all files in a single object", "response": "def list_files(i):\n    \"\"\"\n    Input:  {\n              (repo_uoa)   \n              (module_uoa) \n              (data_uoa)  \n\n              parameters for function 'list_all_files'\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              Output of list all files\n            }\n\n    \"\"\"\n\n    o=i.get('out','')\n\n    ruoa=i.get('repo_uoa','')\n    muoa=i.get('module_uoa','')\n    duoa=i.get('data_uoa','')\n\n    # Get info about entry\n    r=load({'repo_uoa':ruoa, 'module_uoa':muoa, 'data_uoa':duoa})\n    if r['return']>0: return r\n\n    p=r['path']\n\n    # Get files\n    ii={'path':p}\n    if i.get('limit','')!='': ii['limit']=i['limit']\n    if i.get('number','')!='': ii['number']=i['number']\n    if i.get('all','')!='': ii['all']=i['all']\n\n    r=list_all_files(ii)\n    if r['return']>0: return r\n\n    if o=='con':\n       for q in r.get('list',[]):\n           out(q)\n\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef access(i):\n\n    \"\"\"\n    Input:  Can be dictionary or string (string will be converted to dictionary)\n\n            {\n               action\n\n               module_uoa or CID -> converted to cid\n                 or\n               (cid1)            -  if doesn't have = and doesn't start from -- or - or @ -> appended to cids[]\n               (cid2)            -  if doesn't have = and doesn't start from -- or - or @ -> appended to cids[]\n               (cid3)            -  if doesn't have = and doesn't start from -- or - or @ -> appended to cids[]\n                 or\n               (repo_uoa)\n               (module_uoa)\n               (data_uoa)\n\n               (out=type)     Module output\n                              == ''              - none\n                              == 'con'           - console interaction (if from CMD, default)\n                              == 'json'          - return dict as json to console\n                              == 'json_with_sep' - separation line and return dict as json to console\n                              == 'json_file'     - return dict as json to file\n\n               (out_file)     Output file if out=='json_file'\n\n               (con_encoding) - force encoding for IO\n               (ck_profile)   - if 'yes', profile CK\n\n               INPUT TO A GIVEN FUNCTION\n\n\n               NOTE: If INPUT is a string and it will be converted to INPUT dictionary as follows (the same as CK command line):\n\n                     ck key1=value1    -> converted to {key1:value1}\n\n                        -key10         -> converted to {key10:\"yes\"}\n                        -key11=value11 -> converted to {key11:value11}\n\n                        --key12         -> converted to {key12:\"yes\"}\n                        --key13=value13 -> converted to {key13:value13}\n\n\n                     @file_json         -> JSON from this file will be merged with INPUT\n                     @@                 -> CK will ask user ot enter manually JSON from console and merge with INPUT\n                     @@key              -> Enter JSON manually from console and merge with INPUT under this key\n\n                     @@@cmd_json        -> convert string to JSON (special format) and merge with INPUT\n\n                     -- xyz             -> add everything after -- to \"unparsed_cmd\" key in INPUT\n\n                     When string is converted to INPUT dictionary, \"cmd\" variable is set to True\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n\n              OUTPUT FROM A GIVEN FUNCTION\n            }\n\n    \"\"\"\n\n    global con_encoding\n\n#    # Set fresh configuration for each access - very costly\n#    if cfg.get('loading_config','') == '':\n#        cfg['loading_config'] = 'yes'\n#        r=access({'action':'load',\n#                  'repo_uoa':cfg['repo_name_default'],\n#                  'module_uoa':cfg['subdir_kernel'],\n#                  'data_uoa':cfg['subdir_kernel_default']})\n#        if r['return']==0:\n#           cfg.update(r['dict'])\n#\n#        r=access({'action':'load',\n#                  'repo_uoa':cfg['repo_name_local'],\n#                  'module_uoa':cfg['subdir_kernel'],\n#                  'data_uoa':cfg['subdir_kernel_default']})\n#        if r['return']==0:\n#           cfg.update(r['dict'])\n#        cfg['loading_config'] = ''\n\n    rr={'return':0}\n    ii={}\n    cmd=False\n    o=''\n\n    ### If input is string, split into list and process in the next condition\n    if type(i)==str:\n       cmd=True\n       x=i.split(' ')\n       i=x \n\n    ### If input is a list\n    if type(i)==list:\n       if len(i)==1 and i[0].strip()=='test_install':\n          return rr # installation test\n\n       cmd=True\n       rr=convert_ck_list_to_dict(i)\n       if rr['return']==0:\n          i=rr.get('ck_dict',{})\n\n          if i.get('out','')=='': i['out']='con' # Default output is console \n                                                 # if called from CMD or with string\n\n    o=''\n    if rr['return']==0:\n       # Check output mode\n       o=i.get('out','')\n\n       ### If profile\n       cp=i.get('ck_profile','')\n       if cp=='yes':\n          import time\n          start_time = time.time()\n\n       ### Process request ######################################\n\n       if i.get('con_encoding','')!='': con_encoding=i['con_encoding']\n\n       ### Process action ###################################\n       rr=init({})\n       if rr['return']==0:\n          # Run module with a given action\n          rr=perform_action(i)\n          if rr.get('out','')!='': o=rr['out']\n\n       if cp=='yes':\n          elapsed_time=time.time()-start_time\n          rr['ck_profile_time']=elapsed_time\n          if o=='con':\n             out('CK profile time: '+str(elapsed_time)+' sec.')\n\n    # Finalize call (check output) ####################################\n    if o=='json' or o=='json_with_sep':\n       if o=='json_with_sep': out(cfg['json_sep'])\n\n       rr1=dumps_json({'dict':rr})\n       if rr1['return']==0:\n          s=rr1['string']\n          out(s)\n\n    elif o=='json_file':\n       fn=i.get('out_file','')\n       if fn=='':\n          rr['return']=1\n          rr['error']='out==json_file but out_file is not defined in kernel access function'\n       else:\n          rr1=save_json_to_file({'json_file':fn, 'dict':rr})\n          if rr1['return']>0:\n             rr['return']=1\n             rr['error']=rr1['error']\n\n    # If error and CMD, output error to console\n    if cmd:\n       if rr['return']>0:\n          x=''\n          if type(i)==dict:\n             x=i.get('module_uoa','')\n             if x!='':\n                x='['+x+'] '\n          out(cfg['error']+x+rr['error']+'!')\n\n    return rr", "response": "Return a list of all possible values for a given key in a given module."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add(i):\n\n    # Check if global writing is allowed\n    r=ck.check_writing({'module_uoa':work['self_module_uoa']})\n    if r['return']>0: return r\n\n    o=i.get('out','')\n\n    # Find path to module 'module' to get dummies\n    r=ck.access({'action':'load',\n                 'module_uoa':work['self_module_uoa'],\n                 'data_uoa':work['self_module_uoa'],\n                 'common_func':'yes'})\n    if r['return']>0: return r\n    p=r['path']\n\n    pm=os.path.join(p,cfg['dummy_module'])\n    pma=os.path.join(p,cfg['dummy_module_action'])\n\n    # Load module dummy\n    r=ck.load_text_file({'text_file':pm})\n    if r['return']>0: return r\n    spm=r['string']\n\n    # Load module action dummy\n    r=ck.load_text_file({'text_file':pma})\n    if r['return']>0: return r\n    spma=r['string']\n\n    # Prepare meta description\n    desc=i.get('desc','')\n    license=i.get('license','')\n    copyright=i.get('copyright','')\n    developer=i.get('developer','')\n    developer_email=i.get('developer_email','')\n    developer_webpage=i.get('developer_webpage','')\n    actions=i.get('actions',{})\n\n    func=i.get('func','')\n    if func!='':\n       actions[func]={}\n\n    quiet=i.get('quiet','')\n\n    # If console mode, ask some questions\n    if quiet!='yes' and o=='con':\n       if desc=='':\n          ck.out('')\n          r=ck.inp({'text':'Add brief module description: '})\n          desc=r['string']\n\n       if license=='' and ck.cfg.get('default_license','')!='':\n          ck.out('')\n          r=ck.inp({'text':'Add brief module license (or Enter to use \"'+ck.cfg['default_license']+'\"): '})\n          license=r['string']\n          if license=='': license=ck.cfg['default_license']\n\n       if copyright=='' and ck.cfg.get('default_copyright','')!='':\n          ck.out('')\n          r=ck.inp({'text':'Add brief module copyright (or Enter to use \"'+ck.cfg['default_copyright']+'\"): '})\n          copyright=r['string']\n          if copyright=='': copyright=ck.cfg['default_copyright']\n\n       if developer=='' and ck.cfg.get('default_developer','')!='':\n          ck.out('')\n          r=ck.inp({'text':'Add module\\'s developer (or Enter to use \"'+ck.cfg['default_developer']+'\"): '})\n          developer=r['string']\n          if developer=='': developer=ck.cfg['default_developer']\n\n       if developer_email=='' and ck.cfg.get('default_developer_email','')!='':\n          ck.out('')\n          r=ck.inp({'text':'Add module\\'s developer email (or Enter to use \"'+ck.cfg['default_developer_email']+'\"): '})\n          developer_email=r['string']\n          if developer_email=='': developer_email=ck.cfg['default_developer_email']\n\n       if developer_webpage==''  and ck.cfg.get('default_developer_webpage','')!='':\n          ck.out('')\n          r=ck.inp({'text':'Add module\\'s developer webpage (or Enter to use \"'+ck.cfg['default_developer_webpage']+'\"): '})\n          developer_webpage=r['string']\n          if developer_webpage=='': developer_webpage=ck.cfg['default_developer_webpage']\n\n       if len(actions)==0:\n          act='*'\n          while act!='':\n             ck.out('')\n\n             r=ck.inp({'text':'Add action function (or Enter to stop): '})\n             act=r['string']\n             if act!='': \n                actions[act]={}\n\n                r1=ck.inp({'text':'Support web (y/N): '})\n                x=r1['string'].lower()\n                if x=='yes' or x=='y': \n                   fweb='yes'\n                   actions[act]['for_web']=fweb\n\n                r1=ck.inp({'text':'Add action description: '})\n                adesc=r1['string']\n                if adesc!='': \n                   actions[act]['desc']=adesc\n\n    ck.out('')\n\n    # Prepare meta description\n    dd={}\n    if desc!='': \n       dd['desc']=desc\n    spm=spm.replace('$#desc#$', desc)\n\n    if license!='': \n       dd['license']=license\n    spm=spm.replace('$#license#$', license)\n\n    if copyright!='': \n       dd['copyright']=copyright\n    spm=spm.replace('$#copyright#$', copyright)\n\n    dev=''\n    if developer!='': \n       dev=developer\n       dd['developer']=developer\n\n    if developer_email!='': \n       if dev!='': dev+=', '\n       dev+=developer_email\n       dd['developer_email']=developer_email\n\n    if developer_webpage!='': \n       if dev!='': dev+=', '\n       dev+=developer_webpage\n       dd['developer_webpage']=developer_webpage\n\n    spm=spm.replace('$#developer#$', dev)\n\n    dd['actions']=actions\n\n    # Substitute actions\n    for act in actions:\n        adesc=actions[act].get('desc','TBD: action description')\n        spm+='\\n'+spma.replace('$#action#$', act).replace('$#desc#$',adesc)\n\n    dx=i.get('dict',{})\n\n    r=ck.merge_dicts({'dict1':dx, 'dict2':dd})\n    if r['return']>0: return r\n\n    # Add entry (it will ask further questions about alias and user-friendly name)\n    i['common_func']='yes'\n    i['dict']=dx\n    i['sort_keys']='yes'\n    r=ck.access(i)\n    if r['return']>0: return r\n\n    # Add module code\n    p=r['path']\n    pf=os.path.join(p, ck.cfg['module_full_code_name'])\n   \n    if o=='con':\n       ck.out('')\n       ck.out('Creating module code '+pf+' ...')\n\n    # Write module code\n    rx=ck.save_text_file({'text_file':pf, 'string':spm})\n    if rx['return']>0: return rx\n\n    return r", "response": "Add a new entry to the entry in the order they appear."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef show(i):\n\n    o=i.get('out','')\n\n    of=i.get('out_file','')\n    if of!='':\n       xof=os.path.splitext(of)\n\n    html=False\n    if o=='html' or i.get('web','')=='yes':\n       html=True\n\n    h=''\n\n    unique_repo=False\n    if i.get('repo_uoa','')!='': unique_repo=True\n\n    import copy\n    ii=copy.deepcopy(i)\n\n    ii['out']=''\n    ii['action']='list'\n    ii['add_meta']='yes'\n\n    rx=ck.access(ii)\n    if rx['return']>0: return rx\n\n    ll=sorted(rx['lst'], key=lambda k: k['data_uoa'])\n\n    if html:\n       h+='You can install and reuse CK modules as follows:\\n'\n       h+='<pre>\\n'\n       h+=' ck pull repo:{Repo UOA - see below}\\n'\n       h+=' ck help {module UOA - see below}\\n'\n       h+='</pre>\\n'\n\n       h+='You can check a JSON API of a given action of a given module as follows:\\n'\n       h+='<pre>\\n'\n       h+=' ck {module action - see below} {module UOA} --help\\n'\n       h+='</pre>\\n'\n\n       h+='You can add your own dummy CK module as follows:\\n'\n       h+='<pre>\\n'\n       h+=' ck add module:{my module alias}\\n'\n       h+='</pre>\\n'\n\n       h+='You can add a new action to the CK module as follows:\\n'\n       h+='<pre>\\n'\n       h+=' ck add_action module:{my module alias}\\n'\n       h+='</pre>\\n'\n\n       h+='See <a href=\"https://github.com/ctuning/ck/wiki\">CK documentation</a>\\n'\n       h+=' and the latest <a href=\"http://cKnowledge.org/rpi-crowd-tuning\">CK paper</a> for further details.\\n'\n\n       h+='<p>\\n'\n       h+='<table cellpadding=\"4\" border=\"1\" style=\"border-collapse: collapse; border: 1px solid black;\">\\n'\n\n       h+=' <tr>\\n'\n       h+='  <td nowrap><b>#</b></td>\\n'\n       h+='  <td nowrap><b>Module&nbsp;UOA with JSON API<br>(Python module/wrapper/plugin)</b></td>\\n'\n       h+='  <td nowrap><b>Repo UOA</b></td>\\n'\n       h+='  <td><b>Description and actions</b></td>\\n'\n       h+=' </tr>\\n'\n\n    repo_url={}\n    repo_private={}\n\n    private=''\n    num=0\n    for l in ll:\n        ln=l['data_uoa']\n        lr=l['repo_uoa']\n\n        lr_uid=l['repo_uid']\n        url=''\n        if lr=='default':\n           url='https://github.com/ctuning/ck/tree/master/ck/repo'\n        elif lr_uid in repo_url:\n           url=repo_url[lr_uid]\n        else:\n           rx=ck.load_repo_info_from_cache({'repo_uoa':lr_uid})\n           if rx['return']>0: return rx\n           url=rx.get('dict',{}).get('url','')\n           repo_private[lr_uid]=rx.get('dict',{}).get('private','')\n           repo_url[lr_uid]=url\n\n        private=repo_private.get(lr_uid,'')\n\n#        if lr not in cfg.get('skip_repos',[]) and private!='yes' and url!='':\n        if lr not in cfg.get('skip_repos',[]) and private!='yes' and url!='':\n           num+=1\n\n           lm=l['meta']\n           ld=lm.get('desc','')\n\n           actions=lm.get('actions',{})\n\n           if lr=='default':\n              to_get=''\n           elif url.find('github.com/ctuning/')>0:\n              to_get='ck pull repo:'+lr\n           else:\n              to_get='ck pull repo --url='+url\n\n           ###############################################################\n           if html:\n              h+=' <tr>\\n'\n\n              x1=''\n              x2=''\n              z1=''\n              z11=''\n              if url!='':\n                 x1='<a href=\"'+url+'\">'\n                 x2='</a>'\n\n                 url2=url\n\n                 if url2.endswith('.git'):\n                    url2=url2[:-4]\n\n                 if '/tree/master/' not in url2:\n                    url2+='/tree/master/module/'\n                 else:\n                    url2+='/module/'\n\n                 z1='<a href=\"'+url2+ln+'/module.py\">'\n                 z11='<a href=\"'+url2+ln+'/.cm/meta.json\">'\n\n              h+='  <td nowrap valign=\"top\"><a name=\"'+ln+'\">'+str(num)+'</b></td>\\n'\n\n              h+='  <td nowrap valign=\"top\">'+z1+ln+x2+'</b> <i>('+z11+'CK meta'+x2+')</i></td>\\n'\n\n              h+='  <td nowrap valign=\"top\"><b>'+x1+lr+x2+'</b></td>\\n'\n\n              h+='  <td valign=\"top\">'+ld+'\\n'\n\n              if len(actions)>0:\n                 h+='<ul>\\n'\n                 for q in sorted(actions):\n                     qq=actions[q]\n                     qd=qq.get('desc','')\n                     h+='<li>\"ck <i>'+q+'</i> '+ln+'\"'\n                     if qd!='':\n                        h+=' - '+qd\n                 h+='</ul>\\n'\n\n              h+='</td>\\n'\n\n              h+=' </tr>\\n'\n\n           ###############################################################\n           elif o=='mediawiki':\n              x=lr\n              if url!='':\n                 x='['+url+' '+lr+']'\n              ck.out('')\n              ck.out('=== '+ln+' ('+lr+') ===')\n              ck.out('')\n              ck.out('Desc: '+ld)\n              ck.out('<br>CK Repo URL: '+x)\n              if to_get!='':\n                 ck.out('<br>How to get: <i>'+to_get+'</i>')\n              ck.out('')\n              if len(actions)>0:\n\n                 ck.out('Actions (functions):')\n                 ck.out('')\n\n                 for q in sorted(actions):\n                     qq=actions[q]\n                     qd=qq.get('desc','')\n                     ck.out('* \\'\\''+q+'\\'\\' - '+qd)\n\n           ###############################################################\n           elif o=='con' or o=='txt':\n              if unique_repo:\n                 ck.out('')\n                 s=ln+' - '+ld\n\n              else:\n                 ss=''\n                 if len(ln)<35: ss=' '*(35-len(ln))\n\n                 ss1=''\n                 if len(lr)<30: ss1=' '*(30-len(lr))\n\n                 s=ln+ss+'  ('+lr+')'\n                 if ld!='': s+=ss1+'  '+ld\n\n              ck.out(s)\n\n              if len(actions)>0:\n                 ck.out('')\n                 for q in sorted(actions):\n                     qq=actions[q]\n                     qd=qq.get('desc','')\n                     ck.out('  * '+q+' - '+qd)\n\n\n    if html:\n       h+='</table>\\n'\n\n       if of!='':\n          r=ck.save_text_file({'text_file':of, 'string':h})\n          if r['return']>0: return r\n\n    return {'return':0, 'html':h}", "response": "Show the list of available CK modules."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the maximum path similarity between two senses.", "response": "def similarity_by_path(sense1: \"wn.Synset\", sense2: \"wn.Synset\", option: str = \"path\") -> float:\n    \"\"\"\n    Returns maximum path similarity between two senses.\n\n    :param sense1: A synset.\n    :param sense2: A synset.\n    :param option: String, one of ('path', 'wup', 'lch').\n    :return: A float, similarity measurement.\n    \"\"\"\n    if option.lower() in [\"path\", \"path_similarity\"]: # Path similarities.\n        return max(wn.path_similarity(sense1, sense2, if_none_return=0),\n                   wn.path_similarity(sense2, sense1, if_none_return=0))\n    elif option.lower() in [\"wup\", \"wupa\", \"wu-palmer\", \"wu-palmer\"]: # Wu-Palmer\n        return max(wn.wup_similarity(sense1, sense2, if_none_return=0),\n                   wn.wup_similarity(sense2, sense1, if_none_return=0))\n    elif option.lower() in ['lch', \"leacock-chordorow\"]: # Leacock-Chodorow\n        if sense1.pos != sense2.pos: # lch can't do diff POS\n            return 0\n        return wn.lch_similarity(sense1, sense2, if_none_return=0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn similarity scores by information content.", "response": "def similarity_by_infocontent(sense1: \"wn.Synset\", sense2: \"wn.Synset\", option: str) -> float:\n    \"\"\"\n    Returns similarity scores by information content.\n\n    :param sense1: A synset.\n    :param sense2: A synset.\n    :param option: String, one of ('res', 'jcn', 'lin').\n    :return: A float, similarity measurement.\n    \"\"\"\n\n    if sense1.pos != sense2.pos: # infocontent sim can't do diff POS.\n        return 0\n\n    if option in ['res', 'resnik']:\n        if sense1.pos not in wnic_bnc_resnik_add1.ic:\n            return 0\n        return wn.res_similarity(sense1, sense2, wnic_bnc_resnik_add1)\n    #return min(wn.res_similarity(sense1, sense2, wnic.ic(ic)) \\\n    #             for ic in info_contents)\n\n    elif option in ['jcn', \"jiang-conrath\"]:\n        if sense1.pos not in wnic_bnc_add1.ic:\n            return 0\n        return wn.jcn_similarity(sense1, sense2, wnic_bnc_add1)\n\n    elif option in ['lin']:\n        if sense1.pos not in wnic_bnc_add1.ic:\n            return 0\n        return wn.lin_similarity(sense1, sense2, wnic_bnc_add1)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sim(sense1: \"wn.Synset\", sense2: \"wn.Synset\", option: str = \"path\") -> float:\n    option = option.lower()\n    if option.lower() in [\"path\", \"path_similarity\",\n                        \"wup\", \"wupa\", \"wu-palmer\", \"wu-palmer\",\n                        'lch', \"leacock-chordorow\"]:\n        return similarity_by_path(sense1, sense2, option)\n    elif option.lower() in [\"res\", \"resnik\",\n                          \"jcn\",\"jiang-conrath\",\n                          \"lin\"]:\n        return similarity_by_infocontent(sense1, sense2, option)", "response": "Calculates similarity between two synset objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming WSD by maximizing the maximum similarity between all words in a context sentence and a single word.", "response": "def max_similarity(context_sentence: str, ambiguous_word: str, option=\"path\",\n                   lemma=True, context_is_lemmatized=False, pos=None, best=True) -> \"wn.Synset\":\n    \"\"\"\n    Perform WSD by maximizing the sum of maximum similarity between possible\n    synsets of all words in the context sentence and the possible synsets of the\n    ambiguous words (see http://goo.gl/XMq2BI):\n    {argmax}_{synset(a)}(\\sum_{i}^{n}{{max}_{synset(i)}(sim(i,a))}\n\n    :param context_sentence: String, a sentence.\n    :param ambiguous_word: String, a single word.\n    :return: If best, returns only the best Synset, else returns a dict.\n    \"\"\"\n    ambiguous_word = lemmatize(ambiguous_word)\n    # If ambiguous word not in WordNet return None\n    if not wn.synsets(ambiguous_word):\n        return None\n    if context_is_lemmatized:\n        context_sentence = word_tokenize(context_sentence)\n    else:\n        context_sentence = [lemmatize(w) for w in word_tokenize(context_sentence)]\n    result = {}\n    for i in wn.synsets(ambiguous_word, pos=pos):\n        result[i] = 0\n        for j in context_sentence:\n            _result = [0]\n            for k in wn.synsets(j):\n                _result.append(sim(i,k,option))\n            result[i] += max(_result)\n\n    if option in [\"res\",\"resnik\"]: # lower score = more similar\n        result = sorted([(v,k) for k,v in result.items()])\n    else: # higher score = more similar\n        result = sorted([(v,k) for k,v in result.items()],reverse=True)\n\n    return result[0][1] if best else result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_tags(text: str) -> str:\n\n    tags = {i:\" \" for i in re.findall(\"(<[^>\\n]*>)\",text.strip())}\n    no_tag_text = reduce(lambda x, kv:x.replace(*kv), tags.iteritems(), text)\n    return \" \".join(no_tag_text.split())", "response": "Removes tags in angled brackets from text."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a semcor sensekey ID to synset offset.", "response": "def semcor_to_offset(sensekey):\n    \"\"\"\n    Converts SemCor sensekey IDs to synset offset.\n    >>> print semcor_to_offset('live%2:42:06::')\n    02614387-v\n    \"\"\"\n\n    synset = wn.lemma_from_key(sensekey).synset\n    offset = '%08d-%s' % (synset.offset, synset.pos)\n    return offset"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lemmatize(ambiguous_word: str, pos: str = None, neverstem=False,\n              lemmatizer=wnl, stemmer=porter) -> str:\n    \"\"\"\n    Tries to convert a surface word into lemma, and if lemmatize word is not in\n    wordnet then try and convert surface word into its stem.\n\n    This is to handle the case where users input a surface word as an ambiguous\n    word and the surface word is a not a lemma.\n    \"\"\"\n\n    # Try to be a little smarter and use most frequent POS.\n    pos = pos if pos else penn2morphy(pos_tag([ambiguous_word])[0][1],\n                                     default_to_noun=True)\n    lemma = lemmatizer.lemmatize(ambiguous_word, pos=pos)\n    stem = stemmer.stem(ambiguous_word)\n    # Ensure that ambiguous word is a lemma.\n    if not wn.synsets(lemma):\n        if neverstem:\n            return ambiguous_word\n        if not wn.synsets(stem):\n            return ambiguous_word\n        else:\n            return stem\n    else:\n        return lemma", "response": "Tries to convert a surface word into lemma and then try to convert a stem into a lemmatized word."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef penn2morphy(penntag, returnNone=False, default_to_noun=False) -> str:\n    morphy_tag = {'NN':'n', 'JJ':'a', 'VB':'v', 'RB':'r'}\n    try:\n        return morphy_tag[penntag[:2]]\n    except:\n        if returnNone:\n            return None\n        elif default_to_noun:\n            return 'n'\n        else:\n            return ''", "response": "Converts a Penn tag to Morphy tag."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of properties of a WordNet Synset.", "response": "def synset_properties(synset: \"wn.Synset\", parameter: str):\n    \"\"\"\n    Making from NLTK's WordNet Synset's properties to function.\n    Note: This is for compatibility with NLTK 2.x\n    \"\"\"\n\n    return_type = SS_PARAMETERS_TYPE_MAP[parameter]\n    func = 'synset.' + parameter\n\n    return eval(func) if isinstance(eval(func), return_type) else eval(func)()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of synsets of a word after lemmatization.", "response": "def has_synset(word: str) -> list:\n    \"\"\"\" Returns a list of synsets of a word after lemmatization. \"\"\"\n\n    return wn.synsets(lemmatize(word, neverstem=True))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the label for each data point in the cluster.", "response": "def get_label(self, x, w):\n        \"\"\"\n        Computes the label for each data point\n        \"\"\"\n        scores = np.dot(x,w)\n        return np.argmax(scores,axis=1).transpose()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a column of ones to estimate the intercept term for separation boundary", "response": "def add_intercept_term(self, x):\n        \"\"\" \n        Adds a column of ones to estimate the intercept term for \n        separation boundary \n        \"\"\"\n        nr_x,nr_f = x.shape\n        intercept = np.ones([nr_x,1])\n        x = np.hstack((intercept,x))\n        return x"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef evaluate(self, truth, predicted):\n        correct = 0.0\n        total = 0.0\n        for i in range(len(truth)):\n            if(truth[i] == predicted[i]):\n                correct += 1\n            total += 1\n        return 1.0*correct/total", "response": "Evaluates the predicted outputs against the gold data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cosine_similarity(sent1: str, sent2: str) -> float:\n\n    WORD = re.compile(r'\\w+')\n    def get_cosine(vec1, vec2):\n        intersection = set(vec1.keys()) & set(vec2.keys())\n        numerator = sum([vec1[x] * vec2[x] for x in intersection])\n\n        sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n        sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n        denominator = math.sqrt(sum1) * math.sqrt(sum2)\n\n        if not denominator:\n            return 0.0\n        else:\n            return float(numerator) / denominator\n\n    def text_to_vector(text):\n        words = WORD.findall(text)\n        return Counter(words)\n\n    vector1 = text_to_vector(sent1)\n    vector2 = text_to_vector(sent2)\n    cosine = get_cosine(vector1, vector2)\n\n    return cosine", "response": "Calculates cosine similarity between two sentences and documents."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef synset_signatures(ss: \"wn.Synset\", hyperhypo=True, adapted=False,\n                      remove_stopwords=True, to_lemmatize=True, remove_numbers=True,\n                      lowercase=True, original_lesk=False, from_cache=True) -> set:\n    \"\"\"\n    Takes a Synset and returns its signature words.\n\n    :param ss: An instance of wn.Synset.\n    :return: A set of signature strings\n    \"\"\"\n    if from_cache:\n        return synset_signatures_from_cache(ss, hyperhypo, adapted, original_lesk)\n\n    # Collects the signatures from WordNet.\n    signature = []\n\n    # Adds the definition, example sentences and lemma_names.\n    signature += word_tokenize(ss.definition())\n\n    # If the original lesk signature is requested, skip the other signatures.\n    if original_lesk:\n        return set(signature)\n\n    # Adds the examples and lemma names.\n    signature += chain(*[word_tokenize(eg) for eg in ss.examples()])\n    signature += ss.lemma_names()\n\n    # Includes lemma_names of hyper-/hyponyms.\n    if hyperhypo:\n        hyperhyponyms = set(ss.hyponyms() + ss.hypernyms() + ss.instance_hyponyms() + ss.instance_hypernyms())\n        signature += set(chain(*[i.lemma_names() for i in hyperhyponyms]))\n\n    # Includes signatures from related senses as in Adapted Lesk.\n    if adapted:\n        # Includes lemma_names from holonyms, meronyms and similar_tos\n        related_senses = set(ss.member_holonyms() + ss.part_holonyms() + ss.substance_holonyms() + \\\n                             ss.member_meronyms() + ss.part_meronyms() + ss.substance_meronyms() + \\\n                             ss.similar_tos())\n        signature += set(chain(*[i.lemma_names() for i in related_senses]))\n\n    # Lowercase.\n    signature = set(s.lower() for s in signature) if lowercase else signature\n\n    # Removes stopwords.\n    signature = set(signature).difference(EN_STOPWORDS) if remove_stopwords else signature\n\n    # Lemmatized context is preferred over stemmed context.\n    if to_lemmatize:\n        signature = [lemmatize(s) if lowercase else lemmatize(s) # Lowercasing checks here.\n                     for s in signature\n                     # We only throw away if both remove_numbers and s is a digit are true.\n                     if not (remove_numbers and s.isdigit())]\n\n    # Keep only the unique bag-of-words\n    return set(signature)", "response": "Takes a Synset and returns its signature words."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef signatures(ambiguous_word: str, pos: str = None, hyperhypo=True, adapted=False,\n               remove_stopwords=True, to_lemmatize=True, remove_numbers=True,\n               lowercase=True, to_stem=False, original_lesk=False, from_cache=True) -> dict:\n    \"\"\"\n    Takes an ambiguous word and optionally its Part-Of-Speech and returns\n    a dictionary where keys are the synsets and values are sets of signatures.\n\n    :param ambiguous_word: String, a single word.\n    :param pos: String, one of 'a', 'r', 's', 'n', 'v', or None.\n    :return: dict(synset:{signatures}).\n    \"\"\"\n\n    # Ensure that the POS is supported.\n    pos = pos if pos in ['a', 'r', 's', 'n', 'v', None] else None\n\n    # If the POS specified isn't found but other POS is in wordnet.\n    if not wn.synsets(ambiguous_word, pos) and wn.synsets(ambiguous_word):\n        pos = None\n\n    # Holds the synset->signature dictionary.\n    ss_sign = {}\n    for ss in wn.synsets(ambiguous_word, pos):\n        ss_sign[ss] = synset_signatures(ss, hyperhypo=hyperhypo,\n                                        adapted=adapted,\n                                        remove_stopwords=remove_stopwords,\n                                        to_lemmatize=to_lemmatize,\n                                        remove_numbers=remove_numbers,\n                                        lowercase=lowercase,\n                                        original_lesk=original_lesk,\n                                        from_cache=from_cache)\n\n    # Matching exact words may cause sparsity, so optional matching for stems.\n    # Not advisible to use thus left out of the synsets_signatures()\n    if to_stem == True:\n        ss_sign = {ss:[porter.stem(s) for s in signature]\n                   for ss, signature in ss_sign.items()}\n\n    return ss_sign", "response": "Returns a dictionary of synset - > signature pairs for the given ambiguous word."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compare_overlaps_greedy(context: list, synsets_signatures: dict) -> \"wn.Synset\":\n\n    max_overlaps = 0; lesk_sense = None\n    for ss in synsets_signatures:\n        overlaps = set(synsets_signatures[ss]).intersection(context)\n        if len(overlaps) > max_overlaps:\n            lesk_sense = ss\n            max_overlaps = len(overlaps)\n\n    return lesk_sense", "response": "Calculates overlaps between the context sentence and the synset_signatures\n    and returns the synset with the highest number of overlaps."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncompares the context sentence and the synset_signture_ and returns a ranked list of synsets with the highest number of overlaps with its signatures.", "response": "def compare_overlaps(context: list, synsets_signatures: dict,\n                     nbest=False, keepscore=False, normalizescore=False) -> \"wn.Synset\":\n    \"\"\"\n    Calculates overlaps between the context sentence and the synset_signture\n    and returns a ranked list of synsets from highest overlap to lowest.\n\n    :param context: List of strings, tokenized sentence or document.\n    :param synsets_signatures: dict of Synsets and the set of their corresponding signatures.\n    :return: The Synset with the highest number of overlaps with its signatures.\n    \"\"\"\n\n    overlaplen_synsets = [] # a tuple of (len(overlap), synset).\n    for ss in synsets_signatures:\n        overlaps = set(synsets_signatures[ss]).intersection(context)\n        overlaplen_synsets.append((len(overlaps), ss))\n\n    # Rank synsets from highest to lowest overlap.\n    ranked_synsets = sorted(overlaplen_synsets, reverse=True)\n\n    # Normalize scores such that it's between 0 to 1.\n    if normalizescore:\n        total = float(sum(i[0] for i in ranked_synsets))\n        ranked_synsets = [(i/total,j) for i,j in ranked_synsets]\n\n    if not keepscore: # Returns a list of ranked synsets without scores\n        ranked_synsets = [i[1] for i in sorted(overlaplen_synsets, reverse=True)]\n\n    # Returns a ranked list of synsets otherwise only the best sense.\n    return ranked_synsets if nbest else ranked_synsets[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dictionary that includes all signatures of a single word.", "response": "def simple_signatures(ambiguous_word: str, pos: str = None, lemma=True, stem=False,\n                     hyperhypo=True, stop=True, from_cache=True) -> dict:\n    \"\"\"\n    Returns a synsets_signatures dictionary that includes signature words of a\n    sense from its:\n    (i)   definition\n    (ii)  example sentences\n    (iii) hypernyms and hyponyms\n\n    :param ambiguous_word: String, a single word.\n    :param pos: String, one of 'a', 'r', 's', 'n', 'v', or None.\n    :return: dict(synset:{signatures})\n    \"\"\"\n    synsets_signatures = signatures(ambiguous_word, pos=pos, hyperhypo=hyperhypo,\n                            remove_stopwords=stop, to_lemmatize=lemma,\n                            remove_numbers=True, lowercase=True, to_stem=stem,\n                            from_cache=from_cache)\n    return synsets_signatures"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef simple_lesk(context_sentence: str, ambiguous_word: str,\n                pos: str = None, lemma=True, stem=False, hyperhypo=True,\n                stop=True, context_is_lemmatized=False,\n                nbest=False, keepscore=False, normalizescore=False,\n                from_cache=True) -> \"wn.Synset\":\n    \"\"\"\n    Simple Lesk is somewhere in between using more than the\n    original Lesk algorithm (1986) and using less signature\n    words than adapted Lesk (Banerjee and Pederson, 2002)\n\n    :param context_sentence: String, sentence or document.\n    :param ambiguous_word: String, a single word.\n    :param pos: String, one of 'a', 'r', 's', 'n', 'v', or None.\n    :return: A Synset for the estimated best sense.\n    \"\"\"\n    # Ensure that ambiguous word is a lemma.\n    ambiguous_word = lemmatize(ambiguous_word, pos=pos)\n    # If ambiguous word not in WordNet return None\n    if not wn.synsets(ambiguous_word):\n        return None\n    # Get the signatures for each synset.\n    ss_sign = simple_signatures(ambiguous_word, pos, lemma, stem, hyperhypo, stop,\n                                from_cache=from_cache)\n    # Disambiguate the sense in context.\n    context_sentence = context_sentence.split() if context_is_lemmatized else lemmatize_sentence(context_sentence)\n    return compare_overlaps(context_sentence, ss_sign, nbest=nbest,\n                            keepscore=keepscore, normalizescore=normalizescore)", "response": "Simple Lesk algorithm for calculating the best sense for a given word."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating cosine of a single word in a WordNet.", "response": "def cosine_lesk(context_sentence: str, ambiguous_word: str,\n                pos: str = None, lemma=True, stem=True, hyperhypo=True,\n                stop=True, context_is_lemmatized=False,\n                nbest=False, from_cache=True) -> \"wn.Synset\":\n    \"\"\"\n    In line with vector space models, we can use cosine to calculate overlaps\n    instead of using raw overlap counts. Essentially, the idea of using\n    signatures (aka 'sense paraphrases') is lesk-like.\n\n    :param context_sentence: String, sentence or document.\n    :param ambiguous_word: String, a single word.\n    :param pos: String, one of 'a', 'r', 's', 'n', 'v', or None.\n    :return: A Synset for the estimated best sense.\n    \"\"\"\n\n    # Ensure that ambiguous word is a lemma.\n    ambiguous_word = lemmatize(ambiguous_word)\n\n    # If ambiguous word not in WordNet return None\n    if not wn.synsets(ambiguous_word):\n        return None\n    ss_sign = simple_signatures(ambiguous_word, pos, lemma, stem, hyperhypo, stop,\n                                from_cache=from_cache)\n    if context_is_lemmatized:\n        context_sentence = \" \".join(context_sentence.split())\n    else:\n        context_sentence = \" \".join(lemmatize_sentence(context_sentence))\n\n    scores = []\n    for ss, signature in ss_sign.items():\n        # Lowercase and replace \"_\" with spaces.\n        signature = \" \".join(map(str, signature)).lower().replace(\"_\", \" \")\n        scores.append((cos_sim(context_sentence, signature), ss))\n\n    scores = sorted(scores, reverse=True)\n    return scores if nbest else scores[0][1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fileids(self):\n        return [os.path.join(self.path,i) for i in os.listdir(self.path)]", "response": "Returns a list of file ids from SemEval2007 Coarse - grain All - words WSD task."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the file line by line. Use test_file if no filename specified.", "response": "def sents(self, filename=None):\n        \"\"\"\n        Returns the file, line by line. Use test_file if no filename specified.\n        \"\"\"\n        filename = filename if filename else self.test_file\n        with io.open(filename, 'r') as fin:\n            for line in fin:\n                yield line.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_answers(self):\n        inst2ans = {}\n\n        with io.open(self.test_ans, 'r') as fin:\n            for line in fin:\n                line, _, lemma = line.strip().rpartition(' !! ')\n                lemma, pos = lemma[6:].split('#')\n                textid, _, line = line.partition(' ')\n                instid, _, line = line.partition(' ')\n                sensekey = line.split()\n                # What to do if there is no synset to convert to...\n                # synsetkey = [semcor_to_synset(i) for i in sensekey]\n                inst2ans[instid] = Answer(sensekey, lemma, pos)\n        return inst2ans", "response": "Returns a dictionary of Answer objects for all the test analyses."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sentences(self):\n        for sentid, ys in enumerate(self.yield_sentences()):\n            sent, context_sent, context_doc, inst2ans, textid = ys\n            instances = {}\n            for instance in sent.findAll('instance'):\n                instid = instance['id']\n                lemma = instance['lemma']\n                word = instance.text\n                instances[instid] = Instance(instid, lemma, word)\n\n            tokens = []\n            for i in sent: # Iterates through BeautifulSoup object.\n                if str(i).startswith('<instance'): # BeautifulSoup.Tag\n                    instid = sent.find('instance')['id']\n                    inst = instances[instid]\n                    answer = inst2ans[instid]\n                    term = Term(instid, answer.pos, inst.lemma, answer.sensekey,\n                                type='open')\n                    tokens.append(Word(instid, inst.word,\n                                       sentid, textid, term))\n                else: # if BeautifulSoup.NavigableString\n                    tokens+=[Word(None, w, sentid, textid, None)\n                             for w in i.split()]\n            yield tokens", "response": "Yields a list of tokens from the sentences in the document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a random sense.", "response": "def random_sense(ambiguous_word: str, pos=None) -> \"wn.Synset\":\n    \"\"\"\n    Returns a random sense.\n\n    :param ambiguous_word: String, a single word.\n    :param pos: String, one of 'a', 'r', 's', 'n', 'v', or None.\n    :return: A random Synset.\n    \"\"\"\n\n    if pos is None:\n        return custom_random.choice(wn.synsets(ambiguous_word))\n    else:\n        return custom_random.choice(wn.synsets(ambiguous_word, pos))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef first_sense(ambiguous_word: str, pos: str = None) -> \"wn.Synset\":\n\n    if pos is None:\n        return wn.synsets(ambiguous_word)[0]\n    else:\n        return wn.synsets(ambiguous_word, pos)[0]", "response": "Returns the first sense in the list of synsets."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the most common lemma count for a given word.", "response": "def max_lemma_count(ambiguous_word: str) -> \"wn.Synset\":\n    \"\"\"\n    Returns the sense with the highest lemma_name count.\n    The max_lemma_count() can be treated as a rough gauge for the\n    Most Frequent Sense (MFS), if no other sense annotated corpus is available.\n    NOTE: The lemma counts are from the Brown Corpus\n\n    :param ambiguous_word: String, a single word.\n    :return: The estimated most common Synset.\n    \"\"\"\n    sense2lemmacounts = {}\n    for i in wn.synsets(ambiguous_word, pos=None):\n        sense2lemmacounts[i] = sum(j.count() for j in i.lemmas())\n    return max(sense2lemmacounts, key=sense2lemmacounts.get)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef estimate_gaussian(X):\n    mean = np.mean(X,0)\n    variance = np.var(X,0)\n    return Gaussian(mean,variance)", "response": "Estimate the mean and variance of a data set of X assuming that \n is a gaussian distribution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the maximum value of a dictionary.", "response": "def dict_max(dic):\n    \"\"\"\n    Returns maximum value of a dictionary.\n    \"\"\"\n    aux = dict(map(lambda item: (item[1],item[0]),dic.items()))\n    if aux.keys() == []:\n        return 0\n    max_value = max(aux.keys())\n    return max_value,aux[max_value]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nnormalizing an array to sum to one column wise row wise or full array.", "response": "def normalize_array(a,direction=\"column\"):\n    \"\"\"\n    Normalizes an array to sum to one, either column wise, \n    or row wise or the full array.\n    \n    *Directions*\n        Column-wise - 0 default\n        Row-wise - 1 default\n        All - 2 default\n    \"\"\"\n    b = a.copy()\n    if(direction == \"column\"):\n        sums = np.sum(b,0)\n        return np.nan_to_num(b/sums)\n    elif(direction == \"row\"):\n        sums =np.sum(b,1)\n        return  np.nan_to_num((b.transpose() / sums).transpose())\n    elif(direction == \"all\"):\n        sums = np.sum(b)\n        return np.nan_to_num(b / sums)\n    else:\n        print \"Error non existing normalization\"\n        return b"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef has_c_library(library, extension=\".c\"):\n    with tempfile.TemporaryDirectory(dir=\".\") as directory:\n        name = join(directory, \"%s%s\" % (library, extension))\n        with open(name, \"w\") as f:\n            f.write(\"#include <%s.h>\\n\" % library)\n            f.write(\"int main() {}\\n\")\n\n        # Get a compiler instance\n        compiler = ccompiler.new_compiler()\n        assert isinstance(compiler, ccompiler.CCompiler)\n\n        try:\n            # Try to compile the file using the C compiler\n            compiler.link_executable(compiler.compile([name]), name)\n            return True\n        except (CompileError, LinkError):\n            return False", "response": "Check whether a C library is available on the system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the CPM of the counts - per - million on data where the rows are genes.", "response": "def calculate_cpm(x, axis=1):\n    \"\"\"Calculate counts-per-million on data where the rows are genes.\n    \n    Parameters\n    ----------\n    x : array_like\n    axis : int\n        Axis accross which to compute CPM. 0 for genes being in rows and 1 for\n        genes in columns.\n\n    \"\"\"\n    normalization = np.sum(x, axis=axis)\n    # On sparse matrices, the sum will be 2d. We want a 1d array\n    normalization = np.squeeze(np.asarray(normalization))\n    # Straight up division is not an option since this will form a full dense\n    # matrix if `x` is sparse. Divison can be expressed as the dot product with\n    # a reciprocal diagonal matrix\n    normalization = sp.diags(1 / normalization, offsets=0)\n    if axis == 0:\n        cpm_counts = np.dot(x, normalization)\n    elif axis == 1:\n        cpm_counts = np.dot(normalization, x)\n    return cpm_counts * 1e6"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms log transform log ( x + 1.", "response": "def log_normalize(data):\n    \"\"\"Perform log transform log(x + 1).\n    \n    Parameters\n    ----------\n    data : array_like\n    \n    \"\"\"\n    if sp.issparse(data):\n        data = data.copy()\n        data.data = np.log2(data.data + 1)\n        return data\n\n    return np.log2(data.astype(np.float64) + 1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck that the metric is supported by the KNNIndex instance.", "response": "def check_metric(self, metric):\n        \"\"\"Check that the metric is supported by the KNNIndex instance.\"\"\"\n        if metric not in self.VALID_METRICS:\n            raise ValueError(\n                f\"`{self.__class__.__name__}` does not support the `{metric}` \"\n                f\"metric. Please choose one of the supported metrics: \"\n                f\"{', '.join(self.VALID_METRICS)}.\"\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing an embedding using samples from an isotropic Gaussian.", "response": "def random(X, n_components=2, random_state=None):\n    \"\"\"Initialize an embedding using samples from an isotropic Gaussian.\n\n    Parameters\n    ----------\n    X: np.ndarray\n        The data matrix.\n\n    n_components: int\n        The dimension of the embedding space.\n\n    random_state: Union[int, RandomState]\n        If the value is an int, random_state is the seed used by the random\n        number generator. If the value is a RandomState instance, then it will\n        be used as the random number generator. If the value is None, the random\n        number generator is the RandomState instance used by `np.random`.\n\n    Returns\n    -------\n    initialization: np.ndarray\n\n    \"\"\"\n    random_state = check_random_state(random_state)\n    return random_state.normal(0, 1e-2, (X.shape[0], n_components))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing an embedding using the top principal components.", "response": "def pca(X, n_components=2, random_state=None):\n    \"\"\"Initialize an embedding using the top principal components.\n\n    Parameters\n    ----------\n    X: np.ndarray\n        The data matrix.\n\n    n_components: int\n        The dimension of the embedding space.\n\n    random_state: Union[int, RandomState]\n        If the value is an int, random_state is the seed used by the random\n        number generator. If the value is a RandomState instance, then it will\n        be used as the random number generator. If the value is None, the random\n        number generator is the RandomState instance used by `np.random`.\n\n    Returns\n    -------\n    initialization: np.ndarray\n\n    \"\"\"\n    pca_ = PCA(n_components=n_components, random_state=random_state)\n    embedding = pca_.fit_transform(X)\n\n    # The PCA embedding may have high variance, which leads to poor convergence\n    normalization = np.std(embedding[:, 0]) * 100\n    embedding /= normalization\n\n    return embedding"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef weighted_mean(X, embedding, neighbors, distances):\n    n_samples = X.shape[0]\n    n_components = embedding.shape[1]\n\n    partial_embedding = np.zeros((n_samples, n_components))\n    for i in range(n_samples):\n        partial_embedding[i] = np.average(\n            embedding[neighbors[i]], axis=0, weights=distances[i],\n        )\n\n    return partial_embedding", "response": "Initialize points onto an existing embedding by placing them in the weighted mean position of their nearest neighbors on the reference embedding."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npruning the k - neighbors graph back so that nodes have a maximum degree of max_degree.", "response": "def degree_prune(graph, max_degree=20):\n    \"\"\"Prune the k-neighbors graph back so that nodes have a maximum\n    degree of ``max_degree``.\n\n    Parameters\n    ----------\n    graph: sparse matrix\n        The adjacency matrix of the graph\n\n    max_degree: int (optional, default 20)\n        The maximum degree of any node in the pruned graph\n\n    Returns\n    -------\n    result: sparse matrix\n        The pruned graph.\n    \"\"\"\n\n    result = graph.tolil()\n    for i, row_data in enumerate(result.data):\n        if len(row_data) > max_degree:\n            cut_value = np.argsort(row_data)[max_degree]\n            row_data = [x if x <= cut_value else 0.0 for x in row_data]\n            result.data[i] = row_data\n    result = result.tocsr()\n    result.eliminate_zeros()\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming a pruning on a graph.", "response": "def prune(graph, prune_level=0, n_neighbors=10):\n    \"\"\"Perform pruning on the graph so that there are fewer edges to\n    be followed. In practice this operates in two passes. The first pass\n    removes edges such that no node has degree more than ``3 * n_neighbors -\n    prune_level``. The second pass builds up a graph out of spanning trees;\n    each iteration constructs a minimum panning tree of a graph and then\n    removes those edges from the graph. The result is spanning trees that\n    take various paths through the graph. All these spanning trees are merged\n    into the resulting graph. In practice this prunes away a limited number\n    of edges as long as enough iterations are performed. By default we will\n    do ``n_neighbors - prune_level``iterations.\n\n    Parameters\n    ----------\n    graph: sparse matrix\n        The adjacency matrix of the graph\n\n    prune_level: int (optional default 0)\n        How aggressively to prune the graph, larger values perform more\n        aggressive pruning.\n\n    n_neighbors: int (optional 10)\n        The number of neighbors of the k-neighbor graph that was constructed.\n\n    Returns\n    -------\n    result: sparse matrix\n        The pruned graph\n    \"\"\"\n\n    max_degree = max(5, 3 * n_neighbors - prune_level)\n    n_iters = max(3, n_neighbors - prune_level)\n    reduced_graph = degree_prune(graph, max_degree=max_degree)\n    result_graph = lil_matrix((graph.shape[0], graph.shape[0])).tocsr()\n\n    for _ in range(n_iters):\n        mst = minimum_spanning_tree(reduced_graph)\n        result_graph = result_graph.maximum(mst)\n        reduced_graph -= mst\n        reduced_graph.eliminate_zeros()\n\n    return result_graph"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef query(self, query_data, k=10, queue_size=5.0):\n        # query_data = check_array(query_data, dtype=np.float64, order='C')\n        query_data = np.asarray(query_data).astype(np.float32)\n        init = initialise_search(\n            self._rp_forest,\n            self._raw_data,\n            query_data,\n            int(k * queue_size),\n            self._random_init,\n            self._tree_init,\n            self.rng_state,\n        )\n        result = self._search(\n            self._raw_data,\n            self._search_graph.indptr,\n            self._search_graph.indices,\n            init,\n            query_data,\n        )\n\n        indices, dists = deheap_sort(result)\n        return indices[:, :k], dists[:, :k]", "response": "Query the training data for the k nearest neighbors of a specific entry in the base training set."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfits the PyNNDescentTransformer to build KNN graphs with the neighbors given by the dataset X.", "response": "def fit(self, X):\n        \"\"\"Fit the PyNNDescent transformer to build KNN graphs with\n        neighbors given by the dataset X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Sample data\n\n        Returns\n        -------\n        transformer : PyNNDescentTransformer\n            The trained transformer\n        \"\"\"\n        self.n_samples_fit = X.shape[0]\n\n        if self.metric_kwds is None:\n            metric_kwds = {}\n        else:\n            metric_kwds = self.metric_kwds\n\n        self.pynndescent_ = NNDescent(\n            X,\n            self.metric,\n            metric_kwds,\n            self.n_neighbors,\n            self.n_trees,\n            self.leaf_size,\n            self.pruning_level,\n            self.tree_init,\n            self.random_state,\n            self.algorithm,\n            self.max_candidates,\n            self.n_iters,\n            self.early_termination_value,\n            self.sampling_rate,\n        )\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef transform(self, X, y=None):\n\n        if X is None:\n            n_samples_transform = self.n_samples_fit\n        else:\n            n_samples_transform = X.shape[0]\n\n        if X is None:\n            indices, distances = self.pynndescent_._neighbor_graph\n        else:\n            indices, distances = self.pynndescent_.query(\n                X, k=self.n_neighbors, queue_size=self.search_queue_size\n            )\n\n        result = lil_matrix(\n            (n_samples_transform, n_samples_transform), dtype=np.float32\n        )\n        result.rows = indices\n        result.data = distances\n\n        return result.tocsr()", "response": "Computes the weighted graph of Neighbors for points in X."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef weighted_minkowski(x, y, w=_mock_identity, p=2):\n    result = 0.0\n    for i in range(x.shape[0]):\n        result += (w[i] * np.abs(x[i] - y[i])) ** p\n\n    return result ** (1.0 / p)", "response": "A weighted version of Minkowski distance."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts the user friendly params into something the optimizer cancanon canon canononon canon canon", "response": "def _handle_nice_params(optim_params: dict) -> None:\n    \"\"\"Convert the user friendly params into something the optimizer can\n    understand.\"\"\"\n    # Handle callbacks\n    optim_params[\"callbacks\"] = _check_callbacks(optim_params.get(\"callbacks\"))\n    optim_params[\"use_callbacks\"] = optim_params[\"callbacks\"] is not None\n\n    # Handle negative gradient method\n    negative_gradient_method = optim_params.pop(\"negative_gradient_method\")\n    if callable(negative_gradient_method):\n        negative_gradient_method = negative_gradient_method\n    elif negative_gradient_method in {\"bh\", \"BH\", \"barnes-hut\"}:\n        negative_gradient_method = kl_divergence_bh\n    elif negative_gradient_method in {\"fft\", \"FFT\", \"interpolation\"}:\n        negative_gradient_method = kl_divergence_fft\n    else:\n        raise ValueError(\"Unrecognized gradient method. Please choose one of \"\n                         \"the supported methods or provide a valid callback.\")\n    # `gradient_descent` uses the more informative name `objective_function`\n    optim_params[\"objective_function\"] = negative_gradient_method\n\n    # Handle number of jobs\n    n_jobs = optim_params.get(\"n_jobs\", 1)\n    if n_jobs < 0:\n        n_cores = multiprocessing.cpu_count()\n        # Add negative number of n_jobs to the number of cores, but increment by\n        # one because -1 indicates using all cores, -2 all except one, and so on\n        n_jobs = n_cores + n_jobs + 1\n\n    # If the number of jobs, after this correction is still <= 0, then the user\n    # probably thought they had more cores, so we'll default to 1\n    if n_jobs <= 0:\n        log.warning(\"`n_jobs` receieved value %d but only %d cores are available. \"\n                    \"Defaulting to single job.\" % (optim_params[\"n_jobs\"], n_cores))\n        n_jobs = 1\n\n    optim_params[\"n_jobs\"] = n_jobs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\noptimizing the embedding for a given number of steps.", "response": "def optimize(self, n_iter, inplace=False, propagate_exception=False,\n                 **gradient_descent_params):\n        \"\"\"Run optmization on the embedding for a given number of steps.\n\n        Parameters\n        ----------\n        n_iter: int\n            The number of optimization iterations.\n\n        learning_rate: float\n            The learning rate for t-SNE optimization. Typical values range\n            between 100 to 1000. Setting the learning rate too low or too high\n            may result in the points forming a \"ball\". This is also known as the\n            crowding problem.\n\n        exaggeration: float\n            The exaggeration factor is used to increase the attractive forces of\n            nearby points, producing more compact clusters.\n\n        momentum: float\n            Momentum accounts for gradient directions from previous iterations,\n            resulting in faster convergence.\n\n        negative_gradient_method: str\n            Specifies the negative gradient approximation method to use. For\n            smaller data sets, the Barnes-Hut approximation is appropriate and\n            can be set using one of the following aliases: ``bh``, ``BH`` or\n            ``barnes-hut``. For larger data sets, the FFT accelerated\n            interpolation method is more appropriate and can be set using one of\n            the following aliases: ``fft``, ``FFT`` or ``\u00ecnterpolation``.\n\n        theta: float\n            This is the trade-off parameter between speed and accuracy of the\n            tree approximation method. Typical values range from 0.2 to 0.8. The\n            value 0 indicates that no approximation is to be made and produces\n            exact results also producing longer runtime.\n\n        n_interpolation_points: int\n            Only used when ``negative_gradient_method=\"fft\"`` or its other\n            aliases. The number of interpolation points to use within each grid\n            cell for interpolation based t-SNE. It is highly recommended leaving\n            this value at the default 3.\n\n        min_num_intervals: int\n            Only used when ``negative_gradient_method=\"fft\"`` or its other\n            aliases. The minimum number of grid cells to use, regardless of the\n            ``ints_in_interval`` parameter. Higher values provide more accurate\n            gradient estimations.\n\n        inplace: bool\n            Whether or not to create a copy of the embedding or to perform\n            updates inplace.\n\n        propagate_exception: bool\n            The optimization process can be interrupted using callbacks. This\n            flag indicates whether we should propagate that exception or to\n            simply stop optimization and return the resulting embedding.\n\n        random_state: Union[int, RandomState]\n            The random state parameter follows the convention used in\n            scikit-learn. If the value is an int, random_state is the seed used\n            by the random number generator. If the value is a RandomState\n            instance, then it will be used as the random number generator. If\n            the value is None, the random number generator is the RandomState\n            instance used by `np.random`.\n\n        n_jobs: int\n            The number of threads to use while running t-SNE. This follows the\n            scikit-learn convention, ``-1`` meaning all processors, ``-2``\n            meaning all but one, etc.\n\n        callbacks: Callable[[int, float, np.ndarray] -> bool]\n            Callbacks, which will be run every ``callbacks_every_iters``\n            iterations.\n\n        callbacks_every_iters: int\n            How many iterations should pass between each time the callbacks are\n            invoked.\n\n        Returns\n        -------\n        PartialTSNEEmbedding\n            An optimized partial t-SNE embedding.\n\n        Raises\n        ------\n        OptimizationInterrupt\n            If a callback stops the optimization and the ``propagate_exception``\n            flag is set, then an exception is raised.\n\n        \"\"\"\n        # Typically we want to return a new embedding and keep the old one intact\n        if inplace:\n            embedding = self\n        else:\n            embedding = PartialTSNEEmbedding(\n                np.copy(self),\n                self.reference_embedding,\n                self.P,\n                optimizer=self.optimizer.copy(),\n                **self.gradient_descent_params,\n            )\n\n        # If optimization parameters were passed to this funciton, prefer those\n        # over the defaults specified in the TSNE object\n        optim_params = dict(self.gradient_descent_params)\n        optim_params.update(gradient_descent_params)\n        _handle_nice_params(optim_params)\n        optim_params[\"n_iter\"] = n_iter\n\n        try:\n            # Run gradient descent with the embedding optimizer so gains are\n            # properly updated and kept\n            error, embedding = embedding.optimizer(\n                embedding=embedding,\n                reference_embedding=self.reference_embedding,\n                P=self.P,\n                **optim_params,\n            )\n\n        except OptimizationInterrupt as ex:\n            log.info(\"Optimization was interrupted with callback.\")\n            if propagate_exception:\n                raise ex\n            error, embedding = ex.error, ex.final_embedding\n\n        embedding.kl_divergence = error\n\n        return embedding"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transform(self, X, perplexity=5, initialization=\"median\", k=25,\n                  learning_rate=1, n_iter=100, exaggeration=2, momentum=0):\n        \"\"\"Embed new points into the existing embedding.\n\n        This procedure optimizes each point only with respect to the existing\n        embedding i.e. it ignores any interactions between the points in ``X``\n        among themselves.\n\n        Please see the :ref:`parameter-guide` for more information.\n\n        Parameters\n        ----------\n        X: np.ndarray\n            The data matrix to be added to the existing embedding.\n\n        perplexity: float\n            Perplexity can be thought of as the continuous :math:`k` number of\n            nearest neighbors, for which t-SNE will attempt to preserve\n            distances. However, when transforming, we only consider neighbors in\n            the existing embedding i.e. each data point is placed into the\n            embedding, independently of other new data points.\n\n        initialization: Union[np.ndarray, str]\n            The initial point positions to be used in the embedding space. Can\n            be a precomputed numpy array, ``median``, ``weighted`` or\n            ``random``. In all cases, ``median`` of ``weighted`` should be\n            preferred.\n\n        k: int\n            The number of nearest neighbors to consider when initially placing\n            the point onto the embedding. This is different from ``perpelxity``\n            because perplexity affects optimization while this only affects the\n            initial point positions.\n\n        learning_rate: float\n            The learning rate for t-SNE optimization. Typical values range\n            between 100 to 1000. Setting the learning rate too low or too high\n            may result in the points forming a \"ball\". This is also known as the\n            crowding problem.\n\n        n_iter: int\n            The number of iterations to run in the normal optimization regime.\n            Typically, the number of iterations needed when adding new data\n            points is much lower than with regular optimization.\n\n        exaggeration: float\n            The exaggeration factor to use during the normal optimization phase.\n            This can be used to form more densely packed clusters and is useful\n            for large data sets.\n\n        momentum: float\n            The momentum to use during optimization phase.\n\n        Returns\n        -------\n        PartialTSNEEmbedding\n            The positions of the new points in the embedding space.\n\n        \"\"\"\n\n        # We check if the affinity `to_new` methods takes the `perplexity`\n        # parameter and raise an informative error if not. This happes when the\n        # user uses a non-standard affinity class e.g. multiscale, then attempts\n        # to add points via `transform`. These classes take `perplexities` and\n        # fail\n        affinity_signature = inspect.signature(self.affinities.to_new)\n        if \"perplexity\" not in affinity_signature.parameters:\n            raise TypeError(\n                \"`transform` currently does not support non `%s` type affinity \"\n                \"classes. Please use `prepare_partial` and `optimize` to add \"\n                \"points to the embedding.\" % PerplexityBasedNN.__name__\n            )\n\n        embedding = self.prepare_partial(\n            X, perplexity=perplexity, initialization=initialization, k=k\n        )\n\n        try:\n            embedding.optimize(\n                n_iter=n_iter,\n                learning_rate=learning_rate,\n                exaggeration=exaggeration,\n                momentum=momentum,\n                inplace=True,\n                propagate_exception=True,\n            )\n\n        except OptimizationInterrupt as ex:\n            log.info(\"Optimization was interrupted with callback.\")\n            embedding = ex.final_embedding\n\n        return embedding", "response": "This procedure transforms the data matrix X into the existing embedding."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prepare_partial(self, X, initialization=\"median\", k=25, **affinity_params):\n        P, neighbors, distances = self.affinities.to_new(\n            X, return_distances=True, **affinity_params\n        )\n\n        # If initial positions are given in an array, use a copy of that\n        if isinstance(initialization, np.ndarray):\n            init_checks.num_samples(initialization.shape[0], X.shape[0])\n            init_checks.num_dimensions(initialization.shape[1], self.shape[1])\n\n            embedding = np.array(initialization)\n\n        # Random initialization with isotropic normal distribution\n        elif initialization == \"random\":\n            embedding = initialization_scheme.random(X, self.shape[1], self.random_state)\n        elif initialization == \"weighted\":\n            embedding = initialization_scheme.weighted_mean(\n                X, self, neighbors[:, :k], distances[:, :k]\n            )\n        elif initialization == \"median\":\n            embedding = initialization_scheme.median(self, neighbors[:, :k])\n        else:\n            raise ValueError(f\"Unrecognized initialization scheme `{initialization}`.\")\n\n        return PartialTSNEEmbedding(\n            embedding,\n            reference_embedding=self,\n            P=P,\n            **self.gradient_descent_params,\n        )", "response": "Prepares a partial embedding which can be optimized."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfit a t - SNE embedding for a given data set.", "response": "def fit(self, X):\n        \"\"\"Fit a t-SNE embedding for a given data set.\n\n        Runs the standard t-SNE optimization, consisting of the early\n        exaggeration phase and a normal optimization phase.\n\n        Parameters\n        ----------\n        X: np.ndarray\n            The data matrix to be embedded.\n\n        Returns\n        -------\n        TSNEEmbedding\n            A fully optimized t-SNE embedding.\n\n        \"\"\"\n        embedding = self.prepare_initial(X)\n\n        try:\n            # Early exaggeration with lower momentum to allow points to find more\n            # easily move around and find their neighbors\n            embedding.optimize(\n                n_iter=self.early_exaggeration_iter,\n                exaggeration=self.early_exaggeration,\n                momentum=self.initial_momentum,\n                inplace=True,\n                propagate_exception=True,\n            )\n\n            # Restore actual affinity probabilities and increase momentum to get\n            # final, optimized embedding\n            embedding.optimize(\n                n_iter=self.n_iter,\n                exaggeration=self.exaggeration,\n                momentum=self.final_momentum,\n                inplace=True,\n                propagate_exception=True,\n            )\n\n        except OptimizationInterrupt as ex:\n            log.info(\"Optimization was interrupted with callback.\")\n            embedding = ex.final_embedding\n\n        return embedding"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prepare_initial(self, X):\n        # If initial positions are given in an array, use a copy of that\n        if isinstance(self.initialization, np.ndarray):\n            init_checks.num_samples(self.initialization.shape[0], X.shape[0])\n            init_checks.num_dimensions(self.initialization.shape[1], self.n_components)\n\n            embedding = np.array(self.initialization)\n\n            variance = np.var(embedding, axis=0)\n            if any(variance > 1e-4):\n                log.warning(\n                    \"Variance of embedding is greater than 0.0001. Initial \"\n                    \"embeddings with high variance may have display poor convergence.\"\n                )\n\n        elif self.initialization == \"pca\":\n            embedding = initialization_scheme.pca(\n                X, self.n_components, random_state=self.random_state\n            )\n        elif self.initialization == \"random\":\n            embedding = initialization_scheme.random(\n                X, self.n_components, random_state=self.random_state\n            )\n        else:\n            raise ValueError(\n                f\"Unrecognized initialization scheme `{self.initialization}`.\"\n            )\n\n        affinities = PerplexityBasedNN(\n            X,\n            self.perplexity,\n            method=self.neighbors_method,\n            metric=self.metric,\n            metric_params=self.metric_params,\n            n_jobs=self.n_jobs,\n            random_state=self.random_state,\n        )\n\n        gradient_descent_params = {\n            # Degrees of freedom of the Student's t-distribution. The\n            # suggestion degrees_of_freedom = n_components - 1 comes from [3]_.\n            \"dof\":  max(self.n_components - 1, 1),\n\n            \"negative_gradient_method\": self.negative_gradient_method,\n            \"learning_rate\": self.learning_rate,\n            # By default, use the momentum used in unexaggerated phase\n            \"momentum\": self.final_momentum,\n\n            # Barnes-Hut params\n            \"theta\": self.theta,\n            # Interpolation params\n            \"n_interpolation_points\": self.n_interpolation_points,\n            \"min_num_intervals\": self.min_num_intervals,\n            \"ints_in_interval\": self.ints_in_interval,\n\n            \"n_jobs\": self.n_jobs,\n            # Callback params\n            \"callbacks\": self.callbacks,\n            \"callbacks_every_iters\": self.callbacks_every_iters,\n        }\n\n        return TSNEEmbedding(\n            embedding,\n            affinities=affinities,\n            random_state=self.random_state,\n            **gradient_descent_params,\n        )", "response": "Prepares the initial embedding which can be optimized as needed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef joint_probabilities_nn(\n    neighbors,\n    distances,\n    perplexities,\n    symmetrize=True,\n    normalization=\"pair-wise\",\n    n_reference_samples=None,\n    n_jobs=1,\n):\n    \"\"\"Compute the conditional probability matrix P_{j|i}.\n\n    This method computes an approximation to P using the nearest neighbors.\n\n    Parameters\n    ----------\n    neighbors: np.ndarray\n        A `n_samples * k_neighbors` matrix containing the indices to each\n        points\" nearest neighbors in descending order.\n    distances: np.ndarray\n        A `n_samples * k_neighbors` matrix containing the distances to the\n        neighbors at indices defined in the neighbors parameter.\n    perplexities: double\n        The desired perplexity of the probability distribution.\n    symmetrize: bool\n        Whether to symmetrize the probability matrix or not. Symmetrizing is\n        used for typical t-SNE, but does not make sense when embedding new data\n        into an existing embedding.\n    normalization: str\n        The normalization scheme to use for the affinities. Standard t-SNE\n        considers interactions between all the data points, therefore the entire\n        affinity matrix is regarded as a probability distribution, and must sum\n        to 1. When embedding new points, we only consider interactions to\n        existing points, and treat each point separately. In this case, we\n        row-normalize the affinity matrix, meaning each point gets its own\n        probability distribution.\n    n_reference_samples: int\n        The number of samples in the existing (reference) embedding. Needed to\n        properly construct the sparse P matrix.\n    n_jobs: int\n        Number of threads.\n\n    Returns\n    -------\n    csr_matrix\n        A `n_samples * n_reference_samples` matrix containing the probabilities\n        that a new sample would appear as a neighbor of a reference point.\n\n    \"\"\"\n    assert normalization in (\n        \"pair-wise\",\n        \"point-wise\",\n    ), f\"Unrecognized normalization scheme `{normalization}`.\"\n\n    n_samples, k_neighbors = distances.shape\n\n    if n_reference_samples is None:\n        n_reference_samples = n_samples\n\n    # Compute asymmetric pairwise input similarities\n    conditional_P = _tsne.compute_gaussian_perplexity(\n        distances, np.array(perplexities, dtype=float), num_threads=n_jobs\n    )\n    conditional_P = np.asarray(conditional_P)\n\n    P = sp.csr_matrix(\n        (\n            conditional_P.ravel(),\n            neighbors.ravel(),\n            range(0, n_samples * k_neighbors + 1, k_neighbors),\n        ),\n        shape=(n_samples, n_reference_samples),\n    )\n\n    # Symmetrize the probability matrix\n    if symmetrize:\n        P = (P + P.T) / 2\n\n    if normalization == \"pair-wise\":\n        P /= np.sum(P)\n    elif normalization == \"point-wise\":\n        P = sp.diags(np.asarray(1 / P.sum(axis=1)).ravel()) @ P\n\n    return P", "response": "Compute the conditional probability matrix P_j for each new sample."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_perplexity(self, new_perplexity):\n        # If the value hasn't changed, there's nothing to do\n        if new_perplexity == self.perplexity:\n            return\n        # Verify that the perplexity isn't too large\n        new_perplexity = self.check_perplexity(new_perplexity)\n        # Recompute the affinity matrix\n        k_neighbors = min(self.n_samples - 1, int(3 * new_perplexity))\n        if k_neighbors > self.__neighbors.shape[1]:\n            raise RuntimeError(\n                \"The desired perplexity `%.2f` is larger than the initial one \"\n                \"used. This would need to recompute the nearest neighbors, \"\n                \"which is not efficient. Please create a new `%s` instance \"\n                \"with the increased perplexity.\"\n                % (new_perplexity, self.__class__.__name__)\n            )\n\n        self.perplexity = new_perplexity\n        self.P = joint_probabilities_nn(\n            self.__neighbors[:, :k_neighbors],\n            self.__distances[:, :k_neighbors],\n            [self.perplexity],\n            symmetrize=True,\n            n_jobs=self.n_jobs,\n        )", "response": "Change the perplexity of the current instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_new(self, data, perplexity=None, return_distances=False):\n        perplexity = perplexity if perplexity is not None else self.perplexity\n        perplexity = self.check_perplexity(perplexity)\n        k_neighbors = min(self.n_samples - 1, int(3 * perplexity))\n\n        neighbors, distances = self.knn_index.query(data, k_neighbors)\n\n        P = joint_probabilities_nn(\n            neighbors,\n            distances,\n            [perplexity],\n            symmetrize=False,\n            normalization=\"point-wise\",\n            n_reference_samples=self.n_samples,\n            n_jobs=self.n_jobs,\n        )\n\n        if return_distances:\n            return P, neighbors, distances\n\n        return P", "response": "Compute the affinities of new data points to the initial samples."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the affinities of new samples to the initial samples. This is necessary for embedding new data points into an existing embedding. Parameters ---------- data: np.ndarray The data points to be added to the existing embedding. k: int The number of nearest neighbors to consider for each kernel. sigma: float The bandwidth to use for the Gaussian kernels in the ambient space. return_distances: bool If needed, the function can return the indices of the nearest neighbors and their corresponding distances. Returns ------- P: array_like An :math:`N \\\\times M` affinity matrix expressing interactions between :math:`N` new data points the initial :math:`M` data samples. indices: np.ndarray Returned if ``return_distances=True``. The indices of the :math:`k` nearest neighbors in the existing embedding for every new data point. distances: np.ndarray Returned if ``return_distances=True``. The distances to the :math:`k` nearest neighbors in the existing embedding for every new data point.", "response": "def to_new(self, data, k=None, sigma=None, return_distances=False):\n        \"\"\"Compute the affinities of new samples to the initial samples.\n\n        This is necessary for embedding new data points into an existing\n        embedding.\n\n        Parameters\n        ----------\n        data: np.ndarray\n            The data points to be added to the existing embedding.\n\n        k: int\n            The number of nearest neighbors to consider for each kernel.\n\n        sigma: float\n            The bandwidth to use for the Gaussian kernels in the ambient space.\n\n        return_distances: bool\n            If needed, the function can return the indices of the nearest\n            neighbors and their corresponding distances.\n\n        Returns\n        -------\n        P: array_like\n            An :math:`N \\\\times M` affinity matrix expressing interactions\n            between :math:`N` new data points the initial :math:`M` data\n            samples.\n\n        indices: np.ndarray\n            Returned if ``return_distances=True``. The indices of the :math:`k`\n            nearest neighbors in the existing embedding for every new data\n            point.\n\n        distances: np.ndarray\n            Returned if ``return_distances=True``. The distances to the\n            :math:`k` nearest neighbors in the existing embedding for every new\n            data point.\n\n        \"\"\"\n        n_samples = data.shape[0]\n        n_reference_samples = self.n_samples\n\n        if k is None:\n            k = self.k\n        elif k >= n_reference_samples:\n            raise ValueError(\n                \"`k` (%d) cannot be larger than the number of reference \"\n                \"samples (%d).\" % (k, self.n_samples)\n            )\n\n        if sigma is None:\n            sigma = self.sigma\n\n        # Find nearest neighbors and the distances to the new points\n        neighbors, distances = self.knn_index.query(data, k)\n\n        # Compute asymmetric pairwise input similarities\n        conditional_P = np.exp(-distances ** 2 / (2 * sigma ** 2))\n\n        # Convert weights to probabilities\n        conditional_P /= np.sum(conditional_P, axis=1)[:, np.newaxis]\n\n        P = sp.csr_matrix(\n            (conditional_P.ravel(), neighbors.ravel(), range(0, n_samples * k + 1, k)),\n            shape=(n_samples, n_reference_samples),\n        )\n\n        if return_distances:\n            return P, neighbors, distances\n\n        return P"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchanging the perplexities of the affinity matrix.", "response": "def set_perplexities(self, new_perplexities):\n        \"\"\"Change the perplexities of the affinity matrix.\n\n        Note that we only allow lowering the perplexities or restoring them to\n        their original maximum value. This restriction exists because setting a\n        higher perplexity value requires recomputing all the nearest neighbors,\n        which can take a long time. To avoid potential confusion as to why\n        execution time is slow, this is not allowed. If you would like to\n        increase the perplexity above the initial value, simply create a new\n        instance.\n\n        Parameters\n        ----------\n        new_perplexities: List[float]\n            The new list of perplexities.\n\n        \"\"\"\n        if np.array_equal(self.perplexities, new_perplexities):\n            return\n\n        new_perplexities = self.check_perplexities(new_perplexities)\n        max_perplexity = np.max(new_perplexities)\n        k_neighbors = min(self.n_samples - 1, int(3 * max_perplexity))\n\n        if k_neighbors > self.__neighbors.shape[1]:\n            raise RuntimeError(\n                \"The largest perplexity `%.2f` is larger than the initial one \"\n                \"used. This would need to recompute the nearest neighbors, \"\n                \"which is not efficient. Please create a new `%s` instance \"\n                \"with the increased perplexity.\"\n                % (max_perplexity, self.__class__.__name__)\n            )\n\n        self.perplexities = new_perplexities\n        self.P = self._calculate_P(\n            self.__neighbors[:, :k_neighbors],\n            self.__distances[:, :k_neighbors],\n            self.perplexities,\n            symmetrize=True,\n            n_jobs=self.n_jobs,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the affinities of new samples to the initial samples. This is necessary for embedding new data points into an existing embedding. Please see the :ref:`parameter-guide` for more information. Parameters ---------- data: np.ndarray The data points to be added to the existing embedding. perplexities: List[float] A list of perplexity values, which will be used in the multiscale Gaussian kernel. Perplexity can be thought of as the continuous :math:`k` number of nearest neighbors, for which t-SNE will attempt to preserve distances. return_distances: bool If needed, the function can return the indices of the nearest neighbors and their corresponding distances. Returns ------- P: array_like An :math:`N \\\\times M` affinity matrix expressing interactions between :math:`N` new data points the initial :math:`M` data samples. indices: np.ndarray Returned if ``return_distances=True``. The indices of the :math:`k` nearest neighbors in the existing embedding for every new data point. distances: np.ndarray Returned if ``return_distances=True``. The distances to the :math:`k` nearest neighbors in the existing embedding for every new data point.", "response": "def to_new(self, data, perplexities=None, return_distances=False):\n        \"\"\"Compute the affinities of new samples to the initial samples.\n\n        This is necessary for embedding new data points into an existing\n        embedding.\n\n        Please see the :ref:`parameter-guide` for more information.\n\n        Parameters\n        ----------\n        data: np.ndarray\n            The data points to be added to the existing embedding.\n\n        perplexities: List[float]\n            A list of perplexity values, which will be used in the multiscale\n            Gaussian kernel. Perplexity can be thought of as the continuous\n            :math:`k` number of nearest neighbors, for which t-SNE will attempt\n            to preserve distances.\n\n        return_distances: bool\n            If needed, the function can return the indices of the nearest\n            neighbors and their corresponding distances.\n\n        Returns\n        -------\n        P: array_like\n            An :math:`N \\\\times M` affinity matrix expressing interactions\n            between :math:`N` new data points the initial :math:`M` data\n            samples.\n\n        indices: np.ndarray\n            Returned if ``return_distances=True``. The indices of the :math:`k`\n            nearest neighbors in the existing embedding for every new data\n            point.\n\n        distances: np.ndarray\n            Returned if ``return_distances=True``. The distances to the\n            :math:`k` nearest neighbors in the existing embedding for every new\n            data point.\n\n        \"\"\"\n        perplexities = perplexities if perplexities is not None else self.perplexities\n        perplexities = self.check_perplexities(perplexities)\n\n        max_perplexity = np.max(perplexities)\n        k_neighbors = min(self.n_samples - 1, int(3 * max_perplexity))\n\n        neighbors, distances = self.knn_index.query(data, k_neighbors)\n\n        P = self._calculate_P(\n            neighbors,\n            distances,\n            perplexities,\n            symmetrize=False,\n            normalization=\"point-wise\",\n            n_reference_samples=self.n_samples,\n            n_jobs=self.n_jobs,\n        )\n\n        if return_distances:\n            return P, neighbors, distances\n\n        return P"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_perplexities(self, perplexities):\n        usable_perplexities = []\n        for perplexity in sorted(perplexities):\n            if 3 * perplexity > self.n_samples - 1:\n                new_perplexity = (self.n_samples - 1) / 3\n\n                if new_perplexity in usable_perplexities:\n                    log.warning(\n                        \"Perplexity value %d is too high. Dropping \"\n                        \"because the max perplexity is already in the \"\n                        \"list.\" % perplexity\n                    )\n                else:\n                    usable_perplexities.append(new_perplexity)\n                    log.warning(\n                        \"Perplexity value %d is too high. Using \"\n                        \"perplexity %.2f instead\" % (perplexity, new_perplexity)\n                    )\n            else:\n                usable_perplexities.append(perplexity)\n\n        return usable_perplexities", "response": "Check and correct and truncate perplexities."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_heap(n_points, size):\n    result = np.zeros((3, n_points, size))\n    result[0] = -1\n    result[1] = np.infty\n    result[2] = 0\n\n    return result", "response": "This is a helper function that creates a numba enabled heap object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef heap_push(heap, row, weight, index, flag):\n    indices = heap[0, row]\n    weights = heap[1, row]\n    is_new = heap[2, row]\n\n    if weight >= weights[0]:\n        return 0\n\n    # break if we already have this element.\n    for i in range(indices.shape[0]):\n        if index == indices[i]:\n            return 0\n\n    # insert val at position zero\n    weights[0] = weight\n    indices[0] = index\n    is_new[0] = flag\n\n    # descend the heap, swapping values until the max heap criterion is met\n    i = 0\n    while True:\n        ic1 = 2 * i + 1\n        ic2 = ic1 + 1\n\n        if ic1 >= heap.shape[2]:\n            break\n        elif ic2 >= heap.shape[2]:\n            if weights[ic1] > weight:\n                i_swap = ic1\n            else:\n                break\n        elif weights[ic1] >= weights[ic2]:\n            if weight < weights[ic1]:\n                i_swap = ic1\n            else:\n                break\n        else:\n            if weight < weights[ic2]:\n                i_swap = ic2\n            else:\n                break\n\n        weights[i] = weights[i_swap]\n        indices[i] = indices[i_swap]\n        is_new[i] = is_new[i_swap]\n\n        i = i_swap\n\n    weights[i] = weight\n    indices[i] = index\n    is_new[i] = flag\n\n    return 1", "response": "Push a new element onto the heap."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef euclidean_random_projection_split(data, indices, rng_state):\n    dim = data.shape[1]\n\n    # Select two random points, set the hyperplane between them\n    left_index = tau_rand_int(rng_state) % indices.shape[0]\n    right_index = tau_rand_int(rng_state) % indices.shape[0]\n    right_index += left_index == right_index\n    right_index = right_index % indices.shape[0]\n    left = indices[left_index]\n    right = indices[right_index]\n\n    # Compute the normal vector to the hyperplane (the vector between\n    # the two points) and the offset from the origin\n    hyperplane_offset = 0.0\n    hyperplane_vector = np.empty(dim, dtype=np.float32)\n\n    for d in range(dim):\n        hyperplane_vector[d] = data[left, d] - data[right, d]\n        hyperplane_offset -= (\n            hyperplane_vector[d] * (data[left, d] + data[right, d]) / 2.0\n        )\n\n    # For each point compute the margin (project into normal vector, add offset)\n    # If we are on lower side of the hyperplane put in one pile, otherwise\n    # put it in the other pile (if we hit hyperplane on the nose, flip a coin)\n    n_left = 0\n    n_right = 0\n    side = np.empty(indices.shape[0], np.int8)\n    for i in range(indices.shape[0]):\n        margin = hyperplane_offset\n        for d in range(dim):\n            margin += hyperplane_vector[d] * data[indices[i], d]\n\n        if margin == 0:\n            side[i] = tau_rand_int(rng_state) % 2\n            if side[i] == 0:\n                n_left += 1\n            else:\n                n_right += 1\n        elif margin > 0:\n            side[i] = 0\n            n_left += 1\n        else:\n            side[i] = 1\n            n_right += 1\n\n    # Now that we have the counts allocate arrays\n    indices_left = np.empty(n_left, dtype=np.int64)\n    indices_right = np.empty(n_right, dtype=np.int64)\n\n    # Populate the arrays with indices according to which side they fell on\n    n_left = 0\n    n_right = 0\n    for i in range(side.shape[0]):\n        if side[i] == 0:\n            indices_left[n_left] = indices[i]\n            n_left += 1\n        else:\n            indices_right[n_right] = indices[i]\n            n_right += 1\n\n    return indices_left, indices_right, hyperplane_vector, hyperplane_offset", "response": "This function creates a random hyperplane that splits the data into two nodes and returns the indices of the nodes that fall on the left side of the hyperplane and the offset from the origin."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nestimate the autocorrelation function of a time series using the FFT.", "response": "def get_acf(x, axis=0, fast=False):\n    \"\"\"\n    Estimate the autocorrelation function of a time series using the FFT.\n\n    :param x:\n        The time series. If multidimensional, set the time axis using the\n        ``axis`` keyword argument and the function will be computed for every\n        other axis.\n\n    :param axis: (optional)\n        The time axis of ``x``. Assumed to be the first axis if not specified.\n\n    :param fast: (optional)\n        If ``True``, only use the largest ``2^n`` entries for efficiency.\n        (default: False)\n\n    \"\"\"\n    x = np.atleast_1d(x)\n    m = [slice(None), ] * len(x.shape)\n\n    # For computational efficiency, crop the chain to the largest power of\n    # two if requested.\n    if fast:\n        n = int(2 ** np.floor(np.log2(x.shape[axis])))\n        m[axis] = slice(0, n)\n        x = x\n    else:\n        n = x.shape[axis]\n\n    # Compute the FFT and then (from that) the auto-correlation function.\n    f = np.fft.fft(x - np.mean(x, axis=axis), n=2 * n, axis=axis)\n    m[axis] = slice(0, n)\n    acf = np.fft.ifft(f * np.conjugate(f), axis=axis)[tuple(m)].real\n    m[axis] = 0\n    return acf / acf[tuple(m)]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nestimating the integrated autocorrelation time of a time series.", "response": "def get_integrated_act(x, axis=0, window=50, fast=False):\n    \"\"\"\n    Estimate the integrated autocorrelation time of a time series.\n\n    See `Sokal's notes <http://www.stat.unc.edu/faculty/cji/Sokal.pdf>`_ on\n    MCMC and sample estimators for autocorrelation times.\n\n    :param x:\n        The time series. If multidimensional, set the time axis using the\n        ``axis`` keyword argument and the function will be computed for every\n        other axis.\n\n    :param axis: (optional)\n        The time axis of ``x``. Assumed to be the first axis if not specified.\n\n    :param window: (optional)\n        The size of the window to use. (default: 50)\n\n    :param fast: (optional)\n        If ``True``, only use the largest ``2^n`` entries for efficiency.\n        (default: False)\n\n    \"\"\"\n    # Compute the autocorrelation function.\n    f = get_acf(x, axis=axis, fast=fast)\n\n    # Special case 1D for simplicity.\n    if len(f.shape) == 1:\n        return 1 + 2 * np.sum(f[1:window])\n\n    # N-dimensional case.\n    m = [slice(None), ] * len(f.shape)\n    m[axis] = slice(1, window)\n    tau = 1 + 2 * np.sum(f[tuple(m)], axis=axis)\n\n    return tau"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef thermodynamic_integration_log_evidence(betas, logls):\n    if len(betas) != len(logls):\n        raise ValueError('Need the same number of log(L) values as temperatures.')\n\n    order = np.argsort(betas)[::-1]\n    betas = betas[order]\n    logls = logls[order]\n\n    betas0 = np.copy(betas)\n    if betas[-1] != 0:\n        betas = np.concatenate((betas0, [0]))\n        betas2 = np.concatenate((betas0[::2], [0]))\n\n        # Duplicate mean log-likelihood of hottest chain as a best guess for beta = 0.\n        logls2 = np.concatenate((logls[::2], [logls[-1]]))\n        logls = np.concatenate((logls, [logls[-1]]))\n    else:\n        betas2 = np.concatenate((betas0[:-1:2], [0]))\n        logls2 = np.concatenate((logls[:-1:2], [logls[-1]]))\n\n    logZ = -np.trapz(logls, betas)\n    logZ2 = -np.trapz(logls2, betas2)\n    return logZ, np.abs(logZ - logZ2)", "response": "This function calculates the thermodynamic integration estimate of the evidence."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_authorization_header(request):\n    auth = request.META.get('HTTP_AUTHORIZATION', b'')\n\n    if isinstance(auth, type('')):\n        # Work around django test client oddness\n        auth = auth.encode('iso-8859-1')\n\n    return auth", "response": "Returns the Authorization header as a bytestring."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef authenticate_credentials(self, payload):\n        try:\n            user_id = jwt_get_user_id_from_payload(payload)\n\n            if user_id:\n                user = User.objects.get(pk=user_id, is_active=True)\n            else:\n                msg = 'Invalid payload'\n                raise exceptions.AuthenticationFailed(msg)\n        except User.DoesNotExist:\n            msg = 'Invalid signature'\n            raise exceptions.AuthenticationFailed(msg)\n\n        return user", "response": "Authenticate the payload and return an active user that matches the user id and email."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a set of transactions find all patterns that are under the specified support threshold.", "response": "def find_frequent_patterns(transactions, support_threshold):\n    \"\"\"\n    Given a set of transactions, find the patterns in it\n    over the specified support threshold.\n    \"\"\"\n    tree = FPTree(transactions, support_threshold, None, None)\n    return tree.mine_patterns(support_threshold)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a set of frequent itemsets return a dict of association rules in the form where the left and right items are the same.", "response": "def generate_association_rules(patterns, confidence_threshold):\n    \"\"\"\n    Given a set of frequent itemsets, return a dict\n    of association rules in the form\n    {(left): ((right), confidence)}\n    \"\"\"\n    rules = {}\n    for itemset in patterns.keys():\n        upper_support = patterns[itemset]\n\n        for i in range(1, len(itemset)):\n            for antecedent in itertools.combinations(itemset, i):\n                antecedent = tuple(sorted(antecedent))\n                consequent = tuple(sorted(set(itemset) - set(antecedent)))\n\n                if antecedent in patterns:\n                    lower_support = patterns[antecedent]\n                    confidence = float(upper_support) / lower_support\n\n                    if confidence >= confidence_threshold:\n                        rules[antecedent] = (consequent, confidence)\n\n    return rules"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if a node has a particular child.", "response": "def has_child(self, value):\n        \"\"\"\n        Check if node has a particular child node.\n        \"\"\"\n        for node in self.children:\n            if node.value == value:\n                return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_child(self, value):\n        for node in self.children:\n            if node.value == value:\n                return node\n\n        return None", "response": "Return a child node with a particular value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a child node.", "response": "def add_child(self, value):\n        \"\"\"\n        Add a node as a child node.\n        \"\"\"\n        child = FPNode(value, 1, self)\n        self.children.append(child)\n        return child"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding items with occurrences above the threshold.", "response": "def find_frequent_items(transactions, threshold):\n        \"\"\"\n        Create a dictionary of items with occurrences above the threshold.\n        \"\"\"\n        items = {}\n\n        for transaction in transactions:\n            for item in transaction:\n                if item in items:\n                    items[item] += 1\n                else:\n                    items[item] = 1\n\n        for key in list(items.keys()):\n            if items[key] < threshold:\n                del items[key]\n\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds the FP tree and return the root node.", "response": "def build_fptree(self, transactions, root_value,\n                     root_count, frequent, headers):\n        \"\"\"\n        Build the FP tree and return the root node.\n        \"\"\"\n        root = FPNode(root_value, root_count, None)\n\n        for transaction in transactions:\n            sorted_items = [x for x in transaction if x in frequent]\n            sorted_items.sort(key=lambda x: frequent[x], reverse=True)\n            if len(sorted_items) > 0:\n                self.insert_tree(sorted_items, root, headers)\n\n        return root"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef insert_tree(self, items, node, headers):\n        first = items[0]\n        child = node.get_child(first)\n        if child is not None:\n            child.count += 1\n        else:\n            # Add new child.\n            child = node.add_child(first)\n\n            # Link it to header structure.\n            if headers[first] is None:\n                headers[first] = child\n            else:\n                current = headers[first]\n                while current.link is not None:\n                    current = current.link\n                current.link = child\n\n        # Call function recursively.\n        remaining_items = items[1:]\n        if len(remaining_items) > 0:\n            self.insert_tree(remaining_items, child, headers)", "response": "Insert a new item into the FP tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if there is a single path in the tree.", "response": "def tree_has_single_path(self, node):\n        \"\"\"\n        If there is a single path in the tree,\n        return True, else return False.\n        \"\"\"\n        num_children = len(node.children)\n        if num_children > 1:\n            return False\n        elif num_children == 0:\n            return True\n        else:\n            return True and self.tree_has_single_path(node.children[0])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmining the FP tree for frequent patterns.", "response": "def mine_patterns(self, threshold):\n        \"\"\"\n        Mine the constructed FP tree for frequent patterns.\n        \"\"\"\n        if self.tree_has_single_path(self.root):\n            return self.generate_pattern_list()\n        else:\n            return self.zip_patterns(self.mine_sub_trees(threshold))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nzips the patterns in a dictionary.", "response": "def zip_patterns(self, patterns):\n        \"\"\"\n        Append suffix to patterns in dictionary if\n        we are in a conditional FP tree.\n        \"\"\"\n        suffix = self.root.value\n\n        if suffix is not None:\n            # We are in a conditional tree.\n            new_patterns = {}\n            for key in patterns.keys():\n                new_patterns[tuple(sorted(list(key) + [suffix]))] = patterns[key]\n\n            return new_patterns\n\n        return patterns"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a list of patterns with support counts.", "response": "def generate_pattern_list(self):\n        \"\"\"\n        Generate a list of patterns with support counts.\n        \"\"\"\n        patterns = {}\n        items = self.frequent.keys()\n\n        # If we are in a conditional tree,\n        # the suffix is a pattern on its own.\n        if self.root.value is None:\n            suffix_value = []\n        else:\n            suffix_value = [self.root.value]\n            patterns[tuple(suffix_value)] = self.root.count\n\n        for i in range(1, len(items) + 1):\n            for subset in itertools.combinations(items, i):\n                pattern = tuple(sorted(list(subset) + suffix_value))\n                patterns[pattern] = \\\n                    min([self.frequent[x] for x in subset])\n\n        return patterns"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating subtrees and mine them for patterns.", "response": "def mine_sub_trees(self, threshold):\n        \"\"\"\n        Generate subtrees and mine them for patterns.\n        \"\"\"\n        patterns = {}\n        mining_order = sorted(self.frequent.keys(),\n                              key=lambda x: self.frequent[x])\n\n        # Get items in tree in reverse order of occurrences.\n        for item in mining_order:\n            suffixes = []\n            conditional_tree_input = []\n            node = self.headers[item]\n\n            # Follow node links to get a list of\n            # all occurrences of a certain item.\n            while node is not None:\n                suffixes.append(node)\n                node = node.link\n\n            # For each occurrence of the item, \n            # trace the path back to the root node.\n            for suffix in suffixes:\n                frequency = suffix.count\n                path = []\n                parent = suffix.parent\n\n                while parent.parent is not None:\n                    path.append(parent.value)\n                    parent = parent.parent\n\n                for i in range(frequency):\n                    conditional_tree_input.append(path)\n\n            # Now we have the input for a subtree,\n            # so construct it and grab the patterns.\n            subtree = FPTree(conditional_tree_input, threshold,\n                             item, self.frequent[item])\n            subtree_patterns = subtree.mine_patterns(threshold)\n\n            # Insert subtree patterns into main patterns dictionary.\n            for pattern in subtree_patterns.keys():\n                if pattern in patterns:\n                    patterns[pattern] += subtree_patterns[pattern]\n                else:\n                    patterns[pattern] = subtree_patterns[pattern]\n\n        return patterns"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _is_videofile(self, f):\n        if os.path.isfile(f):\n            types = mimetypes.guess_type(f)\n            mtype = types[0]\n            if (mtype and mtype.split('/')[0] == 'video') or (os.path.splitext(f)[1] in self.VIDEO_EXTS):\n                return True\n        return False", "response": "Returns True if f is a video file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _filter_path(self, path):\n        if self._is_videofile(path):\n            return [path, ]\n\n        if os.path.isdir(path):\n            result = []\n            for root, dirs, files in os.walk(path):\n                result.extend(filter(self._is_videofile, map(\n                    lambda f: os.path.join(root, f), files)))\n            return result\n        else:\n            return []", "response": "Filter path to only video files"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _download(self, videofile):\n        basename = os.path.basename(videofile)\n\n        subinfos = []\n        for subsearcher in self.subsearcher:\n            self.logger.info(\n                '{0}\uff1a\u5f00\u59cb\u4f7f\u7528 {1} \u641c\u7d22\u5b57\u5e55'.format(basename, subsearcher))\n            try:\n                subinfos = subsearcher.search_subs(\n                    videofile, self.languages, self.exts)\n            except Exception as e:\n                err = str(e)\n                if self.debug:\n                    err = traceback.format_exc()\n                self.logger.error(\n                    '{}\uff1a\u641c\u7d22\u5b57\u5e55\u53d1\u751f\u9519\u8bef\uff1a {}'.format(basename, err))\n                continue\n            if subinfos:\n                break\n        self.logger.info('{1}\uff1a\u627e\u5230 {0} \u4e2a\u5b57\u5e55, \u51c6\u5907\u4e0b\u8f7d'.format(\n            len(subinfos), basename))\n        try:\n            for subinfo in subinfos:\n                downloaded = subinfo.get('downloaded', False)\n                if downloaded:\n                    if isinstance(subinfo['subname'], (list, tuple)):\n                        self._history[videofile].extend(subinfo['subname'])\n                    else:\n                        self._history[videofile].append(subinfo['subname'])\n                else:\n                    link = subinfo.get('link')\n                    subname = subinfo.get('subname')\n                    subpath = os.path.join(os.path.dirname(videofile), subname)\n                    res = self.session.get(link, stream=True)\n                    with open(subpath, 'wb') as fp:\n                        for chunk in res.iter_content(8192):\n                            fp.write(chunk)\n                    self._history[videofile].append(subpath)\n        except Exception as e:\n            self.logger.error(str(e))", "response": "Download a video file from SubSearcher."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstart the sub finder.", "response": "def start(self):\n        \"\"\" SubFinder \u5165\u53e3\uff0c\u5f00\u59cb\u51fd\u6570\n        \"\"\"\n        self.logger.info('\u5f00\u59cb')\n        videofiles = self._filter_path(self.path)\n        l = len(videofiles)\n        if l == 0:\n            self.logger.info(\n                '\u5728 {} \u4e0b\u6ca1\u6709\u53d1\u73b0\u89c6\u9891\u6587\u4ef6'.format(self.path))\n            return\n        else:\n            self.logger.info('\u627e\u5230 {} \u4e2a\u89c6\u9891\u6587\u4ef6'.format(l))\n\n        for f in videofiles:\n            self._history[f] = []\n            self.pool.spawn(self._download, f)\n        self.pool.join()\n        self.logger.info('='*20 + '\u4e0b\u8f7d\u5b8c\u6210' + '='*20)\n        for v, subs in self._history.items():\n            basename = os.path.basename(v)\n            self.logger.info(\n                '{}: \u4e0b\u8f7d {} \u4e2a\u5b57\u5e55'.format(basename, len(subs)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating filename of subtitles", "response": "def _gen_subname(videofile, language, ext):\n        \"\"\" generate filename of subtitles\n        :TODO: fix the conflict of subname\n        \"\"\"\n        root, basename = os.path.split(videofile)\n        name, _ = os.path.splitext(basename)\n        subname = '{basename}.{language}.{ext}'.format(\n            basename=name,\n            language=language,\n            ext=ext)\n        return subname"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_video_hash(videofile):\n        seek_positions = [None] * 4\n        hash_result = []\n        with open(videofile, 'rb') as fp:\n            total_size = os.fstat(fp.fileno()).st_size\n            if total_size < 8192 + 4096:\n                raise exceptions.InvalidFileError(\n                    'the video[{}] is too small'.format(os.path.basename(videofile)))\n\n            seek_positions[0] = 4096\n            seek_positions[1] = total_size // 3 * 2\n            seek_positions[2] = total_size // 3\n            seek_positions[3] = total_size - 8192\n            for pos in seek_positions:\n                fp.seek(pos, 0)\n                data = fp.read(4096)\n                m = hashlib.md5(data)\n                hash_result.append(m.hexdigest())\n        return ';'.join(hash_result)", "response": "compute videofile s hash\n            reference"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting all subtitles in path recursively", "response": "def rm_subtitles(path):\n    \"\"\" delete all subtitles in path recursively\n    \"\"\"\n    sub_exts = ['ass', 'srt', 'sub']\n    count = 0\n    for root, dirs, files in os.walk(path):\n        for f in files:\n            _, ext = os.path.splitext(f)\n            ext = ext[1:]\n            if ext in sub_exts:\n                p = os.path.join(root, f)\n                count += 1\n                print('Delete {}'.format(p))\n                os.remove(p)\n    return count"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmove videos in sub - directory of path to path.", "response": "def mv_videos(path):\n    \"\"\" move videos in sub-directory of path to path.\n    \"\"\"\n    count = 0\n    for f in os.listdir(path):\n        f = os.path.join(path, f)\n        if os.path.isdir(f):\n            for sf in os.listdir(f):\n                sf = os.path.join(f, sf)\n                if os.path.isfile(sf):\n                    new_name = os.path.join(path, os.path.basename(sf))\n                    try:\n                        os.rename(sf, new_name)\n                    except (WindowsError, OSError) as e:\n                        print('mv {} happens error: {}'.format(sf, e))\n                    else:\n                        count += 1\n                        print('mv {} to {}'.format(sf, new_name))\n    return count"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rename_subtitle(source, template):\n    root = os.path.dirname(source)\n    old_name = os.path.basename(source)\n    old_name, ext = os.path.splitext(old_name)\n    ext = ext[1:]\n    info = BaseSubSearcher._parse_videoname(old_name)\n    s = info['season']\n    e = info['episode']\n\n    _, language = os.path.splitext(old_name)\n    language = language[1:]\n    if language == '':\n        language = 'UNKNOWN'\n\n    new_name = template.format(season=s, episode=e, language=language, ext=ext)\n    new_file = os.path.join(root, new_name)\n    os.rename(source, new_file)\n    return new_file", "response": "\u91cd\u547d\u540d\u5b57\u5e55\u6587\u4ef6\n    @param source: \u8981\u91cd\u547d\u540d\u7684\u5b57\u5e55\u6587\u4ef6\n    @param template: \u6a21\u677f\uff0c\u4f8b\u5982\uff1a'Friends.S{season:02d}.E{episode:02d}.1080p.5.1Ch.BluRay.ReEnc-DeeJayAhmed.{}.{ext}'\n    \u652f\u6301\u7684\u53d8\u91cf\u6709\uff1aseason, episode, language, ext"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_downloadcount(self, text):\n        unit_map = {\n            '\u5343': 1000,\n            '\u4e07': 10000,\n            '\u767e\u4e07': 1000000,\n        }\n\n        m = re.match(r'^(\\d+(?:\\.\\d+)?)(\\w{0,2})$', text, re.UNICODE)\n        if m:\n            n = float(m.group(1))\n            u = m.group(2)\n            u = unit_map.get(u, 1)\n            return int(n * u)\n        else:\n            return 0", "response": "parse download count\n            text format maybe return download count"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_search_results_html(self, doc):\n        subgroups = []\n        soup = bs4.BeautifulSoup(doc, 'lxml')\n        ele_divs = soup.select('div.item.prel')\n        if not ele_divs:\n            return subgroups\n        for item in ele_divs:\n            ele_a = item.select('p.tt > a')\n            if not ele_a:\n                continue\n            link = ele_a[0].get('href')\n            title = ele_a[0].get_text().strip()\n            subgroups.append({\n                'title': title,\n                'link': link,\n            })\n        return subgroups", "response": "parse search result html return list of subgroups"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_subinfo_list(self, videoname):\n        # searching subtitles\n        res = self.session.get(self.API, params={'q': videoname})\n        doc = res.content\n        referer = res.url\n        subgroups = self._parse_search_results_html(doc)\n        if not subgroups:\n            return []\n        subgroup = self._filter_subgroup(subgroups)\n\n        # get subtitles\n        headers = {\n            'Referer': referer\n        }\n        res = self.session.get(self._join_url(self.API, subgroup['link']), headers=headers)\n        doc = res.content\n        referer = res.url\n        subinfo_list = self._parse_sublist_html(doc)\n        for subinfo in subinfo_list:\n            subinfo['link'] = self._join_url(res.url, subinfo['link'])\n        return subinfo_list, referer", "response": "get subinfo list of videoname"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the search result html.", "response": "def _parse_search_result_html(self, doc):\n        \"\"\"\n        \u89e3\u6790\u641c\u7d22\u7ed3\u679c\u9875\u9762\uff0c\u8fd4\u56de\u5b57\u5e55\u4fe1\u606f\u5217\u8868\n        \"\"\"\n        result = []\n        soup = bs4.BeautifulSoup(doc, 'lxml')\n        search_item_div_list = soup.select(\n            '.search-result > ul > li > .search-item')\n        if not search_item_div_list:\n            return []\n        for item in search_item_div_list:\n            subinfo = {\n                'title': '',\n                'link': '',\n                'author': 'zimuzu',\n                'exts': [],\n                'languages': [],\n                'rate': 0,\n                'download_count': 0,\n            }\n            a = item.find('a')\n            if not a:\n                continue\n            url = a.get('href')\n            subinfo['link'] = self._join_url(self.API, url)\n            zh_title = a.get_text()\n            for k, v in self.LANGUAGES_MAP.items():\n                if k in zh_title:\n                    subinfo['languages'].append(v)\n            font = item.find('font', class_=\"f4\")\n            en_title = font.string\n            subinfo['title'] = en_title\n            result.append(subinfo)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_detailpage_html(self, doc):\n        result = {\n            'exts': [],\n            'downloadpage_link': ''\n        }\n        soup = bs4.BeautifulSoup(doc, 'lxml')\n        li_list = soup.select('ul.subtitle-info > li')\n        for li in li_list:\n            if li.string and '\u3010\u683c\u5f0f\u3011' in li.string:\n                s = li.string.lower()\n                for ext in self.SUPPORT_EXTS:\n                    if ext in s:\n                        result['exts'].append(ext)\n\n        a = soup.select('.subtitle-links > a')\n        if a:\n            a = a[0]\n            result['downloadpage_link'] = a.get('href')\n\n        return result", "response": "Parse detail page html."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_downloadpage_html(self, doc):\n        soup = bs4.BeautifulSoup(doc, 'lxml')\n        a = soup.select('.download-box > a.btn-click')\n        if a:\n            a = a[0]\n            link = a.get('href')\n            return link\n        return ''", "response": "Parse the download page html."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets subinfo list from a keyword.", "response": "def _get_subinfo_list(self, keyword):\n        \"\"\"\u6839\u636e\u5173\u952e\u8bcd\u641c\u7d22\uff0c\u8fd4\u56de\u5b57\u5e55\u4fe1\u606f\u5217\u8868\n        \"\"\"\n        res = self.session.get(\n            self.API, params={'keyword': keyword, 'search_type': 'subtitle'})\n        doc = res.content\n        referer = res.url\n        subinfo_list = self._parse_search_result_html(doc)\n\n        return subinfo_list, referer"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _visit_detailpage(self, detailpage_link, referer):\n        headers = {'Referer': referer}\n        res = self.session.get(detailpage_link, headers=headers)\n        referer = res.url\n        doc = res.content\n        result = self._parse_detailpage_html(doc)\n        return result, referer", "response": "Visit detailpage_link and parse it into a dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the keyword from the videoinfo.", "response": "def _get_keyword(self, videoinfo):\n        \"\"\" \u83b7\u53d6\u5173\u952e\u8bcd\n        \"\"\"\n        keyword = videoinfo.get('title')\n        if videoinfo['season'] != 0:\n            keyword += '.S{:02d}'.format(videoinfo['season'])\n        if videoinfo['episode'] != 0:\n            keyword += '.E{:02d}'.format(videoinfo['episode'])\n        # replace space with '+'\n        keyword = re.sub(r'\\s+', '+', keyword)\n        return keyword"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nregisters a subsearcher, the `name` is a key used for searching subsearchers. if the subsearcher named `name` already exists, then it's will overrite the old subsearcher.", "response": "def register_subsearcher(name, subsearcher_cls):\n    \"\"\" register a subsearcher, the `name` is a key used for searching subsearchers.\n    if the subsearcher named `name` already exists, then it's will overrite the old subsearcher.\n    \"\"\"\n    if not issubclass(subsearcher_cls, BaseSubSearcher):\n        raise ValueError(\n            '{} is not a subclass of BaseSubSearcher'.format(subsearcher_cls))\n    registered_subsearchers[name] = subsearcher_cls"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_videoname(cls, videofile):\n        name = os.path.basename(videofile)\n        name = os.path.splitext(name)[0]\n        return name", "response": "parse the videofile and return it s basename"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the videoname and return a dict of the relevant information.", "response": "def _parse_videoname(cls, videoname):\n        \"\"\" parse videoname and return video info dict\n        video info contains:\n        - title, the name of video\n        - sub_title, the sub_title of video\n        - resolution,\n        - source,\n        -\n        - season, defaults to 0\n        - episode, defaults to 0\n        \"\"\"\n        info = {\n            'title': '',\n            'season': 0,\n            'episode': 0,\n            'sub_title': '',\n            'resolution': '',\n            'source': '',\n            'audio_encoding': '',\n            'video_encoding': '',\n        }\n        last_index = 0\n        m = cls.RE_SEASON_EPISODE.search(videoname)\n        if m:\n            info['season'] = int(m.group('season'))\n            info['episode'] = int(m.group('episode'))\n            s, e = m.span()\n            info['title'] = videoname[0:s].strip('.')\n            last_index = e\n        else:\n            m = cls.RE_SEASON.search(videoname)\n            if m:\n                info['season'] = int(m.group('season'))\n                s, e = m.span()\n                info['title'] = videoname[0:s].strip('.')\n                last_index = e\n\n        m = cls.RE_RESOLUTION.search(videoname)\n        if m:\n            info['resolution'] = m.group('resolution')\n            s, e = m.span()\n            if info['title'] == '':\n                info['title'] = videoname[0:s].strip('.')\n\n            if info['season'] > 0 and info['episode'] > 0:\n                info['sub_title'] = videoname[last_index:s].strip('.')\n            last_index = e\n\n        if info['title'] == '':\n            info['title'] = videoname\n\n        m = cls.RE_SOURCE.search(videoname)\n        if m:\n            info['source'] = m.group('source')\n\n        m = cls.RE_AUDIO_ENC.search(videoname)\n        if m:\n            info['audio_encoding'] = m.group('audio_encoding')\n\n        m = cls.RE_VIDEO_ENC.search(videoname)\n        if m:\n            info['video_encoding'] = m.group('video_encoding')\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _extract(cls, compressed_file, videofile, exts):\n        if not CompressedFile.is_compressed_file(compressed_file):\n            return [compressed_file]\n\n        root = os.path.dirname(compressed_file)\n        subs = []\n        cf = CompressedFile(compressed_file)\n        for name in cf.namelist():\n            if cf.isdir(name):\n                continue\n            # make `name` to unicode string\n            orig_name = CompressedFile.decode_file_name(name)\n            _, ext = os.path.splitext(orig_name)\n            ext = ext[1:]\n            if ext not in exts:\n                continue\n            subname = cls._gen_subname(videofile, '', '', orig_name=orig_name)\n            subpath = os.path.join(root, subname)\n            cf.extract(name, subpath)\n            subs.append(subpath)\n        cf.close()\n        return subs", "response": "Extract a file into a list of files."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _filter_subinfo_list(cls, subinfo_list, videoinfo, languages, exts):\n        filter_field_list = [\n            'season',\n            'episode',\n            'resolution',\n            'source',\n            'video_encoding',\n            'audio_encoding'\n        ]\n        filtered_subinfo_list = dict((f, []) for f in filter_field_list)\n        \n        for subinfo in subinfo_list:\n            title = subinfo.get('title')\n            videoinfo_ = cls._parse_videoname(title)\n            last_field = None\n            for field in filter_field_list:\n                if videoinfo.get(field) == videoinfo_.get(field):\n                    last_field = field\n                else:\n                    break\n            if last_field is not None:\n                filtered_subinfo_list[last_field].append(subinfo)\n        for field in filter_field_list[::-1]:\n            if len(filtered_subinfo_list[field]) > 0:\n                # sort by download_count and rate\n                sorted_subinfo_list = sorted(filtered_subinfo_list[field],\n                                     key=lambda item: (\n                                         item['rate'], item['download_count']),\n                                     reverse=True)\n                return sorted_subinfo_list[0]\n        return None", "response": "filter out the subinfo list that contains only the ones that are in the same field as the videoinfo."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndownload subs from a video file.", "response": "def _download_subs(self, download_link, videofile, referer='', sub_title=''):\n        \"\"\" \u4e0b\u8f7d\u5b57\u5e55\n        videofile: \u89c6\u9891\u6587\u4ef6\u8def\u5f84\n        sub_title: \u5b57\u5e55\u6807\u9898\uff08\u6587\u4ef6\u540d\uff09\n        download_link: \u4e0b\u8f7d\u94fe\u63a5\n        referer: referer\n        \"\"\"\n        root = os.path.dirname(videofile)\n        name, _ = os.path.splitext(os.path.basename(videofile))\n        ext = ''\n\n        headers = {\n            'Referer': referer\n        }\n        res = self.session.get(download_link, headers=headers, stream=True)\n        referer = res.url\n\n        # \u5c1d\u8bd5\u4ece Content-Disposition \u4e2d\u83b7\u53d6\u6587\u4ef6\u540e\u7f00\u540d\n        content_disposition = res.headers.get('Content-Disposition', '')\n        if content_disposition:\n            _, params = cgi.parse_header(content_disposition)\n            filename = params.get('filename')\n            if filename:\n                _, ext = os.path.splitext(filename)\n                ext = ext[1:]\n\n        if ext == '':\n            # \u5c1d\u8bd5\u4eceurl \u4e2d\u83b7\u53d6\u6587\u4ef6\u540e\u7f00\u540d\n            p = urlparse.urlparse(res.url)\n            path = p.path\n            if path:\n                _, ext = os.path.splitext(path)\n                ext = ext[1:]\n\n        if ext == '':\n            # \u5c1d\u8bd5\u4ece\u5b57\u5e55\u6807\u9898\u4e2d\u83b7\u53d6\u6587\u4ef6\u540e\u7f00\u540d\n            _, ext = os.path.splitext(sub_title)\n            ext = ext[1:]\n\n        filename = '{}.{}'.format(name, ext)\n        filepath = os.path.join(root, filename)\n        with open(filepath, 'wb') as fp:\n            for chunk in res.iter_content(8192):\n                fp.write(chunk)\n\n        return filepath, referer"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate and return a connection proxy to the sqlite database.", "response": "def connect(\n    database: Union[str, Path], *, loop: asyncio.AbstractEventLoop = None, **kwargs: Any\n) -> Connection:\n    \"\"\"Create and return a connection proxy to the sqlite database.\"\"\"\n    if loop is None:\n        loop = asyncio.get_event_loop()\n\n    def connector() -> sqlite3.Connection:\n        if isinstance(database, str):\n            loc = database\n        elif isinstance(database, bytes):\n            loc = database.decode(\"utf-8\")\n        else:\n            loc = str(database)\n\n        return sqlite3.connect(loc, **kwargs)\n\n    return Connection(connector, loop)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute the given function on the shared connection s thread.", "response": "async def _execute(self, fn, *args, **kwargs):\n        \"\"\"Execute the given function on the shared connection's thread.\"\"\"\n        return await self._conn._execute(fn, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexecuting the given query.", "response": "async def execute(self, sql: str, parameters: Iterable[Any] = None) -> None:\n        \"\"\"Execute the given query.\"\"\"\n        if parameters is None:\n            parameters = []\n        await self._execute(self._cursor.execute, sql, parameters)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes the given multiquery.", "response": "async def executemany(self, sql: str, parameters: Iterable[Iterable[Any]]) -> None:\n        \"\"\"Execute the given multiquery.\"\"\"\n        await self._execute(self._cursor.executemany, sql, parameters)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def executescript(self, sql_script: str) -> None:\n        await self._execute(self._cursor.executescript, sql_script)", "response": "Execute a user script."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def fetchone(self) -> Optional[sqlite3.Row]:\n        return await self._execute(self._cursor.fetchone)", "response": "Fetch a single row."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch up to size number of rows.", "response": "async def fetchmany(self, size: int = None) -> Iterable[sqlite3.Row]:\n        \"\"\"Fetch up to `cursor.arraysize` number of rows.\"\"\"\n        args = ()  # type: Tuple[int, ...]\n        if size is not None:\n            args = (size,)\n        return await self._execute(self._cursor.fetchmany, *args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def fetchall(self) -> Iterable[sqlite3.Row]:\n        return await self._execute(self._cursor.fetchall)", "response": "Fetch all remaining rows."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting function calls on a separate thread.", "response": "def run(self) -> None:\n        \"\"\"Execute function calls on a separate thread.\"\"\"\n        while self._running:\n            try:\n                future, function = self._tx.get(timeout=0.1)\n            except Empty:\n                continue\n\n            try:\n                LOG.debug(\"executing %s\", function)\n                result = function()\n                LOG.debug(\"returning %s\", result)\n                self._loop.call_soon_threadsafe(future.set_result, result)\n            except BaseException as e:\n                LOG.exception(\"returning exception %s\", e)\n                self._loop.call_soon_threadsafe(future.set_exception, e)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def _execute(self, fn, *args, **kwargs):\n        function = partial(fn, *args, **kwargs)\n        future = self._loop.create_future()\n\n        self._tx.put_nowait((future, function))\n\n        return await future", "response": "Queue a function with the given arguments for execution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconnecting to the actual sqlite database.", "response": "async def _connect(self) -> \"Connection\":\n        \"\"\"Connect to the actual sqlite database.\"\"\"\n        if self._connection is None:\n            self._connection = await self._execute(self._connector)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an aiosqlite cursor wrapping a sqlite3 cursor object.", "response": "async def cursor(self) -> Cursor:\n        \"\"\"Create an aiosqlite cursor wrapping a sqlite3 cursor object.\"\"\"\n        return Cursor(self, await self._execute(self._conn.cursor))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def close(self) -> None:\n        await self._execute(self._conn.close)\n        self._running = False\n        self._connection = None", "response": "Complete queued queries/cursors and close the connection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def execute_fetchall(\n        self, sql: str, parameters: Iterable[Any] = None\n    ) -> Iterable[sqlite3.Row]:\n        \"\"\"Helper to execute a query and return all the data.\"\"\"\n        if parameters is None:\n            parameters = []\n        return await self._execute(self._execute_fetchall, sql, parameters)", "response": "Helper to execute a query and return all the data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def executemany(\n        self, sql: str, parameters: Iterable[Iterable[Any]]\n    ) -> Cursor:\n        \"\"\"Helper to create a cursor and execute the given multiquery.\"\"\"\n        cursor = await self._execute(self._conn.executemany, sql, parameters)\n        return Cursor(self, cursor)", "response": "Helper to create a cursor and execute the given multiquery."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef human_time(seconds):\n\t''' DocTests:\n\t>>> human_time(0) == ''\n\tTrue\n\t>>> human_time(122.1) == '2m2s'\n\tTrue\n\t>>> human_time(133) == '2m13s'\n\tTrue\n\t>>> human_time(12345678) == '20W2D21h21m18s'\n\tTrue\n\t'''\n\tisec = int(seconds)\n\ts = isec % 60\n\tm = isec // 60 % 60\n\th = isec // 60 // 60 % 24\n\td = isec // 60 // 60 // 24 % 7\n\tw = isec // 60 // 60 // 24 // 7\n\n\tresult = ''\n\tfor t in [ ('W', w), ('D', d), ('h', h), ('m', m), ('s', s) ]:\n\t\tif t[1]:\n\t\t\tresult += str(t[1]) + t[0]\n\n\treturn result", "response": "Return a human - readable string of the given time."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef limit_unit(timestr, num = 2):\n\t''' DocTests:\n\t>>> limit_unit('1m2s', 1) == '1m'\n\tTrue\n\t>>> limit_unit('1m2s') == '1m2s'\n\tTrue\n\t>>> limit_unit('1m2s', 4) == '1m2s'\n\tTrue\n\t>>> limit_unit('1d2h3m2s') == '1d2h'\n\tTrue\n\t>>> limit_unit('1d2h3m2s', 1) == '1d'\n\tTrue\n\t'''\n\tl = len(timestr)\n\ti = 0\n\tp = 0\n\twhile i < num and p <= l:\n\t\tat = 0\n\t\twhile p < l:\n\t\t\tc = timestr[p]\n\t\t\tif at == 0:\n\t\t\t\tif c.isdigit():\n\t\t\t\t\tp += 1\n\t\t\t\telse:\n\t\t\t\t\tat += 1\n\t\t\telif at == 1:\n\t\t\t\tif not c.isdigit():\n\t\t\t\t\tp += 1\n\t\t\t\telse:\n\t\t\t\t\tat += 1\n\t\t\telse:\n\t\t\t\tbreak\n\n\t\ti += 1\n\n\treturn timestr[:p]", "response": "Limit the number of units in a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef interpret_size(si):\n\t'''\n\t>>> interpret_size(10)\n\t10\n\t>>> interpret_size('10')\n\t10\n\t>>> interpret_size('10b')\n\t10\n\t>>> interpret_size('10k')\n\t10240\n\t>>> interpret_size('10K')\n\t10240\n\t>>> interpret_size('10kb')\n\t10240\n\t>>> interpret_size('10kB')\n\t10240\n\t>>> interpret_size('a10')\n\tTraceback (most recent call last):\n\tValueError\n\t>>> interpret_size('10a')\n\tTraceback (most recent call last):\n\tKeyError: 'A'\n\t'''\n\tm = re.match(r\"\\s*(\\d+)\\s*([ac-z]?)(b?)\\s*$\", str(si), re.I)\n\tif m:\n\t\tif not m.group(2) and m.group(3):\n\t\t\ttimes = 1\n\t\telse:\n\t\t\ttimes = const.SIPrefixTimes[m.group(2).upper()] if m.group(2) else 1\n\t\treturn int(m.group(1)) * times\n\telse:\n\t\traise ValueError", "response": "Interpret the size of the block."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unzip(self, remotepath, subpath = '/', start = 0, limit = 1000):\n\t\t''' Usage: unzip <remotepath> [<subpath> [<start> [<limit>]]]'''\n\t\trpath = get_pcs_path(remotepath)\n\t\treturn self.__panapi_unzip_file(rpath, subpath, start, limit);", "response": "Unzips a file from a remote path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract(self, remotepath, subpath, saveaspath = None):\n\t\t''' Usage: extract <remotepath> <subpath> [<saveaspath>]'''\n\t\trpath = get_pcs_path(remotepath)\n\t\ttopath = get_pcs_path(saveaspath)\n\t\tif not saveaspath:\n\t\t\ttopath = os.path.dirname(rpath) + '/' + subpath\n\t\treturn self.__panapi_unzipcopy_file(rpath, subpath, topath)", "response": "Extract a file from remotepath to subpath."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrevert a revision of a remote file.", "response": "def revert(self, remotepath, revision, dir = None):\n\t\t''' Usage: revert <remotepath> revisionid [dir]'''\n\t\trpath = get_pcs_path(remotepath)\n\t\tdir = get_pcs_path(dir)\n\t\tif not dir:\n\t\t\tdir = os.path.dirname(rpath)\n\t\treturn self.__panapi_revision_revert(rpath, revision, dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main(argv=None): # IGNORE:C0111\n\t''' Main Entry '''\n\n\tby = None\n\treqres = check_requirements()\n\tif reqres == CheckResult.Error:\n\t\tperr(\"Requirement checking failed\")\n\t\tsys.exit(const.EFatal)\n\n\ttry:\n\t\tresult = const.ENoError\n\t\tif argv is None:\n\t\t\targv = sys.argv\n\t\telse:\n\t\t\tsys.argv.extend(argv)\n\n\t\tsetuphandlers()\n\n\t\tparser = getparser()\n\t\targs = parser.parse_args()\n\t\tdl_args = ''\n\t\tif not args.downloader_args:\n\t\t\tif const.DownloaderArgsEnvKey in os.environ:\n\t\t\t\tdl_args = os.environ[const.DownloaderArgsEnvKey]\n\t\telse:\n\t\t\tprefixlen = len(const.DownloaderArgsIsFilePrefix)\n\t\t\tif args.downloader_args[:prefixlen] == const.DownloaderArgsIsFilePrefix: # file\n\t\t\t\twith io.open(args.downloader_args[prefixlen:], 'r', encoding = 'utf-8') as f:\n\t\t\t\t\tdl_args = f.read().strip()\n\t\t\telse:\n\t\t\t\tdl_args = args.downloader_args\n\n\t\t# house-keeping reminder\n\t\t# TODO: may need to move into ByPy for customized config dir\n\t\tif os.path.exists(const.HashCachePath):\n\t\t\tcachesize = getfilesize(const.HashCachePath)\n\t\t\tif cachesize > 10 * const.OneM or cachesize == -1:\n\t\t\t\tpwarn((\n\t\t\t\t   \"*** WARNING ***\\n\"\n\t\t\t\t   \"Hash Cache file '{0}' is very large ({1}).\\n\"\n\t\t\t\t   \"This may affect program's performance (high memory consumption).\\n\"\n\t\t\t\t   \"You can first try to run 'bypy.py cleancache' to slim the file.\\n\"\n\t\t\t\t   \"But if the file size won't reduce (this warning persists),\"\n\t\t\t\t   \" you may consider deleting / moving the Hash Cache file '{0}'\\n\"\n\t\t\t\t   \"*** WARNING ***\\n\\n\\n\").format(const.HashCachePath, human_size(cachesize)))\n\n\t\t# check for situations that require no ByPy object creation first\n\t\tif args.clean >= 1:\n\t\t\treturn clean_prog_files(args.clean, args.verbose, args.configdir)\n\n\t\t# some arguments need some processing\n\t\ttry:\n\t\t\tslice_size = interpret_size(args.slice)\n\t\texcept (ValueError, KeyError):\n\t\t\tperr(\"Error: Invalid slice size specified '{}'\".format(args.slice))\n\t\t\treturn const.EArgument\n\n\t\ttry:\n\t\t\tchunk_size = interpret_size(args.chunk)\n\t\texcept (ValueError, KeyError):\n\t\t\tperr(\"Error: Invalid slice size specified '{}'\".format(args.slice))\n\t\t\treturn const.EArgument\n\n\t\tif len(args.command) <= 0 or \\\n\t\t\t(len(args.command) == 1 and args.command[0].lower() == 'help'):\n\t\t\tparser.print_help()\n\t\t\treturn const.EArgument\n\t\telif len(args.command) == 2 and args.command[0].lower() == 'help':\n\t\t\tByPy.help(args.command[1])\n\t\t\treturn const.EArgument\n\t\telif args.command[0] in ByPy.__dict__: # dir(ByPy), dir(by)\n\t\t\t#timeout = args.timeout or None\n\n\t\t\tcached.usecache = not args.forcehash\n\t\t\tbypyopt = {\n\t\t\t\t'slice_size': slice_size,\n\t\t\t\t'dl_chunk_size': chunk_size,\n\t\t\t\t'verify': args.verify,\n\t\t\t\t'retry': args.retry,\n\t\t\t\t'timeout': args.timeout,\n\t\t\t\t'quit_when_fail': args.quit,\n\t\t\t\t'resumedownload': args.resumedl,\n\t\t\t\t'incregex': args.incregex,\n\t\t\t\t'ondup': args.ondup,\n\t\t\t\t'followlink': args.followlink,\n\t\t\t\t'checkssl': args.checkssl,\n\t\t\t\t'cacerts': args.cacerts,\n\t\t\t\t'rapiduploadonly': args.rapiduploadonly,\n\t\t\t\t'mirror': args.mirror,\n\t\t\t\t'selectmirror': args.selectmirror,\n\t\t\t\t'configdir': args.configdir,\n\t\t\t\t'resumedl_revertcount': args.resumedl_revertcount,\n\t\t\t\t'deletesource': args.deletesource,\n\t\t\t\t'downloader': args.downloader,\n\t\t\t\t'downloader_args': dl_args,\n\t\t\t\t'verbose': args.verbose,\n\t\t\t\t'debug': args.debug}\n\t\t\tif Pool:\n\t\t\t\tbypyopt['processes'] = args.processes\n\n\t\t\t# we construct a ByPy object here.\n\t\t\t# if you want to try PanAPI, simply replace ByPy with PanAPI, and all the bduss related function _should_ work\n\t\t\t# I didn't use PanAPI here as I have never tried out those functions inside\n\t\t\tby = ByPy(**bypyopt)\n\t\t\tuargs = []\n\t\t\tfor arg in args.command[1:]:\n\t\t\t\tif sys.version_info[0] < 3:\n\t\t\t\t\tuargs.append(unicode(arg, gvar.SystemEncoding))\n\t\t\t\telse:\n\t\t\t\t\tuargs.append(arg)\n\t\t\tresult = getattr(by, args.command[0])(*uargs)\n\t\t\tif result != const.ENoError:\n\t\t\t\terrmsg = '-' * 64 + \"\\nError {}{}\".format(result, ': ' + const.ErrorExplanations[result] if result in const.ErrorExplanations else '')\n\t\t\t\tperr(errmsg)\n\t\telse:\n\t\t\tperr(\"Error: Command '{}' not available.\".format(args.command[0]))\n\t\t\tparser.print_help()\n\t\t\treturn const.EParameter\n\n\texcept KeyboardInterrupt:\n\t\t# handle keyboard interrupt\n\t\tpr(\"KeyboardInterrupt\")\n\t\tpr(\"Abort\")\n\texcept Exception as ex:\n\t\t# NOTE: Capturing the exeption as 'ex' seems matters, otherwise this:\n\t\t# except Exception ex:\n\t\t# will sometimes give exception ...\n\t\tperr(\"Exception occurred:\\n{}\".format(formatex(ex)))\n\t\tpr(\"Abort\")\n\t\traise\n\tfinally:\n\t\tif by:\n\t\t\tby.quit(result)", "response": "Main entry point for the main function."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhelps for a given command", "response": "def help(command): # this comes first to make it easy to spot\n\t\t''' Usage: help <command> - provide some information for the command '''\n\t\tfor i, v in ByPy.__dict__.items():\n\t\t\tif callable(v) and v.__doc__ and v.__name__ == command :\n\t\t\t\thelp = v.__doc__.strip()\n\t\t\t\tpos = help.find(const.HelpMarker)\n\t\t\t\tif pos != -1:\n\t\t\t\t\tpr(\"Usage: \" + help[pos + len(const.HelpMarker):].strip())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list(self, remotepath = '',\n\t\tfmt = '$t $f $s $m $d',\n\t\tsort = 'name', order = 'asc'):\n\t\t''' Usage: list/ls [remotepath] [format] [sort] [order] - list the 'remotepath' directory at Baidu PCS\n    remotepath - the remote path at Baidu PCS. default: root directory '/'\n\tformat - specifies how the list are displayed\n\t  $t - Type: Directory ('D') or File ('F')\n\t  $f - File name\n\t  $c - Creation time\n\t  $m - Modification time\n\t  $d - MD5 hash\n\t  $s - Size\n\t  $$ - The '$' sign\n\t  So '$t - $f - $s - $$' will display \"Type - File - Size - $'\n\t  Default format: '$t $f $s $m $d'\n    sort - sorting by [name, time, size]. default: 'name'\n    order - sorting order [asc, desc]. default: 'asc'\n\t\t'''\n\t\trpath = get_pcs_path(remotepath)\n\n\t\tpars = {\n\t\t\t'method' : 'list',\n\t\t\t'path' : rpath,\n\t\t\t'by' : sort,\n\t\t\t'order' : order }\n\n\t\treturn self.__get(pcsurl + 'file', pars, self.__list_act, (rpath, fmt))", "response": "List the files in the remote path at Baidu PCS."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the metadata of a given remote path", "response": "def meta(self, remotepath = '/', fmt = '$t $u $f $s $c $m $i $b'):\n\t\t''' Usage: meta <remotepath> [format] - \\\nget information of the given path (dir / file) at Baidu Yun.\n  remotepath - the remote path\n  format - specifies how the list are displayed\n    it supports all the format variables in the 'list' command, and additionally the followings:\n\t$i - fs_id\n\t$b - MD5 block_list\n\t$u - Has sub directory or not\n'''\n\t\trpath = get_pcs_path(remotepath)\n\t\treturn self.__meta(rpath, fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuploads a file or directory to the remote server.", "response": "def upload(self, localpath = '', remotepath = '', ondup = \"overwrite\"):\n\t\t''' Usage: upload [localpath] [remotepath] [ondup] - \\\nupload a file or directory (recursively)\n    localpath - local path, is the current directory '.' if not specified\n    remotepath - remote path at Baidu Yun (after app root directory at Baidu Yun)\n    ondup - what to do upon duplication ('overwrite' or 'newcopy'), default: 'overwrite'\n\t\t'''\n\t\t# copying since Python is call-by-reference by default,\n\t\t# so we shall not modify the passed-in parameters\n\t\tlpath = localpath.rstrip('\\\\/ ') # no trailing slashes\n\t\tlpathbase = os.path.basename(lpath)\n\t\trpath = remotepath\n\t\tif not lpath:\n\t\t\t# so, if you don't specify the local path, it will always be the current directory\n\t\t\t# and thus isdir(localpath) is always true\n\t\t\tlpath = os.path.abspath(\".\")\n\t\t\tself.pd(\"localpath not set, set it to current directory '{}'\".format(localpath))\n\n\t\tif os.path.isfile(lpath):\n\t\t\tself.pd(\"Uploading file '{}'\".format(lpath))\n\t\t\tif not rpath or rpath == '/': # to root we go\n\t\t\t\trpath = lpathbase\n\t\t\tif rpath[-1] == '/': # user intends to upload to this DIR\n\t\t\t\trpath = get_pcs_path(rpath + lpathbase)\n\t\t\telse:\n\t\t\t\trpath = get_pcs_path(rpath)\n\t\t\t\t# avoid uploading a file and destroy a directory by accident\n\t\t\t\tsubresult = self.__get_file_info(rpath)\n\t\t\t\tif subresult == const.ENoError: # remote path exists, check is dir or file\n\t\t\t\t\tif self.__remote_json['isdir']: # do this only for dir\n\t\t\t\t\t\trpath += '/' + lpathbase # rpath is guaranteed no '/' ended\n\t\t\tself.pd(\"remote path is '{}'\".format(rpath))\n\t\t\treturn self.__upload_file(lpath, rpath, ondup)\n\t\telif os.path.isdir(lpath):\n\t\t\tself.pd(\"Uploading directory '{}' recursively\".format(lpath))\n\t\t\trpath = get_pcs_path(rpath)\n\t\t\treturn self.__upload_dir(lpath, rpath, ondup)\n\t\telse:\n\t\t\tperr(\"Error: invalid local path '{}' for uploading specified.\".format(localpath))\n\t\t\treturn const.EParameter"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef combine(self, remotefile, localfile = '*', *args):\n\t\t''' Usage: combine <remotefile> [localfile] [md5s] - \\\ntry to create a file at PCS by combining slices, having MD5s specified\n  remotefile - remote file at Baidu Yun (after app root directory at Baidu Yun)\n  localfile - local file to verify against, passing in a star '*' or '/dev/null' means no verification\n  md5s - MD5 digests of the slices, can be:\n    - list of MD5 hex strings separated by spaces\n    - a string in the form of 'l<path>' where <path> points to a text file containing MD5 hex strings separated by spaces or line-by-line\n\t\t'''\n\t\tself.__slice_md5s = []\n\t\tif args:\n\t\t\tif args[0].upper() == 'L':\n\t\t\t\ttry:\n\t\t\t\t\twith io.open(args[1:], 'r', encoding = 'utf-8') as f:\n\t\t\t\t\t\tcontents = f.read()\n\t\t\t\t\t\tdigests = filter(None, contents.split())\n\t\t\t\t\t\tfor d in digests:\n\t\t\t\t\t\t\tself.__slice_md5s.append(d)\n\t\t\t\texcept IOError as ex:\n\t\t\t\t\tperr(\"Exception occured while reading file '{}'.\\n{}\".format(\n\t\t\t\t\t\tlocalfile, formatex(ex)))\n\t\t\telse:\n\t\t\t\tfor arg in args:\n\t\t\t\t\tself.__slice_md5s.append(arg)\n\t\telse:\n\t\t\tperr(\"You MUST provide the MD5s hex strings through arguments or a file.\")\n\t\t\treturn const.EArgument\n\n\t\toriginal_verify = self.__verify\n\t\tif not localfile or localfile == '*' or localfile == '/dev/null':\n\t\t\tself.__current_file = '/dev/null' # Force no verify\n\t\t\tself.__verify = False\n\t\telse:\n\t\t\tself.__current_file = localfile\n\t\t\tself.__current_file_size = getfilesize(localfile)\n\n\t\tresult = self.__combine_file(get_pcs_path(remotefile))\n\t\tself.__verify = original_verify\n\t\treturn result", "response": "This function is used to combine a file at PCS by combining the file with the given file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef downfile(self, remotefile, localpath = ''):\n\t\t''' Usage: downfile <remotefile> [localpath] - \\\ndownload a remote file.\n  remotefile - remote file at Baidu Yun (after app root directory at Baidu Yun)\n  localpath - local path.\n    if it ends with '/' or '\\\\', it specifies the local directory\n    if it specifies an existing directory, it is the local directory\n    if not specified, the local directory is the current directory '.'\n    otherwise, it specifies the local file name\nTo stream a file using downfile, you can use the 'mkfifo' trick with omxplayer etc.:\n  mkfifo /tmp/omx\n  bypy.py downfile <remotepath> /tmp/omx &\n  omxplayer /tmp/omx\n\t\t'''\n\t\tlocalfile = localpath\n\t\tif not localpath:\n\t\t\tlocalfile = os.path.basename(remotefile)\n\t\telif localpath[-1] == '\\\\' or \\\n\t\t\tlocalpath[-1] == '/' or \\\n\t\t\tos.path.isdir(localpath):\n\t\t\t#localfile = os.path.join(localpath, os.path.basename(remotefile))\n\t\t\tlocalfile = joinpath(localpath, os.path.basename(remotefile))\n\t\telse:\n\t\t\tlocalfile = localpath\n\n\t\tpcsrpath = get_pcs_path(remotefile)\n\t\treturn self.__downfile(pcsrpath, localfile)", "response": "Download a file from the remote file system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstream a file to a pipe", "response": "def streaming(self, remotefile, localpipe, fmt = 'M3U8_480_360', chunk = 4 * const.OneM):\n\t\t''' Usage: stream <remotefile> <localpipe> [format] [chunk] - \\\nstream a video / audio file converted to M3U format at cloud side, to a pipe.\n  remotefile - remote file at Baidu Yun (after app root directory at Baidu Yun)\n  localpipe - the local pipe file to write to\n  format - output video format (M3U8_320_240 | M3U8_480_224 | \\\nM3U8_480_360 | M3U8_640_480 | M3U8_854_480). Default: M3U8_480_360\n  chunk - chunk (initial buffering) size for streaming (default: 4M)\nTo stream a file, you can use the 'mkfifo' trick with omxplayer etc.:\n  mkfifo /tmp/omx\n  bypy.py downfile <remotepath> /tmp/omx &\n  omxplayer /tmp/omx\n  *** NOT WORKING YET ****\n\t\t'''\n\t\tpars = {\n\t\t\t'method' : 'streaming',\n\t\t\t'path' : get_pcs_path(remotefile),\n\t\t\t'type' : fmt }\n\n\t\treturn self.__get(pcsurl + 'file', pars,\n\t\t\tself.__streaming_act, (localpipe, chunk), stream = True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndownloads a remote directory and return the local directory.", "response": "def downdir(self, remotepath = None, localpath = None):\n\t\t''' Usage: downdir [remotedir] [localdir] - \\\ndownload a remote directory (recursively)\n  remotedir - remote directory at Baidu Yun (after app root directory), if not specified, it is set to the root directory at Baidu Yun\n  localdir - local directory. if not specified, it is set to the current directory\n\t\t'''\n\t\trpath = get_pcs_path(remotepath)\n\t\tlpath = localpath\n\t\tif not lpath:\n\t\t\tlpath = '' # empty string does it, no need '.'\n\t\tlpath = lpath.rstrip('/\\\\ ')\n\t\treturn self.__downdir(rpath, lpath)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef download(self, remotepath = '/', localpath = ''):\n\t\t''' Usage: download [remotepath] [localpath] - \\\ndownload a remote directory (recursively) / file\n  remotepath - remote path at Baidu Yun (after app root directory), if not specified, it is set to the root directory at Baidu Yun\n  localpath - local path. if not specified, it is set to the current directory\n\t\t'''\n\t\tsubr = self.get_file_info(remotepath)\n\t\tif const.ENoError == subr:\n\t\t\tif 'isdir' in self.__remote_json:\n\t\t\t\tif self.__remote_json['isdir']:\n\t\t\t\t\treturn self.downdir(remotepath, localpath)\n\t\t\t\telse:\n\t\t\t\t\treturn self.downfile(remotepath, localpath)\n\t\t\telse:\n\t\t\t\tperr(\"Malformed path info JSON '{}' returned\".format(self.__remote_json))\n\t\t\t\treturn const.EFatal\n\t\telif const.EFileNotFound == subr:\n\t\t\tperr(\"Remote path '{}' does not exist\".format(remotepath))\n\t\t\treturn subr\n\t\telse:\n\t\t\tperr(\"Error {} while getting info for remote path '{}'\".format(subr, remotepath))\n\t\t\treturn subr", "response": "Download a file from the remote path to the local path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmove a file from one location to another", "response": "def move(self, fromp, to):\n\t\t''' Usage: move/mv/rename/ren <from> <to> - \\\nmove a file / dir remotely at Baidu Yun\n  from - source path (file / dir)\n  to - destination path (file / dir)\n\t\t'''\n\t\tfrompp = get_pcs_path(fromp)\n\t\ttop = get_pcs_path(to)\n\t\tpars = {\n\t\t\t'method' : 'move',\n\t\t\t'from' : frompp,\n\t\t\t'to' : top }\n\n\t\tself.pd(\"Remote moving: '{}' =mm=> '{}'\".format(fromp, to))\n\t\treturn self.__post(pcsurl + 'file', pars, self.__move_act)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copy(self, fromp, to):\n\t\t''' Usage: copy/cp <from> <to> - \\\ncopy a file / dir remotely at Baidu Yun\n  from - source path (file / dir)\n  to - destination path (file / dir)\n\t\t'''\n\t\tfrompp = get_pcs_path(fromp)\n\t\ttop = get_pcs_path(to)\n\t\tpars = {\n\t\t\t'method' : 'copy',\n\t\t\t'from' : frompp,\n\t\t\t'to' : top }\n\n\t\tself.pd(\"Remote copying '{}' =cc=> '{}'\".format(frompp, top))\n\t\treturn self.__post(pcsurl + 'file', pars, self.__copy_act)", "response": "Copy a file from one location to another"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef search(self, keyword, remotepath = None, recursive = True):\n\t\t''' Usage: search <keyword> [remotepath] [recursive] - \\\nsearch for a file using keyword at Baidu Yun\n  keyword - the keyword to search\n  remotepath - remote path at Baidu Yun, if not specified, it's app's root directory\n  resursive - search recursively or not. default is true\n\t\t'''\n\t\trpath = get_pcs_path(remotepath)\n\n\t\tpars = {\n\t\t\t'method' : 'search',\n\t\t\t'path' : rpath,\n\t\t\t'wd' : keyword,\n\t\t\t're' : '1' if str2bool(recursive) else '0'}\n\n\t\tself.pd(\"Searching: '{}'\".format(rpath))\n\t\treturn self.__get(pcsurl + 'file', pars, self.__search_act)", "response": "Search for a file using a keyword at Baidu Yun and return the file object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef listrecycle(self, start = 0, limit = 1000):\n\t\t''' Usage: listrecycle [start] [limit] - \\\nlist the recycle contents\n  start - starting point, default: 0\n  limit - maximum number of items to display. default: 1000\n\t\t'''\n\t\tpars = {\n\t\t\t'method' : 'listrecycle',\n\t\t\t'start' : str2int(start),\n\t\t\t'limit' : str2int(limit) }\n\n\t\tself.pd(\"Listing recycle '{}'\")\n\t\treturn self.__get(pcsurl + 'file', pars, self.__listrecycle_act)", "response": "This method returns a list of the contents of the recycle contents."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef restore(self, remotepath):\n\t\t''' Usage: restore <remotepath> - \\\nrestore a file from the recycle bin\n  remotepath - the remote path to restore\n\t\t'''\n\t\trpath = get_pcs_path(remotepath)\n\t\t# by default, only 1000 items, more than that sounds a bit crazy\n\t\tpars = {\n\t\t\t'method' : 'listrecycle' }\n\n\t\tself.pd(\"Searching for fs_id to restore\")\n\t\treturn self.__get(pcsurl + 'file', pars, self.__restore_search_act, rpath)", "response": "This method will restore a file from the recycle bin\n"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompare the remote directory with the local directory.", "response": "def compare(self, remotedir = None, localdir = None, skip_remote_only_dirs = False):\n\t\t''' Usage: compare [remotedir] [localdir] - \\\ncompare the remote directory with the local directory\n  remotedir - the remote directory at Baidu Yun (after app's directory). \\\nif not specified, it defaults to the root directory.\n  localdir - the local directory, if not specified, it defaults to the current directory.\n  skip_remote_only_dirs - skip remote-only sub-directories (faster if the remote \\\ndirectory is much larger than the local one). it defaults to False.\n\t\t'''\n\t\tsame, diff, local, remote = self.__compare(get_pcs_path(remotedir), localdir, str2bool(skip_remote_only_dirs))\n\n\t\tpr(\"==== Same files ===\")\n\t\tfor c in same:\n\t\t\tpr(\"{} - {}\".format(c[0], c[1]))\n\n\t\tpr(\"==== Different files ===\")\n\t\tfor d in diff:\n\t\t\tpr(\"{} - {}\".format(d[0], d[1]))\n\n\t\tpr(\"==== Local only ====\")\n\t\tfor l in local:\n\t\t\tpr(\"{} - {}\".format(l[0], l[1]))\n\n\t\tpr(\"==== Remote only ====\")\n\t\tfor r in remote:\n\t\t\tpr(\"{} - {}\".format(r[0], r[1]))\n\n\t\tpr(\"\\nStatistics:\")\n\t\tpr(\"--------------------------------\")\n\t\tpr(\"Same: {}\".format(len(same)))\n\t\tpr(\"Different: {}\".format(len(diff)))\n\t\tpr(\"Local only: {}\".format(len(local)))\n\t\tpr(\"Remote only: {}\".format(len(remote)))\n\n\t\tself.result['same'] = same\n\t\tself.result['diff'] = diff\n\t\tself.result['local'] = local\n\t\tself.result['remote'] = remote\n\n\t\treturn const.ENoError"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef syncup(self, localdir = '', remotedir = '', deleteremote = False):\n\t\t''' Usage: syncup [localdir] [remotedir] [deleteremote] - \\\nsync up from the local directory to the remote directory\n  localdir - the local directory to sync from if not specified, it defaults to the current directory.\n  remotedir - the remote directory at Baidu Yun (after app's directory) to sync to. \\\nif not specified, it defaults to the root directory\n  deleteremote - delete remote files that are not inside the local directory, default is False\n\t\t'''\n\t\tresult = const.ENoError\n\t\trpath = get_pcs_path(remotedir)\n\t\t#rpartialdir = remotedir.rstrip('/ ')\n\t\tcompare_result = self.__compare(rpath, localdir, True)\n\t\tsame, diff, local, remote = compare_result\n\t\tif Pool and self.processes > 1:\n\t\t\tsubresult = self.__syncup_multi(localdir, rpath, compare_result)\n\t\t\tif subresult != const.ENoError:\n\t\t\t\tresult = subresult\n\t\telse:\n\t\t\tsubresult = self.__syncup_single(localdir, rpath, compare_result)\n\t\t\tif subresult != const.ENoError:\n\t\t\t\tresult = subresult\n\t\tif str2bool(deleteremote):\n\t\t\tsubresult = self.__syncup_delete_remote(rpath, remote)\n\t\t\tif subresult != const.ENoError:\n\t\t\t\tresult = subresult\n\t\tif result == const.ENoError:\n\t\t\tself.__remove_local_on_success(localdir)\n\t\treturn result", "response": "Syncs the local directory to the remote directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dumpcache(self):\n\t\t''' Usage: dumpcache - display file hash cache'''\n\t\tif cached.cacheloaded:\n\t\t\t#pprint.pprint(cached.cache)\n\t\t\tMyPrettyPrinter().pprint(cached.cache)\n\t\t\treturn const.ENoError\n\t\telse:\n\t\t\tperr(\"Cache not loaded.\")\n\t\t\treturn const.ECacheNotLoaded", "response": "Display file hash cache."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cleancache(self):\n\t\t''' Usage: cleancache - remove invalid entries from hash cache file'''\n\t\tif os.path.exists(self.__hashcachepath):\n\t\t\ttry:\n\t\t\t\t# backup first\n\t\t\t\tbackup = self.__hashcachepath + '.lastclean'\n\t\t\t\tshutil.copy(self.__hashcachepath, backup)\n\t\t\t\tself.pd(\"Hash Cache file '{}' backed up as '{}\".format(\n\t\t\t\t\tself.__hashcachepath, backup))\n\t\t\t\tcached.cleancache()\n\t\t\t\treturn const.ENoError\n\t\t\texcept Exception as ex:\n\t\t\t\tperr(formatex(ex))\n\t\t\t\treturn const.EException\n\t\telse:\n\t\t\treturn const.EFileNotFound", "response": "This function is used to clean up invalid entries from hash cache file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a file from a remote URL to the local CDL file.", "response": "def cdl_add(self, source_url, save_path = '/', timeout = 3600):\n\t\t''' Usage: cdl_add <source_url> [save_path] [timeout] - add an offline (cloud) download task\n  source_url - the URL to download file from.\n  save_path - path on PCS to save file to. default is to save to root directory '/'.\n  timeout - timeout in seconds. default is 3600 seconds.\n\t\t'''\n\t\trpath = self.__get_cdl_dest(source_url, save_path)\n\t\treturn self.__cdl_add(source_url, rpath, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a new download task and monitor the download progress", "response": "def cdl_addmon(self, source_url, save_path = '/', timeout = 3600):\n\t\t''' Usage: cdl_addmon <source_url> [save_path] [timeout] - add an offline (cloud) download task and monitor the download progress\n  source_url - the URL to download file from.\n  save_path - path on PCS to save file to. default is to save to root directory '/'.\n  timeout - timeout in seconds. default is 3600 seconds.\n\t\t'''\n\t\trpath = self.__get_cdl_dest(source_url, save_path)\n\t\treturn self.__cdl_addmon(source_url, rpath, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef log_multivariate_normal_density(X, means, covars, covariance_type='diag'):\n    log_multivariate_normal_density_dict = {\n        'spherical': _log_multivariate_normal_density_spherical,\n        'tied': _log_multivariate_normal_density_tied,\n        'diag': _log_multivariate_normal_density_diag,\n        'full': _log_multivariate_normal_density_full}\n    return log_multivariate_normal_density_dict[covariance_type](\n        X, means, covars\n    )", "response": "Compute the log probability under a multivariate Gaussian distribution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing Gaussian log - density at X for a diagonal model.", "response": "def _log_multivariate_normal_density_diag(X, means, covars):\n    \"\"\"Compute Gaussian log-density at X for a diagonal model.\"\"\"\n    n_samples, n_dim = X.shape\n    lpr = -0.5 * (n_dim * np.log(2 * np.pi) + np.sum(np.log(covars), 1)\n                  + np.sum((means ** 2) / covars, 1)\n                  - 2 * np.dot(X, (means / covars).T)\n                  + np.dot(X ** 2, (1.0 / covars).T))\n    return lpr"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing Gaussian log - density at X for a spherical model.", "response": "def _log_multivariate_normal_density_spherical(X, means, covars):\n    \"\"\"Compute Gaussian log-density at X for a spherical model.\"\"\"\n    cv = covars.copy()\n    if covars.ndim == 1:\n        cv = cv[:, np.newaxis]\n    if cv.shape[1] == 1:\n        cv = np.tile(cv, (1, X.shape[-1]))\n    return _log_multivariate_normal_density_diag(X, means, cv)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing Gaussian log - density at X for a tied model.", "response": "def _log_multivariate_normal_density_tied(X, means, covars):\n    \"\"\"Compute Gaussian log-density at X for a tied model.\"\"\"\n    cv = np.tile(covars, (means.shape[0], 1, 1))\n    return _log_multivariate_normal_density_full(X, means, cv)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlog probability for full covariance matrices.", "response": "def _log_multivariate_normal_density_full(X, means, covars, min_covar=1.e-7):\n    \"\"\"Log probability for full covariance matrices.\"\"\"\n    n_samples, n_dim = X.shape\n    nmix = len(means)\n    log_prob = np.empty((n_samples, nmix))\n    for c, (mu, cv) in enumerate(zip(means, covars)):\n        try:\n            cv_chol = linalg.cholesky(cv, lower=True)\n        except linalg.LinAlgError:\n            # The model is most probably stuck in a component with too\n            # few observations, we need to reinitialize this components\n            try:\n                cv_chol = linalg.cholesky(cv + min_covar * np.eye(n_dim),\n                                          lower=True)\n            except linalg.LinAlgError:\n                raise ValueError(\"'covars' must be symmetric, \"\n                                 \"positive-definite\")\n\n        cv_log_det = 2 * np.sum(np.log(np.diagonal(cv_chol)))\n        cv_sol = linalg.solve_triangular(cv_chol, (X - mu).T, lower=True).T\n        log_prob[:, c] = - .5 * (np.sum(cv_sol ** 2, axis=1) +\n                                 n_dim * np.log(2 * np.pi) + cv_log_det)\n\n    return log_prob"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef report(self, logprob):\n        if self.verbose:\n            delta = logprob - self.history[-1] if self.history else np.nan\n            message = self._template.format(\n                iter=self.iter + 1, logprob=logprob, delta=delta)\n            print(message, file=sys.stderr)\n\n        self.history.append(logprob)\n        self.iter += 1", "response": "Reports convergence to sys. stderr."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntrue if the EM algorithm converged and False otherwise.", "response": "def converged(self):\n        \"\"\"``True`` if the EM algorithm converged and ``False`` otherwise.\"\"\"\n        # XXX we might want to check that ``logprob`` is non-decreasing.\n        return (self.iter == self.n_iter or\n                (len(self.history) == 2 and\n                 self.history[1] - self.history[0] < self.tol))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the stationary distribution of states.", "response": "def get_stationary_distribution(self):\n        \"\"\"Compute the stationary distribution of states.\n        \"\"\"\n        # The stationary distribution is proportional to the left-eigenvector\n        # associated with the largest eigenvalue (i.e., 1) of the transition\n        # matrix.\n        check_is_fitted(self, \"transmat_\")\n        eigvals, eigvecs = np.linalg.eig(self.transmat_.T)\n        eigvec = np.real_if_close(eigvecs[:, np.argmax(eigvals)])\n        return eigvec / eigvec.sum()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef score_samples(self, X, lengths=None):\n        check_is_fitted(self, \"startprob_\")\n        self._check()\n\n        X = check_array(X)\n        n_samples = X.shape[0]\n        logprob = 0\n        posteriors = np.zeros((n_samples, self.n_components))\n        for i, j in iter_from_X_lengths(X, lengths):\n            framelogprob = self._compute_log_likelihood(X[i:j])\n            logprobij, fwdlattice = self._do_forward_pass(framelogprob)\n            logprob += logprobij\n\n            bwdlattice = self._do_backward_pass(framelogprob)\n            posteriors[i:j] = self._compute_posteriors(fwdlattice, bwdlattice)\n        return logprob, posteriors", "response": "Compute the log probability under the model and compute posteriors."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef score(self, X, lengths=None):\n        check_is_fitted(self, \"startprob_\")\n        self._check()\n\n        X = check_array(X)\n        # XXX we can unroll forward pass for speed and memory efficiency.\n        logprob = 0\n        for i, j in iter_from_X_lengths(X, lengths):\n            framelogprob = self._compute_log_likelihood(X[i:j])\n            logprobij, _fwdlattice = self._do_forward_pass(framelogprob)\n            logprob += logprobij\n        return logprob", "response": "Compute the log probability under the model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decode(self, X, lengths=None, algorithm=None):\n        check_is_fitted(self, \"startprob_\")\n        self._check()\n\n        algorithm = algorithm or self.algorithm\n        if algorithm not in DECODER_ALGORITHMS:\n            raise ValueError(\"Unknown decoder {!r}\".format(algorithm))\n\n        decoder = {\n            \"viterbi\": self._decode_viterbi,\n            \"map\": self._decode_map\n        }[algorithm]\n\n        X = check_array(X)\n        n_samples = X.shape[0]\n        logprob = 0\n        state_sequence = np.empty(n_samples, dtype=int)\n        for i, j in iter_from_X_lengths(X, lengths):\n            # XXX decoder works on a single sample at a time!\n            logprobij, state_sequenceij = decoder(X[i:j])\n            logprob += logprobij\n            state_sequence[i:j] = state_sequenceij\n\n        return logprob, state_sequence", "response": "This method returns the most likely state sequence corresponding to X."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef predict(self, X, lengths=None):\n        _, state_sequence = self.decode(X, lengths)\n        return state_sequence", "response": "Predict the most likely state sequence corresponding to X."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the posterior probability for each state in the model.", "response": "def predict_proba(self, X, lengths=None):\n        \"\"\"Compute the posterior probability for each state in the model.\n\n        X : array-like, shape (n_samples, n_features)\n            Feature matrix of individual samples.\n\n        lengths : array-like of integers, shape (n_sequences, ), optional\n            Lengths of the individual sequences in ``X``. The sum of\n            these should be ``n_samples``.\n\n        Returns\n        -------\n        posteriors : array, shape (n_samples, n_components)\n            State-membership probabilities for each sample from ``X``.\n        \"\"\"\n        _, posteriors = self.score_samples(X, lengths)\n        return posteriors"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate random samples from the model.", "response": "def sample(self, n_samples=1, random_state=None):\n        \"\"\"Generate random samples from the model.\n\n        Parameters\n        ----------\n        n_samples : int\n            Number of samples to generate.\n\n        random_state : RandomState or an int seed\n            A random number generator instance. If ``None``, the object's\n            ``random_state`` is used.\n\n        Returns\n        -------\n        X : array, shape (n_samples, n_features)\n            Feature matrix.\n\n        state_sequence : array, shape (n_samples, )\n            State sequence produced by the model.\n        \"\"\"\n        check_is_fitted(self, \"startprob_\")\n        self._check()\n\n        if random_state is None:\n            random_state = self.random_state\n        random_state = check_random_state(random_state)\n\n        startprob_cdf = np.cumsum(self.startprob_)\n        transmat_cdf = np.cumsum(self.transmat_, axis=1)\n\n        currstate = (startprob_cdf > random_state.rand()).argmax()\n        state_sequence = [currstate]\n        X = [self._generate_sample_from_state(\n            currstate, random_state=random_state)]\n\n        for t in range(n_samples - 1):\n            currstate = (transmat_cdf[currstate] > random_state.rand()) \\\n                .argmax()\n            state_sequence.append(currstate)\n            X.append(self._generate_sample_from_state(\n                currstate, random_state=random_state))\n\n        return np.atleast_2d(X), np.array(state_sequence, dtype=int)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nestimate model parameters for the set of individual sequences in X.", "response": "def fit(self, X, lengths=None):\n        \"\"\"Estimate model parameters.\n\n        An initialization step is performed before entering the\n        EM algorithm. If you want to avoid this step for a subset of\n        the parameters, pass proper ``init_params`` keyword argument\n        to estimator's constructor.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Feature matrix of individual samples.\n\n        lengths : array-like of integers, shape (n_sequences, )\n            Lengths of the individual sequences in ``X``. The sum of\n            these should be ``n_samples``.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        X = check_array(X)\n        self._init(X, lengths=lengths)\n        self._check()\n\n        self.monitor_._reset()\n        for iter in range(self.n_iter):\n            stats = self._initialize_sufficient_statistics()\n            curr_logprob = 0\n            for i, j in iter_from_X_lengths(X, lengths):\n                framelogprob = self._compute_log_likelihood(X[i:j])\n                logprob, fwdlattice = self._do_forward_pass(framelogprob)\n                curr_logprob += logprob\n                bwdlattice = self._do_backward_pass(framelogprob)\n                posteriors = self._compute_posteriors(fwdlattice, bwdlattice)\n                self._accumulate_sufficient_statistics(\n                    stats, X[i:j], framelogprob, posteriors, fwdlattice,\n                    bwdlattice)\n\n            # XXX must be before convergence check, because otherwise\n            #     there won't be any updates for the case ``n_iter=1``.\n            self._do_mstep(stats)\n\n            self.monitor_.report(curr_logprob)\n            if self.monitor_.converged:\n                break\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing the model parameters prior to fitting.", "response": "def _init(self, X, lengths):\n        \"\"\"Initializes model parameters prior to fitting.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Feature matrix of individual samples.\n\n        lengths : array-like of integers, shape (n_sequences, )\n            Lengths of the individual sequences in ``X``. The sum of\n            these should be ``n_samples``.\n        \"\"\"\n        init = 1. / self.n_components\n        if 's' in self.init_params or not hasattr(self, \"startprob_\"):\n            self.startprob_ = np.full(self.n_components, init)\n        if 't' in self.init_params or not hasattr(self, \"transmat_\"):\n            self.transmat_ = np.full((self.n_components, self.n_components),\n                                     init)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates the parameters prior to fitting.", "response": "def _check(self):\n        \"\"\"Validates model parameters prior to fitting.\n\n        Raises\n        ------\n\n        ValueError\n            If any of the parameters are invalid, e.g. if :attr:`startprob_`\n            don't sum to 1.\n        \"\"\"\n        self.startprob_ = np.asarray(self.startprob_)\n        if len(self.startprob_) != self.n_components:\n            raise ValueError(\"startprob_ must have length n_components\")\n        if not np.allclose(self.startprob_.sum(), 1.0):\n            raise ValueError(\"startprob_ must sum to 1.0 (got {:.4f})\"\n                             .format(self.startprob_.sum()))\n\n        self.transmat_ = np.asarray(self.transmat_)\n        if self.transmat_.shape != (self.n_components, self.n_components):\n            raise ValueError(\n                \"transmat_ must have shape (n_components, n_components)\")\n        if not np.allclose(self.transmat_.sum(axis=1), 1.0):\n            raise ValueError(\"rows of transmat_ must sum to 1.0 (got {})\"\n                             .format(self.transmat_.sum(axis=1)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _initialize_sufficient_statistics(self):\n        stats = {'nobs': 0,\n                 'start': np.zeros(self.n_components),\n                 'trans': np.zeros((self.n_components, self.n_components))}\n        return stats", "response": "Initializes the sufficient statistics required for the M - step."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the sufficient statistics for a given sample.", "response": "def _accumulate_sufficient_statistics(self, stats, X, framelogprob,\n                                          posteriors, fwdlattice, bwdlattice):\n        \"\"\"Updates sufficient statistics from a given sample.\n\n        Parameters\n        ----------\n        stats : dict\n            Sufficient statistics as returned by\n            :meth:`~base._BaseHMM._initialize_sufficient_statistics`.\n\n        X : array, shape (n_samples, n_features)\n            Sample sequence.\n\n        framelogprob : array, shape (n_samples, n_components)\n            Log-probabilities of each sample under each of the model states.\n\n        posteriors : array, shape (n_samples, n_components)\n            Posterior probabilities of each sample being generated by each\n            of the model states.\n\n        fwdlattice, bwdlattice : array, shape (n_samples, n_components)\n            Log-forward and log-backward probabilities.\n        \"\"\"\n        stats['nobs'] += 1\n        if 's' in self.params:\n            stats['start'] += posteriors[0]\n        if 't' in self.params:\n            n_samples, n_components = framelogprob.shape\n            # when the sample is of length 1, it contains no transitions\n            # so there is no reason to update our trans. matrix estimate\n            if n_samples <= 1:\n                return\n\n            log_xi_sum = np.full((n_components, n_components), -np.inf)\n            _hmmc._compute_log_xi_sum(n_samples, n_components, fwdlattice,\n                                      log_mask_zero(self.transmat_),\n                                      bwdlattice, framelogprob,\n                                      log_xi_sum)\n            with np.errstate(under=\"ignore\"):\n                stats['trans'] += np.exp(log_xi_sum)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _do_mstep(self, stats):\n        # The ``np.where`` calls guard against updating forbidden states\n        # or transitions in e.g. a left-right HMM.\n        if 's' in self.params:\n            startprob_ = self.startprob_prior - 1.0 + stats['start']\n            self.startprob_ = np.where(self.startprob_ == 0.0,\n                                       self.startprob_, startprob_)\n            normalize(self.startprob_)\n        if 't' in self.params:\n            transmat_ = self.transmat_prior - 1.0 + stats['trans']\n            self.transmat_ = np.where(self.transmat_ == 0.0,\n                                      self.transmat_, transmat_)\n            normalize(self.transmat_, axis=1)", "response": "Performs the M - step of EM algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate_covars(covars, covariance_type, n_components):\n    from scipy import linalg\n    if covariance_type == 'spherical':\n        if len(covars) != n_components:\n            raise ValueError(\"'spherical' covars have length n_components\")\n        elif np.any(covars <= 0):\n            raise ValueError(\"'spherical' covars must be non-negative\")\n    elif covariance_type == 'tied':\n        if covars.shape[0] != covars.shape[1]:\n            raise ValueError(\"'tied' covars must have shape (n_dim, n_dim)\")\n        elif (not np.allclose(covars, covars.T)\n              or np.any(linalg.eigvalsh(covars) <= 0)):\n            raise ValueError(\"'tied' covars must be symmetric, \"\n                             \"positive-definite\")\n    elif covariance_type == 'diag':\n        if len(covars.shape) != 2:\n            raise ValueError(\"'diag' covars must have shape \"\n                             \"(n_components, n_dim)\")\n        elif np.any(covars <= 0):\n            raise ValueError(\"'diag' covars must be non-negative\")\n    elif covariance_type == 'full':\n        if len(covars.shape) != 3:\n            raise ValueError(\"'full' covars must have shape \"\n                             \"(n_components, n_dim, n_dim)\")\n        elif covars.shape[1] != covars.shape[2]:\n            raise ValueError(\"'full' covars must have shape \"\n                             \"(n_components, n_dim, n_dim)\")\n        for n, cv in enumerate(covars):\n            if (not np.allclose(cv, cv.T)\n                    or np.any(linalg.eigvalsh(cv) <= 0)):\n                raise ValueError(\"component %d of 'full' covars must be \"\n                                 \"symmetric, positive-definite\" % n)\n    else:\n        raise ValueError(\"covariance_type must be one of \" +\n                         \"'spherical', 'tied', 'diag', 'full'\")", "response": "Validate the matrix covariance sizes and values."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates all the covariance matrices from a given template.", "response": "def distribute_covar_matrix_to_match_covariance_type(\n        tied_cv, covariance_type, n_components):\n    \"\"\"Create all the covariance matrices from a given template.\"\"\"\n    if covariance_type == 'spherical':\n        cv = np.tile(tied_cv.mean() * np.ones(tied_cv.shape[1]),\n                     (n_components, 1))\n    elif covariance_type == 'tied':\n        cv = tied_cv\n    elif covariance_type == 'diag':\n        cv = np.tile(np.diag(tied_cv), (n_components, 1))\n    elif covariance_type == 'full':\n        cv = np.tile(tied_cv, (n_components, 1, 1))\n    else:\n        raise ValueError(\"covariance_type must be one of \" +\n                         \"'spherical', 'tied', 'diag', 'full'\")\n    return cv"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef normalize(a, axis=None):\n    a_sum = a.sum(axis)\n    if axis and a.ndim > 1:\n        # Make sure we don't divide by zero.\n        a_sum[a_sum == 0] = 1\n        shape = list(a.shape)\n        shape[axis] = 1\n        a_sum.shape = shape\n\n    a /= a_sum", "response": "Normalizes the input array so that it sums to 1."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef log_normalize(a, axis=None):\n    with np.errstate(under=\"ignore\"):\n        a_lse = logsumexp(a, axis)\n    a -= a_lse[:, np.newaxis]", "response": "Normalizes the input array so that the exponent of the sum is 1."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef log_mask_zero(a):\n    a = np.asarray(a)\n    with np.errstate(divide=\"ignore\"):\n        return np.log(a)", "response": "Computes the log of input probabilities masking divide by zero in log."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning covars as a full matrix.", "response": "def covars_(self):\n        \"\"\"Return covars as a full matrix.\"\"\"\n        return fill_covars(self._covars_, self.covariance_type,\n                           self.n_components, self.n_features)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if X is a sample from a Multinomial distribution.", "response": "def _check_input_symbols(self, X):\n        \"\"\"Check if ``X`` is a sample from a Multinomial distribution.\n\n        That is ``X`` should be an array of non-negative integers from\n        range ``[min(X), max(X)]``, such that each integer from the range\n        occurs in ``X`` at least once.\n\n        For example ``[0, 0, 2, 1, 3, 1, 1]`` is a valid sample from a\n        Multinomial distribution, while ``[0, 0, 3, 5, 10]`` is not.\n        \"\"\"\n        symbols = np.concatenate(X)\n        if (len(symbols) == 1                                # not enough data\n            or not np.issubdtype(symbols.dtype, np.integer)  # not an integer\n            or (symbols < 0).any()):                         # not positive\n            return False\n        u = np.unique(symbols)\n        return u[0] == 0 and u[-1] == len(u) - 1"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new_from_sha(cls, repo, sha1):\n        if sha1 == cls.NULL_BIN_SHA:\n            # the NULL binsha is always the root commit\n            return get_object_type_by_name(b'commit')(repo, sha1)\n        # END handle special case\n        oinfo = repo.odb.info(sha1)\n        inst = get_object_type_by_name(oinfo.type)(repo, oinfo.binsha)\n        inst.size = oinfo.size\n        return inst", "response": "Returns a new object instance of a type appropriate to represent the given sha1."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _set_cache_(self, attr):\n        if attr == \"size\":\n            oinfo = self.repo.odb.info(self.binsha)\n            self.size = oinfo.size\n            # assert oinfo.type == self.type, _assertion_msg_format % (self.binsha, oinfo.type, self.type)\n        else:\n            super(Object, self)._set_cache_(attr)", "response": "Retrieve object information and set size if needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stream_data(self, ostream):\n        istream = self.repo.odb.stream(self.binsha)\n        stream_copy(istream, ostream)\n        return self", "response": "Writes our data directly to the given output stream"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mime_type(self):\n        guesses = None\n        if self.path:\n            guesses = guess_type(self.path)\n        return guesses and guesses[0] or self.DEFAULT_MIME_TYPE", "response": "Returns the mime type of this file based on the filename."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\niterate remote references and constrain them to the given remote", "response": "def iter_items(cls, repo, common_path=None, remote=None):\n        \"\"\"Iterate remote references, and if given, constrain them to the given remote\"\"\"\n        common_path = common_path or cls._common_path_default\n        if remote is not None:\n            common_path = join_path(common_path, str(remote))\n        # END handle remote constraint\n        return super(RemoteReference, cls).iter_items(repo, common_path)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(cls, repo, *refs, **kwargs):\n        repo.git.branch(\"-d\", \"-r\", *refs)\n        # the official deletion method will ignore remote symbolic refs - these\n        # are generally ignored in the refs/ folder. We don't though\n        # and delete remainders manually\n        for ref in refs:\n            try:\n                os.remove(osp.join(repo.common_dir, ref.path))\n            except OSError:\n                pass\n            try:\n                os.remove(osp.join(repo.git_dir, ref.path))\n            except OSError:\n                pass", "response": "Delete the given remote references."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize external projects by putting them into the path", "response": "def _init_externals():\n    \"\"\"Initialize external projects by putting them into the path\"\"\"\n    if __version__ == 'git':\n        sys.path.insert(0, osp.join(osp.dirname(__file__), 'ext', 'gitdb'))\n\n    try:\n        import gitdb\n    except ImportError:\n        raise ImportError(\"'gitdb' could not be found in your PYTHONPATH\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _git_dir(repo, path):\n    name = \"%s\" % (path,)\n    if name in ['HEAD', 'ORIG_HEAD', 'FETCH_HEAD', 'index', 'logs']:\n        return repo.git_dir\n    return repo.common_dir", "response": "Find the git dir that s appropriate for the path"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _iter_packed_refs(cls, repo):\n        try:\n            with open(cls._get_packed_refs_path(repo), 'rt') as fp:\n                for line in fp:\n                    line = line.strip()\n                    if not line:\n                        continue\n                    if line.startswith('#'):\n                        # \"# pack-refs with: peeled fully-peeled sorted\"\n                        # the git source code shows \"peeled\",\n                        # \"fully-peeled\" and \"sorted\" as the keywords\n                        # that can go on this line, as per comments in git file\n                        # refs/packed-backend.c\n                        # I looked at master on 2017-10-11,\n                        # commit 111ef79afe, after tag v2.15.0-rc1\n                        # from repo https://github.com/git/git.git\n                        if line.startswith('# pack-refs with:') and 'peeled' not in line:\n                            raise TypeError(\"PackingType of packed-Refs not understood: %r\" % line)\n                        # END abort if we do not understand the packing scheme\n                        continue\n                    # END parse comment\n\n                    # skip dereferenced tag object entries - previous line was actual\n                    # tag reference for it\n                    if line[0] == '^':\n                        continue\n\n                    yield tuple(line.split(' ', 1))\n                # END for each line\n        except (OSError, IOError):\n            return", "response": "Returns an iterator yielding sha1 and path pairs for the corresponding tag objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dereference_recursive(cls, repo, ref_path):\n        while True:\n            hexsha, ref_path = cls._get_ref_info(repo, ref_path)\n            if hexsha is not None:\n                return hexsha", "response": "Recursively dereferences all the all the all the other objects at ref_path"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns (str(sha), str(target_ref_path)) if available, the sha the file at rela_path points to, or None. target_ref_path is the reference we point to, or None", "response": "def _get_ref_info_helper(cls, repo, ref_path):\n        \"\"\"Return: (str(sha), str(target_ref_path)) if available, the sha the file at\n        rela_path points to, or None. target_ref_path is the reference we\n        point to, or None\"\"\"\n        tokens = None\n        repodir = _git_dir(repo, ref_path)\n        try:\n            with open(osp.join(repodir, ref_path), 'rt') as fp:\n                value = fp.read().rstrip()\n            # Don't only split on spaces, but on whitespace, which allows to parse lines like\n            # 60b64ef992065e2600bfef6187a97f92398a9144                branch 'master' of git-server:/path/to/repo\n            tokens = value.split()\n            assert(len(tokens) != 0)\n        except (OSError, IOError):\n            # Probably we are just packed, find our entry in the packed refs file\n            # NOTE: We are not a symbolic ref if we are in a packed file, as these\n            # are excluded explicitly\n            for sha, path in cls._iter_packed_refs(repo):\n                if path != ref_path:\n                    continue\n                # sha will be used\n                tokens = sha, path\n                break\n            # END for each packed ref\n        # END handle packed refs\n        if tokens is None:\n            raise ValueError(\"Reference at %r does not exist\" % ref_path)\n\n        # is it a reference ?\n        if tokens[0] == 'ref:':\n            return (None, tokens[1])\n\n        # its a commit\n        if repo.re_hexsha_only.match(tokens[0]):\n            return (tokens[0], None)\n\n        raise ValueError(\"Failed to parse reference information from %r\" % ref_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the object we re pointing to.", "response": "def _get_object(self):\n        \"\"\"\n        :return:\n            The object our ref currently refers to. Refs can be cached, they will\n            always point to the actual object as it gets re-created on each query\"\"\"\n        # have to be dynamic here as we may be a tag which can point to anything\n        # Our path will be resolved to the hexsha which will be used accordingly\n        return Object.new_from_sha(self.repo, hex_to_bin(self.dereference_recursive(self.repo, self.path)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_commit(self):\n        obj = self._get_object()\n        if obj.type == 'tag':\n            obj = obj.object\n        # END dereference tag\n\n        if obj.type != Commit.type:\n            raise TypeError(\"Symbolic Reference pointed to object %r, commit was required\" % obj)\n        # END handle type\n        return obj", "response": "Returns the object that represents the symbolic reference."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_commit(self, commit, logmsg=None):\n        # check the type - assume the best if it is a base-string\n        invalid_type = False\n        if isinstance(commit, Object):\n            invalid_type = commit.type != Commit.type\n        elif isinstance(commit, SymbolicReference):\n            invalid_type = commit.object.type != Commit.type\n        else:\n            try:\n                invalid_type = self.repo.rev_parse(commit).type != Commit.type\n            except (BadObject, BadName):\n                raise ValueError(\"Invalid object: %s\" % commit)\n            # END handle exception\n        # END verify type\n\n        if invalid_type:\n            raise ValueError(\"Need commit, got %r\" % commit)\n        # END handle raise\n\n        # we leave strings to the rev-parse method below\n        self.set_object(commit, logmsg)\n\n        return self", "response": "As set_object but restricts the type of object to be a Commit object and logmsg."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_object(self, object, logmsg=None):  # @ReservedAssignment\n        if isinstance(object, SymbolicReference):\n            object = object.object  # @ReservedAssignment\n        # END resolve references\n\n        is_detached = True\n        try:\n            is_detached = self.is_detached\n        except ValueError:\n            pass\n        # END handle non-existing ones\n\n        if is_detached:\n            return self.set_reference(object, logmsg)\n\n        # set the commit on our reference\n        return self._get_reference().set_object(object, logmsg)", "response": "Set the object we point to possibly dereference our symbolic reference first."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the Reference Object we point to", "response": "def _get_reference(self):\n        \"\"\":return: Reference Object we point to\n        :raise TypeError: If this symbolic reference is detached, hence it doesn't point\n            to a reference, but to a commit\"\"\"\n        sha, target_ref_path = self._get_ref_info(self.repo, self.path)\n        if target_ref_path is None:\n            raise TypeError(\"%s is a detached symbolic reference as it points to %r\" % (self, sha))\n        return self.from_path(self.repo, target_ref_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets our internal state to the given symbolic reference.", "response": "def set_reference(self, ref, logmsg=None):\n        \"\"\"Set ourselves to the given ref. It will stay a symbol if the ref is a Reference.\n        Otherwise an Object, given as Object instance or refspec, is assumed and if valid,\n        will be set which effectively detaches the refererence if it was a purely\n        symbolic one.\n\n        :param ref: SymbolicReference instance, Object instance or refspec string\n            Only if the ref is a SymbolicRef instance, we will point to it. Everything\n            else is dereferenced to obtain the actual object.\n        :param logmsg: If set to a string, the message will be used in the reflog.\n            Otherwise, a reflog entry is not written for the changed reference.\n            The previous commit of the entry will be the commit we point to now.\n\n            See also: log_append()\n\n        :return: self\n        :note: This symbolic reference will not be dereferenced. For that, see\n            ``set_object(...)``\"\"\"\n        write_value = None\n        obj = None\n        if isinstance(ref, SymbolicReference):\n            write_value = \"ref: %s\" % ref.path\n        elif isinstance(ref, Object):\n            obj = ref\n            write_value = ref.hexsha\n        elif isinstance(ref, string_types):\n            try:\n                obj = self.repo.rev_parse(ref + \"^{}\")    # optionally deref tags\n                write_value = obj.hexsha\n            except (BadObject, BadName):\n                raise ValueError(\"Could not extract object from %s\" % ref)\n            # END end try string\n        else:\n            raise ValueError(\"Unrecognized Value: %r\" % ref)\n        # END try commit attribute\n\n        # typecheck\n        if obj is not None and self._points_to_commits_only and obj.type != Commit.type:\n            raise TypeError(\"Require commit, got %r\" % obj)\n        # END verify type\n\n        oldbinsha = None\n        if logmsg is not None:\n            try:\n                oldbinsha = self.commit.binsha\n            except ValueError:\n                oldbinsha = Commit.NULL_BIN_SHA\n            # END handle non-existing\n        # END retrieve old hexsha\n\n        fpath = self.abspath\n        assure_directory_exists(fpath, is_file=True)\n\n        lfd = LockedFD(fpath)\n        fd = lfd.open(write=True, stream=True)\n        ok = True\n        try:\n            fd.write(write_value.encode('ascii') + b'\\n')\n            lfd.commit()\n            ok = True\n        finally:\n            if not ok:\n                lfd.rollback()\n        # Adjust the reflog\n        if logmsg is not None:\n            self.log_append(oldbinsha, logmsg)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef log_append(self, oldbinsha, message, newbinsha=None):\n        # NOTE: we use the committer of the currently active commit - this should be\n        # correct to allow overriding the committer on a per-commit level.\n        # See https://github.com/gitpython-developers/GitPython/pull/146\n        try:\n            committer_or_reader = self.commit.committer\n        except ValueError:\n            committer_or_reader = self.repo.config_reader()\n        # end handle newly cloned repositories\n        return RefLog.append_entry(committer_or_reader, RefLog.path(self), oldbinsha,\n                                   (newbinsha is None and self.commit.binsha) or newbinsha,\n                                   message)", "response": "Append a logentry to the logfile of this ref."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a path to a full repository - relative path which can be used to initialize a Reference instance by using Reference. from_path", "response": "def to_full_path(cls, path):\n        \"\"\"\n        :return: string with a full repository-relative path which can be used to initialize\n            a Reference instance, for instance by using ``Reference.from_path``\"\"\"\n        if isinstance(path, SymbolicReference):\n            path = path.path\n        full_ref_path = path\n        if not cls._common_path_default:\n            return full_ref_path\n        if not path.startswith(cls._common_path_default + \"/\"):\n            full_ref_path = '%s/%s' % (cls._common_path_default, path)\n        return full_ref_path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete(cls, repo, path):\n        full_ref_path = cls.to_full_path(path)\n        abs_path = osp.join(repo.common_dir, full_ref_path)\n        if osp.exists(abs_path):\n            os.remove(abs_path)\n        else:\n            # check packed refs\n            pack_file_path = cls._get_packed_refs_path(repo)\n            try:\n                with open(pack_file_path, 'rb') as reader:\n                    new_lines = []\n                    made_change = False\n                    dropped_last_line = False\n                    for line in reader:\n                        # keep line if it is a comment or if the ref to delete is not\n                        # in the line\n                        # If we deleted the last line and this one is a tag-reference object,\n                        # we drop it as well\n                        line = line.decode(defenc)\n                        if (line.startswith('#') or full_ref_path not in line) and \\\n                                (not dropped_last_line or dropped_last_line and not line.startswith('^')):\n                            new_lines.append(line)\n                            dropped_last_line = False\n                            continue\n                        # END skip comments and lines without our path\n\n                        # drop this line\n                        made_change = True\n                        dropped_last_line = True\n\n                # write the new lines\n                if made_change:\n                    # write-binary is required, otherwise windows will\n                    # open the file in text mode and change LF to CRLF !\n                    with open(pack_file_path, 'wb') as fd:\n                        fd.writelines(l.encode(defenc) for l in new_lines)\n\n            except (OSError, IOError):\n                pass  # it didn't exist at all\n\n        # delete the reflog\n        reflog_path = RefLog.path(cls(repo, full_ref_path))\n        if osp.isfile(reflog_path):\n            os.remove(reflog_path)", "response": "Delete the symbolic reference at the given path"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create(cls, repo, path, resolve, reference, force, logmsg=None):\n        git_dir = _git_dir(repo, path)\n        full_ref_path = cls.to_full_path(path)\n        abs_ref_path = osp.join(git_dir, full_ref_path)\n\n        # figure out target data\n        target = reference\n        if resolve:\n            target = repo.rev_parse(str(reference))\n\n        if not force and osp.isfile(abs_ref_path):\n            target_data = str(target)\n            if isinstance(target, SymbolicReference):\n                target_data = target.path\n            if not resolve:\n                target_data = \"ref: \" + target_data\n            with open(abs_ref_path, 'rb') as fd:\n                existing_data = fd.read().decode(defenc).strip()\n            if existing_data != target_data:\n                raise OSError(\"Reference at %r does already exist, pointing to %r, requested was %r\" %\n                              (full_ref_path, existing_data, target_data))\n        # END no force handling\n\n        ref = cls(repo, full_ref_path)\n        ref.set_reference(target, logmsg)\n        return ref", "response": "internal method used to create a new symbolic reference at the given path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new symbolic reference in the repository at the given path.", "response": "def create(cls, repo, path, reference='HEAD', force=False, logmsg=None):\n        \"\"\"Create a new symbolic reference, hence a reference pointing to another reference.\n\n        :param repo:\n            Repository to create the reference in\n\n        :param path:\n            full path at which the new symbolic reference is supposed to be\n            created at, i.e. \"NEW_HEAD\" or \"symrefs/my_new_symref\"\n\n        :param reference:\n            The reference to which the new symbolic reference should point to.\n            If it is a commit'ish, the symbolic ref will be detached.\n\n        :param force:\n            if True, force creation even if a symbolic reference with that name already exists.\n            Raise OSError otherwise\n\n        :param logmsg:\n            If not None, the message to append to the reflog. Otherwise no reflog\n            entry is written.\n\n        :return: Newly created symbolic Reference\n\n        :raise OSError:\n            If a (Symbolic)Reference with the same name but different contents\n            already exists.\n\n        :note: This does not alter the current HEAD, index or Working Tree\"\"\"\n        return cls._create(repo, path, cls._resolve_ref_on_create, reference, force, logmsg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rename(self, new_path, force=False):\n        new_path = self.to_full_path(new_path)\n        if self.path == new_path:\n            return self\n\n        new_abs_path = osp.join(_git_dir(self.repo, new_path), new_path)\n        cur_abs_path = osp.join(_git_dir(self.repo, self.path), self.path)\n        if osp.isfile(new_abs_path):\n            if not force:\n                # if they point to the same file, its not an error\n                with open(new_abs_path, 'rb') as fd1:\n                    f1 = fd1.read().strip()\n                with open(cur_abs_path, 'rb') as fd2:\n                    f2 = fd2.read().strip()\n                if f1 != f2:\n                    raise OSError(\"File at path %r already exists\" % new_abs_path)\n                # else: we could remove ourselves and use the otherone, but\n                # but clarity we just continue as usual\n            # END not force handling\n            os.remove(new_abs_path)\n        # END handle existing target file\n\n        dname = osp.dirname(new_abs_path)\n        if not osp.isdir(dname):\n            os.makedirs(dname)\n        # END create directory\n\n        os.rename(cur_abs_path, new_abs_path)\n        self.path = new_path\n\n        return self", "response": "Rename the current object to a new path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef iter_items(cls, repo, common_path=None):\n        return (r for r in cls._iter_items(repo, common_path) if r.__class__ == SymbolicReference or not r.is_detached)", "response": "Iterate over all refs in the repository"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reset(self, commit='HEAD', index=True, working_tree=False,\n              paths=None, **kwargs):\n        \"\"\"Reset our HEAD to the given commit optionally synchronizing\n        the index and working tree. The reference we refer to will be set to\n        commit as well.\n\n        :param commit:\n            Commit object, Reference Object or string identifying a revision we\n            should reset HEAD to.\n\n        :param index:\n            If True, the index will be set to match the given commit. Otherwise\n            it will not be touched.\n\n        :param working_tree:\n            If True, the working tree will be forcefully adjusted to match the given\n            commit, possibly overwriting uncommitted changes without warning.\n            If working_tree is True, index must be true as well\n\n        :param paths:\n            Single path or list of paths relative to the git root directory\n            that are to be reset. This allows to partially reset individual files.\n\n        :param kwargs:\n            Additional arguments passed to git-reset.\n\n        :return: self\"\"\"\n        mode = \"--soft\"\n        if index:\n            mode = \"--mixed\"\n\n            # it appears, some git-versions declare mixed and paths deprecated\n            # see http://github.com/Byron/GitPython/issues#issue/2\n            if paths:\n                mode = None\n            # END special case\n        # END handle index\n\n        if working_tree:\n            mode = \"--hard\"\n            if not index:\n                raise ValueError(\"Cannot reset the working tree if the index is not reset as well\")\n\n        # END working tree handling\n\n        try:\n            self.repo.git.reset(mode, commit, '--', paths, **kwargs)\n        except GitCommandError as e:\n            # git nowadays may use 1 as status to indicate there are still unstaged\n            # modifications after the reset\n            if e.status != 1:\n                raise\n        # END handle exception\n\n        return self", "response": "Reset our HEAD to the given commit optionally synchronizing the index and working tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes the given heads from the given repository.", "response": "def delete(cls, repo, *heads, **kwargs):\n        \"\"\"Delete the given heads\n\n        :param force:\n            If True, the heads will be deleted even if they are not yet merged into\n            the main development stream.\n            Default False\"\"\"\n        force = kwargs.get(\"force\", False)\n        flag = \"-d\"\n        if force:\n            flag = \"-D\"\n        repo.git.branch(flag, *heads)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_tracking_branch(self, remote_reference):\n        from .remote import RemoteReference\n        if remote_reference is not None and not isinstance(remote_reference, RemoteReference):\n            raise ValueError(\"Incorrect parameter type: %r\" % remote_reference)\n        # END handle type\n\n        with self.config_writer() as writer:\n            if remote_reference is None:\n                writer.remove_option(self.k_config_remote)\n                writer.remove_option(self.k_config_remote_ref)\n                if len(writer.options()) == 0:\n                    writer.remove_section()\n            else:\n                writer.set_value(self.k_config_remote, remote_reference.remote_name)\n                writer.set_value(self.k_config_remote_ref, Head.to_full_path(remote_reference.remote_head))\n\n        return self", "response": "Configure this branch to track the given remote reference."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tracking_branch(self):\n        from .remote import RemoteReference\n        reader = self.config_reader()\n        if reader.has_option(self.k_config_remote) and reader.has_option(self.k_config_remote_ref):\n            ref = Head(self.repo, Head.to_full_path(strip_quotes(reader.get_value(self.k_config_remote_ref))))\n            remote_refpath = RemoteReference.to_full_path(join_path(reader.get_value(self.k_config_remote), ref.name))\n            return RemoteReference(self.repo, remote_refpath)\n        # END handle have tracking branch\n\n        # we are not a tracking branch\n        return None", "response": "Return the remote_reference we are tracking branch or None if we are not tracking branch"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rename(self, new_path, force=False):\n        flag = \"-m\"\n        if force:\n            flag = \"-M\"\n\n        self.repo.git.branch(flag, self, new_path)\n        self.path = \"%s/%s\" % (self._common_path_default, new_path)\n        return self", "response": "Rename the current object to a new path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _deserialize(self, stream):\n        self.version, self.entries, self._extension_data, conten_sha = read_cache(stream)  # @UnusedVariable\n        return self", "response": "Initialize this instance with index values read from the given stream"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _entries_sorted(self):\n        return sorted(self.entries.values(), key=lambda e: (e.path, e.stage))", "response": "return list of entries in a sorted fashion"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write(self, file_path=None, ignore_extension_data=False):\n        # make sure we have our entries read before getting a write lock\n        # else it would be done when streaming. This can happen\n        # if one doesn't change the index, but writes it right away\n        self.entries\n        lfd = LockedFD(file_path or self._file_path)\n        stream = lfd.open(write=True, stream=True)\n\n        ok = False\n        try:\n            self._serialize(stream, ignore_extension_data)\n            ok = True\n        finally:\n            if not ok:\n                lfd.rollback()\n\n        lfd.commit()\n\n        # make sure we represent what we have written\n        if file_path is not None:\n            self._file_path = file_path", "response": "Writes the current state of the index to the given file path or to the given one."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef merge_tree(self, rhs, base=None):\n        # -i : ignore working tree status\n        # --aggressive : handle more merge cases\n        # -m : do an actual merge\n        args = [\"--aggressive\", \"-i\", \"-m\"]\n        if base is not None:\n            args.append(base)\n        args.append(rhs)\n\n        self.repo.git.read_tree(args)\n        return self", "response": "Merge the given rhs treeish into the current index."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new index file from the given treeish revisions.", "response": "def new(cls, repo, *tree_sha):\n        \"\"\" Merge the given treeish revisions into a new index which is returned.\n        This method behaves like git-read-tree --aggressive when doing the merge.\n\n        :param repo: The repository treeish are located in.\n\n        :param tree_sha:\n            20 byte or 40 byte tree sha or tree objects\n\n        :return:\n            New IndexFile instance. Its path will be undefined.\n            If you intend to write such a merged Index, supply an alternate file_path\n            to its 'write' method.\"\"\"\n        base_entries = aggressive_tree_merge(repo.odb, [to_bin_sha(str(t)) for t in tree_sha])\n\n        inst = cls(repo)\n        # convert to entries dict\n        entries = dict(izip(((e.path, e.stage) for e in base_entries),\n                            (IndexEntry.from_base(e) for e in base_entries)))\n\n        inst.entries = entries\n        return inst"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_tree(cls, repo, *treeish, **kwargs):\n        if len(treeish) == 0 or len(treeish) > 3:\n            raise ValueError(\"Please specify between 1 and 3 treeish, got %i\" % len(treeish))\n\n        arg_list = []\n        # ignore that working tree and index possibly are out of date\n        if len(treeish) > 1:\n            # drop unmerged entries when reading our index and merging\n            arg_list.append(\"--reset\")\n            # handle non-trivial cases the way a real merge does\n            arg_list.append(\"--aggressive\")\n        # END merge handling\n\n        # tmp file created in git home directory to be sure renaming\n        # works - /tmp/ dirs could be on another device\n        tmp_index = tempfile.mktemp('', '', repo.git_dir)\n        arg_list.append(\"--index-output=%s\" % tmp_index)\n        arg_list.extend(treeish)\n\n        # move current index out of the way - otherwise the merge may fail\n        # as it considers existing entries. moving it essentially clears the index.\n        # Unfortunately there is no 'soft' way to do it.\n        # The TemporaryFileSwap assure the original file get put back\n        index_handler = TemporaryFileSwap(join_path_native(repo.git_dir, 'index'))\n        try:\n            repo.git.read_tree(*arg_list, **kwargs)\n            index = cls(repo, tmp_index)\n            index.entries       # force it to read the file as we will delete the temp-file\n            del(index_handler)  # release as soon as possible\n        finally:\n            if osp.exists(tmp_index):\n                os.remove(tmp_index)\n        # END index merge handling\n\n        return index", "response": "Merge the given treeish revisions into a new index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\niterate over the list of paths and yield the corresponding paths accordingly.", "response": "def _iter_expand_paths(self, paths):\n        \"\"\"Expand the directories in list of paths to the corresponding paths accordingly,\n\n        Note: git will add items multiple times even if a glob overlapped\n        with manually specified paths or if paths where specified multiple\n        times - we respect that and do not prune\"\"\"\n        def raise_exc(e):\n            raise e\n        r = self.repo.working_tree_dir\n        rs = r + os.sep\n        for path in paths:\n            abs_path = path\n            if not osp.isabs(abs_path):\n                abs_path = osp.join(r, path)\n            # END make absolute path\n\n            try:\n                st = os.lstat(abs_path)     # handles non-symlinks as well\n            except OSError:\n                # the lstat call may fail as the path may contain globs as well\n                pass\n            else:\n                if S_ISLNK(st.st_mode):\n                    yield abs_path.replace(rs, '')\n                    continue\n            # end check symlink\n\n            # resolve globs if possible\n            if '?' in path or '*' in path or '[' in path:\n                resolved_paths = glob.glob(abs_path)\n                # not abs_path in resolved_paths:\n                #   a glob() resolving to the same path we are feeding it with\n                #   is a glob() that failed to resolve. If we continued calling\n                #   ourselves we'd endlessly recurse. If the condition below\n                #   evaluates to true then we are likely dealing with a file\n                #   whose name contains wildcard characters.\n                if abs_path not in resolved_paths:\n                    for f in self._iter_expand_paths(glob.glob(abs_path)):\n                        yield f.replace(rs, '')\n                    continue\n            # END glob handling\n            try:\n                for root, dirs, files in os.walk(abs_path, onerror=raise_exc):  # @UnusedVariable\n                    for rela_file in files:\n                        # add relative paths only\n                        yield osp.join(root.replace(rs, ''), rela_file)\n                    # END for each file in subdir\n                # END for each subdirectory\n            except OSError:\n                # was a file or something that could not be iterated\n                yield path.replace(rs, '')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites path to proc. stdin and make sure it processes the item and return stdout string.", "response": "def _write_path_to_stdin(self, proc, filepath, item, fmakeexc, fprogress,\n                             read_from_stdout=True):\n        \"\"\"Write path to proc.stdin and make sure it processes the item, including progress.\n\n        :return: stdout string\n        :param read_from_stdout: if True, proc.stdout will be read after the item\n            was sent to stdin. In that case, it will return None\n        :note: There is a bug in git-update-index that prevents it from sending\n            reports just in time. This is why we have a version that tries to\n            read stdout and one which doesn't. In fact, the stdout is not\n            important as the piped-in files are processed anyway and just in time\n        :note: Newlines are essential here, gits behaviour is somewhat inconsistent\n            on this depending on the version, hence we try our best to deal with\n            newlines carefully. Usually the last newline will not be sent, instead\n            we will close stdin to break the pipe.\"\"\"\n\n        fprogress(filepath, False, item)\n        rval = None\n        try:\n            proc.stdin.write((\"%s\\n\" % filepath).encode(defenc))\n        except IOError:\n            # pipe broke, usually because some error happened\n            raise fmakeexc()\n        # END write exception handling\n        proc.stdin.flush()\n        if read_from_stdout:\n            rval = proc.stdout.readline().strip()\n        fprogress(filepath, True, item)\n        return rval"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iter_blobs(self, predicate=lambda t: True):\n        for entry in mviter(self.entries):\n            blob = entry.to_blob(self.repo)\n            blob.size = entry.size\n            output = (entry.stage, blob)\n            if predicate(output):\n                yield output", "response": "Iterate over the blobs in the archive."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unmerged_blobs(self):\n        is_unmerged_blob = lambda t: t[0] != 0\n        path_map = {}\n        for stage, blob in self.iter_blobs(is_unmerged_blob):\n            path_map.setdefault(blob.path, []).append((stage, blob))\n        # END for each unmerged blob\n        for l in mviter(path_map):\n            l.sort()\n        return path_map", "response": "Iterator yielding the unmerged blobs in the index."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resolve_blobs(self, iter_blobs):\n        for blob in iter_blobs:\n            stage_null_key = (blob.path, 0)\n            if stage_null_key in self.entries:\n                raise ValueError(\"Path %r already exists at stage 0\" % blob.path)\n            # END assert blob is not stage 0 already\n\n            # delete all possible stages\n            for stage in (1, 2, 3):\n                try:\n                    del(self.entries[(blob.path, stage)])\n                except KeyError:\n                    pass\n                # END ignore key errors\n            # END for each possible stage\n\n            self.entries[stage_null_key] = IndexEntry.from_blob(blob)\n        # END for each blob\n\n        return self", "response": "Resolve the blobs given in blob iterator."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_tree(self):\n        # we obtain no lock as we just flush our contents to disk as tree\n        # If we are a new index, the entries access will load our data accordingly\n        mdb = MemoryDB()\n        entries = self._entries_sorted()\n        binsha, tree_items = write_tree_from_cache(entries, mdb, slice(0, len(entries)))\n\n        # copy changed trees only\n        mdb.stream_copy(mdb.sha_iter(), self.repo.odb)\n\n        # note: additional deserialization could be saved if write_tree_from_cache\n        # would return sorted tree entries\n        root_tree = Tree(self.repo, binsha, path='')\n        root_tree._cache = tree_items\n        return root_tree", "response": "Writes this index to a corresponding Tree object into the repository s object database and return it."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a path to a relative version of the git directory.", "response": "def _to_relative_path(self, path):\n        \"\"\":return: Version of path relative to our git directory or raise ValueError\n        if it is not within our git direcotory\"\"\"\n        if not osp.isabs(path):\n            return path\n        if self.repo.bare:\n            raise InvalidGitRepositoryError(\"require non-bare repository\")\n        if not path.startswith(self.repo.working_tree_dir):\n            raise ValueError(\"Absolute path %r is not in git repository at %r\" % (path, self.repo.working_tree_dir))\n        return os.path.relpath(path, self.repo.working_tree_dir)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsplits the items into two lists of path strings and BaseEntries.", "response": "def _preprocess_add_items(self, items):\n        \"\"\" Split the items into two lists of path strings and BaseEntries. \"\"\"\n        paths = []\n        entries = []\n\n        for item in items:\n            if isinstance(item, string_types):\n                paths.append(self._to_relative_path(item))\n            elif isinstance(item, (Blob, Submodule)):\n                entries.append(BaseIndexEntry.from_blob(item))\n            elif isinstance(item, BaseIndexEntry):\n                entries.append(item)\n            else:\n                raise TypeError(\"Invalid Type: %r\" % item)\n        # END for each item\n        return (paths, entries)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _store_path(self, filepath, fprogress):\n        st = os.lstat(filepath)     # handles non-symlinks as well\n        if S_ISLNK(st.st_mode):\n            # in PY3, readlink is string, but we need bytes. In PY2, it's just OS encoded bytes, we assume UTF-8\n            open_stream = lambda: BytesIO(force_bytes(os.readlink(filepath), encoding=defenc))\n        else:\n            open_stream = lambda: open(filepath, 'rb')\n        with open_stream() as stream:\n            fprogress(filepath, False, filepath)\n            istream = self.repo.odb.store(IStream(Blob.type, st.st_size, stream))\n            fprogress(filepath, True, filepath)\n        return BaseIndexEntry((stat_mode_to_index_mode(st.st_mode),\n                               istream.binsha, 0, to_native_path_linux(filepath)))", "response": "Store a file at filepath in the database and return the base index entry"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, items, force=True, fprogress=lambda *args: None, path_rewriter=None,\n            write=True, write_extension_data=False):\n        \"\"\"Add files from the working tree, specific blobs or BaseIndexEntries\n        to the index.\n\n        :param items:\n            Multiple types of items are supported, types can be mixed within one call.\n            Different types imply a different handling. File paths may generally be\n            relative or absolute.\n\n            - path string\n                strings denote a relative or absolute path into the repository pointing to\n                an existing file, i.e. CHANGES, lib/myfile.ext, '/home/gitrepo/lib/myfile.ext'.\n\n                Absolute paths must start with working tree directory of this index's repository\n                to be considered valid. For example, if it was initialized with a non-normalized path, like\n                `/root/repo/../repo`, absolute paths to be added must start with `/root/repo/../repo`.\n\n                Paths provided like this must exist. When added, they will be written\n                into the object database.\n\n                PathStrings may contain globs, such as 'lib/__init__*' or can be directories\n                like 'lib', the latter ones will add all the files within the dirctory and\n                subdirectories.\n\n                This equals a straight git-add.\n\n                They are added at stage 0\n\n            - Blob or Submodule object\n                Blobs are added as they are assuming a valid mode is set.\n                The file they refer to may or may not exist in the file system, but\n                must be a path relative to our repository.\n\n                If their sha is null ( 40*0 ), their path must exist in the file system\n                relative to the git repository as an object will be created from\n                the data at the path.\n                The handling now very much equals the way string paths are processed, except that\n                the mode you have set will be kept. This allows you to create symlinks\n                by settings the mode respectively and writing the target of the symlink\n                directly into the file. This equals a default Linux-Symlink which\n                is not dereferenced automatically, except that it can be created on\n                filesystems not supporting it as well.\n\n                Please note that globs or directories are not allowed in Blob objects.\n\n                They are added at stage 0\n\n            - BaseIndexEntry or type\n                Handling equals the one of Blob objects, but the stage may be\n                explicitly set. Please note that Index Entries require binary sha's.\n\n        :param force:\n            **CURRENTLY INEFFECTIVE**\n            If True, otherwise ignored or excluded files will be\n            added anyway.\n            As opposed to the git-add command, we enable this flag by default\n            as the API user usually wants the item to be added even though\n            they might be excluded.\n\n        :param fprogress:\n            Function with signature f(path, done=False, item=item) called for each\n            path to be added, one time once it is about to be added where done==False\n            and once after it was added where done=True.\n            item is set to the actual item we handle, either a Path or a BaseIndexEntry\n            Please note that the processed path is not guaranteed to be present\n            in the index already as the index is currently being processed.\n\n        :param path_rewriter:\n            Function with signature (string) func(BaseIndexEntry) function returning a path\n            for each passed entry which is the path to be actually recorded for the\n            object created from entry.path. This allows you to write an index which\n            is not identical to the layout of the actual files on your hard-disk.\n            If not None and ``items`` contain plain paths, these paths will be\n            converted to Entries beforehand and passed to the path_rewriter.\n            Please note that entry.path is relative to the git repository.\n\n        :param write:\n            If True, the index will be written once it was altered. Otherwise\n            the changes only exist in memory and are not available to git commands.\n\n        :param write_extension_data:\n            If True, extension data will be written back to the index. This can lead to issues in case\n            it is containing the 'TREE' extension, which will cause the `git commit` command to write an\n            old tree, instead of a new one representing the now changed index.\n            This doesn't matter if you use `IndexFile.commit()`, which ignores the `TREE` extension altogether.\n            You should set it to True if you intend to use `IndexFile.commit()` exclusively while maintaining\n            support for third-party extensions. Besides that, you can usually safely ignore the built-in\n            extensions when using GitPython on repositories that are not handled manually at all.\n            All current built-in extensions are listed here:\n            http://opensource.apple.com/source/Git/Git-26/src/git-htmldocs/technical/index-format.txt\n\n        :return:\n            List(BaseIndexEntries) representing the entries just actually added.\n\n        :raise OSError:\n            if a supplied Path did not exist. Please note that BaseIndexEntry\n            Objects that do not have a null sha will be added even if their paths\n            do not exist.\n        \"\"\"\n        # sort the entries into strings and Entries, Blobs are converted to entries\n        # automatically\n        # paths can be git-added, for everything else we use git-update-index\n        paths, entries = self._preprocess_add_items(items)\n        entries_added = []\n        # This code needs a working tree, therefore we try not to run it unless required.\n        # That way, we are OK on a bare repository as well.\n        # If there are no paths, the rewriter has nothing to do either\n        if paths:\n            entries_added.extend(self._entries_for_paths(paths, path_rewriter, fprogress, entries))\n\n        # HANDLE ENTRIES\n        if entries:\n            null_mode_entries = [e for e in entries if e.mode == 0]\n            if null_mode_entries:\n                raise ValueError(\n                    \"At least one Entry has a null-mode - please use index.remove to remove files for clarity\")\n            # END null mode should be remove\n\n            # HANLDE ENTRY OBJECT CREATION\n            # create objects if required, otherwise go with the existing shas\n            null_entries_indices = [i for i, e in enumerate(entries) if e.binsha == Object.NULL_BIN_SHA]\n            if null_entries_indices:\n                @git_working_dir\n                def handle_null_entries(self):\n                    for ei in null_entries_indices:\n                        null_entry = entries[ei]\n                        new_entry = self._store_path(null_entry.path, fprogress)\n\n                        # update null entry\n                        entries[ei] = BaseIndexEntry(\n                            (null_entry.mode, new_entry.binsha, null_entry.stage, null_entry.path))\n                    # END for each entry index\n                # end closure\n                handle_null_entries(self)\n            # END null_entry handling\n\n            # REWRITE PATHS\n            # If we have to rewrite the entries, do so now, after we have generated\n            # all object sha's\n            if path_rewriter:\n                for i, e in enumerate(entries):\n                    entries[i] = BaseIndexEntry((e.mode, e.binsha, e.stage, path_rewriter(e)))\n                # END for each entry\n            # END handle path rewriting\n\n            # just go through the remaining entries and provide progress info\n            for i, entry in enumerate(entries):\n                progress_sent = i in null_entries_indices\n                if not progress_sent:\n                    fprogress(entry.path, False, entry)\n                    fprogress(entry.path, True, entry)\n                # END handle progress\n            # END for each enty\n            entries_added.extend(entries)\n        # END if there are base entries\n\n        # FINALIZE\n        # add the new entries to this instance\n        for entry in entries_added:\n            self.entries[(entry.path, 0)] = IndexEntry.from_base(entry)\n\n        if write:\n            self.write(ignore_extension_data=not write_extension_data)\n        # END handle write\n\n        return entries_added", "response": "Add files from the working tree and specific blobs or BaseIndexEntries to the index."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _items_to_rela_paths(self, items):\n        paths = []\n        for item in items:\n            if isinstance(item, (BaseIndexEntry, (Blob, Submodule))):\n                paths.append(self._to_relative_path(item.path))\n            elif isinstance(item, string_types):\n                paths.append(self._to_relative_path(item))\n            else:\n                raise TypeError(\"Invalid item type: %r\" % item)\n        # END for each item\n        return paths", "response": "Returns a list of repo - relative paths from the given items which\n        may be absolute or relative paths entries or blobs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving the given items from the index and optionally from the working tree as well.", "response": "def remove(self, items, working_tree=False, **kwargs):\n        \"\"\"Remove the given items from the index and optionally from\n        the working tree as well.\n\n        :param items:\n            Multiple types of items are supported which may be be freely mixed.\n\n            - path string\n                Remove the given path at all stages. If it is a directory, you must\n                specify the r=True keyword argument to remove all file entries\n                below it. If absolute paths are given, they will be converted\n                to a path relative to the git repository directory containing\n                the working tree\n\n                The path string may include globs, such as *.c.\n\n            - Blob Object\n                Only the path portion is used in this case.\n\n            - BaseIndexEntry or compatible type\n                The only relevant information here Yis the path. The stage is ignored.\n\n        :param working_tree:\n            If True, the entry will also be removed from the working tree, physically\n            removing the respective file. This may fail if there are uncommitted changes\n            in it.\n\n        :param kwargs:\n            Additional keyword arguments to be passed to git-rm, such\n            as 'r' to allow recursive removal of\n\n        :return:\n            List(path_string, ...) list of repository relative paths that have\n            been removed effectively.\n            This is interesting to know in case you have provided a directory or\n            globs. Paths are relative to the repository. \"\"\"\n        args = []\n        if not working_tree:\n            args.append(\"--cached\")\n        args.append(\"--\")\n\n        # preprocess paths\n        paths = self._items_to_rela_paths(items)\n        removed_paths = self.repo.git.rm(args, paths, **kwargs).splitlines()\n\n        # process output to gain proper paths\n        # rm 'path'\n        return [p[4:-1] for p in removed_paths]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrenaming the items whereas the last item is considered the destination of the move operation.", "response": "def move(self, items, skip_errors=False, **kwargs):\n        \"\"\"Rename/move the items, whereas the last item is considered the destination of\n        the move operation. If the destination is a file, the first item ( of two )\n        must be a file as well. If the destination is a directory, it may be preceded\n        by one or more directories or files.\n\n        The working tree will be affected in non-bare repositories.\n\n        :parma items:\n            Multiple types of items are supported, please see the 'remove' method\n            for reference.\n        :param skip_errors:\n            If True, errors such as ones resulting from missing source files will\n            be skipped.\n        :param kwargs:\n            Additional arguments you would like to pass to git-mv, such as dry_run\n            or force.\n\n        :return:List(tuple(source_path_string, destination_path_string), ...)\n            A list of pairs, containing the source file moved as well as its\n            actual destination. Relative to the repository root.\n\n        :raise ValueError: If only one item was given\n            GitCommandError: If git could not handle your request\"\"\"\n        args = []\n        if skip_errors:\n            args.append('-k')\n\n        paths = self._items_to_rela_paths(items)\n        if len(paths) < 2:\n            raise ValueError(\"Please provide at least one source and one destination of the move operation\")\n\n        was_dry_run = kwargs.pop('dry_run', kwargs.pop('n', None))\n        kwargs['dry_run'] = True\n\n        # first execute rename in dryrun so the command tells us what it actually does\n        # ( for later output )\n        out = []\n        mvlines = self.repo.git.mv(args, paths, **kwargs).splitlines()\n\n        # parse result - first 0:n/2 lines are 'checking ', the remaining ones\n        # are the 'renaming' ones which we parse\n        for ln in xrange(int(len(mvlines) / 2), len(mvlines)):\n            tokens = mvlines[ln].split(' to ')\n            assert len(tokens) == 2, \"Too many tokens in %s\" % mvlines[ln]\n\n            # [0] = Renaming x\n            # [1] = y\n            out.append((tokens[0][9:], tokens[1]))\n        # END for each line to parse\n\n        # either prepare for the real run, or output the dry-run result\n        if was_dry_run:\n            return out\n        # END handle dryrun\n\n        # now apply the actual operation\n        kwargs.pop('dry_run')\n        self.repo.git.mv(args, paths, **kwargs)\n\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef commit(self, message, parent_commits=None, head=True, author=None,\n               committer=None, author_date=None, commit_date=None,\n               skip_hooks=False):\n        \"\"\"Commit the current default index file, creating a commit object.\n        For more information on the arguments, see tree.commit.\n\n        :note: If you have manually altered the .entries member of this instance,\n               don't forget to write() your changes to disk beforehand.\n               Passing skip_hooks=True is the equivalent of using `-n`\n               or `--no-verify` on the command line.\n        :return: Commit object representing the new commit\"\"\"\n        if not skip_hooks:\n            run_commit_hook('pre-commit', self)\n\n            self._write_commit_editmsg(message)\n            run_commit_hook('commit-msg', self, self._commit_editmsg_filepath())\n            message = self._read_commit_editmsg()\n            self._remove_commit_editmsg()\n        tree = self.write_tree()\n        rval = Commit.create_from_tree(self.repo, tree, message, parent_commits,\n                                       head, author=author, committer=committer,\n                                       author_date=author_date, commit_date=commit_date)\n        if not skip_hooks:\n            run_commit_hook('post-commit', self)\n        return rval", "response": "Commit the current default index file creating a new commit object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset(self, commit='HEAD', working_tree=False, paths=None, head=False, **kwargs):\n        # what we actually want to do is to merge the tree into our existing\n        # index, which is what git-read-tree does\n        new_inst = type(self).from_tree(self.repo, commit)\n        if not paths:\n            self.entries = new_inst.entries\n        else:\n            nie = new_inst.entries\n            for path in paths:\n                path = self._to_relative_path(path)\n                try:\n                    key = entry_key(path, 0)\n                    self.entries[key] = nie[key]\n                except KeyError:\n                    # if key is not in theirs, it musn't be in ours\n                    try:\n                        del(self.entries[key])\n                    except KeyError:\n                        pass\n                    # END handle deletion keyerror\n                # END handle keyerror\n            # END for each path\n        # END handle paths\n        self.write()\n\n        if working_tree:\n            self.checkout(paths=paths, force=True)\n        # END handle working tree\n\n        if head:\n            self.repo.head.set_commit(self.repo.commit(commit), logmsg=\"%s: Updating HEAD\" % commit)\n        # END handle head change\n\n        return self", "response": "Reset the index to reflect the tree at the given commit."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, previous_commit=None, recursive=True, force_remove=False, init=True,\n               to_latest_revision=False, progress=None, dry_run=False, force_reset=False,\n               keep_going=False):\n        \"\"\"Update the submodules of this repository to the current HEAD commit.\n        This method behaves smartly by determining changes of the path of a submodules\n        repository, next to changes to the to-be-checked-out commit or the branch to be\n        checked out. This works if the submodules ID does not change.\n        Additionally it will detect addition and removal of submodules, which will be handled\n        gracefully.\n\n        :param previous_commit: If set to a commit'ish, the commit we should use\n            as the previous commit the HEAD pointed to before it was set to the commit it points to now.\n            If None, it defaults to HEAD@{1} otherwise\n        :param recursive: if True, the children of submodules will be updated as well\n            using the same technique\n        :param force_remove: If submodules have been deleted, they will be forcibly removed.\n            Otherwise the update may fail if a submodule's repository cannot be deleted as\n            changes have been made to it (see Submodule.update() for more information)\n        :param init: If we encounter a new module which would need to be initialized, then do it.\n        :param to_latest_revision: If True, instead of checking out the revision pointed to\n            by this submodule's sha, the checked out tracking branch will be merged with the\n            latest remote branch fetched from the repository's origin.\n            Unless force_reset is specified, a local tracking branch will never be reset into its past, therefore\n            the remote branch must be in the future for this to have an effect.\n        :param force_reset: if True, submodules may checkout or reset their branch even if the repository has\n            pending changes that would be overwritten, or if the local tracking branch is in the future of the\n            remote tracking branch and would be reset into its past.\n        :param progress: RootUpdateProgress instance or None if no progress should be sent\n        :param dry_run: if True, operations will not actually be performed. Progress messages\n            will change accordingly to indicate the WOULD DO state of the operation.\n        :param keep_going: if True, we will ignore but log all errors, and keep going recursively.\n            Unless dry_run is set as well, keep_going could cause subsequent/inherited errors you wouldn't see\n            otherwise.\n            In conjunction with dry_run, it can be useful to anticipate all errors when updating submodules\n        :return: self\"\"\"\n        if self.repo.bare:\n            raise InvalidGitRepositoryError(\"Cannot update submodules in bare repositories\")\n        # END handle bare\n\n        if progress is None:\n            progress = RootUpdateProgress()\n        # END assure progress is set\n\n        prefix = ''\n        if dry_run:\n            prefix = 'DRY-RUN: '\n\n        repo = self.repo\n\n        try:\n            # SETUP BASE COMMIT\n            ###################\n            cur_commit = repo.head.commit\n            if previous_commit is None:\n                try:\n                    previous_commit = repo.commit(repo.head.log_entry(-1).oldhexsha)\n                    if previous_commit.binsha == previous_commit.NULL_BIN_SHA:\n                        raise IndexError\n                    # END handle initial commit\n                except IndexError:\n                    # in new repositories, there is no previous commit\n                    previous_commit = cur_commit\n                # END exception handling\n            else:\n                previous_commit = repo.commit(previous_commit)   # obtain commit object\n            # END handle previous commit\n\n            psms = self.list_items(repo, parent_commit=previous_commit)\n            sms = self.list_items(repo)\n            spsms = set(psms)\n            ssms = set(sms)\n\n            # HANDLE REMOVALS\n            ###################\n            rrsm = (spsms - ssms)\n            len_rrsm = len(rrsm)\n\n            for i, rsm in enumerate(rrsm):\n                op = REMOVE\n                if i == 0:\n                    op |= BEGIN\n                # END handle begin\n\n                # fake it into thinking its at the current commit to allow deletion\n                # of previous module. Trigger the cache to be updated before that\n                progress.update(op, i, len_rrsm, prefix + \"Removing submodule %r at %s\" % (rsm.name, rsm.abspath))\n                rsm._parent_commit = repo.head.commit\n                rsm.remove(configuration=False, module=True, force=force_remove, dry_run=dry_run)\n\n                if i == len_rrsm - 1:\n                    op |= END\n                # END handle end\n                progress.update(op, i, len_rrsm, prefix + \"Done removing submodule %r\" % rsm.name)\n            # END for each removed submodule\n\n            # HANDLE PATH RENAMES\n            #####################\n            # url changes + branch changes\n            csms = (spsms & ssms)\n            len_csms = len(csms)\n            for i, csm in enumerate(csms):\n                psm = psms[csm.name]\n                sm = sms[csm.name]\n\n                # PATH CHANGES\n                ##############\n                if sm.path != psm.path and psm.module_exists():\n                    progress.update(BEGIN | PATHCHANGE, i, len_csms, prefix +\n                                    \"Moving repository of submodule %r from %s to %s\"\n                                    % (sm.name, psm.abspath, sm.abspath))\n                    # move the module to the new path\n                    if not dry_run:\n                        psm.move(sm.path, module=True, configuration=False)\n                    # END handle dry_run\n                    progress.update(\n                        END | PATHCHANGE, i, len_csms, prefix + \"Done moving repository of submodule %r\" % sm.name)\n                # END handle path changes\n\n                if sm.module_exists():\n                    # HANDLE URL CHANGE\n                    ###################\n                    if sm.url != psm.url:\n                        # Add the new remote, remove the old one\n                        # This way, if the url just changes, the commits will not\n                        # have to be re-retrieved\n                        nn = '__new_origin__'\n                        smm = sm.module()\n                        rmts = smm.remotes\n\n                        # don't do anything if we already have the url we search in place\n                        if len([r for r in rmts if r.url == sm.url]) == 0:\n                            progress.update(BEGIN | URLCHANGE, i, len_csms, prefix +\n                                            \"Changing url of submodule %r from %s to %s\" % (sm.name, psm.url, sm.url))\n\n                            if not dry_run:\n                                assert nn not in [r.name for r in rmts]\n                                smr = smm.create_remote(nn, sm.url)\n                                smr.fetch(progress=progress)\n\n                                # If we have a tracking branch, it should be available\n                                # in the new remote as well.\n                                if len([r for r in smr.refs if r.remote_head == sm.branch_name]) == 0:\n                                    raise ValueError(\n                                        \"Submodule branch named %r was not available in new submodule remote at %r\"\n                                        % (sm.branch_name, sm.url)\n                                    )\n                                # END head is not detached\n\n                                # now delete the changed one\n                                rmt_for_deletion = None\n                                for remote in rmts:\n                                    if remote.url == psm.url:\n                                        rmt_for_deletion = remote\n                                        break\n                                    # END if urls match\n                                # END for each remote\n\n                                # if we didn't find a matching remote, but have exactly one,\n                                # we can safely use this one\n                                if rmt_for_deletion is None:\n                                    if len(rmts) == 1:\n                                        rmt_for_deletion = rmts[0]\n                                    else:\n                                        # if we have not found any remote with the original url\n                                        # we may not have a name. This is a special case,\n                                        # and its okay to fail here\n                                        # Alternatively we could just generate a unique name and leave all\n                                        # existing ones in place\n                                        raise InvalidGitRepositoryError(\n                                            \"Couldn't find original remote-repo at url %r\" % psm.url)\n                                    # END handle one single remote\n                                # END handle check we found a remote\n\n                                orig_name = rmt_for_deletion.name\n                                smm.delete_remote(rmt_for_deletion)\n                                # NOTE: Currently we leave tags from the deleted remotes\n                                # as well as separate tracking branches in the possibly totally\n                                # changed repository ( someone could have changed the url to\n                                # another project ). At some point, one might want to clean\n                                # it up, but the danger is high to remove stuff the user\n                                # has added explicitly\n\n                                # rename the new remote back to what it was\n                                smr.rename(orig_name)\n\n                                # early on, we verified that the our current tracking branch\n                                # exists in the remote. Now we have to assure that the\n                                # sha we point to is still contained in the new remote\n                                # tracking branch.\n                                smsha = sm.binsha\n                                found = False\n                                rref = smr.refs[self.branch_name]\n                                for c in rref.commit.traverse():\n                                    if c.binsha == smsha:\n                                        found = True\n                                        break\n                                    # END traverse all commits in search for sha\n                                # END for each commit\n\n                                if not found:\n                                    # adjust our internal binsha to use the one of the remote\n                                    # this way, it will be checked out in the next step\n                                    # This will change the submodule relative to us, so\n                                    # the user will be able to commit the change easily\n                                    log.warn(\"Current sha %s was not contained in the tracking\\\n             branch at the new remote, setting it the the remote's tracking branch\", sm.hexsha)\n                                    sm.binsha = rref.commit.binsha\n                                # END reset binsha\n\n                                # NOTE: All checkout is performed by the base implementation of update\n                            # END handle dry_run\n                            progress.update(\n                                END | URLCHANGE, i, len_csms, prefix + \"Done adjusting url of submodule %r\" % (sm.name))\n                        # END skip remote handling if new url already exists in module\n                    # END handle url\n\n                    # HANDLE PATH CHANGES\n                    #####################\n                    if sm.branch_path != psm.branch_path:\n                        # finally, create a new tracking branch which tracks the\n                        # new remote branch\n                        progress.update(BEGIN | BRANCHCHANGE, i, len_csms, prefix +\n                                        \"Changing branch of submodule %r from %s to %s\"\n                                        % (sm.name, psm.branch_path, sm.branch_path))\n                        if not dry_run:\n                            smm = sm.module()\n                            smmr = smm.remotes\n                            # As the branch might not exist yet, we will have to fetch all remotes to be sure ... .\n                            for remote in smmr:\n                                remote.fetch(progress=progress)\n                            # end for each remote\n\n                            try:\n                                tbr = git.Head.create(smm, sm.branch_name, logmsg='branch: Created from HEAD')\n                            except OSError:\n                                # ... or reuse the existing one\n                                tbr = git.Head(smm, sm.branch_path)\n                            # END assure tracking branch exists\n\n                            tbr.set_tracking_branch(find_first_remote_branch(smmr, sm.branch_name))\n                            # NOTE: All head-resetting is done in the base implementation of update\n                            # but we will have to checkout the new branch here. As it still points to the currently\n                            # checkout out commit, we don't do any harm.\n                            # As we don't want to update working-tree or index, changing the ref is all there is to do\n                            smm.head.reference = tbr\n                        # END handle dry_run\n\n                        progress.update(\n                            END | BRANCHCHANGE, i, len_csms, prefix + \"Done changing branch of submodule %r\" % sm.name)\n                    # END handle branch\n                # END handle\n            # END for each common submodule\n        except Exception as err:\n            if not keep_going:\n                raise\n            log.error(str(err))\n        # end handle keep_going\n\n        # FINALLY UPDATE ALL ACTUAL SUBMODULES\n        ######################################\n        for sm in sms:\n            # update the submodule using the default method\n            sm.update(recursive=False, init=init, to_latest_revision=to_latest_revision,\n                      progress=progress, dry_run=dry_run, force=force_reset, keep_going=keep_going)\n\n            # update recursively depth first - question is which inconsitent\n            # state will be better in case it fails somewhere. Defective branch\n            # or defective depth. The RootSubmodule type will never process itself,\n            # which was done in the previous expression\n            if recursive:\n                # the module would exist by now if we are not in dry_run mode\n                if sm.module_exists():\n                    type(self)(sm.module()).update(recursive=True, force_remove=force_remove,\n                                                   init=init, to_latest_revision=to_latest_revision,\n                                                   progress=progress, dry_run=dry_run, force_reset=force_reset,\n                                                   keep_going=keep_going)\n                # END handle dry_run\n            # END handle recursive\n        # END for each submodule to update\n\n        return self", "response": "Update the submodules of this repository to the current HEAD commit."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef commit(self):\n        obj = self.object\n        while obj.type != 'commit':\n            if obj.type == \"tag\":\n                # it is a tag object which carries the commit as an object - we can point to anything\n                obj = obj.object\n            else:\n                raise ValueError((\"Cannot resolve commit as tag %s points to a %s object - \" +\n                                  \"use the `.object` property instead to access it\") % (self, obj.type))\n        return obj", "response": "Return the object the tag ref points to\n        \n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new tag reference.", "response": "def create(cls, repo, path, ref='HEAD', message=None, force=False, **kwargs):\n        \"\"\"Create a new tag reference.\n\n        :param path:\n            The name of the tag, i.e. 1.0 or releases/1.0.\n            The prefix refs/tags is implied\n\n        :param ref:\n            A reference to the object you want to tag. It can be a commit, tree or\n            blob.\n\n        :param message:\n            If not None, the message will be used in your tag object. This will also\n            create an additional tag object that allows to obtain that information, i.e.::\n\n                tagref.tag.message\n\n        :param force:\n            If True, to force creation of a tag even though that tag already exists.\n\n        :param kwargs:\n            Additional keyword arguments to be passed to git-tag\n\n        :return: A new TagReference\"\"\"\n        args = (path, ref)\n        if message:\n            kwargs['m'] = message\n        if force:\n            kwargs['f'] = True\n\n        repo.git.tag(*args, **kwargs)\n        return TagReference(repo, \"%s/%s\" % (cls._common_path_default, path))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tree_to_stream(entries, write):\n    ord_zero = ord('0')\n    bit_mask = 7            # 3 bits set\n\n    for binsha, mode, name in entries:\n        mode_str = b''\n        for i in xrange(6):\n            mode_str = bchr(((mode >> (i * 3)) & bit_mask) + ord_zero) + mode_str\n        # END for each 8 octal value\n\n        # git slices away the first octal if its zero\n        if byte_ord(mode_str[0]) == ord_zero:\n            mode_str = mode_str[1:]\n        # END save a byte\n\n        # here it comes:  if the name is actually unicode, the replacement below\n        # will not work as the binsha is not part of the ascii unicode encoding -\n        # hence we must convert to an utf8 string for it to work properly.\n        # According to my tests, this is exactly what git does, that is it just\n        # takes the input literally, which appears to be utf8 on linux.\n        if isinstance(name, text_type):\n            name = name.encode(defenc)\n        write(b''.join((mode_str, b' ', name, b'\\0', binsha)))", "response": "Write the given list of entries into a stream using the given write method."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the binary representation of a tree and returns a list of tuples of Tree items.", "response": "def tree_entries_from_data(data):\n    \"\"\"Reads the binary representation of a tree and returns tuples of Tree items\n    :param data: data block with tree data (as bytes)\n    :return: list(tuple(binsha, mode, tree_relative_path), ...)\"\"\"\n    ord_zero = ord('0')\n    space_ord = ord(' ')\n    len_data = len(data)\n    i = 0\n    out = []\n    while i < len_data:\n        mode = 0\n\n        # read mode\n        # Some git versions truncate the leading 0, some don't\n        # The type will be extracted from the mode later\n        while byte_ord(data[i]) != space_ord:\n            # move existing mode integer up one level being 3 bits\n            # and add the actual ordinal value of the character\n            mode = (mode << 3) + (byte_ord(data[i]) - ord_zero)\n            i += 1\n        # END while reading mode\n\n        # byte is space now, skip it\n        i += 1\n\n        # parse name, it is NULL separated\n\n        ns = i\n        while byte_ord(data[i]) != 0:\n            i += 1\n        # END while not reached NULL\n\n        # default encoding for strings in git is utf8\n        # Only use the respective unicode object if the byte stream was encoded\n        name = data[ns:i]\n        name = safe_decode(name)\n\n        # byte is NULL, get next 20\n        i += 1\n        sha = data[i:i + 20]\n        i = i + 20\n        out.append((sha, mode, name))\n    # END for each byte in data stream\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the data entry matching the given name and tree mode or None", "response": "def _find_by_name(tree_data, name, is_dir, start_at):\n    \"\"\"return data entry matching the given name and tree mode\n    or None.\n    Before the item is returned, the respective data item is set\n    None in the tree_data list to mark it done\"\"\"\n    try:\n        item = tree_data[start_at]\n        if item and item[2] == name and S_ISDIR(item[1]) == is_dir:\n            tree_data[start_at] = None\n            return item\n    except IndexError:\n        pass\n    # END exception handling\n    for index, item in enumerate(tree_data):\n        if item and item[2] == name and S_ISDIR(item[1]) == is_dir:\n            tree_data[index] = None\n            return item\n        # END if item matches\n    # END for each item\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef traverse_tree_recursive(odb, tree_sha, path_prefix):\n    entries = []\n    data = tree_entries_from_data(odb.stream(tree_sha).read())\n\n    # unpacking/packing is faster than accessing individual items\n    for sha, mode, name in data:\n        if S_ISDIR(mode):\n            entries.extend(traverse_tree_recursive(odb, sha, path_prefix + name + '/'))\n        else:\n            entries.append((sha, mode, path_prefix + name))\n    # END for each item\n\n    return entries", "response": "Recursively traverses the tree tree_sha and returns a list of entries"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef needs_values(func):\n\n    @wraps(func)\n    def assure_data_present(self, *args, **kwargs):\n        self.read()\n        return func(self, *args, **kwargs)\n    # END wrapper method\n    return assure_data_present", "response": "Returns method assuring we read values on demand before we try to access them"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a method that checks whether given function may be called.", "response": "def set_dirty_and_flush_changes(non_const_func):\n    \"\"\"Return method that checks whether given non constant function may be called.\n    If so, the instance will be set dirty.\n    Additionally, we flush the changes right to disk\"\"\"\n\n    def flush_changes(self, *args, **kwargs):\n        rval = non_const_func(self, *args, **kwargs)\n        self._dirty = True\n        self.write()\n        return rval\n    # END wrapper method\n    flush_changes.__name__ = non_const_func.__name__\n    return flush_changes"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls the configuration at the given method which must take a section name as first argument", "response": "def _call_config(self, method, *args, **kwargs):\n        \"\"\"Call the configuration at the given method which must take a section name\n        as first argument\"\"\"\n        return getattr(self._config, method)(self._section_name, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nflushing changes and release the configuration write lock.", "response": "def release(self):\n        \"\"\"Flush changes and release the configuration write lock. This instance must not be used anymore afterwards.\n        In Python 3, it's required to explicitly release locks and flush changes, as __del__ is not called\n        deterministically anymore.\"\"\"\n        # checking for the lock here makes sure we do not raise during write()\n        # in case an invalid parser was created who could not get a lock\n        if self.read_only or (self._lock and not self._lock._has_lock()):\n            return\n\n        try:\n            try:\n                self.write()\n            except IOError:\n                log.error(\"Exception during destruction of GitConfigParser\", exc_info=True)\n            except ReferenceError:\n                # This happens in PY3 ... and usually means that some state cannot be written\n                # as the sections dict cannot be iterated\n                # Usually when shutting down the interpreter, don'y know how to fix this\n                pass\n        finally:\n            self._lock._release_lock()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read(self, fp, fpname):\n        cursect = None                            # None, or a dictionary\n        optname = None\n        lineno = 0\n        is_multi_line = False\n        e = None                                  # None, or an exception\n\n        def string_decode(v):\n            if v[-1] == '\\\\':\n                v = v[:-1]\n            # end cut trailing escapes to prevent decode error\n\n            if PY3:\n                return v.encode(defenc).decode('unicode_escape')\n            else:\n                return v.decode('string_escape')\n            # end\n        # end\n\n        while True:\n            # we assume to read binary !\n            line = fp.readline().decode(defenc)\n            if not line:\n                break\n            lineno = lineno + 1\n            # comment or blank line?\n            if line.strip() == '' or self.re_comment.match(line):\n                continue\n            if line.split(None, 1)[0].lower() == 'rem' and line[0] in \"rR\":\n                # no leading whitespace\n                continue\n\n            # is it a section header?\n            mo = self.SECTCRE.match(line.strip())\n            if not is_multi_line and mo:\n                sectname = mo.group('header').strip()\n                if sectname in self._sections:\n                    cursect = self._sections[sectname]\n                elif sectname == cp.DEFAULTSECT:\n                    cursect = self._defaults\n                else:\n                    cursect = self._dict((('__name__', sectname),))\n                    self._sections[sectname] = cursect\n                    self._proxies[sectname] = None\n                # So sections can't start with a continuation line\n                optname = None\n            # no section header in the file?\n            elif cursect is None:\n                raise cp.MissingSectionHeaderError(fpname, lineno, line)\n            # an option line?\n            elif not is_multi_line:\n                mo = self.OPTCRE.match(line)\n                if mo:\n                    # We might just have handled the last line, which could contain a quotation we want to remove\n                    optname, vi, optval = mo.group('option', 'vi', 'value')\n                    if vi in ('=', ':') and ';' in optval and not optval.strip().startswith('\"'):\n                        pos = optval.find(';')\n                        if pos != -1 and optval[pos - 1].isspace():\n                            optval = optval[:pos]\n                    optval = optval.strip()\n                    if optval == '\"\"':\n                        optval = ''\n                    # end handle empty string\n                    optname = self.optionxform(optname.rstrip())\n                    if len(optval) > 1 and optval[0] == '\"' and optval[-1] != '\"':\n                        is_multi_line = True\n                        optval = string_decode(optval[1:])\n                    # end handle multi-line\n                    cursect[optname] = optval\n                else:\n                    # check if it's an option with no value - it's just ignored by git\n                    if not self.OPTVALUEONLY.match(line):\n                        if not e:\n                            e = cp.ParsingError(fpname)\n                        e.append(lineno, repr(line))\n                    continue\n            else:\n                line = line.rstrip()\n                if line.endswith('\"'):\n                    is_multi_line = False\n                    line = line[:-1]\n                # end handle quotations\n                cursect[optname] += string_decode(line)\n            # END parse section or option\n        # END while reading\n\n        # if any parsing errors occurred, raise an exception\n        if e:\n            raise e", "response": "A direct copy of the py2. 4 version of the super class s _read method that uses ordered dicts."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read(self):\n        if self._is_initialized:\n            return\n        self._is_initialized = True\n\n        if not isinstance(self._file_or_files, (tuple, list)):\n            files_to_read = [self._file_or_files]\n        else:\n            files_to_read = list(self._file_or_files)\n        # end assure we have a copy of the paths to handle\n\n        seen = set(files_to_read)\n        num_read_include_files = 0\n        while files_to_read:\n            file_path = files_to_read.pop(0)\n            fp = file_path\n            file_ok = False\n\n            if hasattr(fp, \"seek\"):\n                self._read(fp, fp.name)\n            else:\n                # assume a path if it is not a file-object\n                try:\n                    with open(file_path, 'rb') as fp:\n                        file_ok = True\n                        self._read(fp, fp.name)\n                except IOError:\n                    continue\n\n            # Read includes and append those that we didn't handle yet\n            # We expect all paths to be normalized and absolute (and will assure that is the case)\n            if self._has_includes():\n                for _, include_path in self.items('include'):\n                    if include_path.startswith('~'):\n                        include_path = osp.expanduser(include_path)\n                    if not osp.isabs(include_path):\n                        if not file_ok:\n                            continue\n                        # end ignore relative paths if we don't know the configuration file path\n                        assert osp.isabs(file_path), \"Need absolute paths to be sure our cycle checks will work\"\n                        include_path = osp.join(osp.dirname(file_path), include_path)\n                    # end make include path absolute\n                    include_path = osp.normpath(include_path)\n                    if include_path in seen or not os.access(include_path, os.R_OK):\n                        continue\n                    seen.add(include_path)\n                    # insert included file to the top to be considered first\n                    files_to_read.insert(0, include_path)\n                    num_read_include_files += 1\n                # each include path in configuration file\n            # end handle includes\n        # END for each file object to read\n\n        # If there was no file included, we can safely write back (potentially) the configuration file\n        # without altering it's meaning\n        if num_read_include_files == 0:\n            self._merge_includes = False", "response": "Reads the data stored in the files we have been initialized with. It will ignore files that cannot be read possibly leaving an empty configuration."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _write(self, fp):\n        def write_section(name, section_dict):\n            fp.write((\"[%s]\\n\" % name).encode(defenc))\n            for (key, value) in section_dict.items():\n                if key != \"__name__\":\n                    fp.write((\"\\t%s = %s\\n\" % (key, self._value_to_string(value).replace('\\n', '\\n\\t'))).encode(defenc))\n                # END if key is not __name__\n        # END section writing\n\n        if self._defaults:\n            write_section(cp.DEFAULTSECT, self._defaults)\n        for name, value in self._sections.items():\n            write_section(name, value)", "response": "Write an. ini - format representation of the configuration state in\n            git compatible format"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of all items in the given section", "response": "def items(self, section_name):\n        \"\"\":return: list((option, value), ...) pairs of all items in the given section\"\"\"\n        return [(k, v) for k, v in super(GitConfigParser, self).items(section_name) if k != '__name__']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting changes to our file.", "response": "def write(self):\n        \"\"\"Write changes to our file, if there are changes at all\n\n        :raise IOError: if this is a read-only writer instance or if we could not obtain\n            a file lock\"\"\"\n        self._assure_writable(\"write\")\n        if not self._dirty:\n            return\n\n        if isinstance(self._file_or_files, (list, tuple)):\n            raise AssertionError(\"Cannot write back if there is not exactly a single file to write to, have %i files\"\n                                 % len(self._file_or_files))\n        # end assert multiple files\n\n        if self._has_includes():\n            log.debug(\"Skipping write-back of configuration file as include files were merged in.\" +\n                      \"Set merge_includes=False to prevent this.\")\n            return\n        # end\n\n        fp = self._file_or_files\n\n        # we have a physical file on disk, so get a lock\n        is_file_lock = isinstance(fp, string_types + (FileType, ))\n        if is_file_lock:\n            self._lock._obtain_lock()\n        if not hasattr(fp, \"seek\"):\n            with open(self._file_or_files, \"wb\") as fp:\n                self._write(fp)\n        else:\n            fp.seek(0)\n            # make sure we do not overwrite into an existing file\n            if hasattr(fp, 'truncate'):\n                fp.truncate()\n            self._write(fp)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_value(self, section, option, default=None):\n        try:\n            valuestr = self.get(section, option)\n        except Exception:\n            if default is not None:\n                return default\n            raise\n\n        types = (int, float)\n        for numtype in types:\n            try:\n                val = numtype(valuestr)\n\n                # truncated value ?\n                if val != float(valuestr):\n                    continue\n\n                return val\n            except (ValueError, TypeError):\n                continue\n        # END for each numeric type\n\n        # try boolean values as git uses them\n        vl = valuestr.lower()\n        if vl == 'false':\n            return False\n        if vl == 'true':\n            return True\n\n        if not isinstance(valuestr, string_types):\n            raise TypeError(\"Invalid value type: only int, long, float and str are allowed\", valuestr)\n\n        return valuestr", "response": "Returns a properly typed value for the given option in the given section."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_value(self, section, option, value):\n        if not self.has_section(section):\n            self.add_section(section)\n        self.set(section, option, self._value_to_string(value))\n        return self", "response": "Sets the given option in section to the given value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrename the given section to new_name", "response": "def rename_section(self, section, new_name):\n        \"\"\"rename the given section to new_name\n        :raise ValueError: if section doesn't exit\n        :raise ValueError: if a section with new_name does already exist\n        :return: this instance\n        \"\"\"\n        if not self.has_section(section):\n            raise ValueError(\"Source section '%s' doesn't exist\" % section)\n        if self.has_section(new_name):\n            raise ValueError(\"Destination section '%s' already exists\" % new_name)\n\n        super(GitConfigParser, self).add_section(new_name)\n        for k, v in self.items(section):\n            self.set(new_name, k, self._value_to_string(v))\n        # end for each value to copy\n\n        # This call writes back the changes, which is why we don't have the respective decorator\n        self.remove_section(section)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning the commit hook of the given name.", "response": "def run_commit_hook(name, index, *args):\n    \"\"\"Run the commit hook of the given name. Silently ignores hooks that do not exist.\n    :param name: name of hook, like 'pre-commit'\n    :param index: IndexFile instance\n    :param args: arguments passed to hook file\n    :raises HookExecutionError: \"\"\"\n    hp = hook_path(name, index.repo.git_dir)\n    if not os.access(hp, os.X_OK):\n        return\n\n    env = os.environ.copy()\n    env['GIT_INDEX_FILE'] = safe_decode(index.path) if PY3 else safe_encode(index.path)\n    env['GIT_EDITOR'] = ':'\n    try:\n        cmd = subprocess.Popen([hp] + list(args),\n                               env=env,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               cwd=index.repo.working_dir,\n                               close_fds=is_posix,\n                               creationflags=PROC_CREATIONFLAGS,)\n    except Exception as ex:\n        raise HookExecutionError(hp, ex)\n    else:\n        stdout = []\n        stderr = []\n        handle_process_output(cmd, stdout.append, stderr.append, finalize_process)\n        stdout = ''.join(stdout)\n        stderr = ''.join(stderr)\n        if cmd.returncode != 0:\n            stdout = force_text(stdout, defenc)\n            stderr = force_text(stderr, defenc)\n            raise HookExecutionError(hp, cmd.returncode, stderr, stdout)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stat_mode_to_index_mode(mode):\n    if S_ISLNK(mode):   # symlinks\n        return S_IFLNK\n    if S_ISDIR(mode) or S_IFMT(mode) == S_IFGITLINK:    # submodules\n        return S_IFGITLINK\n    return S_IFREG | 0o644 | (mode & 0o111)", "response": "Convert the given mode from a stat call to the corresponding index mode\n    and return it"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite the cache represented by entries to a stream.", "response": "def write_cache(entries, stream, extension_data=None, ShaStreamCls=IndexFileSHA1Writer):\n    \"\"\"Write the cache represented by entries to a stream\n\n    :param entries: **sorted** list of entries\n    :param stream: stream to wrap into the AdapterStreamCls - it is used for\n        final output.\n\n    :param ShaStreamCls: Type to use when writing to the stream. It produces a sha\n        while writing to it, before the data is passed on to the wrapped stream\n\n    :param extension_data: any kind of data to write as a trailer, it must begin\n        a 4 byte identifier, followed by its size ( 4 bytes )\"\"\"\n    # wrap the stream into a compatible writer\n    stream = ShaStreamCls(stream)\n\n    tell = stream.tell\n    write = stream.write\n\n    # header\n    version = 2\n    write(b\"DIRC\")\n    write(pack(\">LL\", version, len(entries)))\n\n    # body\n    for entry in entries:\n        beginoffset = tell()\n        write(entry[4])         # ctime\n        write(entry[5])         # mtime\n        path = entry[3]\n        path = force_bytes(path, encoding=defenc)\n        plen = len(path) & CE_NAMEMASK      # path length\n        assert plen == len(path), \"Path %s too long to fit into index\" % entry[3]\n        flags = plen | (entry[2] & CE_NAMEMASK_INV)     # clear possible previous values\n        write(pack(\">LLLLLL20sH\", entry[6], entry[7], entry[0],\n                   entry[8], entry[9], entry[10], entry[1], flags))\n        write(path)\n        real_size = ((tell() - beginoffset + 8) & ~7)\n        write(b\"\\0\" * ((beginoffset + real_size) - tell()))\n    # END for each entry\n\n    # write previously cached extensions data\n    if extension_data is not None:\n        stream.write(extension_data)\n\n    # write the sha over the content\n    stream.write_sha()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the index file header from the given stream", "response": "def read_header(stream):\n    \"\"\"Return tuple(version_long, num_entries) from the given stream\"\"\"\n    type_id = stream.read(4)\n    if type_id != b\"DIRC\":\n        raise AssertionError(\"Invalid index file header: %r\" % type_id)\n    version, num_entries = unpack(\">LL\", stream.read(4 * 2))\n\n    # TODO: handle version 3: extended data, see read-cache.c\n    assert version in (1, 2)\n    return version, num_entries"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef entry_key(*entry):\n    if len(entry) == 1:\n        return (entry[0].path, entry[0].stage)\n    else:\n        return tuple(entry)", "response": "Return the key suitable to be used for the index. entries dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads a cache file from the given stream", "response": "def read_cache(stream):\n    \"\"\"Read a cache file from the given stream\n    :return: tuple(version, entries_dict, extension_data, content_sha)\n    * version is the integer version number\n    * entries dict is a dictionary which maps IndexEntry instances to a path at a stage\n    * extension_data is '' or 4 bytes of type + 4 bytes of size + size bytes\n    * content_sha is a 20 byte sha on all cache file contents\"\"\"\n    version, num_entries = read_header(stream)\n    count = 0\n    entries = {}\n\n    read = stream.read\n    tell = stream.tell\n    while count < num_entries:\n        beginoffset = tell()\n        ctime = unpack(\">8s\", read(8))[0]\n        mtime = unpack(\">8s\", read(8))[0]\n        (dev, ino, mode, uid, gid, size, sha, flags) = \\\n            unpack(\">LLLLLL20sH\", read(20 + 4 * 6 + 2))\n        path_size = flags & CE_NAMEMASK\n        path = read(path_size).decode(defenc)\n\n        real_size = ((tell() - beginoffset + 8) & ~7)\n        read((beginoffset + real_size) - tell())\n        entry = IndexEntry((mode, sha, flags, path, ctime, mtime, dev, ino, uid, gid, size))\n        # entry_key would be the method to use, but we safe the effort\n        entries[(path, entry.stage)] = entry\n        count += 1\n    # END for each entry\n\n    # the footer contains extension data and a sha on the content so far\n    # Keep the extension footer,and verify we have a sha in the end\n    # Extension data format is:\n    # 4 bytes ID\n    # 4 bytes length of chunk\n    # repeated 0 - N times\n    extension_data = stream.read(~0)\n    assert len(extension_data) > 19, \"Index Footer was not at least a sha on content as it was only %i bytes in size\"\\\n                                     % len(extension_data)\n\n    content_sha = extension_data[-20:]\n\n    # truncate the sha in the end as we will dynamically create it anyway\n    extension_data = extension_data[:-20]\n\n    return (version, entries, extension_data, content_sha)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_tree_from_cache(entries, odb, sl, si=0):\n    tree_items = []\n    tree_items_append = tree_items.append\n    ci = sl.start\n    end = sl.stop\n    while ci < end:\n        entry = entries[ci]\n        if entry.stage != 0:\n            raise UnmergedEntriesError(entry)\n        # END abort on unmerged\n        ci += 1\n        rbound = entry.path.find('/', si)\n        if rbound == -1:\n            # its not a tree\n            tree_items_append((entry.binsha, entry.mode, entry.path[si:]))\n        else:\n            # find common base range\n            base = entry.path[si:rbound]\n            xi = ci\n            while xi < end:\n                oentry = entries[xi]\n                orbound = oentry.path.find('/', si)\n                if orbound == -1 or oentry.path[si:orbound] != base:\n                    break\n                # END abort on base mismatch\n                xi += 1\n            # END find common base\n\n            # enter recursion\n            # ci - 1 as we want to count our current item as well\n            sha, tree_entry_list = write_tree_from_cache(entries, odb, slice(ci - 1, xi), rbound + 1)  # @UnusedVariable\n            tree_items_append((sha, S_IFDIR, base))\n\n            # skip ahead\n            ci = xi\n        # END handle bounds\n    # END for each entry\n\n    # finally create the tree\n    sio = BytesIO()\n    tree_to_stream(tree_items, sio.write)\n    sio.seek(0)\n\n    istream = odb.store(IStream(str_tree_type, len(sio.getvalue()), sio))\n    return (istream.binsha, tree_items)", "response": "Create a tree from the given sorted list of entries and put the respective\n            trees into the given object database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef aggressive_tree_merge(odb, tree_shas):\n    out = []\n    out_append = out.append\n\n    # one and two way is the same for us, as we don't have to handle an existing\n    # index, instrea\n    if len(tree_shas) in (1, 2):\n        for entry in traverse_tree_recursive(odb, tree_shas[-1], ''):\n            out_append(_tree_entry_to_baseindexentry(entry, 0))\n        # END for each entry\n        return out\n    # END handle single tree\n\n    if len(tree_shas) > 3:\n        raise ValueError(\"Cannot handle %i trees at once\" % len(tree_shas))\n\n    # three trees\n    for base, ours, theirs in traverse_trees_recursive(odb, tree_shas, ''):\n        if base is not None:\n            # base version exists\n            if ours is not None:\n                # ours exists\n                if theirs is not None:\n                    # it exists in all branches, if it was changed in both\n                    # its a conflict, otherwise we take the changed version\n                    # This should be the most common branch, so it comes first\n                    if(base[0] != ours[0] and base[0] != theirs[0] and ours[0] != theirs[0]) or \\\n                            (base[1] != ours[1] and base[1] != theirs[1] and ours[1] != theirs[1]):\n                        # changed by both\n                        out_append(_tree_entry_to_baseindexentry(base, 1))\n                        out_append(_tree_entry_to_baseindexentry(ours, 2))\n                        out_append(_tree_entry_to_baseindexentry(theirs, 3))\n                    elif base[0] != ours[0] or base[1] != ours[1]:\n                        # only we changed it\n                        out_append(_tree_entry_to_baseindexentry(ours, 0))\n                    else:\n                        # either nobody changed it, or they did. In either\n                        # case, use theirs\n                        out_append(_tree_entry_to_baseindexentry(theirs, 0))\n                    # END handle modification\n                else:\n\n                    if ours[0] != base[0] or ours[1] != base[1]:\n                        # they deleted it, we changed it, conflict\n                        out_append(_tree_entry_to_baseindexentry(base, 1))\n                        out_append(_tree_entry_to_baseindexentry(ours, 2))\n                    # else:\n                    #   we didn't change it, ignore\n                    #   pass\n                    # END handle our change\n                # END handle theirs\n            else:\n                if theirs is None:\n                    # deleted in both, its fine - its out\n                    pass\n                else:\n                    if theirs[0] != base[0] or theirs[1] != base[1]:\n                        # deleted in ours, changed theirs, conflict\n                        out_append(_tree_entry_to_baseindexentry(base, 1))\n                        out_append(_tree_entry_to_baseindexentry(theirs, 3))\n                    # END theirs changed\n                    # else:\n                    #   theirs didn't change\n                    #   pass\n                # END handle theirs\n            # END handle ours\n        else:\n            # all three can't be None\n            if ours is None:\n                # added in their branch\n                out_append(_tree_entry_to_baseindexentry(theirs, 0))\n            elif theirs is None:\n                # added in our branch\n                out_append(_tree_entry_to_baseindexentry(ours, 0))\n            else:\n                # both have it, except for the base, see whether it changed\n                if ours[0] != theirs[0] or ours[1] != theirs[1]:\n                    out_append(_tree_entry_to_baseindexentry(ours, 2))\n                    out_append(_tree_entry_to_baseindexentry(theirs, 3))\n                else:\n                    # it was added the same in both\n                    out_append(_tree_entry_to_baseindexentry(ours, 0))\n                # END handle two items\n            # END handle heads\n        # END handle base exists\n    # END for each entries tuple\n\n    return out", "response": "This function performs an aggressive merge of the given tree_shas and returns a list of BaseIndexEntries representing the aggressive merge of the given tree_shas."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_blob(cls, blob, stage=0):\n        return cls((blob.mode, blob.binsha, stage << CE_STAGESHIFT, blob.path))", "response": "Return a Fully equipped BaseIndexEntry from a blob."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a Blob object using the information of this index entry", "response": "def to_blob(self, repo):\n        \"\"\":return: Blob using the information of this index entry\"\"\"\n        return Blob(repo, self.binsha, self.mode, self.path)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_base(cls, base):\n        time = pack(\">LL\", 0, 0)\n        return IndexEntry((base.mode, base.binsha, base.flags, base.path, time, time, 0, 0, 0, 0, 0))", "response": "Returns a new index entry from a BaseIndexEntry instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a Minimal entry resembling the given blob object.", "response": "def from_blob(cls, blob, stage=0):\n        \"\"\":return: Minimal entry resembling the given blob object\"\"\"\n        time = pack(\">LL\", 0, 0)\n        return IndexEntry((blob.mode, blob.binsha, stage << CE_STAGESHIFT, blob.path,\n                           time, time, 0, 0, 0, 0, blob.size))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _index_by_name(self, name):\n        for i, t in enumerate(self._cache):\n            if t[2] == name:\n                return i\n            # END found item\n        # END for each item in cache\n        return -1", "response": "return the index of an item with name or - 1 if not found"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd the given item to the tree.", "response": "def add(self, sha, mode, name, force=False):\n        \"\"\"Add the given item to the tree. If an item with the given name already\n        exists, nothing will be done, but a ValueError will be raised if the\n        sha and mode of the existing item do not match the one you add, unless\n        force is True\n\n        :param sha: The 20 or 40 byte sha of the item to add\n        :param mode: int representing the stat compatible mode of the item\n        :param force: If True, an item with your name and information will overwrite\n            any existing item with the same name, no matter which information it has\n        :return: self\"\"\"\n        if '/' in name:\n            raise ValueError(\"Name must not contain '/' characters\")\n        if (mode >> 12) not in Tree._map_id_to_type:\n            raise ValueError(\"Invalid object type according to mode %o\" % mode)\n\n        sha = to_bin_sha(sha)\n        index = self._index_by_name(name)\n        item = (sha, mode, name)\n        if index == -1:\n            self._cache.append(item)\n        else:\n            if force:\n                self._cache[index] = item\n            else:\n                ex_item = self._cache[index]\n                if ex_item[0] != sha or ex_item[1] != mode:\n                    raise ValueError(\"Item %r existed with different properties\" % name)\n                # END handle mismatch\n            # END handle force\n        # END handle name exists\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding the given item to the tree.", "response": "def add_unchecked(self, binsha, mode, name):\n        \"\"\"Add the given item to the tree, its correctness is assumed, which\n        puts the caller into responsibility to assure the input is correct.\n        For more information on the parameters, see ``add``\n        :param binsha: 20 byte binary sha\"\"\"\n        self._cache.append((binsha, mode, name))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _iter_convert_to_object(self, iterable):\n        for binsha, mode, name in iterable:\n            path = join_path(self.path, name)\n            try:\n                yield self._map_id_to_type[mode >> 12](self.repo, binsha, mode, path)\n            except KeyError:\n                raise TypeError(\"Unknown mode %o found in tree data for path '%s'\" % (mode, path))", "response": "Iterate over the tree data and convert them to objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the named object in this tree or tree s contents AttributeNames", "response": "def join(self, file):\n        \"\"\"Find the named object in this tree's contents\n        :return: ``git.Blob`` or ``git.Tree`` or ``git.Submodule``\n\n        :raise KeyError: if given file or tree does not exist in tree\"\"\"\n        msg = \"Blob or Tree named %r not found\"\n        if '/' in file:\n            tree = self\n            item = self\n            tokens = file.split('/')\n            for i, token in enumerate(tokens):\n                item = tree[token]\n                if item.type == 'tree':\n                    tree = item\n                else:\n                    # safety assertion - blobs are at the end of the path\n                    if i != len(tokens) - 1:\n                        raise KeyError(msg % file)\n                    return item\n                # END handle item type\n            # END for each token of split path\n            if item == self:\n                raise KeyError(msg % file)\n            return item\n        else:\n            for info in self._cache:\n                if info[2] == file:     # [2] == name\n                    return self._map_id_to_type[info[1] >> 12](self.repo, info[0], info[1],\n                                                               join_path(self.path, info[2]))\n            # END for each obj\n            raise KeyError(msg % file)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef traverse(self, predicate=lambda i, d: True,\n                 prune=lambda i, d: False, depth=-1, branch_first=True,\n                 visit_once=False, ignore_self=1):\n        \"\"\"For documentation, see util.Traversable.traverse\n        Trees are set to visit_once = False to gain more performance in the traversal\"\"\"\n        return super(Tree, self).traverse(predicate, prune, depth, branch_first, visit_once, ignore_self)", "response": "For documentation, see util.Traversable.traverse\n        Trees are set to visit_once = False to gain more performance in the traversal"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an OStream object for the given sha.", "response": "def stream(self, sha):\n        \"\"\"For now, all lookup is done by git itself\"\"\"\n        hexsha, typename, size, stream = self._git.stream_object_data(bin_to_hex(sha))\n        return OStream(hex_to_bin(hexsha), typename, size, stream)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef partial_to_complete_sha_hex(self, partial_hexsha):\n        try:\n            hexsha, typename, size = self._git.get_object_header(partial_hexsha)  # @UnusedVariable\n            return hex_to_bin(hexsha)\n        except (GitCommandError, ValueError):\n            raise BadObject(partial_hexsha)", "response": "Return the full binary 20 byte sha from the given partial hexsha."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a mode string like 755 or 644 to int", "response": "def mode_str_to_int(modestr):\n    \"\"\"\n    :param modestr: string like 755 or 644 or 100644 - only the last 6 chars will be used\n    :return:\n        String identifying a mode compatible to the mode methods ids of the\n        stat module regarding the rwx permissions for user, group and other,\n        special flags and file system flags, i.e. whether it is a symlink\n        for example.\"\"\"\n    mode = 0\n    for iteration, char in enumerate(reversed(modestr[-6:])):\n        mode += int(char) << iteration * 3\n    # END for each char\n    return mode"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the object type suitable to handle the given object type name.", "response": "def get_object_type_by_name(object_type_name):\n    \"\"\"\n    :return: type suitable to handle the given object type name.\n        Use the type to create new instances.\n\n    :param object_type_name: Member of TYPES\n\n    :raise ValueError: In case object_type_name is unknown\"\"\"\n    if object_type_name == b\"commit\":\n        from . import commit\n        return commit.Commit\n    elif object_type_name == b\"tag\":\n        from . import tag\n        return tag.TagObject\n    elif object_type_name == b\"blob\":\n        from . import blob\n        return blob.Blob\n    elif object_type_name == b\"tree\":\n        from . import tree\n        return tree.Tree\n    else:\n        raise ValueError(\"Cannot handle unknown object type: %s\" % object_type_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nverify that the given offset is correct for the UTCTZ format.", "response": "def verify_utctz(offset):\n    \"\"\":raise ValueError: if offset is incorrect\n    :return: offset\"\"\"\n    fmt_exc = ValueError(\"Invalid timezone offset format: %s\" % offset)\n    if len(offset) != 5:\n        raise fmt_exc\n    if offset[0] not in \"+-\":\n        raise fmt_exc\n    if offset[1] not in digits or\\\n       offset[2] not in digits or\\\n       offset[3] not in digits or\\\n       offset[4] not in digits:\n        raise fmt_exc\n    # END for each char\n    return offset"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a timestamp + tz_offset into an aware datetime instance.", "response": "def from_timestamp(timestamp, tz_offset):\n    \"\"\"Converts a timestamp + tz_offset into an aware datetime instance.\"\"\"\n    utc_dt = datetime.fromtimestamp(timestamp, utc)\n    try:\n        local_dt = utc_dt.astimezone(tzoffset(tz_offset))\n        return local_dt\n    except ValueError:\n        return utc_dt"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a date string into a tuple of timestamp and offset.", "response": "def parse_date(string_date):\n    \"\"\"\n    Parse the given date as one of the following\n\n        * Git internal format: timestamp offset\n        * RFC 2822: Thu, 07 Apr 2005 22:13:13 +0200.\n        * ISO 8601 2005-04-07T22:13:13\n            The T can be a space as well\n\n    :return: Tuple(int(timestamp_UTC), int(offset)), both in seconds since epoch\n    :raise ValueError: If the format could not be understood\n    :note: Date can also be YYYY.MM.DD, MM/DD/YYYY and DD.MM.YYYY.\n    \"\"\"\n    # git time\n    try:\n        if string_date.count(' ') == 1 and string_date.rfind(':') == -1:\n            timestamp, offset = string_date.split()\n            timestamp = int(timestamp)\n            return timestamp, utctz_to_altz(verify_utctz(offset))\n        else:\n            offset = \"+0000\"                    # local time by default\n            if string_date[-5] in '-+':\n                offset = verify_utctz(string_date[-5:])\n                string_date = string_date[:-6]  # skip space as well\n            # END split timezone info\n            offset = utctz_to_altz(offset)\n\n            # now figure out the date and time portion - split time\n            date_formats = []\n            splitter = -1\n            if ',' in string_date:\n                date_formats.append(\"%a, %d %b %Y\")\n                splitter = string_date.rfind(' ')\n            else:\n                # iso plus additional\n                date_formats.append(\"%Y-%m-%d\")\n                date_formats.append(\"%Y.%m.%d\")\n                date_formats.append(\"%m/%d/%Y\")\n                date_formats.append(\"%d.%m.%Y\")\n\n                splitter = string_date.rfind('T')\n                if splitter == -1:\n                    splitter = string_date.rfind(' ')\n                # END handle 'T' and ' '\n            # END handle rfc or iso\n\n            assert splitter > -1\n\n            # split date and time\n            time_part = string_date[splitter + 1:]    # skip space\n            date_part = string_date[:splitter]\n\n            # parse time\n            tstruct = time.strptime(time_part, \"%H:%M:%S\")\n\n            for fmt in date_formats:\n                try:\n                    dtstruct = time.strptime(date_part, fmt)\n                    utctime = calendar.timegm((dtstruct.tm_year, dtstruct.tm_mon, dtstruct.tm_mday,\n                                               tstruct.tm_hour, tstruct.tm_min, tstruct.tm_sec,\n                                               dtstruct.tm_wday, dtstruct.tm_yday, tstruct.tm_isdst))\n                    return int(utctime), offset\n                except ValueError:\n                    continue\n                # END exception handling\n            # END for each fmt\n\n            # still here ? fail\n            raise ValueError(\"no format matched\")\n        # END handle format\n    except Exception:\n        raise ValueError(\"Unsupported date format: %s\" % string_date)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_actor_and_date(line):\n    actor, epoch, offset = '', 0, 0\n    m = _re_actor_epoch.search(line)\n    if m:\n        actor, epoch, offset = m.groups()\n    else:\n        m = _re_only_actor.search(line)\n        actor = m.group(1) if m else line or ''\n    return (Actor._from_string(actor), int(epoch), utctz_to_altz(offset))", "response": "Parse out the actor and date from a line like author Tom Preston - Werner <tom@mojombo. com > 1191999972 - 0700"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an iterableList with the results of the traversal as produced by traverse", "response": "def list_traverse(self, *args, **kwargs):\n        \"\"\"\n        :return: IterableList with the results of the traversal as produced by\n            traverse()\"\"\"\n        out = IterableList(self._id_attribute_)\n        out.extend(self.traverse(*args, **kwargs))\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntraversing the tree for items that are found at the specified depth", "response": "def traverse(self, predicate=lambda i, d: True,\n                 prune=lambda i, d: False, depth=-1, branch_first=True,\n                 visit_once=True, ignore_self=1, as_edge=False):\n        \"\"\":return: iterator yielding of items found when traversing self\n\n        :param predicate: f(i,d) returns False if item i at depth d should not be included in the result\n\n        :param prune:\n            f(i,d) return True if the search should stop at item i at depth d.\n            Item i will not be returned.\n\n        :param depth:\n            define at which level the iteration should not go deeper\n            if -1, there is no limit\n            if 0, you would effectively only get self, the root of the iteration\n            i.e. if 1, you would only get the first level of predecessors/successors\n\n        :param branch_first:\n            if True, items will be returned branch first, otherwise depth first\n\n        :param visit_once:\n            if True, items will only be returned once, although they might be encountered\n            several times. Loops are prevented that way.\n\n        :param ignore_self:\n            if True, self will be ignored and automatically pruned from\n            the result. Otherwise it will be the first item to be returned.\n            If as_edge is True, the source of the first edge is None\n\n        :param as_edge:\n            if True, return a pair of items, first being the source, second the\n            destination, i.e. tuple(src, dest) with the edge spanning from\n            source to destination\"\"\"\n        visited = set()\n        stack = Deque()\n        stack.append((0, self, None))       # self is always depth level 0\n\n        def addToStack(stack, item, branch_first, depth):\n            lst = self._get_intermediate_items(item)\n            if not lst:\n                return\n            if branch_first:\n                stack.extendleft((depth, i, item) for i in lst)\n            else:\n                reviter = ((depth, lst[i], item) for i in range(len(lst) - 1, -1, -1))\n                stack.extend(reviter)\n        # END addToStack local method\n\n        while stack:\n            d, item, src = stack.pop()          # depth of item, item, item_source\n\n            if visit_once and item in visited:\n                continue\n\n            if visit_once:\n                visited.add(item)\n\n            rval = (as_edge and (src, item)) or item\n            if prune(rval, d):\n                continue\n\n            skipStartItem = ignore_self and (item is self)\n            if not skipStartItem and predicate(rval, d):\n                yield rval\n\n            # only continue to next level if this is appropriate !\n            nd = d + 1\n            if depth > -1 and nd > depth:\n                continue\n\n            addToStack(stack, item, branch_first, nd)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_process_output(process, stdout_handler, stderr_handler,\n                          finalizer=None, decode_streams=True):\n    \"\"\"Registers for notifications to lean that process output is ready to read, and dispatches lines to\n    the respective line handlers.\n    This function returns once the finalizer returns\n\n    :return: result of finalizer\n    :param process: subprocess.Popen instance\n    :param stdout_handler: f(stdout_line_string), or None\n    :param stderr_handler: f(stderr_line_string), or None\n    :param finalizer: f(proc) - wait for proc to finish\n    :param decode_streams:\n        Assume stdout/stderr streams are binary and decode them before pushing \\\n        their contents to handlers.\n        Set it to False if `universal_newline == True` (then streams are in text-mode)\n        or if decoding must happen later (i.e. for Diffs).\n    \"\"\"\n    # Use 2 \"pump\" threads and wait for both to finish.\n    def pump_stream(cmdline, name, stream, is_decode, handler):\n        try:\n            for line in stream:\n                if handler:\n                    if is_decode:\n                        line = line.decode(defenc)\n                    handler(line)\n        except Exception as ex:\n            log.error(\"Pumping %r of cmd(%s) failed due to: %r\", name, cmdline, ex)\n            raise CommandError(['<%s-pump>' % name] + cmdline, ex)\n        finally:\n            stream.close()\n\n    cmdline = getattr(process, 'args', '')  # PY3+ only\n    if not isinstance(cmdline, (tuple, list)):\n        cmdline = cmdline.split()\n\n    pumps = []\n    if process.stdout:\n        pumps.append(('stdout', process.stdout, stdout_handler))\n    if process.stderr:\n        pumps.append(('stderr', process.stderr, stderr_handler))\n\n    threads = []\n\n    for name, stream, handler in pumps:\n        t = threading.Thread(target=pump_stream,\n                             args=(cmdline, name, stream, decode_streams, handler))\n        t.setDaemon(True)\n        t.start()\n        threads.append(t)\n\n    ## FIXME: Why Join??  Will block if `stdin` needs feeding...\n    #\n    for t in threads:\n        t.join()\n\n    if finalizer:\n        return finalizer(process)", "response": "This function handles the output of a process and dispatches lines to the corresponding line handlers."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nspecifies command line options to the git executable for subsequent subcommands calls", "response": "def set_persistent_git_options(self, **kwargs):\n        \"\"\"Specify command line options to the git executable\n        for subsequent subcommand calls\n\n        :param kwargs:\n            is a dict of keyword arguments.\n            these arguments are passed as in _call_process\n            but will be passed to the git command rather than\n            the subcommand.\n        \"\"\"\n\n        self._persistent_git_options = self.transform_kwargs(\n            split_single_char_options=True, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef execute(self, command,\n                istream=None,\n                with_extended_output=False,\n                with_exceptions=True,\n                as_process=False,\n                output_stream=None,\n                stdout_as_string=True,\n                kill_after_timeout=None,\n                with_stdout=True,\n                universal_newlines=False,\n                shell=None,\n                env=None,\n                max_chunk_size=io.DEFAULT_BUFFER_SIZE,\n                **subprocess_kwargs\n                ):\n        \"\"\"Handles executing the command on the shell and consumes and returns\n        the returned information (stdout)\n\n        :param command:\n            The command argument list to execute.\n            It should be a string, or a sequence of program arguments. The\n            program to execute is the first item in the args sequence or string.\n\n        :param istream:\n            Standard input filehandle passed to subprocess.Popen.\n\n        :param with_extended_output:\n            Whether to return a (status, stdout, stderr) tuple.\n\n        :param with_exceptions:\n            Whether to raise an exception when git returns a non-zero status.\n\n        :param as_process:\n            Whether to return the created process instance directly from which\n            streams can be read on demand. This will render with_extended_output and\n            with_exceptions ineffective - the caller will have\n            to deal with the details himself.\n            It is important to note that the process will be placed into an AutoInterrupt\n            wrapper that will interrupt the process once it goes out of scope. If you\n            use the command in iterators, you should pass the whole process instance\n            instead of a single stream.\n\n        :param output_stream:\n            If set to a file-like object, data produced by the git command will be\n            output to the given stream directly.\n            This feature only has any effect if as_process is False. Processes will\n            always be created with a pipe due to issues with subprocess.\n            This merely is a workaround as data will be copied from the\n            output pipe to the given output stream directly.\n            Judging from the implementation, you shouldn't use this flag !\n\n        :param stdout_as_string:\n            if False, the commands standard output will be bytes. Otherwise, it will be\n            decoded into a string using the default encoding (usually utf-8).\n            The latter can fail, if the output contains binary data.\n\n        :param env:\n            A dictionary of environment variables to be passed to `subprocess.Popen`.\n            \n        :param max_chunk_size:\n            Maximum number of bytes in one chunk of data passed to the output_stream in\n            one invocation of write() method. If the given number is not positive then\n            the default value is used.\n\n        :param subprocess_kwargs:\n            Keyword arguments to be passed to subprocess.Popen. Please note that\n            some of the valid kwargs are already set by this method, the ones you\n            specify may not be the same ones.\n\n        :param with_stdout: If True, default True, we open stdout on the created process\n        :param universal_newlines:\n            if True, pipes will be opened as text, and lines are split at\n            all known line endings.\n        :param shell:\n            Whether to invoke commands through a shell (see `Popen(..., shell=True)`).\n            It overrides :attr:`USE_SHELL` if it is not `None`.\n        :param kill_after_timeout:\n            To specify a timeout in seconds for the git command, after which the process\n            should be killed. This will have no effect if as_process is set to True. It is\n            set to None by default and will let the process run until the timeout is\n            explicitly specified. This feature is not supported on Windows. It's also worth\n            noting that kill_after_timeout uses SIGKILL, which can have negative side\n            effects on a repository. For example, stale locks in case of git gc could\n            render the repository incapable of accepting changes until the lock is manually\n            removed.\n\n        :return:\n            * str(output) if extended_output = False (Default)\n            * tuple(int(status), str(stdout), str(stderr)) if extended_output = True\n\n            if output_stream is True, the stdout value will be your output stream:\n            * output_stream if extended_output = False\n            * tuple(int(status), output_stream, str(stderr)) if extended_output = True\n\n            Note git is executed with LC_MESSAGES=\"C\" to ensure consistent\n            output regardless of system language.\n\n        :raise GitCommandError:\n\n        :note:\n           If you add additional keyword arguments to the signature of this method,\n           you must update the execute_kwargs tuple housed in this module.\"\"\"\n        if self.GIT_PYTHON_TRACE and (self.GIT_PYTHON_TRACE != 'full' or as_process):\n            log.info(' '.join(command))\n\n        # Allow the user to have the command executed in their working dir.\n        cwd = self._working_dir or os.getcwd()\n\n        # Start the process\n        inline_env = env\n        env = os.environ.copy()\n        # Attempt to force all output to plain ascii english, which is what some parsing code\n        # may expect.\n        # According to stackoverflow (http://goo.gl/l74GC8), we are setting LANGUAGE as well\n        # just to be sure.\n        env[\"LANGUAGE\"] = \"C\"\n        env[\"LC_ALL\"] = \"C\"\n        env.update(self._environment)\n        if inline_env is not None:\n            env.update(inline_env)\n\n        if is_win:\n            cmd_not_found_exception = OSError\n            if kill_after_timeout:\n                raise GitCommandError(command, '\"kill_after_timeout\" feature is not supported on Windows.')\n        else:\n            if sys.version_info[0] > 2:\n                cmd_not_found_exception = FileNotFoundError  # NOQA # exists, flake8 unknown @UndefinedVariable\n            else:\n                cmd_not_found_exception = OSError\n        # end handle\n\n        stdout_sink = (PIPE\n                       if with_stdout\n                       else getattr(subprocess, 'DEVNULL', None) or open(os.devnull, 'wb'))\n        log.debug(\"Popen(%s, cwd=%s, universal_newlines=%s, shell=%s)\",\n                  command, cwd, universal_newlines, shell)\n        try:\n            proc = Popen(command,\n                         env=env,\n                         cwd=cwd,\n                         bufsize=-1,\n                         stdin=istream,\n                         stderr=PIPE,\n                         stdout=stdout_sink,\n                         shell=shell is not None and shell or self.USE_SHELL,\n                         close_fds=is_posix,  # unsupported on windows\n                         universal_newlines=universal_newlines,\n                         creationflags=PROC_CREATIONFLAGS,\n                         **subprocess_kwargs\n                         )\n        except cmd_not_found_exception as err:\n            raise GitCommandNotFound(command, err)\n\n        if as_process:\n            return self.AutoInterrupt(proc, command)\n\n        def _kill_process(pid):\n            \"\"\" Callback method to kill a process. \"\"\"\n            p = Popen(['ps', '--ppid', str(pid)], stdout=PIPE,\n                      creationflags=PROC_CREATIONFLAGS)\n            child_pids = []\n            for line in p.stdout:\n                if len(line.split()) > 0:\n                    local_pid = (line.split())[0]\n                    if local_pid.isdigit():\n                        child_pids.append(int(local_pid))\n            try:\n                # Windows does not have SIGKILL, so use SIGTERM instead\n                sig = getattr(signal, 'SIGKILL', signal.SIGTERM)\n                os.kill(pid, sig)\n                for child_pid in child_pids:\n                    try:\n                        os.kill(child_pid, sig)\n                    except OSError:\n                        pass\n                kill_check.set()    # tell the main routine that the process was killed\n            except OSError:\n                # It is possible that the process gets completed in the duration after timeout\n                # happens and before we try to kill the process.\n                pass\n            return\n        # end\n\n        if kill_after_timeout:\n            kill_check = threading.Event()\n            watchdog = threading.Timer(kill_after_timeout, _kill_process, args=(proc.pid,))\n\n        # Wait for the process to return\n        status = 0\n        stdout_value = b''\n        stderr_value = b''\n        try:\n            if output_stream is None:\n                if kill_after_timeout:\n                    watchdog.start()\n                stdout_value, stderr_value = proc.communicate()\n                if kill_after_timeout:\n                    watchdog.cancel()\n                    if kill_check.isSet():\n                        stderr_value = ('Timeout: the command \"%s\" did not complete in %d '\n                                        'secs.' % (\" \".join(command), kill_after_timeout)).encode(defenc)\n                # strip trailing \"\\n\"\n                if stdout_value.endswith(b\"\\n\"):\n                    stdout_value = stdout_value[:-1]\n                if stderr_value.endswith(b\"\\n\"):\n                    stderr_value = stderr_value[:-1]\n                status = proc.returncode\n            else:\n                max_chunk_size = max_chunk_size if max_chunk_size and max_chunk_size > 0 else io.DEFAULT_BUFFER_SIZE\n                stream_copy(proc.stdout, output_stream, max_chunk_size)\n                stdout_value = proc.stdout.read()\n                stderr_value = proc.stderr.read()\n                # strip trailing \"\\n\"\n                if stderr_value.endswith(b\"\\n\"):\n                    stderr_value = stderr_value[:-1]\n                status = proc.wait()\n            # END stdout handling\n        finally:\n            proc.stdout.close()\n            proc.stderr.close()\n\n        if self.GIT_PYTHON_TRACE == 'full':\n            cmdstr = \" \".join(command)\n\n            def as_text(stdout_value):\n                return not output_stream and safe_decode(stdout_value) or '<OUTPUT_STREAM>'\n            # end\n\n            if stderr_value:\n                log.info(\"%s -> %d; stdout: '%s'; stderr: '%s'\",\n                         cmdstr, status, as_text(stdout_value), safe_decode(stderr_value))\n            elif stdout_value:\n                log.info(\"%s -> %d; stdout: '%s'\", cmdstr, status, as_text(stdout_value))\n            else:\n                log.info(\"%s -> %d\", cmdstr, status)\n        # END handle debug printing\n\n        if with_exceptions and status != 0:\n            raise GitCommandError(command, status, stderr_value, stdout_value)\n\n        if isinstance(stdout_value, bytes) and stdout_as_string:  # could also be output_stream\n            stdout_value = safe_decode(stdout_value)\n\n        # Allow access to the command's status code\n        if with_extended_output:\n            return (status, stdout_value, safe_decode(stderr_value))\n        else:\n            return stdout_value", "response": "Executes a git command and returns the output of the process instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_environment(self, **kwargs):\n        old_env = {}\n        for key, value in kwargs.items():\n            # set value if it is None\n            if value is not None:\n                old_env[key] = self._environment.get(key)\n                self._environment[key] = value\n            # remove key from environment if its value is None\n            elif key in self._environment:\n                old_env[key] = self._environment[key]\n                del self._environment[key]\n        return old_env", "response": "Update the environment variables for future git invocations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef custom_environment(self, **kwargs):\n        old_env = self.update_environment(**kwargs)\n        try:\n            yield\n        finally:\n            self.update_environment(**old_env)", "response": "A context manager around the above update_environment method to restore the environment back to its previous state after operation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntransform Python style kwargs into git command line options.", "response": "def transform_kwargs(self, split_single_char_options=True, **kwargs):\n        \"\"\"Transforms Python style kwargs into git command line options.\"\"\"\n        args = []\n        kwargs = OrderedDict(sorted(kwargs.items(), key=lambda x: x[0]))\n        for k, v in kwargs.items():\n            if isinstance(v, (list, tuple)):\n                for value in v:\n                    args += self.transform_kwarg(k, value, split_single_char_options)\n            else:\n                args += self.transform_kwarg(k, v, split_single_char_options)\n        return args"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _call_process(self, method, *args, **kwargs):\n        # Handle optional arguments prior to calling transform_kwargs\n        # otherwise these'll end up in args, which is bad.\n        exec_kwargs = {k: v for k, v in kwargs.items() if k in execute_kwargs}\n        opts_kwargs = {k: v for k, v in kwargs.items() if k not in execute_kwargs}\n\n        insert_after_this_arg = opts_kwargs.pop('insert_kwargs_after', None)\n\n        # Prepare the argument list\n        opt_args = self.transform_kwargs(**opts_kwargs)\n        ext_args = self.__unpack_args([a for a in args if a is not None])\n\n        if insert_after_this_arg is None:\n            args = opt_args + ext_args\n        else:\n            try:\n                index = ext_args.index(insert_after_this_arg)\n            except ValueError:\n                raise ValueError(\"Couldn't find argument '%s' in args %s to insert cmd options after\"\n                                 % (insert_after_this_arg, str(ext_args)))\n            # end handle error\n            args = ext_args[:index + 1] + opt_args + ext_args[index + 1:]\n        # end handle opts_kwargs\n\n        call = [self.GIT_PYTHON_GIT_EXECUTABLE]\n\n        # add persistent git options\n        call.extend(self._persistent_git_options)\n\n        # add the git options, then reset to empty\n        # to avoid side_effects\n        call.extend(self._git_options)\n        self._git_options = ()\n\n        call.append(dashify(method))\n        call.extend(args)\n\n        return self.execute(call, **exec_kwargs)", "response": "Execute the given git command with the specified arguments and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_object_header(self, header_line):\n        tokens = header_line.split()\n        if len(tokens) != 3:\n            if not tokens:\n                raise ValueError(\"SHA could not be resolved, git returned: %r\" % (header_line.strip()))\n            else:\n                raise ValueError(\"SHA %s could not be resolved, git returned: %r\" % (tokens[0], header_line.strip()))\n            # END handle actual return value\n        # END error handling\n\n        if len(tokens[0]) != 40:\n            raise ValueError(\"Failed to parse header: %r\" % header_line)\n        return (tokens[0], tokens[1], int(tokens[2]))", "response": "Parse the object header line."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuses this method to quickly examine the type and size of the object behind the given ref. :note: The method will only suffer from the costs of command invocation once and reuses the command in subsequent calls. :return: (hexsha, type_string, size_as_int)", "response": "def get_object_header(self, ref):\n        \"\"\" Use this method to quickly examine the type and size of the object behind\n        the given ref.\n\n        :note: The method will only suffer from the costs of command invocation\n            once and reuses the command in subsequent calls.\n\n        :return: (hexsha, type_string, size_as_int)\"\"\"\n        cmd = self._get_persistent_cmd(\"cat_file_header\", \"cat_file\", batch_check=True)\n        return self.__get_object_header(cmd, ref)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nclears all kinds of internal caches to release resources.", "response": "def clear_cache(self):\n        \"\"\"Clear all kinds of internal caches to release resources.\n\n        Currently persistent commands will be interrupted.\n\n        :return: self\"\"\"\n        for cmd in (self.cat_file_all, self.cat_file_header):\n            if cmd:\n                cmd.__del__()\n\n        self.cat_file_all = None\n        self.cat_file_header = None\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remote(self, name='origin'):\n        r = Remote(self, name)\n        if not r.exists():\n            raise ValueError(\"Remote named '%s' didn't exist\" % name)\n        return r", "response": "Returns a Remote object with the specified name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_head(self, path, commit='HEAD', force=False, logmsg=None):\n        return Head.create(self, path, commit, force, logmsg)", "response": "Create a new head within the repository."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_tag(self, path, ref='HEAD', message=None, force=False, **kwargs):\n        return TagReference.create(self, path, ref, message, force, **kwargs)", "response": "Create a new tag reference."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_remote(self, name, url, **kwargs):\n        return Remote.create(self, name, url, **kwargs)", "response": "Create a new remote object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef config_reader(self, config_level=None):\n        files = None\n        if config_level is None:\n            files = [self._get_config_path(f) for f in self.config_level]\n        else:\n            files = [self._get_config_path(config_level)]\n        return GitConfigParser(files, read_only=True)", "response": "Returns a GitConfigParser that can read the full git configuration but not to write it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef commit(self, rev=None):\n        if rev is None:\n            return self.head.commit\n        else:\n            return self.rev_parse(text_type(rev) + \"^0\")", "response": "Returns the Commit object for the specified revision."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an iterator yielding Tree objects for all the commits in this repository.", "response": "def iter_trees(self, *args, **kwargs):\n        \"\"\":return: Iterator yielding Tree objects\n        :note: Takes all arguments known to iter_commits method\"\"\"\n        return (c.tree for c in self.iter_commits(*args, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef iter_commits(self, rev=None, paths='', **kwargs):\n        if rev is None:\n            rev = self.head.commit\n\n        return Commit.iter_items(self, rev, paths, **kwargs)", "response": "Return an iterator over the commits in a given revision."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merge_base(self, *rev, **kwargs):\n        if len(rev) < 2:\n            raise ValueError(\"Please specify at least two revs, got only %i\" % len(rev))\n        # end handle input\n\n        res = []\n        try:\n            lines = self.git.merge_base(*rev, **kwargs).splitlines()\n        except GitCommandError as err:\n            if err.status == 128:\n                raise\n            # end handle invalid rev\n            # Status code 1 is returned if there is no merge-base\n            # (see https://github.com/git/git/blob/master/builtin/merge-base.c#L16)\n            return res\n        # end exception handling\n\n        for line in lines:\n            res.append(self.commit(line))\n        # end for each merge-base\n\n        return res", "response": "Find the common ancestor for the given revision."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_ancestor(self, ancestor_rev, rev):\n        try:\n            self.git.merge_base(ancestor_rev, rev, is_ancestor=True)\n        except GitCommandError as err:\n            if err.status == 1:\n                return False\n            raise\n        return True", "response": "Check if a commit is an ancestor of another commit"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_alternates(self):\n        alternates_path = osp.join(self.git_dir, 'objects', 'info', 'alternates')\n\n        if osp.exists(alternates_path):\n            with open(alternates_path, 'rb') as f:\n                alts = f.read().decode(defenc)\n            return alts.strip().splitlines()\n        else:\n            return []", "response": "Returns the list of strings being pathnames of alternates"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the alternates at which git should look for objects.", "response": "def _set_alternates(self, alts):\n        \"\"\"Sets the alternates\n\n        :param alts:\n            is the array of string paths representing the alternates at which\n            git should look for objects, i.e. /home/user/repo/.git/objects\n\n        :raise NoSuchPathError:\n        :note:\n            The method does not check for the existence of the paths in alts\n            as the caller is responsible.\"\"\"\n        alternates_path = osp.join(self.common_dir, 'objects', 'info', 'alternates')\n        if not alts:\n            if osp.isfile(alternates_path):\n                os.remove(alternates_path)\n        else:\n            with open(alternates_path, 'wb') as f:\n                f.write(\"\\n\".join(alts).encode(defenc))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the repository is dirty.", "response": "def is_dirty(self, index=True, working_tree=True, untracked_files=False,\n                 submodules=True, path=None):\n        \"\"\"\n        :return:\n            ``True``, the repository is considered dirty. By default it will react\n            like a git-status without untracked files, hence it is dirty if the\n            index or the working copy have changes.\"\"\"\n        if self._bare:\n            # Bare repositories with no associated working directory are\n            # always consired to be clean.\n            return False\n\n        # start from the one which is fastest to evaluate\n        default_args = ['--abbrev=40', '--full-index', '--raw']\n        if not submodules:\n            default_args.append('--ignore-submodules')\n        if path:\n            default_args.append(path)\n        if index:\n            # diff index against HEAD\n            if osp.isfile(self.index.path) and \\\n                    len(self.git.diff('--cached', *default_args)):\n                return True\n        # END index handling\n        if working_tree:\n            # diff index against working tree\n            if len(self.git.diff(*default_args)):\n                return True\n        # END working tree handling\n        if untracked_files:\n            if len(self._get_untracked_files(path, ignore_submodules=not submodules)):\n                return True\n        # END untracked files\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef blame_incremental(self, rev, file, **kwargs):\n        data = self.git.blame(rev, '--', file, p=True, incremental=True, stdout_as_string=False, **kwargs)\n        commits = {}\n\n        stream = (line for line in data.split(b'\\n') if line)\n        while True:\n            try:\n                line = next(stream)  # when exhausted, causes a StopIteration, terminating this function\n            except StopIteration:\n                return\n            hexsha, orig_lineno, lineno, num_lines = line.split()\n            lineno = int(lineno)\n            num_lines = int(num_lines)\n            orig_lineno = int(orig_lineno)\n            if hexsha not in commits:\n                # Now read the next few lines and build up a dict of properties\n                # for this commit\n                props = {}\n                while True:\n                    try:\n                        line = next(stream)\n                    except StopIteration:\n                        return\n                    if line == b'boundary':\n                        # \"boundary\" indicates a root commit and occurs\n                        # instead of the \"previous\" tag\n                        continue\n\n                    tag, value = line.split(b' ', 1)\n                    props[tag] = value\n                    if tag == b'filename':\n                        # \"filename\" formally terminates the entry for --incremental\n                        orig_filename = value\n                        break\n\n                c = Commit(self, hex_to_bin(hexsha),\n                           author=Actor(safe_decode(props[b'author']),\n                                        safe_decode(props[b'author-mail'].lstrip(b'<').rstrip(b'>'))),\n                           authored_date=int(props[b'author-time']),\n                           committer=Actor(safe_decode(props[b'committer']),\n                                           safe_decode(props[b'committer-mail'].lstrip(b'<').rstrip(b'>'))),\n                           committed_date=int(props[b'committer-time']))\n                commits[hexsha] = c\n            else:\n                # Discard all lines until we find \"filename\" which is\n                # guaranteed to be the last line\n                while True:\n                    try:\n                        line = next(stream)  # will fail if we reach the EOF unexpectedly\n                    except StopIteration:\n                        return\n                    tag, value = line.split(b' ', 1)\n                    if tag == b'filename':\n                        orig_filename = value\n                        break\n\n            yield BlameEntry(commits[hexsha],\n                             range(lineno, lineno + num_lines),\n                             safe_decode(orig_filename),\n                             range(orig_lineno, orig_lineno + num_lines))", "response": "Iterator for incremental blame information for the given file at the given revision."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the blame information for the given file at the given revision.", "response": "def blame(self, rev, file, incremental=False, **kwargs):\n        \"\"\"The blame information for the given file at the given revision.\n\n        :param rev: revision specifier, see git-rev-parse for viable options.\n        :return:\n            list: [git.Commit, list: [<line>]]\n            A list of tuples associating a Commit object with a list of lines that\n            changed within the given commit. The Commit objects will be given in order\n            of appearance.\"\"\"\n        if incremental:\n            return self.blame_incremental(rev, file, **kwargs)\n\n        data = self.git.blame(rev, '--', file, p=True, stdout_as_string=False, **kwargs)\n        commits = {}\n        blames = []\n        info = None\n\n        keepends = True\n        for line in data.splitlines(keepends):\n            try:\n                line = line.rstrip().decode(defenc)\n            except UnicodeDecodeError:\n                firstpart = ''\n                is_binary = True\n            else:\n                # As we don't have an idea when the binary data ends, as it could contain multiple newlines\n                # in the process. So we rely on being able to decode to tell us what is is.\n                # This can absolutely fail even on text files, but even if it does, we should be fine treating it\n                # as binary instead\n                parts = self.re_whitespace.split(line, 1)\n                firstpart = parts[0]\n                is_binary = False\n            # end handle decode of line\n\n            if self.re_hexsha_only.search(firstpart):\n                # handles\n                # 634396b2f541a9f2d58b00be1a07f0c358b999b3 1 1 7        - indicates blame-data start\n                # 634396b2f541a9f2d58b00be1a07f0c358b999b3 2 2          - indicates\n                # another line of blame with the same data\n                digits = parts[-1].split(\" \")\n                if len(digits) == 3:\n                    info = {'id': firstpart}\n                    blames.append([None, []])\n                elif info['id'] != firstpart:\n                    info = {'id': firstpart}\n                    blames.append([commits.get(firstpart), []])\n                # END blame data initialization\n            else:\n                m = self.re_author_committer_start.search(firstpart)\n                if m:\n                    # handles:\n                    # author Tom Preston-Werner\n                    # author-mail <tom@mojombo.com>\n                    # author-time 1192271832\n                    # author-tz -0700\n                    # committer Tom Preston-Werner\n                    # committer-mail <tom@mojombo.com>\n                    # committer-time 1192271832\n                    # committer-tz -0700  - IGNORED BY US\n                    role = m.group(0)\n                    if firstpart.endswith('-mail'):\n                        info[\"%s_email\" % role] = parts[-1]\n                    elif firstpart.endswith('-time'):\n                        info[\"%s_date\" % role] = int(parts[-1])\n                    elif role == firstpart:\n                        info[role] = parts[-1]\n                    # END distinguish mail,time,name\n                else:\n                    # handle\n                    # filename lib/grit.rb\n                    # summary add Blob\n                    # <and rest>\n                    if firstpart.startswith('filename'):\n                        info['filename'] = parts[-1]\n                    elif firstpart.startswith('summary'):\n                        info['summary'] = parts[-1]\n                    elif firstpart == '':\n                        if info:\n                            sha = info['id']\n                            c = commits.get(sha)\n                            if c is None:\n                                c = Commit(self, hex_to_bin(sha),\n                                           author=Actor._from_string(info['author'] + ' ' + info['author_email']),\n                                           authored_date=info['author_date'],\n                                           committer=Actor._from_string(\n                                               info['committer'] + ' ' + info['committer_email']),\n                                           committed_date=info['committer_date'])\n                                commits[sha] = c\n                            # END if commit objects needs initial creation\n                            if not is_binary:\n                                if line and line[0] == '\\t':\n                                    line = line[1:]\n                            else:\n                                # NOTE: We are actually parsing lines out of binary data, which can lead to the\n                                # binary being split up along the newline separator. We will append this to the blame\n                                # we are currently looking at, even though it should be concatenated with the last line\n                                # we have seen.\n                                pass\n                            # end handle line contents\n                            blames[-1][0] = c\n                            blames[-1][1].append(line)\n                            info = {'id': sha}\n                        # END if we collected commit info\n                    # END distinguish filename,summary,rest\n                # END distinguish author|committer vs filename,summary,rest\n            # END distinguish hexsha vs other information\n        return blames"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef init(cls, path=None, mkdir=True, odbt=GitCmdObjectDB, expand_vars=True, **kwargs):\n        if path:\n            path = expand_path(path, expand_vars)\n        if mkdir and path and not osp.exists(path):\n            os.makedirs(path, 0o755)\n\n        # git command automatically chdir into the directory\n        git = Git(path)\n        git.init(**kwargs)\n        return cls(path, odbt=odbt)", "response": "Initialize a git repository at the given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a clone from this repository.", "response": "def clone(self, path, progress=None, **kwargs):\n        \"\"\"Create a clone from this repository.\n\n        :param path: is the full path of the new repo (traditionally ends with ./<name>.git).\n        :param progress: See 'git.remote.Remote.push'.\n        :param kwargs:\n            * odbt = ObjectDatabase Type, allowing to determine the object database\n              implementation used by the returned Repo instance\n            * All remaining keyword arguments are given to the git-clone command\n\n        :return: ``git.Repo`` (the newly cloned repo)\"\"\"\n        return self._clone(self.git, self.common_dir, path, type(self.odb), progress, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a clone from the given URL and save it to the given path.", "response": "def clone_from(cls, url, to_path, progress=None, env=None, **kwargs):\n        \"\"\"Create a clone from the given URL\n\n        :param url: valid git url, see http://www.kernel.org/pub/software/scm/git/docs/git-clone.html#URLS\n        :param to_path: Path to which the repository should be cloned to\n        :param progress: See 'git.remote.Remote.push'.\n        :param env: Optional dictionary containing the desired environment variables.\n        :param kwargs: see the ``clone`` method\n        :return: Repo instance pointing to the cloned directory\"\"\"\n        git = Git(os.getcwd())\n        if env is not None:\n            git.update_environment(**env)\n        return cls._clone(git, url, to_path, GitCmdObjectDB, progress, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\narchives the tree at the given revision.", "response": "def archive(self, ostream, treeish=None, prefix=None, **kwargs):\n        \"\"\"Archive the tree at the given revision.\n\n        :param ostream: file compatible stream object to which the archive will be written as bytes\n        :param treeish: is the treeish name/id, defaults to active branch\n        :param prefix: is the optional prefix to prepend to each filename in the archive\n        :param kwargs: Additional arguments passed to git-archive\n\n            * Use the 'format' argument to define the kind of format. Use\n              specialized ostreams to write any format supported by python.\n            * You may specify the special **path** keyword, which may either be a repository-relative\n              path to a directory or file to place into the archive, or a list or tuple of multiple paths.\n\n        :raise GitCommandError: in case something went wrong\n        :return: self\"\"\"\n        if treeish is None:\n            treeish = self.head.commit\n        if prefix and 'prefix' not in kwargs:\n            kwargs['prefix'] = prefix\n        kwargs['output_stream'] = ostream\n        path = kwargs.pop('path', [])\n        if not isinstance(path, (tuple, list)):\n            path = [path]\n        # end assure paths is list\n\n        self.git.archive(treeish, *path, **kwargs)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef has_separate_working_tree(self):\n        if self.bare:\n            return False\n        return osp.isfile(osp.join(self.working_tree_dir, '.git'))", "response": "Return True if our git_dir is at the root of our working_tree_dir and agnositic symbolic link."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef safe_decode(s):\n    if isinstance(s, unicode):\n        return s\n    elif isinstance(s, bytes):\n        return s.decode(defenc, 'surrogateescape')\n    elif s is not None:\n        raise TypeError('Expected bytes or text, but got %r' % (s,))", "response": "Safely decodes a binary string to unicode"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef win_encode(s):\n    if isinstance(s, unicode):\n        return s.encode(locale.getpreferredencoding(False))\n    elif isinstance(s, bytes):\n        return s\n    elif s is not None:\n        raise TypeError('Expected bytes or text, but got %r' % (s,))", "response": "Encode unicodes for process arguments on Windows."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new object with the given metaclass", "response": "def with_metaclass(meta, *bases):\n    \"\"\"copied from https://github.com/Byron/bcore/blob/master/src/python/butility/future.py#L15\"\"\"\n    class metaclass(meta):\n        __call__ = type.__call__\n        __init__ = type.__init__\n\n        def __new__(cls, name, nbases, d):\n            if nbases is None:\n                return type.__new__(cls, name, (), d)\n            # There may be clients who rely on this attribute to be set to a reasonable value, which is why\n            # we set the __metaclass__ attribute explicitly\n            if not PY3 and '___metaclass__' not in d:\n                d['__metaclass__'] = meta\n            return meta(name, bases, d)\n    return metaclass(meta.__name__ + 'Helper', None, {})"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreplaces surrogate characters in a string with a more logical bytes.", "response": "def replace_surrogate_encode(mystring, exc):\n    \"\"\"\n    Returns a (unicode) string, not the more logical bytes, because the codecs\n    register_error functionality expects this.\n    \"\"\"\n    decoded = []\n    for ch in mystring:\n        # if PY3:\n        #     code = ch\n        # else:\n        code = ord(ch)\n\n        # The following magic comes from Py3.3's Python/codecs.c file:\n        if not 0xD800 <= code <= 0xDCFF:\n            # Not a surrogate. Fail with the original exception.\n            raise exc\n        # mybytes = [0xe0 | (code >> 12),\n        #            0x80 | ((code >> 6) & 0x3f),\n        #            0x80 | (code & 0x3f)]\n        # Is this a good idea?\n        if 0xDC00 <= code <= 0xDC7F:\n            decoded.append(_unichr(code - 0xDC00))\n        elif code <= 0xDCFF:\n            decoded.append(_unichr(code - 0xDC00))\n        else:\n            raise NotASurrogateError\n    return str().join(decoded)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecode a unicode string with the surrogates.", "response": "def replace_surrogate_decode(mybytes):\n    \"\"\"\n    Returns a (unicode) string\n    \"\"\"\n    decoded = []\n    for ch in mybytes:\n        # We may be parsing newbytes (in which case ch is an int) or a native\n        # str on Py2\n        if isinstance(ch, int):\n            code = ch\n        else:\n            code = ord(ch)\n        if 0x80 <= code <= 0xFF:\n            decoded.append(_unichr(0xDC00 + code))\n        elif code <= 0x7F:\n            decoded.append(_unichr(code))\n        else:\n            # # It may be a bad byte\n            # # Try swallowing it.\n            # continue\n            # print(\"RAISE!\")\n            raise NotASurrogateError\n    return str().join(decoded)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncounts the number of commits reachable from this commit.", "response": "def count(self, paths='', **kwargs):\n        \"\"\"Count the number of commits reachable from this commit\n\n        :param paths:\n            is an optional path or a list of paths restricting the return value\n            to commits actually containing the paths\n\n        :param kwargs:\n            Additional options to be passed to git-rev-list. They must not alter\n            the output style of the command, or parsing will yield incorrect results\n        :return: int defining the number of reachable commits\"\"\"\n        # yes, it makes a difference whether empty paths are given or not in our case\n        # as the empty paths version will ignore merge commits for some reason.\n        if paths:\n            return len(self.repo.git.rev_list(self.hexsha, '--', paths, **kwargs).splitlines())\n        else:\n            return len(self.repo.git.rev_list(self.hexsha, **kwargs).splitlines())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an iterator yielding all Commit items that match the given criteria.", "response": "def iter_items(cls, repo, rev, paths='', **kwargs):\n        \"\"\"Find all commits matching the given criteria.\n\n        :param repo: is the Repo\n        :param rev: revision specifier, see git-rev-parse for viable options\n        :param paths:\n            is an optional path or list of paths, if set only Commits that include the path\n            or paths will be considered\n        :param kwargs:\n            optional keyword arguments to git rev-list where\n            ``max_count`` is the maximum number of commits to fetch\n            ``skip`` is the number of commits to skip\n            ``since`` all commits since i.e. '1970-01-01'\n        :return: iterator yielding Commit items\"\"\"\n        if 'pretty' in kwargs:\n            raise ValueError(\"--pretty cannot be used as parsing expects single sha's only\")\n        # END handle pretty\n\n        # use -- in any case, to prevent possibility of ambiguous arguments\n        # see https://github.com/gitpython-developers/GitPython/issues/264\n        args = ['--']\n        if paths:\n            args.extend((paths, ))\n        # END if paths\n\n        proc = repo.git.rev_list(rev, args, as_process=True, **kwargs)\n        return cls._iter_from_process_or_stream(repo, proc)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iter_parents(self, paths='', **kwargs):\n        # skip ourselves\n        skip = kwargs.get(\"skip\", 1)\n        if skip == 0:   # skip ourselves\n            skip = 1\n        kwargs['skip'] = skip\n\n        return self.iter_items(self.repo, self, paths, **kwargs)", "response": "Iterate _all_ parents of this commit."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a git stat from changes between this commit and its first parent .", "response": "def stats(self):\n        \"\"\"Create a git stat from changes between this commit and its first parent\n        or from all changes done if this is the very first commit.\n\n        :return: git.Stats\"\"\"\n        if not self.parents:\n            text = self.repo.git.diff_tree(self.hexsha, '--', numstat=True, root=True)\n            text2 = \"\"\n            for line in text.splitlines()[1:]:\n                (insertions, deletions, filename) = line.split(\"\\t\")\n                text2 += \"%s\\t%s\\t%s\\n\" % (insertions, deletions, filename)\n            text = text2\n        else:\n            text = self.repo.git.diff(self.parents[0].hexsha, self.hexsha, '--', numstat=True)\n        return Stats._list_from_string(self.repo, text)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _iter_from_process_or_stream(cls, repo, proc_or_stream):\n        stream = proc_or_stream\n        if not hasattr(stream, 'readline'):\n            stream = proc_or_stream.stdout\n\n        readline = stream.readline\n        while True:\n            line = readline()\n            if not line:\n                break\n            hexsha = line.strip()\n            if len(hexsha) > 40:\n                # split additional information, as returned by bisect for instance\n                hexsha, _ = line.split(None, 1)\n            # END handle extra info\n\n            assert len(hexsha) == 40, \"Invalid line: %s\" % hexsha\n            yield Commit(repo, hex_to_bin(hexsha))\n        # END for each line in stream\n        # TODO: Review this - it seems process handling got a bit out of control\n        # due to many developers trying to fix the open file handles issue\n        if hasattr(proc_or_stream, 'wait'):\n            finalize_process(proc_or_stream)", "response": "Parse out commit information from a git - rev - list process instance or a stream."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_from_tree(cls, repo, tree, message, parent_commits=None, head=False, author=None, committer=None,\n                         author_date=None, commit_date=None):\n        \"\"\"Commit the given tree, creating a commit object.\n\n        :param repo: Repo object the commit should be part of\n        :param tree: Tree object or hex or bin sha\n            the tree of the new commit\n        :param message: Commit message. It may be an empty string if no message is provided.\n            It will be converted to a string in any case.\n        :param parent_commits:\n            Optional Commit objects to use as parents for the new commit.\n            If empty list, the commit will have no parents at all and become\n            a root commit.\n            If None , the current head commit will be the parent of the\n            new commit object\n        :param head:\n            If True, the HEAD will be advanced to the new commit automatically.\n            Else the HEAD will remain pointing on the previous commit. This could\n            lead to undesired results when diffing files.\n        :param author: The name of the author, optional. If unset, the repository\n            configuration is used to obtain this value.\n        :param committer: The name of the committer, optional. If unset, the\n            repository configuration is used to obtain this value.\n        :param author_date: The timestamp for the author field\n        :param commit_date: The timestamp for the committer field\n\n        :return: Commit object representing the new commit\n\n        :note:\n            Additional information about the committer and Author are taken from the\n            environment or from the git configuration, see git-commit-tree for\n            more information\"\"\"\n        if parent_commits is None:\n            try:\n                parent_commits = [repo.head.commit]\n            except ValueError:\n                # empty repositories have no head commit\n                parent_commits = []\n            # END handle parent commits\n        else:\n            for p in parent_commits:\n                if not isinstance(p, cls):\n                    raise ValueError(\"Parent commit '%r' must be of type %s\" % (p, cls))\n            # end check parent commit types\n        # END if parent commits are unset\n\n        # retrieve all additional information, create a commit object, and\n        # serialize it\n        # Generally:\n        # * Environment variables override configuration values\n        # * Sensible defaults are set according to the git documentation\n\n        # COMMITER AND AUTHOR INFO\n        cr = repo.config_reader()\n        env = os.environ\n\n        committer = committer or Actor.committer(cr)\n        author = author or Actor.author(cr)\n\n        # PARSE THE DATES\n        unix_time = int(time())\n        is_dst = daylight and localtime().tm_isdst > 0\n        offset = altzone if is_dst else timezone\n\n        author_date_str = env.get(cls.env_author_date, '')\n        if author_date:\n            author_time, author_offset = parse_date(author_date)\n        elif author_date_str:\n            author_time, author_offset = parse_date(author_date_str)\n        else:\n            author_time, author_offset = unix_time, offset\n        # END set author time\n\n        committer_date_str = env.get(cls.env_committer_date, '')\n        if commit_date:\n            committer_time, committer_offset = parse_date(commit_date)\n        elif committer_date_str:\n            committer_time, committer_offset = parse_date(committer_date_str)\n        else:\n            committer_time, committer_offset = unix_time, offset\n        # END set committer time\n\n        # assume utf8 encoding\n        enc_section, enc_option = cls.conf_encoding.split('.')\n        conf_encoding = cr.get_value(enc_section, enc_option, cls.default_encoding)\n\n        # if the tree is no object, make sure we create one - otherwise\n        # the created commit object is invalid\n        if isinstance(tree, str):\n            tree = repo.tree(tree)\n        # END tree conversion\n\n        # CREATE NEW COMMIT\n        new_commit = cls(repo, cls.NULL_BIN_SHA, tree,\n                         author, author_time, author_offset,\n                         committer, committer_time, committer_offset,\n                         message, parent_commits, conf_encoding)\n\n        stream = BytesIO()\n        new_commit._serialize(stream)\n        streamlen = stream.tell()\n        stream.seek(0)\n\n        istream = repo.odb.store(IStream(cls.type, streamlen, stream))\n        new_commit.binsha = istream.binsha\n\n        if head:\n            # need late import here, importing git at the very beginning throws\n            # as well ...\n            import git.refs\n            try:\n                repo.head.set_commit(new_commit, logmsg=message)\n            except ValueError:\n                # head is not yet set to the ref our HEAD points to\n                # Happens on first commit\n                master = git.refs.Head.create(repo, repo.head.ref, new_commit, logmsg=\"commit (initial): %s\" % message)\n                repo.head.set_reference(master, logmsg='commit: Switching to %s' % master)\n            # END handle empty repositories\n        # END advance head handling\n\n        return new_commit", "response": "Create a new commit object from a tree."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_worktree_git_dir(dotgit):\n    try:\n        statbuf = os.stat(dotgit)\n    except OSError:\n        return None\n    if not stat.S_ISREG(statbuf.st_mode):\n        return None\n\n    try:\n        lines = open(dotgit, 'r').readlines()\n        for key, value in [line.strip().split(': ') for line in lines]:\n            if key == 'gitdir':\n                return value\n    except ValueError:\n        pass\n    return None", "response": "Search for a gitdir for this worktree."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_submodule_git_dir(d):\n    if is_git_dir(d):\n        return d\n\n    try:\n        with open(d) as fp:\n            content = fp.read().rstrip()\n    except (IOError, OSError):\n        # it's probably not a file\n        pass\n    else:\n        if content.startswith('gitdir: '):\n            path = content[8:]\n\n            if Git.is_cygwin():\n                ## Cygwin creates submodules prefixed with `/cygdrive/...` suffixes.\n                path = decygpath(path)\n            if not osp.isabs(path):\n                path = osp.join(osp.dirname(d), path)\n            return find_submodule_git_dir(path)\n    # end handle exception\n    return None", "response": "Search for a submodule repo."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a name returns the object specified by it.", "response": "def name_to_object(repo, name, return_ref=False):\n    \"\"\"\n    :return: object specified by the given name, hexshas ( short and long )\n        as well as references are supported\n    :param return_ref: if name specifies a reference, we will return the reference\n        instead of the object. Otherwise it will raise BadObject or BadName\n    \"\"\"\n    hexsha = None\n\n    # is it a hexsha ? Try the most common ones, which is 7 to 40\n    if repo.re_hexsha_shortened.match(name):\n        if len(name) != 40:\n            # find long sha for short sha\n            hexsha = short_to_long(repo.odb, name)\n        else:\n            hexsha = name\n        # END handle short shas\n    # END find sha if it matches\n\n    # if we couldn't find an object for what seemed to be a short hexsha\n    # try to find it as reference anyway, it could be named 'aaa' for instance\n    if hexsha is None:\n        for base in ('%s', 'refs/%s', 'refs/tags/%s', 'refs/heads/%s', 'refs/remotes/%s', 'refs/remotes/%s/HEAD'):\n            try:\n                hexsha = SymbolicReference.dereference_recursive(repo, base % name)\n                if return_ref:\n                    return SymbolicReference(repo, base % name)\n                # END handle symbolic ref\n                break\n            except ValueError:\n                pass\n        # END for each base\n    # END handle hexsha\n\n    # didn't find any ref, this is an error\n    if return_ref:\n        raise BadObject(\"Couldn't find reference named %r\" % name)\n    # END handle return ref\n\n    # tried everything ? fail\n    if hexsha is None:\n        raise BadName(name)\n    # END assert hexsha was found\n\n    return Object.new_from_sha(repo, hex_to_bin(hexsha))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert the given object to a commit if possible and return it", "response": "def to_commit(obj):\n    \"\"\"Convert the given object to a commit if possible and return it\"\"\"\n    if obj.type == 'tag':\n        obj = deref_tag(obj)\n\n    if obj.type != \"commit\":\n        raise ValueError(\"Cannot convert object %r to type commit\" % obj)\n    # END verify type\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a git - rev - parse compatible revision specification as string.", "response": "def rev_parse(repo, rev):\n    \"\"\"\n    :return: Object at the given revision, either Commit, Tag, Tree or Blob\n    :param rev: git-rev-parse compatible revision specification as string, please see\n        http://www.kernel.org/pub/software/scm/git/docs/git-rev-parse.html\n        for details\n    :raise BadObject: if the given revision could not be found\n    :raise ValueError: If rev couldn't be parsed\n    :raise IndexError: If invalid reflog index is specified\"\"\"\n\n    # colon search mode ?\n    if rev.startswith(':/'):\n        # colon search mode\n        raise NotImplementedError(\"commit by message search ( regex )\")\n    # END handle search\n\n    obj = None\n    ref = None\n    output_type = \"commit\"\n    start = 0\n    parsed_to = 0\n    lr = len(rev)\n    while start < lr:\n        if rev[start] not in \"^~:@\":\n            start += 1\n            continue\n        # END handle start\n\n        token = rev[start]\n\n        if obj is None:\n            # token is a rev name\n            if start == 0:\n                ref = repo.head.ref\n            else:\n                if token == '@':\n                    ref = name_to_object(repo, rev[:start], return_ref=True)\n                else:\n                    obj = name_to_object(repo, rev[:start])\n                # END handle token\n            # END handle refname\n\n            if ref is not None:\n                obj = ref.commit\n            # END handle ref\n        # END initialize obj on first token\n\n        start += 1\n\n        # try to parse {type}\n        if start < lr and rev[start] == '{':\n            end = rev.find('}', start)\n            if end == -1:\n                raise ValueError(\"Missing closing brace to define type in %s\" % rev)\n            output_type = rev[start + 1:end]  # exclude brace\n\n            # handle type\n            if output_type == 'commit':\n                pass  # default\n            elif output_type == 'tree':\n                try:\n                    obj = to_commit(obj).tree\n                except (AttributeError, ValueError):\n                    pass    # error raised later\n                # END exception handling\n            elif output_type in ('', 'blob'):\n                if obj.type == 'tag':\n                    obj = deref_tag(obj)\n                else:\n                    # cannot do anything for non-tags\n                    pass\n                # END handle tag\n            elif token == '@':\n                # try single int\n                assert ref is not None, \"Requre Reference to access reflog\"\n                revlog_index = None\n                try:\n                    # transform reversed index into the format of our revlog\n                    revlog_index = -(int(output_type) + 1)\n                except ValueError:\n                    # TODO: Try to parse the other date options, using parse_date\n                    # maybe\n                    raise NotImplementedError(\"Support for additional @{...} modes not implemented\")\n                # END handle revlog index\n\n                try:\n                    entry = ref.log_entry(revlog_index)\n                except IndexError:\n                    raise IndexError(\"Invalid revlog index: %i\" % revlog_index)\n                # END handle index out of bound\n\n                obj = Object.new_from_sha(repo, hex_to_bin(entry.newhexsha))\n\n                # make it pass the following checks\n                output_type = None\n            else:\n                raise ValueError(\"Invalid output type: %s ( in %s )\" % (output_type, rev))\n            # END handle output type\n\n            # empty output types don't require any specific type, its just about dereferencing tags\n            if output_type and obj.type != output_type:\n                raise ValueError(\"Could not accommodate requested object type %r, got %s\" % (output_type, obj.type))\n            # END verify output type\n\n            start = end + 1                   # skip brace\n            parsed_to = start\n            continue\n        # END parse type\n\n        # try to parse a number\n        num = 0\n        if token != \":\":\n            found_digit = False\n            while start < lr:\n                if rev[start] in digits:\n                    num = num * 10 + int(rev[start])\n                    start += 1\n                    found_digit = True\n                else:\n                    break\n                # END handle number\n            # END number parse loop\n\n            # no explicit number given, 1 is the default\n            # It could be 0 though\n            if not found_digit:\n                num = 1\n            # END set default num\n        # END number parsing only if non-blob mode\n\n        parsed_to = start\n        # handle hierarchy walk\n        try:\n            if token == \"~\":\n                obj = to_commit(obj)\n                for _ in xrange(num):\n                    obj = obj.parents[0]\n                # END for each history item to walk\n            elif token == \"^\":\n                obj = to_commit(obj)\n                # must be n'th parent\n                if num:\n                    obj = obj.parents[num - 1]\n            elif token == \":\":\n                if obj.type != \"tree\":\n                    obj = obj.tree\n                # END get tree type\n                obj = obj[rev[start:]]\n                parsed_to = lr\n            else:\n                raise ValueError(\"Invalid token: %r\" % token)\n            # END end handle tag\n        except (IndexError, AttributeError):\n            raise BadName(\"Invalid revision spec '%s' - not enough parent commits to reach '%s%i'\" % (rev, token, num))\n        # END exception handling\n    # END parse loop\n\n    # still no obj ? Its probably a simple name\n    if obj is None:\n        obj = name_to_object(repo, rev)\n        parsed_to = lr\n    # END handle simple name\n\n    if obj is None:\n        raise ValueError(\"Revision specifier could not be parsed: %s\" % rev)\n\n    if parsed_to != lr:\n        raise ValueError(\"Didn't consume complete rev spec %s, consumed part: %s\" % (rev, rev[:parsed_to]))\n\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_progress(kwargs, git, progress):\n    if progress is not None:\n        v = git.version_info[:2]\n        if v >= (1, 7):\n            kwargs['progress'] = True\n        # END handle --progress\n    # END handle progress\n    return kwargs", "response": "Add the progress flag to the given kwargs dict if supported by the\n    git command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remote_ref(self):\n        # translate heads to a local remote, tags stay as they are\n        if self.remote_ref_string.startswith(\"refs/tags\"):\n            return TagReference(self._remote.repo, self.remote_ref_string)\n        elif self.remote_ref_string.startswith(\"refs/heads\"):\n            remote_ref = Reference(self._remote.repo, self.remote_ref_string)\n            return RemoteReference(self._remote.repo, \"refs/remotes/%s/%s\" % (str(self._remote), remote_ref.name))\n        else:\n            raise ValueError(\"Could not handle remote ref: %r\" % self.remote_ref_string)", "response": "Returns the RemoteReference or TagReference in the local repository corresponding\n            to the remote_ref_string kept in this instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new PushInfo instance from a line of text.", "response": "def _from_line(cls, remote, line):\n        \"\"\"Create a new PushInfo instance as parsed from line which is expected to be like\n            refs/heads/master:refs/heads/master 05d2687..1d0568e as bytes\"\"\"\n        control_character, from_to, summary = line.split('\\t', 3)\n        flags = 0\n\n        # control character handling\n        try:\n            flags |= cls._flag_map[control_character]\n        except KeyError:\n            raise ValueError(\"Control character %r unknown as parsed from line %r\" % (control_character, line))\n        # END handle control character\n\n        # from_to handling\n        from_ref_string, to_ref_string = from_to.split(':')\n        if flags & cls.DELETED:\n            from_ref = None\n        else:\n            from_ref = Reference.from_path(remote.repo, from_ref_string)\n\n        # commit handling, could be message or commit info\n        old_commit = None\n        if summary.startswith('['):\n            if \"[rejected]\" in summary:\n                flags |= cls.REJECTED\n            elif \"[remote rejected]\" in summary:\n                flags |= cls.REMOTE_REJECTED\n            elif \"[remote failure]\" in summary:\n                flags |= cls.REMOTE_FAILURE\n            elif \"[no match]\" in summary:\n                flags |= cls.ERROR\n            elif \"[new tag]\" in summary:\n                flags |= cls.NEW_TAG\n            elif \"[new branch]\" in summary:\n                flags |= cls.NEW_HEAD\n            # uptodate encoded in control character\n        else:\n            # fast-forward or forced update - was encoded in control character,\n            # but we parse the old and new commit\n            split_token = \"...\"\n            if control_character == \" \":\n                split_token = \"..\"\n            old_sha, new_sha = summary.split(' ')[0].split(split_token)  # @UnusedVariable\n            # have to use constructor here as the sha usually is abbreviated\n            old_commit = old_sha\n        # END message handling\n\n        return PushInfo(flags, from_ref, to_ref_string, remote, old_commit, summary)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef refresh(cls):\n        # clear the old values in _flag_map\n        try:\n            del cls._flag_map[\"t\"]\n        except KeyError:\n            pass\n\n        try:\n            del cls._flag_map[\"-\"]\n        except KeyError:\n            pass\n\n        # set the value given the git version\n        if Git().version_info[:2] >= (2, 10):\n            cls._flag_map[\"t\"] = cls.TAG_UPDATE\n        else:\n            cls._flag_map[\"-\"] = cls.TAG_UPDATE\n\n        return True", "response": "This function is called by the top level base class to refresh the internal flag map."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _from_line(cls, repo, line, fetch_line):\n        match = cls._re_fetch_result.match(line)\n        if match is None:\n            raise ValueError(\"Failed to parse line: %r\" % line)\n\n        # parse lines\n        control_character, operation, local_remote_ref, remote_local_ref, note = match.groups()\n        try:\n            new_hex_sha, fetch_operation, fetch_note = fetch_line.split(\"\\t\")  # @UnusedVariable\n            ref_type_name, fetch_note = fetch_note.split(' ', 1)\n        except ValueError:  # unpack error\n            raise ValueError(\"Failed to parse FETCH_HEAD line: %r\" % fetch_line)\n\n        # parse flags from control_character\n        flags = 0\n        try:\n            flags |= cls._flag_map[control_character]\n        except KeyError:\n            raise ValueError(\"Control character %r unknown as parsed from line %r\" % (control_character, line))\n        # END control char exception handling\n\n        # parse operation string for more info - makes no sense for symbolic refs, but we parse it anyway\n        old_commit = None\n        is_tag_operation = False\n        if 'rejected' in operation:\n            flags |= cls.REJECTED\n        if 'new tag' in operation:\n            flags |= cls.NEW_TAG\n            is_tag_operation = True\n        if 'tag update' in operation:\n            flags |= cls.TAG_UPDATE\n            is_tag_operation = True\n        if 'new branch' in operation:\n            flags |= cls.NEW_HEAD\n        if '...' in operation or '..' in operation:\n            split_token = '...'\n            if control_character == ' ':\n                split_token = split_token[:-1]\n            old_commit = repo.rev_parse(operation.split(split_token)[0])\n        # END handle refspec\n\n        # handle FETCH_HEAD and figure out ref type\n        # If we do not specify a target branch like master:refs/remotes/origin/master,\n        # the fetch result is stored in FETCH_HEAD which destroys the rule we usually\n        # have. In that case we use a symbolic reference which is detached\n        ref_type = None\n        if remote_local_ref == \"FETCH_HEAD\":\n            ref_type = SymbolicReference\n        elif ref_type_name == \"tag\" or is_tag_operation:\n            # the ref_type_name can be branch, whereas we are still seeing a tag operation. It happens during\n            # testing, which is based on actual git operations\n            ref_type = TagReference\n        elif ref_type_name in (\"remote-tracking\", \"branch\"):\n            # note: remote-tracking is just the first part of the 'remote-tracking branch' token.\n            # We don't parse it correctly, but its enough to know what to do, and its new in git 1.7something\n            ref_type = RemoteReference\n        elif '/' in ref_type_name:\n            # If the fetch spec look something like this '+refs/pull/*:refs/heads/pull/*', and is thus pretty\n            # much anything the user wants, we will have trouble to determine what's going on\n            # For now, we assume the local ref is a Head\n            ref_type = Head\n        else:\n            raise TypeError(\"Cannot handle reference type: %r\" % ref_type_name)\n        # END handle ref type\n\n        # create ref instance\n        if ref_type is SymbolicReference:\n            remote_local_ref = ref_type(repo, \"FETCH_HEAD\")\n        else:\n            # determine prefix. Tags are usually pulled into refs/tags, they may have subdirectories.\n            # It is not clear sometimes where exactly the item is, unless we have an absolute path as indicated\n            # by the 'ref/' prefix. Otherwise even a tag could be in refs/remotes, which is when it will have the\n            # 'tags/' subdirectory in its path.\n            # We don't want to test for actual existence, but try to figure everything out analytically.\n            ref_path = None\n            remote_local_ref = remote_local_ref.strip()\n            if remote_local_ref.startswith(Reference._common_path_default + \"/\"):\n                # always use actual type if we get absolute paths\n                # Will always be the case if something is fetched outside of refs/remotes (if its not a tag)\n                ref_path = remote_local_ref\n                if ref_type is not TagReference and not \\\n                   remote_local_ref.startswith(RemoteReference._common_path_default + \"/\"):\n                    ref_type = Reference\n                # END downgrade remote reference\n            elif ref_type is TagReference and 'tags/' in remote_local_ref:\n                # even though its a tag, it is located in refs/remotes\n                ref_path = join_path(RemoteReference._common_path_default, remote_local_ref)\n            else:\n                ref_path = join_path(ref_type._common_path_default, remote_local_ref)\n            # END obtain refpath\n\n            # even though the path could be within the git conventions, we make\n            # sure we respect whatever the user wanted, and disabled path checking\n            remote_local_ref = ref_type(repo, ref_path, check_path=False)\n        # END create ref instance\n\n        note = (note and note.strip()) or ''\n\n        return cls(remote_local_ref, flags, note, old_commit, local_remote_ref)", "response": "Parse the line returned by git - fetch and return a FetchInfo object representing the information."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if a remote. nacite entry exists in the repository.", "response": "def exists(self):\n        \"\"\"\n        :return: True if this is a valid, existing remote.\n            Valid remotes have an entry in the repository's configuration\"\"\"\n        try:\n            self.config_reader.get('url')\n            return True\n        except cp.NoOptionError:\n            # we have the section at least ...\n            return True\n        except cp.NoSectionError:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\niterate over all Remote objects in the given repository.", "response": "def iter_items(cls, repo):\n        \"\"\":return: Iterator yielding Remote objects of the given repository\"\"\"\n        for section in repo.config_reader(\"repository\").sections():\n            if not section.startswith('remote '):\n                continue\n            lbound = section.find('\"')\n            rbound = section.rfind('\"')\n            if lbound == -1 or rbound == -1:\n                raise ValueError(\"Remote-Section has invalid format: %r\" % section)\n            yield Remote(repo, section[lbound + 1:rbound])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_url(self, new_url, old_url=None, **kwargs):\n        scmd = 'set-url'\n        kwargs['insert_kwargs_after'] = scmd\n        if old_url:\n            self.repo.git.remote(scmd, self.name, new_url, old_url, **kwargs)\n        else:\n            self.repo.git.remote(scmd, self.name, new_url, **kwargs)\n        return self", "response": "Configure URLs on current remote"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef urls(self):\n        try:\n            remote_details = self.repo.git.remote(\"get-url\", \"--all\", self.name)\n            for line in remote_details.split('\\n'):\n                yield line\n        except GitCommandError as ex:\n            ## We are on git < 2.7 (i.e TravisCI as of Oct-2016),\n            #  so `get-utl` command does not exist yet!\n            #    see: https://github.com/gitpython-developers/GitPython/pull/528#issuecomment-252976319\n            #    and: http://stackoverflow.com/a/32991784/548792\n            #\n            if 'Unknown subcommand: get-url' in str(ex):\n                try:\n                    remote_details = self.repo.git.remote(\"show\", self.name)\n                    for line in remote_details.split('\\n'):\n                        if '  Push  URL:' in line:\n                            yield line.split(': ')[-1]\n                except GitCommandError as ex:\n                    if any(msg in str(ex) for msg in ['correct access rights', 'cannot run ssh']):\n                        # If ssh is not setup to access this repository, see issue 694\n                        result = Git().execute(\n                            ['git', 'config', '--get', 'remote.%s.url' % self.name]\n                        )\n                        yield result\n                    else:\n                        raise ex\n            else:\n                raise ex", "response": "Yields all configured URL targets on a remote as strings"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef refs(self):\n        out_refs = IterableList(RemoteReference._id_attribute_, \"%s/\" % self.name)\n        out_refs.extend(RemoteReference.list_items(self.repo, remote=self.name))\n        return out_refs", "response": "Returns an IterableList of RemoteReference objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of RemoteReference objects that are not available locally on the remote side.", "response": "def stale_refs(self):\n        \"\"\"\n        :return:\n            IterableList RemoteReference objects that do not have a corresponding\n            head in the remote reference anymore as they have been deleted on the\n            remote side, but are still available locally.\n\n            The IterableList is prefixed, hence the 'origin' must be omitted. See\n            'refs' property for an example.\n\n            To make things more complicated, it can be possible for the list to include\n            other kinds of references, for example, tag references, if these are stale\n            as well. This is a fix for the issue described here:\n            https://github.com/gitpython-developers/GitPython/issues/260\n            \"\"\"\n        out_refs = IterableList(RemoteReference._id_attribute_, \"%s/\" % self.name)\n        for line in self.repo.git.remote(\"prune\", \"--dry-run\", self).splitlines()[2:]:\n            # expecting\n            # * [would prune] origin/new_branch\n            token = \" * [would prune] \"\n            if not line.startswith(token):\n                raise ValueError(\"Could not parse git-remote prune result: %r\" % line)\n            ref_name = line.replace(token, \"\")\n            # sometimes, paths start with a full ref name, like refs/tags/foo, see #260\n            if ref_name.startswith(Reference._common_path_default + '/'):\n                out_refs.append(SymbolicReference.from_path(self.repo, ref_name))\n            else:\n                fqhn = \"%s/%s\" % (RemoteReference._common_path_default, ref_name)\n                out_refs.append(RemoteReference(self.repo, fqhn))\n            # end special case handling\n        # END for each line\n        return out_refs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new remote to the given repository", "response": "def create(cls, repo, name, url, **kwargs):\n        \"\"\"Create a new remote to the given repository\n        :param repo: Repository instance that is to receive the new remote\n        :param name: Desired name of the remote\n        :param url: URL which corresponds to the remote's name\n        :param kwargs: Additional arguments to be passed to the git-remote add command\n        :return: New Remote instance\n        :raise GitCommandError: in case an origin with that name already exists\"\"\"\n        scmd = 'add'\n        kwargs['insert_kwargs_after'] = scmd\n        repo.git.remote(scmd, name, Git.polish_url(url), **kwargs)\n        return cls(repo, name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving the remote with the given name", "response": "def remove(cls, repo, name):\n        \"\"\"Remove the remote with the given name\n        :return: the passed remote name to remove\n        \"\"\"\n        repo.git.remote(\"rm\", name)\n        if isinstance(name, cls):\n            name._clear_cache()\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rename(self, new_name):\n        if self.name == new_name:\n            return self\n\n        self.repo.git.remote(\"rename\", self.name, new_name)\n        self.name = new_name\n        self._clear_cache()\n\n        return self", "response": "Rename the current object to the given new_name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, **kwargs):\n        scmd = 'update'\n        kwargs['insert_kwargs_after'] = scmd\n        self.repo.git.remote(scmd, self.name, **kwargs)\n        return self", "response": "Fetch all changes for this remote including new branches which will\n        be forced in."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nturning out we can t deal with remotes if the refspec is missing", "response": "def _assert_refspec(self):\n        \"\"\"Turns out we can't deal with remotes if the refspec is missing\"\"\"\n        config = self.config_reader\n        unset = 'placeholder'\n        try:\n            if config.get_value('fetch', default=unset) is unset:\n                msg = \"Remote '%s' has no refspec set.\\n\"\n                msg += \"You can set it as follows:\"\n                msg += \" 'git config --add \\\"remote.%s.fetch +refs/heads/*:refs/heads/*\\\"'.\"\n                raise AssertionError(msg % (self.name, self.name))\n        finally:\n            config.release()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfetch the latest changes for this remote and return a list of FetchInfo instances.", "response": "def fetch(self, refspec=None, progress=None, **kwargs):\n        \"\"\"Fetch the latest changes for this remote\n\n        :param refspec:\n            A \"refspec\" is used by fetch and push to describe the mapping\n            between remote ref and local ref. They are combined with a colon in\n            the format <src>:<dst>, preceded by an optional plus sign, +.\n            For example: git fetch $URL refs/heads/master:refs/heads/origin means\n            \"grab the master branch head from the $URL and store it as my origin\n            branch head\". And git push $URL refs/heads/master:refs/heads/to-upstream\n            means \"publish my master branch head as to-upstream branch at $URL\".\n            See also git-push(1).\n\n            Taken from the git manual\n\n            Fetch supports multiple refspecs (as the\n            underlying git-fetch does) - supplying a list rather than a string\n            for 'refspec' will make use of this facility.\n        :param progress: See 'push' method\n        :param kwargs: Additional arguments to be passed to git-fetch\n        :return:\n            IterableList(FetchInfo, ...) list of FetchInfo instances providing detailed\n            information about the fetch results\n\n        :note:\n            As fetch does not provide progress information to non-ttys, we cannot make\n            it available here unfortunately as in the 'push' method.\"\"\"\n        if refspec is None:\n            # No argument refspec, then ensure the repo's config has a fetch refspec.\n            self._assert_refspec()\n        kwargs = add_progress(kwargs, self.repo.git, progress)\n        if isinstance(refspec, list):\n            args = refspec\n        else:\n            args = [refspec]\n\n        proc = self.repo.git.fetch(self, *args, as_process=True, with_stdout=False,\n                                   universal_newlines=True, v=True, **kwargs)\n        res = self._get_fetch_info_from_stderr(proc, progress)\n        if hasattr(self.repo.odb, 'update_cache'):\n            self.repo.odb.update_cache()\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npushes changes from source branch in refspec to target branch in refspec.", "response": "def push(self, refspec=None, progress=None, **kwargs):\n        \"\"\"Push changes from source branch in refspec to target branch in refspec.\n\n        :param refspec: see 'fetch' method\n        :param progress:\n            Can take one of many value types:\n\n            * None to discard progress information\n            * A function (callable) that is called with the progress information.\n\n              Signature: ``progress(op_code, cur_count, max_count=None, message='')``.\n\n             `Click here <http://goo.gl/NPa7st>`_ for a description of all arguments\n              given to the function.\n            * An instance of a class derived from ``git.RemoteProgress`` that\n              overrides the ``update()`` function.\n\n        :note: No further progress information is returned after push returns.\n        :param kwargs: Additional arguments to be passed to git-push\n        :return:\n            IterableList(PushInfo, ...) iterable list of PushInfo instances, each\n            one informing about an individual head which had been updated on the remote\n            side.\n            If the push contains rejected heads, these will have the PushInfo.ERROR bit set\n            in their flags.\n            If the operation fails completely, the length of the returned IterableList will\n            be null.\"\"\"\n        kwargs = add_progress(kwargs, self.repo.git, progress)\n        proc = self.repo.git.push(self, refspec, porcelain=True, as_process=True,\n                                  universal_newlines=True, **kwargs)\n        return self._get_push_info(proc, progress)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef config_writer(self):\n        writer = self.repo.config_writer()\n\n        # clear our cache to assure we re-read the possibly changed configuration\n        self._clear_cache()\n        return SectionConstraint(writer, self._config_section_name())", "response": "Returns a GitConfigParser compatible object able to write options for this remote."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _config_parser(cls, repo, parent_commit, read_only):\n        parent_matches_head = True\n        if parent_commit is not None:\n            try:\n                parent_matches_head = repo.head.commit == parent_commit\n            except ValueError:\n                # We are most likely in an empty repository, so the HEAD doesn't point to a valid ref\n                pass\n        # end handle parent_commit\n\n        if not repo.bare and parent_matches_head:\n            fp_module = osp.join(repo.working_tree_dir, cls.k_modules_file)\n        else:\n            assert parent_commit is not None, \"need valid parent_commit in bare repositories\"\n            try:\n                fp_module = cls._sio_modules(parent_commit)\n            except KeyError:\n                raise IOError(\"Could not find %s file in the tree of parent commit %s\" %\n                              (cls.k_modules_file, parent_commit))\n            # END handle exceptions\n        # END handle non-bare working tree\n\n        if not read_only and (repo.bare or not parent_matches_head):\n            raise ValueError(\"Cannot write blobs of 'historical' submodule configurations\")\n        # END handle writes of historical submodules\n\n        return SubmoduleConfigParser(fp_module, read_only=read_only)", "response": "Returns a ConfigParser constrained to our submodule in read or write mode."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the configuration file as BytesIO - we only access it through the respective blob s data", "response": "def _sio_modules(cls, parent_commit):\n        \"\"\":return: Configuration file as BytesIO - we only access it through the respective blob's data\"\"\"\n        sio = BytesIO(parent_commit.tree[cls.k_modules_file].data_stream.read())\n        sio.name = cls.k_modules_file\n        return sio"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a ConfigParser constrained to our submodule in read or write mode", "response": "def _config_parser_constrained(self, read_only):\n        \"\"\":return: Config Parser constrained to our submodule in read or write mode\"\"\"\n        try:\n            pc = self.parent_commit\n        except ValueError:\n            pc = None\n        # end handle empty parent repository\n        parser = self._config_parser(self.repo, pc, read_only)\n        parser.set_submodule(self)\n        return SectionConstraint(parser, sm_section(self.name))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _clone_repo(cls, repo, url, path, name, **kwargs):\n        module_abspath = cls._module_abspath(repo, path, name)\n        module_checkout_path = module_abspath\n        if cls._need_gitfile_submodules(repo.git):\n            kwargs['separate_git_dir'] = module_abspath\n            module_abspath_dir = osp.dirname(module_abspath)\n            if not osp.isdir(module_abspath_dir):\n                os.makedirs(module_abspath_dir)\n            module_checkout_path = osp.join(repo.working_tree_dir, path)\n        # end\n\n        clone = git.Repo.clone_from(url, module_checkout_path, **kwargs)\n        if cls._need_gitfile_submodules(repo.git):\n            cls._write_git_file_and_module_config(module_checkout_path, module_abspath)\n        # end\n        return clone", "response": "Clones a repository from url to path with optional arguments"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a path to a relative path", "response": "def _to_relative_path(cls, parent_repo, path):\n        \"\"\":return: a path guaranteed  to be relative to the given parent-repository\n        :raise ValueError: if path is not contained in the parent repository's working tree\"\"\"\n        path = to_native_path_linux(path)\n        if path.endswith('/'):\n            path = path[:-1]\n        # END handle trailing slash\n\n        if osp.isabs(path):\n            working_tree_linux = to_native_path_linux(parent_repo.working_tree_dir)\n            if not path.startswith(working_tree_linux):\n                raise ValueError(\"Submodule checkout path '%s' needs to be within the parents repository at '%s'\"\n                                 % (working_tree_linux, path))\n            path = path[len(working_tree_linux.rstrip('/')) + 1:]\n            if not path:\n                raise ValueError(\"Absolute submodule path '%s' didn't yield a valid relative path\" % path)\n            # end verify converted relative path makes sense\n        # end convert to a relative path\n\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _write_git_file_and_module_config(cls, working_tree_dir, module_abspath):\n        git_file = osp.join(working_tree_dir, '.git')\n        rela_path = osp.relpath(module_abspath, start=working_tree_dir)\n        if is_win:\n            if osp.isfile(git_file):\n                os.remove(git_file)\n        with open(git_file, 'wb') as fp:\n            fp.write((\"gitdir: %s\" % rela_path).encode(defenc))\n\n        with GitConfigParser(osp.join(module_abspath, 'config'),\n                             read_only=False, merge_includes=False) as writer:\n            writer.set_value('core', 'worktree',\n                             to_native_path_linux(osp.relpath(working_tree_dir, start=module_abspath)))", "response": "Writes a. git file containing a relative path to the actual git module repository and the module configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add(cls, repo, name, path, url=None, branch=None, no_checkout=False):\n        if repo.bare:\n            raise InvalidGitRepositoryError(\"Cannot add submodules to bare repositories\")\n        # END handle bare repos\n\n        path = cls._to_relative_path(repo, path)\n\n        # assure we never put backslashes into the url, as some operating systems\n        # like it ...\n        if url is not None:\n            url = to_native_path_linux(url)\n        # END assure url correctness\n\n        # INSTANTIATE INTERMEDIATE SM\n        sm = cls(repo, cls.NULL_BIN_SHA, cls.k_default_mode, path, name, url='invalid-temporary')\n        if sm.exists():\n            # reretrieve submodule from tree\n            try:\n                sm = repo.head.commit.tree[path]\n                sm._name = name\n                return sm\n            except KeyError:\n                # could only be in index\n                index = repo.index\n                entry = index.entries[index.entry_key(path, 0)]\n                sm.binsha = entry.binsha\n                return sm\n            # END handle exceptions\n        # END handle existing\n\n        # fake-repo - we only need the functionality on the branch instance\n        br = git.Head(repo, git.Head.to_full_path(str(branch) or cls.k_head_default))\n        has_module = sm.module_exists()\n        branch_is_default = branch is None\n        if has_module and url is not None:\n            if url not in [r.url for r in sm.module().remotes]:\n                raise ValueError(\n                    \"Specified URL '%s' does not match any remote url of the repository at '%s'\" % (url, sm.abspath))\n            # END check url\n        # END verify urls match\n\n        mrepo = None\n        if url is None:\n            if not has_module:\n                raise ValueError(\"A URL was not given and existing repository did not exsit at %s\" % path)\n            # END check url\n            mrepo = sm.module()\n            urls = [r.url for r in mrepo.remotes]\n            if not urls:\n                raise ValueError(\"Didn't find any remote url in repository at %s\" % sm.abspath)\n            # END verify we have url\n            url = urls[0]\n        else:\n            # clone new repo\n            kwargs = {'n': no_checkout}\n            if not branch_is_default:\n                kwargs['b'] = br.name\n            # END setup checkout-branch\n\n            # _clone_repo(cls, repo, url, path, name, **kwargs):\n            mrepo = cls._clone_repo(repo, url, path, name, **kwargs)\n        # END verify url\n\n        ## See #525 for ensuring git urls in config-files valid under Windows.\n        url = Git.polish_url(url)\n\n        # It's important to add the URL to the parent config, to let `git submodule` know.\n        # otherwise there is a '-' character in front of the submodule listing\n        #  a38efa84daef914e4de58d1905a500d8d14aaf45 mymodule (v0.9.0-1-ga38efa8)\n        # -a38efa84daef914e4de58d1905a500d8d14aaf45 submodules/intermediate/one\n        with sm.repo.config_writer() as writer:\n            writer.set_value(sm_section(name), 'url', url)\n\n        # update configuration and index\n        index = sm.repo.index\n        with sm.config_writer(index=index, write=False) as writer:\n            writer.set_value('url', url)\n            writer.set_value('path', path)\n\n            sm._url = url\n            if not branch_is_default:\n                # store full path\n                writer.set_value(cls.k_head_option, br.path)\n                sm._branch_path = br.path\n\n        # we deliberately assume that our head matches our index !\n        sm.binsha = mrepo.head.commit.binsha\n        index.add([sm], write=True)\n\n        return sm", "response": "Add a new submodule to the given repository."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the repository of this submodule.", "response": "def update(self, recursive=False, init=True, to_latest_revision=False, progress=None, dry_run=False,\n               force=False, keep_going=False):\n        \"\"\"Update the repository of this submodule to point to the checkout\n        we point at with the binsha of this instance.\n\n        :param recursive: if True, we will operate recursively and update child-\n            modules as well.\n        :param init: if True, the module repository will be cloned into place if necessary\n        :param to_latest_revision: if True, the submodule's sha will be ignored during checkout.\n            Instead, the remote will be fetched, and the local tracking branch updated.\n            This only works if we have a local tracking branch, which is the case\n            if the remote repository had a master branch, or of the 'branch' option\n            was specified for this submodule and the branch existed remotely\n        :param progress: UpdateProgress instance or None if no progress should be shown\n        :param dry_run: if True, the operation will only be simulated, but not performed.\n            All performed operations are read-only\n        :param force:\n            If True, we may reset heads even if the repository in question is dirty. Additinoally we will be allowed\n            to set a tracking branch which is ahead of its remote branch back into the past or the location of the\n            remote branch. This will essentially 'forget' commits.\n            If False, local tracking branches that are in the future of their respective remote branches will simply\n            not be moved.\n        :param keep_going: if True, we will ignore but log all errors, and keep going recursively.\n            Unless dry_run is set as well, keep_going could cause subsequent/inherited errors you wouldn't see\n            otherwise.\n            In conjunction with dry_run, it can be useful to anticipate all errors when updating submodules\n        :note: does nothing in bare repositories\n        :note: method is definitely not atomic if recurisve is True\n        :return: self\"\"\"\n        if self.repo.bare:\n            return self\n        # END pass in bare mode\n\n        if progress is None:\n            progress = UpdateProgress()\n        # END handle progress\n        prefix = ''\n        if dry_run:\n            prefix = \"DRY-RUN: \"\n        # END handle prefix\n\n        # to keep things plausible in dry-run mode\n        if dry_run:\n            mrepo = None\n        # END init mrepo\n\n        try:\n            # ASSURE REPO IS PRESENT AND UPTODATE\n            #####################################\n            try:\n                mrepo = self.module()\n                rmts = mrepo.remotes\n                len_rmts = len(rmts)\n                for i, remote in enumerate(rmts):\n                    op = FETCH\n                    if i == 0:\n                        op |= BEGIN\n                    # END handle start\n\n                    progress.update(op, i, len_rmts, prefix + \"Fetching remote %s of submodule %r\"\n                                    % (remote, self.name))\n                    #===============================\n                    if not dry_run:\n                        remote.fetch(progress=progress)\n                    # END handle dry-run\n                    #===============================\n                    if i == len_rmts - 1:\n                        op |= END\n                    # END handle end\n                    progress.update(op, i, len_rmts, prefix + \"Done fetching remote of submodule %r\" % self.name)\n                # END fetch new data\n            except InvalidGitRepositoryError:\n                if not init:\n                    return self\n                # END early abort if init is not allowed\n\n                # there is no git-repository yet - but delete empty paths\n                checkout_module_abspath = self.abspath\n                if not dry_run and osp.isdir(checkout_module_abspath):\n                    try:\n                        os.rmdir(checkout_module_abspath)\n                    except OSError:\n                        raise OSError(\"Module directory at %r does already exist and is non-empty\"\n                                      % checkout_module_abspath)\n                    # END handle OSError\n                # END handle directory removal\n\n                # don't check it out at first - nonetheless it will create a local\n                # branch according to the remote-HEAD if possible\n                progress.update(BEGIN | CLONE, 0, 1, prefix + \"Cloning url '%s' to '%s' in submodule %r\" %\n                                (self.url, checkout_module_abspath, self.name))\n                if not dry_run:\n                    mrepo = self._clone_repo(self.repo, self.url, self.path, self.name, n=True)\n                # END handle dry-run\n                progress.update(END | CLONE, 0, 1, prefix + \"Done cloning to %s\" % checkout_module_abspath)\n\n                if not dry_run:\n                    # see whether we have a valid branch to checkout\n                    try:\n                        # find  a remote which has our branch - we try to be flexible\n                        remote_branch = find_first_remote_branch(mrepo.remotes, self.branch_name)\n                        local_branch = mkhead(mrepo, self.branch_path)\n\n                        # have a valid branch, but no checkout - make sure we can figure\n                        # that out by marking the commit with a null_sha\n                        local_branch.set_object(Object(mrepo, self.NULL_BIN_SHA))\n                        # END initial checkout + branch creation\n\n                        # make sure HEAD is not detached\n                        mrepo.head.set_reference(local_branch, logmsg=\"submodule: attaching head to %s\" % local_branch)\n                        mrepo.head.ref.set_tracking_branch(remote_branch)\n                    except (IndexError, InvalidGitRepositoryError):\n                        log.warn(\"Failed to checkout tracking branch %s\", self.branch_path)\n                    # END handle tracking branch\n\n                    # NOTE: Have to write the repo config file as well, otherwise\n                    # the default implementation will be offended and not update the repository\n                    # Maybe this is a good way to assure it doesn't get into our way, but\n                    # we want to stay backwards compatible too ... . Its so redundant !\n                    with self.repo.config_writer() as writer:\n                        writer.set_value(sm_section(self.name), 'url', self.url)\n                # END handle dry_run\n            # END handle initialization\n\n            # DETERMINE SHAS TO CHECKOUT\n            ############################\n            binsha = self.binsha\n            hexsha = self.hexsha\n            if mrepo is not None:\n                # mrepo is only set if we are not in dry-run mode or if the module existed\n                is_detached = mrepo.head.is_detached\n            # END handle dry_run\n\n            if mrepo is not None and to_latest_revision:\n                msg_base = \"Cannot update to latest revision in repository at %r as \" % mrepo.working_dir\n                if not is_detached:\n                    rref = mrepo.head.ref.tracking_branch()\n                    if rref is not None:\n                        rcommit = rref.commit\n                        binsha = rcommit.binsha\n                        hexsha = rcommit.hexsha\n                    else:\n                        log.error(\"%s a tracking branch was not set for local branch '%s'\", msg_base, mrepo.head.ref)\n                    # END handle remote ref\n                else:\n                    log.error(\"%s there was no local tracking branch\", msg_base)\n                # END handle detached head\n            # END handle to_latest_revision option\n\n            # update the working tree\n            # handles dry_run\n            if mrepo is not None and mrepo.head.commit.binsha != binsha:\n                # We must assure that our destination sha (the one to point to) is in the future of our current head.\n                # Otherwise, we will reset changes that might have been done on the submodule, but were not yet pushed\n                # We also handle the case that history has been rewritten, leaving no merge-base. In that case\n                # we behave conservatively, protecting possible changes the user had done\n                may_reset = True\n                if mrepo.head.commit.binsha != self.NULL_BIN_SHA:\n                    base_commit = mrepo.merge_base(mrepo.head.commit, hexsha)\n                    if len(base_commit) == 0 or base_commit[0].hexsha == hexsha:\n                        if force:\n                            msg = \"Will force checkout or reset on local branch that is possibly in the future of\"\n                            msg += \"the commit it will be checked out to, effectively 'forgetting' new commits\"\n                            log.debug(msg)\n                        else:\n                            msg = \"Skipping %s on branch '%s' of submodule repo '%s' as it contains un-pushed commits\"\n                            msg %= (is_detached and \"checkout\" or \"reset\", mrepo.head, mrepo)\n                            log.info(msg)\n                            may_reset = False\n                        # end handle force\n                    # end handle if we are in the future\n\n                    if may_reset and not force and mrepo.is_dirty(index=True, working_tree=True, untracked_files=True):\n                        raise RepositoryDirtyError(mrepo, \"Cannot reset a dirty repository\")\n                    # end handle force and dirty state\n                # end handle empty repo\n\n                # end verify future/past\n                progress.update(BEGIN | UPDWKTREE, 0, 1, prefix +\n                                \"Updating working tree at %s for submodule %r to revision %s\"\n                                % (self.path, self.name, hexsha))\n\n                if not dry_run and may_reset:\n                    if is_detached:\n                        # NOTE: for now we force, the user is no supposed to change detached\n                        # submodules anyway. Maybe at some point this becomes an option, to\n                        # properly handle user modifications - see below for future options\n                        # regarding rebase and merge.\n                        mrepo.git.checkout(hexsha, force=force)\n                    else:\n                        mrepo.head.reset(hexsha, index=True, working_tree=True)\n                    # END handle checkout\n                # if we may reset/checkout\n                progress.update(END | UPDWKTREE, 0, 1, prefix + \"Done updating working tree for submodule %r\"\n                                % self.name)\n            # END update to new commit only if needed\n        except Exception as err:\n            if not keep_going:\n                raise\n            log.error(str(err))\n        # end handle keep_going\n\n        # HANDLE RECURSION\n        ##################\n        if recursive:\n            # in dry_run mode, the module might not exist\n            if mrepo is not None:\n                for submodule in self.iter_items(self.module()):\n                    submodule.update(recursive, init, to_latest_revision, progress=progress, dry_run=dry_run,\n                                     force=force, keep_going=keep_going)\n                # END handle recursive update\n            # END handle dry run\n        # END for each submodule\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmove the submodule to another path.", "response": "def move(self, module_path, configuration=True, module=True):\n        \"\"\"Move the submodule to a another module path. This involves physically moving\n        the repository at our current path, changing the configuration, as well as\n        adjusting our index entry accordingly.\n\n        :param module_path: the path to which to move our module in the parent repostory's working tree,\n            given as repository-relative or absolute path. Intermediate directories will be created\n            accordingly. If the path already exists, it must be empty.\n            Trailing (back)slashes are removed automatically\n        :param configuration: if True, the configuration will be adjusted to let\n            the submodule point to the given path.\n        :param module: if True, the repository managed by this submodule\n            will be moved as well. If False, we don't move the submodule's checkout, which may leave\n            the parent repository in an inconsistent state.\n        :return: self\n        :raise ValueError: if the module path existed and was not empty, or was a file\n        :note: Currently the method is not atomic, and it could leave the repository\n            in an inconsistent state if a sub-step fails for some reason\n        \"\"\"\n        if module + configuration < 1:\n            raise ValueError(\"You must specify to move at least the module or the configuration of the submodule\")\n        # END handle input\n\n        module_checkout_path = self._to_relative_path(self.repo, module_path)\n\n        # VERIFY DESTINATION\n        if module_checkout_path == self.path:\n            return self\n        # END handle no change\n\n        module_checkout_abspath = join_path_native(self.repo.working_tree_dir, module_checkout_path)\n        if osp.isfile(module_checkout_abspath):\n            raise ValueError(\"Cannot move repository onto a file: %s\" % module_checkout_abspath)\n        # END handle target files\n\n        index = self.repo.index\n        tekey = index.entry_key(module_checkout_path, 0)\n        # if the target item already exists, fail\n        if configuration and tekey in index.entries:\n            raise ValueError(\"Index entry for target path did already exist\")\n        # END handle index key already there\n\n        # remove existing destination\n        if module:\n            if osp.exists(module_checkout_abspath):\n                if len(os.listdir(module_checkout_abspath)):\n                    raise ValueError(\"Destination module directory was not empty\")\n                # END handle non-emptiness\n\n                if osp.islink(module_checkout_abspath):\n                    os.remove(module_checkout_abspath)\n                else:\n                    os.rmdir(module_checkout_abspath)\n                # END handle link\n            else:\n                # recreate parent directories\n                # NOTE: renames() does that now\n                pass\n            # END handle existence\n        # END handle module\n\n        # move the module into place if possible\n        cur_path = self.abspath\n        renamed_module = False\n        if module and osp.exists(cur_path):\n            os.renames(cur_path, module_checkout_abspath)\n            renamed_module = True\n\n            if osp.isfile(osp.join(module_checkout_abspath, '.git')):\n                module_abspath = self._module_abspath(self.repo, self.path, self.name)\n                self._write_git_file_and_module_config(module_checkout_abspath, module_abspath)\n            # end handle git file rewrite\n        # END move physical module\n\n        # rename the index entry - have to manipulate the index directly as\n        # git-mv cannot be used on submodules ... yeah\n        previous_sm_path = self.path\n        try:\n            if configuration:\n                try:\n                    ekey = index.entry_key(self.path, 0)\n                    entry = index.entries[ekey]\n                    del(index.entries[ekey])\n                    nentry = git.IndexEntry(entry[:3] + (module_checkout_path,) + entry[4:])\n                    index.entries[tekey] = nentry\n                except KeyError:\n                    raise InvalidGitRepositoryError(\"Submodule's entry at %r did not exist\" % (self.path))\n                # END handle submodule doesn't exist\n\n                # update configuration\n                with self.config_writer(index=index) as writer:        # auto-write\n                    writer.set_value('path', module_checkout_path)\n                    self.path = module_checkout_path\n            # END handle configuration flag\n        except Exception:\n            if renamed_module:\n                os.renames(module_checkout_abspath, cur_path)\n            # END undo module renaming\n            raise\n        # END handle undo rename\n\n        # Auto-rename submodule if it's name was 'default', that is, the checkout directory\n        if previous_sm_path == self.name:\n            self.rename(module_checkout_path)\n        # end\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove(self, module=True, force=False, configuration=True, dry_run=False):\n        if not (module or configuration):\n            raise ValueError(\"Need to specify to delete at least the module, or the configuration\")\n        # END handle parameters\n\n        # Recursively remove children of this submodule\n        nc = 0\n        for csm in self.children():\n            nc += 1\n            csm.remove(module, force, configuration, dry_run)\n            del(csm)\n        # end\n        if configuration and not dry_run and nc > 0:\n            # Assure we don't leave the parent repository in a dirty state, and commit our changes\n            # It's important for recursive, unforced, deletions to work as expected\n            self.module().index.commit(\"Removed at least one of child-modules of '%s'\" % self.name)\n        # end handle recursion\n\n        # DELETE REPOSITORY WORKING TREE\n        ################################\n        if module and self.module_exists():\n            mod = self.module()\n            git_dir = mod.git_dir\n            if force:\n                # take the fast lane and just delete everything in our module path\n                # TODO: If we run into permission problems, we have a highly inconsistent\n                # state. Delete the .git folders last, start with the submodules first\n                mp = self.abspath\n                method = None\n                if osp.islink(mp):\n                    method = os.remove\n                elif osp.isdir(mp):\n                    method = rmtree\n                elif osp.exists(mp):\n                    raise AssertionError(\"Cannot forcibly delete repository as it was neither a link, nor a directory\")\n                # END handle brutal deletion\n                if not dry_run:\n                    assert method\n                    method(mp)\n                # END apply deletion method\n            else:\n                # verify we may delete our module\n                if mod.is_dirty(index=True, working_tree=True, untracked_files=True):\n                    raise InvalidGitRepositoryError(\n                        \"Cannot delete module at %s with any modifications, unless force is specified\"\n                        % mod.working_tree_dir)\n                # END check for dirt\n\n                # figure out whether we have new commits compared to the remotes\n                # NOTE: If the user pulled all the time, the remote heads might\n                # not have been updated, so commits coming from the remote look\n                # as if they come from us. But we stay strictly read-only and\n                # don't fetch beforehand.\n                for remote in mod.remotes:\n                    num_branches_with_new_commits = 0\n                    rrefs = remote.refs\n                    for rref in rrefs:\n                        num_branches_with_new_commits += len(mod.git.cherry(rref)) != 0\n                    # END for each remote ref\n                    # not a single remote branch contained all our commits\n                    if len(rrefs) and num_branches_with_new_commits == len(rrefs):\n                        raise InvalidGitRepositoryError(\n                            \"Cannot delete module at %s as there are new commits\" % mod.working_tree_dir)\n                    # END handle new commits\n                    # have to manually delete references as python's scoping is\n                    # not existing, they could keep handles open ( on windows this is a problem )\n                    if len(rrefs):\n                        del(rref)\n                    # END handle remotes\n                    del(rrefs)\n                    del(remote)\n                # END for each remote\n\n                # finally delete our own submodule\n                if not dry_run:\n                    self._clear_cache()\n                    wtd = mod.working_tree_dir\n                    del(mod)        # release file-handles (windows)\n                    import gc\n                    gc.collect()\n                    try:\n                        rmtree(wtd)\n                    except Exception as ex:\n                        if HIDE_WINDOWS_KNOWN_ERRORS:\n                            raise SkipTest(\"FIXME: fails with: PermissionError\\n  %s\", ex)\n                        else:\n                            raise\n                # END delete tree if possible\n            # END handle force\n\n            if not dry_run and osp.isdir(git_dir):\n                self._clear_cache()\n                try:\n                    rmtree(git_dir)\n                except Exception as ex:\n                    if HIDE_WINDOWS_KNOWN_ERRORS:\n                        raise SkipTest(\"FIXME: fails with: PermissionError\\n  %s\", ex)\n                    else:\n                        raise\n            # end handle separate bare repository\n        # END handle module deletion\n\n        # void our data not to delay invalid access\n        if not dry_run:\n            self._clear_cache()\n\n        # DELETE CONFIGURATION\n        ######################\n        if configuration and not dry_run:\n            # first the index-entry\n            parent_index = self.repo.index\n            try:\n                del(parent_index.entries[parent_index.entry_key(self.path, 0)])\n            except KeyError:\n                pass\n            # END delete entry\n            parent_index.write()\n\n            # now git config - need the config intact, otherwise we can't query\n            # information anymore\n            with self.repo.config_writer() as writer:\n                writer.remove_section(sm_section(self.name))\n\n            with self.config_writer() as writer:\n                writer.remove_section()\n        # END delete configuration\n\n        return self", "response": "Removes this submodule from the repository."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset this instance to use the given commit whose tree is supposed to contain the. gitmodules blob.", "response": "def set_parent_commit(self, commit, check=True):\n        \"\"\"Set this instance to use the given commit whose tree is supposed to\n        contain the .gitmodules blob.\n\n        :param commit:\n            Commit'ish reference pointing at the root_tree, or None to always point to the\n            most recent commit\n        :param check:\n            if True, relatively expensive checks will be performed to verify\n            validity of the submodule.\n        :raise ValueError: if the commit's tree didn't contain the .gitmodules blob.\n        :raise ValueError:\n            if the parent commit didn't store this submodule under the current path\n        :return: self\"\"\"\n        if commit is None:\n            self._parent_commit = None\n            return self\n        # end handle None\n        pcommit = self.repo.commit(commit)\n        pctree = pcommit.tree\n        if self.k_modules_file not in pctree:\n            raise ValueError(\"Tree of commit %s did not contain the %s file\" % (commit, self.k_modules_file))\n        # END handle exceptions\n\n        prev_pc = self._parent_commit\n        self._parent_commit = pcommit\n\n        if check:\n            parser = self._config_parser(self.repo, self._parent_commit, read_only=True)\n            if not parser.has_section(sm_section(self.name)):\n                self._parent_commit = prev_pc\n                raise ValueError(\"Submodule at path %r did not exist in parent commit %s\" % (self.path, commit))\n            # END handle submodule did not exist\n        # END handle checking mode\n\n        # update our sha, it could have changed\n        # If check is False, we might see a parent-commit that doesn't even contain the submodule anymore.\n        # in that case, mark our sha as being NULL\n        try:\n            self.binsha = pctree[self.path].binsha\n        except KeyError:\n            self.binsha = self.NULL_BIN_SHA\n        # end\n\n        self._clear_cache()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef config_writer(self, index=None, write=True):\n        writer = self._config_parser_constrained(read_only=False)\n        if index is not None:\n            writer.config._index = index\n        writer.config._auto_write = write\n        return writer", "response": "returns a config writer instance allowing you to read and write the data\n            from the gitmodules file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrename this submodule and return the new name.", "response": "def rename(self, new_name):\n        \"\"\"Rename this submodule\n        :note: This method takes care of renaming the submodule in various places, such as\n\n            * $parent_git_dir/config\n            * $working_tree_dir/.gitmodules\n            * (git >=v1.8.0: move submodule repository to new name)\n\n        As .gitmodules will be changed, you would need to make a commit afterwards. The changed .gitmodules file\n        will already be added to the index\n\n        :return: this submodule instance\n        \"\"\"\n        if self.name == new_name:\n            return self\n\n        # .git/config\n        with self.repo.config_writer() as pw:\n            # As we ourselves didn't write anything about submodules into the parent .git/config,\n            # we will not require it to exist, and just ignore missing entries.\n            if pw.has_section(sm_section(self.name)):\n                pw.rename_section(sm_section(self.name), sm_section(new_name))\n\n        # .gitmodules\n        with self.config_writer(write=True).config as cw:\n            cw.rename_section(sm_section(self.name), sm_section(new_name))\n\n        self._name = new_name\n\n        # .git/modules\n        mod = self.module()\n        if mod.has_separate_working_tree():\n            destination_module_abspath = self._module_abspath(self.repo, self.path, new_name)\n            source_dir = mod.git_dir\n            # Let's be sure the submodule name is not so obviously tied to a directory\n            if destination_module_abspath.startswith(mod.git_dir):\n                tmp_dir = self._module_abspath(self.repo, self.path, str(uuid.uuid4()))\n                os.renames(source_dir, tmp_dir)\n                source_dir = tmp_dir\n            # end handle self-containment\n            os.renames(source_dir, destination_module_abspath)\n            self._write_git_file_and_module_config(mod.working_tree_dir, destination_module_abspath)\n        # end move separate git repository\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef module(self):\n        # late import to workaround circular dependencies\n        module_checkout_abspath = self.abspath\n        try:\n            repo = git.Repo(module_checkout_abspath)\n            if repo != self.repo:\n                return repo\n            # END handle repo uninitialized\n        except (InvalidGitRepositoryError, NoSuchPathError):\n            raise InvalidGitRepositoryError(\"No valid repository at %s\" % module_checkout_abspath)\n        else:\n            raise InvalidGitRepositoryError(\"Repository at %r was not yet checked out\" % module_checkout_abspath)", "response": "Returns a Repo instance initialized from the repository at our submodule path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if the submodule exists False otherwise.", "response": "def exists(self):\n        \"\"\"\n        :return: True if the submodule exists, False otherwise. Please note that\n            a submodule may exist (in the .gitmodules file) even though its module\n            doesn't exist on disk\"\"\"\n        # keep attributes for later, and restore them if we have no valid data\n        # this way we do not actually alter the state of the object\n        loc = locals()\n        for attr in self._cache_attrs:\n            try:\n                if hasattr(self, attr):\n                    loc[attr] = getattr(self, attr)\n                # END if we have the attribute cache\n            except (cp.NoSectionError, ValueError):\n                # on PY3, this can happen apparently ... don't know why this doesn't happen on PY2\n                pass\n        # END for each attr\n        self._clear_cache()\n\n        try:\n            try:\n                self.path\n                return True\n            except Exception:\n                return False\n            # END handle exceptions\n        finally:\n            for attr in self._cache_attrs:\n                if attr in loc:\n                    setattr(self, attr, loc[attr])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nyields all Submodule instances available in the given repository", "response": "def iter_items(cls, repo, parent_commit='HEAD'):\n        \"\"\":return: iterator yielding Submodule instances available in the given repository\"\"\"\n        pc = repo.commit(parent_commit)         # parent commit instance\n        try:\n            parser = cls._config_parser(repo, pc, read_only=True)\n        except IOError:\n            return\n        # END handle empty iterator\n\n        rt = pc.tree                                # root tree\n\n        for sms in parser.sections():\n            n = sm_name(sms)\n            p = parser.get(sms, 'path')\n            u = parser.get(sms, 'url')\n            b = cls.k_head_default\n            if parser.has_option(sms, cls.k_head_option):\n                b = str(parser.get(sms, cls.k_head_option))\n            # END handle optional information\n\n            # get the binsha\n            index = repo.index\n            try:\n                sm = rt[p]\n            except KeyError:\n                # try the index, maybe it was just added\n                try:\n                    entry = index.entries[index.entry_key(p, 0)]\n                    sm = Submodule(repo, entry.binsha, entry.mode, entry.path)\n                except KeyError:\n                    raise InvalidGitRepositoryError(\n                        \"Gitmodule path %r did not exist in revision of parent commit %s\" % (p, parent_commit))\n                # END handle keyerror\n            # END handle critical error\n\n            # fill in remaining info - saves time as it doesn't have to be parsed again\n            sm._name = n\n            if pc != repo.commit():\n                sm._parent_commit = pc\n            # end set only if not most recent !\n            sm._branch_path = git.Head.to_full_path(b)\n            sm._url = u\n\n            yield sm"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a diff between two items being trees and an index or an index.", "response": "def diff(self, other=Index, paths=None, create_patch=False, **kwargs):\n        \"\"\"Creates diffs between two items being trees, trees and index or an\n        index and the working tree. It will detect renames automatically.\n\n        :param other:\n            Is the item to compare us with.\n            If None, we will be compared to the working tree.\n            If Treeish, it will be compared against the respective tree\n            If Index ( type ), it will be compared against the index.\n            If git.NULL_TREE, it will compare against the empty tree.\n            It defaults to Index to assure the method will not by-default fail\n            on bare repositories.\n\n        :param paths:\n            is a list of paths or a single path to limit the diff to.\n            It will only include at least one of the given path or paths.\n\n        :param create_patch:\n            If True, the returned Diff contains a detailed patch that if applied\n            makes the self to other. Patches are somewhat costly as blobs have to be read\n            and diffed.\n\n        :param kwargs:\n            Additional arguments passed to git-diff, such as\n            R=True to swap both sides of the diff.\n\n        :return: git.DiffIndex\n\n        :note:\n            On a bare repository, 'other' needs to be provided as Index or as\n            as Tree/Commit, or a git command error will occur\"\"\"\n        args = []\n        args.append(\"--abbrev=40\")        # we need full shas\n        args.append(\"--full-index\")       # get full index paths, not only filenames\n\n        args.append(\"-M\")                 # check for renames, in both formats\n        if create_patch:\n            args.append(\"-p\")\n        else:\n            args.append(\"--raw\")\n\n        # in any way, assure we don't see colored output,\n        # fixes https://github.com/gitpython-developers/GitPython/issues/172\n        args.append('--no-color')\n\n        if paths is not None and not isinstance(paths, (tuple, list)):\n            paths = [paths]\n\n        diff_cmd = self.repo.git.diff\n        if other is self.Index:\n            args.insert(0, '--cached')\n        elif other is NULL_TREE:\n            args.insert(0, '-r')  # recursive diff-tree\n            args.insert(0, '--root')\n            diff_cmd = self.repo.git.diff_tree\n        elif other is not None:\n            args.insert(0, '-r')  # recursive diff-tree\n            args.insert(0, other)\n            diff_cmd = self.repo.git.diff_tree\n\n        args.insert(0, self)\n\n        # paths is list here or None\n        if paths:\n            args.append(\"--\")\n            args.extend(paths)\n        # END paths handling\n\n        kwargs['as_process'] = True\n        proc = diff_cmd(*self._process_diff_args(args), **kwargs)\n\n        diff_method = (Diff._index_from_patch_format\n                       if create_patch\n                       else Diff._index_from_raw_format)\n        index = diff_method(self.repo, proc)\n\n        proc.wait()\n        return index"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iter_change_type(self, change_type):\n        if change_type not in self.change_type:\n            raise ValueError(\"Invalid change type: %s\" % change_type)\n\n        for diff in self:\n            if diff.change_type == change_type:\n                yield diff\n            elif change_type == \"A\" and diff.new_file:\n                yield diff\n            elif change_type == \"D\" and diff.deleted_file:\n                yield diff\n            elif change_type == \"R\" and diff.renamed:\n                yield diff\n            elif change_type == \"M\" and diff.a_blob and diff.b_blob and diff.a_blob != diff.b_blob:\n                yield diff", "response": "Iterate over the diff instances of the given type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new DiffIndex from the given text which must be in patch format.", "response": "def _index_from_patch_format(cls, repo, proc):\n        \"\"\"Create a new DiffIndex from the given text which must be in patch format\n        :param repo: is the repository we are operating on - it is required\n        :param stream: result of 'git diff' as a stream (supporting file protocol)\n        :return: git.DiffIndex \"\"\"\n\n        ## FIXME: Here SLURPING raw, need to re-phrase header-regexes linewise.\n        text = []\n        handle_process_output(proc, text.append, None, finalize_process, decode_streams=False)\n\n        # for now, we have to bake the stream\n        text = b''.join(text)\n        index = DiffIndex()\n        previous_header = None\n        for header in cls.re_header.finditer(text):\n            a_path_fallback, b_path_fallback, \\\n                old_mode, new_mode, \\\n                rename_from, rename_to, \\\n                new_file_mode, deleted_file_mode, \\\n                a_blob_id, b_blob_id, b_mode, \\\n                a_path, b_path = header.groups()\n\n            new_file, deleted_file = bool(new_file_mode), bool(deleted_file_mode)\n\n            a_path = cls._pick_best_path(a_path, rename_from, a_path_fallback)\n            b_path = cls._pick_best_path(b_path, rename_to, b_path_fallback)\n\n            # Our only means to find the actual text is to see what has not been matched by our regex,\n            # and then retro-actively assign it to our index\n            if previous_header is not None:\n                index[-1].diff = text[previous_header.end():header.start()]\n            # end assign actual diff\n\n            # Make sure the mode is set if the path is set. Otherwise the resulting blob is invalid\n            # We just use the one mode we should have parsed\n            a_mode = old_mode or deleted_file_mode or (a_path and (b_mode or new_mode or new_file_mode))\n            b_mode = b_mode or new_mode or new_file_mode or (b_path and a_mode)\n            index.append(Diff(repo,\n                              a_path,\n                              b_path,\n                              a_blob_id and a_blob_id.decode(defenc),\n                              b_blob_id and b_blob_id.decode(defenc),\n                              a_mode and a_mode.decode(defenc),\n                              b_mode and b_mode.decode(defenc),\n                              new_file, deleted_file,\n                              rename_from,\n                              rename_to,\n                              None, None, None))\n\n            previous_header = header\n        # end for each header we parse\n        if index:\n            index[-1].diff = text[header.end():]\n        # end assign last diff\n\n        return index"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _index_from_raw_format(cls, repo, proc):\n        # handles\n        # :100644 100644 687099101... 37c5e30c8... M    .gitignore\n\n        index = DiffIndex()\n\n        def handle_diff_line(line):\n            line = line.decode(defenc)\n            if not line.startswith(\":\"):\n                return\n\n            meta, _, path = line[1:].partition('\\t')\n            old_mode, new_mode, a_blob_id, b_blob_id, _change_type = meta.split(None, 4)\n            # Change type can be R100\n            # R: status letter\n            # 100: score (in case of copy and rename)\n            change_type = _change_type[0]\n            score_str = ''.join(_change_type[1:])\n            score = int(score_str) if score_str.isdigit() else None\n            path = path.strip()\n            a_path = path.encode(defenc)\n            b_path = path.encode(defenc)\n            deleted_file = False\n            new_file = False\n            rename_from = None\n            rename_to = None\n\n            # NOTE: We cannot conclude from the existence of a blob to change type\n            # as diffs with the working do not have blobs yet\n            if change_type == 'D':\n                b_blob_id = None\n                deleted_file = True\n            elif change_type == 'A':\n                a_blob_id = None\n                new_file = True\n            elif change_type == 'R':\n                a_path, b_path = path.split('\\t', 1)\n                a_path = a_path.encode(defenc)\n                b_path = b_path.encode(defenc)\n                rename_from, rename_to = a_path, b_path\n            elif change_type == 'T':\n                # Nothing to do\n                pass\n            # END add/remove handling\n\n            diff = Diff(repo, a_path, b_path, a_blob_id, b_blob_id, old_mode, new_mode,\n                        new_file, deleted_file, rename_from, rename_to, '',\n                        change_type, score)\n            index.append(diff)\n\n        handle_process_output(proc, handle_diff_line, None, finalize_process, decode_streams=False)\n\n        return index", "response": "Create a new git. DiffIndex from the given stream which must be in raw format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_cache_(self, attr):\n        if attr in TagObject.__slots__:\n            ostream = self.repo.odb.stream(self.binsha)\n            lines = ostream.read().decode(defenc).splitlines()\n\n            obj, hexsha = lines[0].split(\" \")       # object <hexsha> @UnusedVariable\n            type_token, type_name = lines[1].split(\" \")  # type <type_name> @UnusedVariable\n            self.object = \\\n                get_object_type_by_name(type_name.encode('ascii'))(self.repo, hex_to_bin(hexsha))\n\n            self.tag = lines[2][4:]  # tag <tag name>\n\n            tagger_info = lines[3]  # tagger <actor> <date>\n            self.tagger, self.tagged_date, self.tagger_tz_offset = parse_actor_and_date(tagger_info)\n\n            # line 4 empty - it could mark the beginning of the next header\n            # in case there really is no message, it would not exist. Otherwise\n            # a newline separates header from message\n            if len(lines) > 5:\n                self.message = \"\\n\".join(lines[5:])\n            else:\n                self.message = ''\n        # END check our attributes\n        else:\n            super(TagObject, self)._set_cache_(attr)", "response": "Cache all our attributes at once"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_object(self, object, logmsg=None):  # @ReservedAssignment\n        oldbinsha = None\n        if logmsg is not None:\n            head = self.repo.head\n            if not head.is_detached and head.ref == self:\n                oldbinsha = self.commit.binsha\n            # END handle commit retrieval\n        # END handle message is set\n\n        super(Reference, self).set_object(object, logmsg)\n\n        if oldbinsha is not None:\n            # /* from refs.c in git-source\n            # * Special hack: If a branch is updated directly and HEAD\n            # * points to it (may happen on the remote side of a push\n            # * for example) then logically the HEAD reflog should be\n            # * updated too.\n            # * A generic solution implies reverse symref information,\n            # * but finding all symrefs pointing to the given branch\n            # * would be rather costly for this rare event (the direct\n            # * update of a branch) to be worth it.  So let's cheat and\n            # * check with HEAD only which should cover 99% of all usage\n            # * scenarios (even 100% of the default ones).\n            # */\n            self.repo.head.log_append(oldbinsha, logmsg)\n        # END check if the head\n\n        return self", "response": "Set the object of the branch."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the shortest name of this reference - it may contain path components", "response": "def name(self):\n        \"\"\":return: (shortest) Name of this reference - it may contain path components\"\"\"\n        # first two path tokens are can be removed as they are\n        # refs/heads or refs/tags or refs/remotes\n        tokens = self.path.split('/')\n        if len(tokens) < 3:\n            return self.path           # could be refs/HEAD\n        return '/'.join(tokens[2:])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post_clear_cache(func):\n\n    @wraps(func)\n    def post_clear_cache_if_not_raised(self, *args, **kwargs):\n        rval = func(self, *args, **kwargs)\n        self._delete_entries_cache()\n        return rval\n    # END wrapper method\n\n    return post_clear_cache_if_not_raised", "response": "Decorator for functions that alter the index using the git command. This decorator is used to clear the index if the function is not raised."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef default_index(func):\n\n    @wraps(func)\n    def check_default_index(self, *args, **kwargs):\n        if self._file_path != self._index_path():\n            raise AssertionError(\n                \"Cannot call %r on indices that do not represent the default git index\" % func.__name__)\n        return func(self, *args, **kwargs)\n    # END wrapper method\n\n    return check_default_index", "response": "Decorator that ensures that the wrapped method is only run if we are the default git index."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mkhead(repo, path):\n    return git.Head(repo, git.Head.to_full_path(path))", "response": "Create a new branch or head instance"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the first remote branch in a list of remotes.", "response": "def find_first_remote_branch(remotes, branch_name):\n    \"\"\"Find the remote branch matching the name of the given branch or raise InvalidGitRepositoryError\"\"\"\n    for remote in remotes:\n        try:\n            return remote.refs[branch_name]\n        except IndexError:\n            continue\n        # END exception handling\n    # END for remote\n    raise InvalidGitRepositoryError(\"Didn't find remote branch '%r' in any of the given remotes\" % branch_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nflush changes in our configuration file to the index", "response": "def flush_to_index(self):\n        \"\"\"Flush changes in our configuration file to the index\"\"\"\n        assert self._smref is not None\n        # should always have a file here\n        assert not isinstance(self._file_or_files, BytesIO)\n\n        sm = self._smref()\n        if sm is not None:\n            index = self._index\n            if index is None:\n                index = sm.repo.index\n            # END handle index\n            index.add([sm.k_modules_file], write=self._auto_write)\n            sm._clear_cache()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format(self):\n        act = self.actor\n        time = self.time\n        return u\"{} {} {} <{}> {!s} {}\\t{}\\n\".format(self.oldhexsha,\n                                                     self.newhexsha,\n                                                     act.name,\n                                                     act.email,\n                                                     time[0],\n                                                     altz_to_utctz_str(time[1]),\n                                                     self.message)", "response": "return a string suitable to be placed in a reflog file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef new(self, oldhexsha, newhexsha, actor, time, tz_offset, message):\n        if not isinstance(actor, Actor):\n            raise ValueError(\"Need actor instance, got %s\" % actor)\n        # END check types\n        return RefLogEntry((oldhexsha, newhexsha, actor, (time, tz_offset), message))", "response": "Create a new RefLogEntry instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a line of revlog into a new RefLogEntry instance.", "response": "def from_line(cls, line):\n        \"\"\":return: New RefLogEntry instance from the given revlog line.\n        :param line: line bytes without trailing newline\n        :raise ValueError: If line could not be parsed\"\"\"\n        line = line.decode(defenc)\n        fields = line.split('\\t', 1)\n        if len(fields) == 1:\n            info, msg = fields[0], None\n        elif len(fields) == 2:\n            info, msg = fields\n        else:\n            raise ValueError(\"Line must have up to two TAB-separated fields.\"\n                             \" Got %s\" % repr(line))\n        # END handle first split\n\n        oldhexsha = info[:40]\n        newhexsha = info[41:81]\n        for hexsha in (oldhexsha, newhexsha):\n            if not cls._re_hexsha_only.match(hexsha):\n                raise ValueError(\"Invalid hexsha: %r\" % (hexsha,))\n            # END if hexsha re doesn't match\n        # END for each hexsha\n\n        email_end = info.find('>', 82)\n        if email_end == -1:\n            raise ValueError(\"Missing token: >\")\n        # END handle missing end brace\n\n        actor = Actor._from_string(info[82:email_end + 1])\n        time, tz_offset = parse_date(info[email_end + 2:])\n\n        return RefLogEntry((oldhexsha, newhexsha, actor, (time, tz_offset), msg))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef path(cls, ref):\n        return osp.join(ref.repo.git_dir, \"logs\", to_native_path(ref.path))", "response": "Returns the absolute path to the reflog of the given ref\n            instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iter_entries(cls, stream):\n        new_entry = RefLogEntry.from_line\n        if isinstance(stream, string_types):\n            stream = file_contents_ro_filepath(stream)\n        # END handle stream type\n        while True:\n            line = stream.readline()\n            if not line:\n                return\n            yield new_entry(line.strip())\n        # END endless loop\n        stream.close()", "response": "Iterate over the entries of the reflog from the given stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the RefLogEntry at the given index", "response": "def entry_at(cls, filepath, index):\n        \"\"\":return: RefLogEntry at the given index\n        :param filepath: full path to the index file from which to read the entry\n        :param index: python list compatible index, i.e. it may be negative to\n            specify an entry counted from the end of the list\n\n        :raise IndexError: If the entry didn't exist\n\n        .. note:: This method is faster as it only parses the entry at index, skipping\n            all other lines. Nonetheless, the whole file has to be read if\n            the index is negative\n        \"\"\"\n        fp = open(filepath, 'rb')\n        if index < 0:\n            return RefLogEntry.from_line(fp.readlines()[index].strip())\n        else:\n            # read until index is reached\n            for i in xrange(index + 1):\n                line = fp.readline()\n                if not line:\n                    break\n                # END abort on eof\n            # END handle runup\n\n            if i != index or not line:\n                raise IndexError\n            # END handle exception\n\n            return RefLogEntry.from_line(line.strip())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite the contents of the reflog instance to a file at the given filepath.", "response": "def to_file(self, filepath):\n        \"\"\"Write the contents of the reflog instance to a file at the given filepath.\n        :param filepath: path to file, parent directories are assumed to exist\"\"\"\n        lfd = LockedFD(filepath)\n        assure_directory_exists(filepath, is_file=True)\n\n        fp = lfd.open(write=True, stream=True)\n        try:\n            self._serialize(fp)\n            lfd.commit()\n        except Exception:\n            # on failure it rolls back automatically, but we make it clear\n            lfd.rollback()\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nappend a new entry to the revlog at filepath.", "response": "def append_entry(cls, config_reader, filepath, oldbinsha, newbinsha, message):\n        \"\"\"Append a new log entry to the revlog at filepath.\n\n        :param config_reader: configuration reader of the repository - used to obtain\n            user information. May also be an Actor instance identifying the committer directly.\n            May also be None\n        :param filepath: full path to the log file\n        :param oldbinsha: binary sha of the previous commit\n        :param newbinsha: binary sha of the current commit\n        :param message: message describing the change to the reference\n        :param write: If True, the changes will be written right away. Otherwise\n            the change will not be written\n        :return: RefLogEntry objects which was appended to the log\n        :note: As we are append-only, concurrent access is not a problem as we\n            do not interfere with readers.\"\"\"\n        if len(oldbinsha) != 20 or len(newbinsha) != 20:\n            raise ValueError(\"Shas need to be given in binary format\")\n        # END handle sha type\n        assure_directory_exists(filepath, is_file=True)\n        first_line = message.split('\\n')[0]\n        committer = isinstance(config_reader, Actor) and config_reader or Actor.committer(config_reader)\n        entry = RefLogEntry((\n            bin_to_hex(oldbinsha).decode('ascii'),\n            bin_to_hex(newbinsha).decode('ascii'),\n            committer, (int(time.time()), time.altzone), first_line\n        ))\n\n        lf = LockFile(filepath)\n        lf._obtain_lock_or_raise()\n        fd = open(filepath, 'ab')\n        try:\n            fd.write(entry.format().encode(defenc))\n        finally:\n            fd.close()\n            lf._release_lock()\n        # END handle write operation\n\n        return entry"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites this instance s data to the file we are originating from .", "response": "def write(self):\n        \"\"\"Write this instance's data to the file we are originating from\n        :return: self\"\"\"\n        if self._path is None:\n            raise ValueError(\"Instance was not initialized with a path, use to_file(...) instead\")\n        # END assert path\n        self.to_file(self._path)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rmtree(path):\n\n    def onerror(func, path, exc_info):\n        # Is the error an access error ?\n        os.chmod(path, stat.S_IWUSR)\n\n        try:\n            func(path)  # Will scream if still not possible to delete.\n        except Exception as ex:\n            if HIDE_WINDOWS_KNOWN_ERRORS:\n                raise SkipTest(\"FIXME: fails with: PermissionError\\n  %s\", ex)\n            else:\n                raise\n\n    return shutil.rmtree(path, False, onerror)", "response": "Remove the given directory recursively."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rmfile(path):\n    if osp.isfile(path):\n        if is_win:\n            os.chmod(path, 0o777)\n        os.remove(path)", "response": "Ensure file deleted also on Windows where read - only files need special treatment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncopy all data from the source stream into the destination stream in chunks of size chunk_size", "response": "def stream_copy(source, destination, chunk_size=512 * 1024):\n    \"\"\"Copy all data from the source stream into the destination stream in chunks\n    of size chunk_size\n\n    :return: amount of bytes written\"\"\"\n    br = 0\n    while True:\n        chunk = source.read(chunk_size)\n        destination.write(chunk)\n        br += len(chunk)\n        if len(chunk) < chunk_size:\n            break\n    # END reading output stream\n    return br"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef join_path(a, *p):\n    path = a\n    for b in p:\n        if len(b) == 0:\n            continue\n        if b.startswith('/'):\n            path += b[1:]\n        elif path == '' or path.endswith('/'):\n            path += b\n        else:\n            path += '/' + b\n    # END for each path token to add\n    return path", "response": "Join path tokens together similar to osp. join but always use\n    '/' instead of possibly '\\' on windows."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassure that the directory pointed to by path exists.", "response": "def assure_directory_exists(path, is_file=False):\n    \"\"\"Assure that the directory pointed to by path exists.\n\n    :param is_file: If True, path is assumed to be a file and handled correctly.\n        Otherwise it must be a directory\n    :return: True if the directory was created, False if it already existed\"\"\"\n    if is_file:\n        path = osp.dirname(path)\n    # END handle file\n    if not osp.isdir(path):\n        os.makedirs(path)\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nusing cygpath to find the correct path.", "response": "def cygpath(path):\n    \"\"\"Use :meth:`git.cmd.Git.polish_url()` instead, that works on any environment.\"\"\"\n    if not path.startswith(('/cygdrive', '//')):\n        for regex, parser, recurse in _cygpath_parsers:\n            match = regex.match(path)\n            if match:\n                path = parser(*match.groups())\n                if recurse:\n                    path = cygpath(path)\n                break\n        else:\n            path = _cygexpath(None, path)\n\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the line that contains a progress line.", "response": "def _parse_progress_line(self, line):\n        \"\"\"Parse progress information from the given line as retrieved by git-push\n        or git-fetch.\n\n        - Lines that do not contain progress info are stored in :attr:`other_lines`.\n        - Lines that seem to contain an error (i.e. start with error: or fatal:) are stored\n        in :attr:`error_lines`.\n\n        :return: list(line, ...) list of lines that could not be processed\"\"\"\n        # handle\n        # Counting objects: 4, done.\n        # Compressing objects:  50% (1/2)   \\rCompressing objects: 100% (2/2)   \\rCompressing objects: 100% (2/2), done.\n        self._cur_line = line = line.decode('utf-8') if isinstance(line, bytes) else line\n        if len(self.error_lines) > 0 or self._cur_line.startswith(('error:', 'fatal:')):\n            self.error_lines.append(self._cur_line)\n            return []\n\n        sub_lines = line.split('\\r')\n        failed_lines = []\n        for sline in sub_lines:\n            # find escape characters and cut them away - regex will not work with\n            # them as they are non-ascii. As git might expect a tty, it will send them\n            last_valid_index = None\n            for i, c in enumerate(reversed(sline)):\n                if ord(c) < 32:\n                    # its a slice index\n                    last_valid_index = -i - 1\n                # END character was non-ascii\n            # END for each character in sline\n            if last_valid_index is not None:\n                sline = sline[:last_valid_index]\n            # END cut away invalid part\n            sline = sline.rstrip()\n\n            cur_count, max_count = None, None\n            match = self.re_op_relative.match(sline)\n            if match is None:\n                match = self.re_op_absolute.match(sline)\n\n            if not match:\n                self.line_dropped(sline)\n                failed_lines.append(sline)\n                continue\n            # END could not get match\n\n            op_code = 0\n            remote, op_name, percent, cur_count, max_count, message = match.groups()  # @UnusedVariable\n\n            # get operation id\n            if op_name == \"Counting objects\":\n                op_code |= self.COUNTING\n            elif op_name == \"Compressing objects\":\n                op_code |= self.COMPRESSING\n            elif op_name == \"Writing objects\":\n                op_code |= self.WRITING\n            elif op_name == 'Receiving objects':\n                op_code |= self.RECEIVING\n            elif op_name == 'Resolving deltas':\n                op_code |= self.RESOLVING\n            elif op_name == 'Finding sources':\n                op_code |= self.FINDING_SOURCES\n            elif op_name == 'Checking out files':\n                op_code |= self.CHECKING_OUT\n            else:\n                # Note: On windows it can happen that partial lines are sent\n                # Hence we get something like \"CompreReceiving objects\", which is\n                # a blend of \"Compressing objects\" and \"Receiving objects\".\n                # This can't really be prevented, so we drop the line verbosely\n                # to make sure we get informed in case the process spits out new\n                # commands at some point.\n                self.line_dropped(sline)\n                # Note: Don't add this line to the failed lines, as we have to silently\n                # drop it\n                self.other_lines.extend(failed_lines)\n                return failed_lines\n            # END handle op code\n\n            # figure out stage\n            if op_code not in self._seen_ops:\n                self._seen_ops.append(op_code)\n                op_code |= self.BEGIN\n            # END begin opcode\n\n            if message is None:\n                message = ''\n            # END message handling\n\n            message = message.strip()\n            if message.endswith(self.DONE_TOKEN):\n                op_code |= self.END\n                message = message[:-len(self.DONE_TOKEN)]\n            # END end message handling\n            message = message.strip(self.TOKEN_SEPARATOR)\n\n            self.update(op_code,\n                        cur_count and float(cur_count),\n                        max_count and float(max_count),\n                        message)\n        # END for each sub line\n        self.other_lines.extend(failed_lines)\n        return failed_lines"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an Actor from a string.", "response": "def _from_string(cls, string):\n        \"\"\"Create an Actor from a string.\n        :param string: is the string, which is expected to be in regular git format\n\n                John Doe <jdoe@example.com>\n\n        :return: Actor \"\"\"\n        m = cls.name_email_regex.search(string)\n        if m:\n            name, email = m.groups()\n            return Actor(name, email)\n        else:\n            m = cls.name_only_regex.search(string)\n            if m:\n                return Actor(m.group(1), None)\n            else:\n                # assume best and use the whole string as name\n                return Actor(string, None)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an actor instance corresponding to the configured committer.", "response": "def committer(cls, config_reader=None):\n        \"\"\"\n        :return: Actor instance corresponding to the configured committer. It behaves\n            similar to the git implementation, such that the environment will override\n            configuration values of config_reader. If no value is set at all, it will be\n            generated\n        :param config_reader: ConfigReader to use to retrieve the values from in case\n            they are not set in the environment\"\"\"\n        return cls._main_actor(cls.env_committer_name, cls.env_committer_email, config_reader)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a Stat object from a git - diff output retrieved by git - diff.", "response": "def _list_from_string(cls, repo, text):\n        \"\"\"Create a Stat object from output retrieved by git-diff.\n\n        :return: git.Stat\"\"\"\n        hsh = {'total': {'insertions': 0, 'deletions': 0, 'lines': 0, 'files': 0}, 'files': {}}\n        for line in text.splitlines():\n            (raw_insertions, raw_deletions, filename) = line.split(\"\\t\")\n            insertions = raw_insertions != '-' and int(raw_insertions) or 0\n            deletions = raw_deletions != '-' and int(raw_deletions) or 0\n            hsh['total']['insertions'] += insertions\n            hsh['total']['deletions'] += deletions\n            hsh['total']['lines'] += insertions + deletions\n            hsh['total']['files'] += 1\n            hsh['files'][filename.strip()] = {'insertions': insertions,\n                                              'deletions': deletions,\n                                              'lines': insertions + deletions}\n        return Stats(hsh['total'], hsh['files'])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _release_lock(self):\n        if not self._has_lock():\n            return\n\n        # if someone removed our file beforhand, lets just flag this issue\n        # instead of failing, to make it more usable.\n        lfp = self._lock_file_path()\n        try:\n            rmfile(lfp)\n        except OSError:\n            pass\n        self._owns_lock = False", "response": "Release our lock if we have one."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _obtain_lock(self):\n        starttime = time.time()\n        maxtime = starttime + float(self._max_block_time)\n        while True:\n            try:\n                super(BlockingLockFile, self)._obtain_lock()\n            except IOError:\n                # synity check: if the directory leading to the lockfile is not\n                # readable anymore, raise an exception\n                curtime = time.time()\n                if not osp.isdir(osp.dirname(self._lock_file_path())):\n                    msg = \"Directory containing the lockfile %r was not readable anymore after waiting %g seconds\" % (\n                        self._lock_file_path(), curtime - starttime)\n                    raise IOError(msg)\n                # END handle missing directory\n\n                if curtime >= maxtime:\n                    msg = \"Waited %g seconds for lock at %r\" % (maxtime - starttime, self._lock_file_path())\n                    raise IOError(msg)\n                # END abort if we wait too long\n                time.sleep(self._check_interval)\n            else:\n                break", "response": "This method blocks until it obtained the lock or raises IOError if it ran out of time or the parent directory is not available anymore."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_items(cls, repo, *args, **kwargs):\n        out_list = IterableList(cls._id_attribute_)\n        out_list.extend(cls.iter_items(repo, *args, **kwargs))\n        return out_list", "response": "Return a list of items of this type in the repository."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(input_string):\r\n\r\n    global  exprStack\r\n    global targetvar\r\n\r\n    # Start with a blank exprStack and a blank targetvar\r\n    exprStack = []\r\n    targetvar=None\r\n\r\n    if input_string != '':\r\n      # try parsing the input string\r\n      try:\r\n        L=equation.parseString( input_string )\r\n      except ParseException as err:\r\n        print('Parse Failure', file=sys.stderr)\r\n        print(err.line, file=sys.stderr)\r\n        print(\" \"*(err.column-1) + \"^\", file=sys.stderr)\r\n        print(err, file=sys.stderr)\r\n        raise\r\n\r\n      # show result of parsing the input string\r\n      if debug_flag:\r\n        print(input_string, \"->\", L)\r\n        print(\"exprStack=\", exprStack)\r\n\r\n      # Evaluate the stack of parsed operands, emitting C code.\r\n      try:\r\n        result=_evaluateStack(exprStack)\r\n      except TypeError:\r\n        print(\"Unsupported operation on right side of '%s'.\\nCheck for missing or incorrect tags on non-scalar operands.\"%input_string, file=sys.stderr)\r\n        raise\r\n      except UnaryUnsupportedError:\r\n        print(\"Unary negation is not supported for vectors and matrices: '%s'\"%input_string, file=sys.stderr)\r\n        raise\r\n\r\n      # Create final assignment and print it.\r\n      if debug_flag: print(\"var=\",targetvar)\r\n      if targetvar != None:\r\n          try:\r\n            result = _assignfunc(targetvar,result)\r\n          except TypeError:\r\n            print(\"Left side tag does not match right side of '%s'\"%input_string, file=sys.stderr)\r\n            raise\r\n          except UnaryUnsupportedError:\r\n            print(\"Unary negation is not supported for vectors and matrices: '%s'\"%input_string, file=sys.stderr)\r\n            raise\r\n\r\n          return  result\r\n      else:\r\n        print(\"Empty left side in '%s'\"%input_string, file=sys.stderr)\r\n        raise TypeError", "response": "Parse a string containing an LT equation and returns a list of functions that implement the expression."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nscan an input file for LA equations between double square brackets, e.g. [[ M3_mymatrix = M3_anothermatrix^-1 ]], and replaces the expression with a comment containing the equation followed by nested function calls that implement the equation as C code. A trailing semi-colon is appended. The equation within [[ ]] should NOT end with a semicolon as that will raise a ParseException. However, it is ok to have a semicolon after the right brackets. Other text in the file is unaltered. The arguments are file objects (NOT file names) opened for reading and writing, respectively.", "response": "def fprocess(infilep,outfilep):\r\n   \"\"\"\r\n   Scans an input file for LA equations between double square brackets,\r\n   e.g. [[ M3_mymatrix = M3_anothermatrix^-1 ]], and replaces the expression\r\n   with a comment containing the equation followed by nested function calls\r\n   that implement the equation as C code. A trailing semi-colon is appended.\r\n   The equation within [[ ]] should NOT end with a semicolon as that will raise\r\n   a ParseException. However, it is ok to have a semicolon after the right brackets.\r\n\r\n   Other text in the file is unaltered.\r\n\r\n   The arguments are file objects (NOT file names) opened for reading and\r\n   writing, respectively.\r\n   \"\"\"\r\n   pattern = r'\\[\\[\\s*(.*?)\\s*\\]\\]'\r\n   eqn = re.compile(pattern,re.DOTALL)\r\n   s = infilep.read()\r\n   def parser(mo):\r\n      ccode = parse(mo.group(1))\r\n      return \"/* %s */\\n%s;\\nLAParserBufferReset();\\n\"%(mo.group(1),ccode)\r\n\r\n   content = eqn.sub(parser,s)\r\n   outfilep.write(content)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a list of files and return a dictionary of ParseResults.", "response": "def main(testfiles=None, action=printer):\r\n    \"\"\"testfiles can be None, in which case the command line arguments are used as filenames.\r\n    testfiles can be a string, in which case that file is parsed.\r\n    testfiles can be a list.\r\n    In all cases, the filenames will be globbed.\r\n    If more than one file is parsed successfully, a dictionary of ParseResults is returned.\r\n    Otherwise, a simple ParseResults is returned.\r\n    \"\"\"\r\n    testfiles = get_filename_list(testfiles)\r\n    print(testfiles)\r\n\r\n    if action:\r\n        for i in (simple_identifier, value, item_list):\r\n            i.setParseAction(action)\r\n\r\n    success = 0\r\n    failures = []\r\n\r\n    retval = {}\r\n    for f in testfiles:\r\n        try:\r\n            retval[f] = object_definition.parseFile(f)\r\n            success += 1\r\n        except Exception:\r\n            failures.append(f)\r\n\r\n    if failures:\r\n        print('\\nfailed while processing %s' % ', '.join(failures))\r\n    print('\\nsucceeded on %d of %d files' %(success, len(testfiles)))\r\n\r\n    if len(retval) == 1 and len(testfiles) == 1:\r\n        # if only one file is parsed, return the parseResults directly\r\n        return retval[list(retval.keys())[0]]\r\n\r\n    # else, return a dictionary of parseResults\r\n    return retval"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef expand_state_definition(source, loc, tokens):\n    indent = \" \" * (pp.col(loc, source) - 1)\n    statedef = []\n\n    # build list of states\n    states = set()\n    fromTo = {}\n    for tn in tokens.transitions:\n        states.add(tn.from_state)\n        states.add(tn.to_state)\n        fromTo[tn.from_state] = tn.to_state\n\n    # define base class for state classes\n    baseStateClass = tokens.name\n    statedef.extend([\n        \"class %s(object):\" % baseStateClass,\n        \"    def __str__(self):\",\n        \"        return self.__class__.__name__\",\n\n        \"    @classmethod\",\n        \"    def states(cls):\",\n        \"        return list(cls.__subclasses__())\",\n\n        \"    def next_state(self):\",\n        \"        return self._next_state_class()\",\n    ])\n\n    # define all state classes\n    statedef.extend(\"class {0}({1}): pass\".format(s, baseStateClass) for s in states)\n\n    # define state->state transitions\n    statedef.extend(\"{0}._next_state_class = {1}\".format(s, fromTo[s]) for s in states if s in fromTo)\n\n    statedef.extend([\n        \"class {baseStateClass}Mixin:\".format(baseStateClass=baseStateClass),\n        \"    def __init__(self):\",\n        \"        self._state = None\",\n\n        \"    def initialize_state(self, init_state):\",\n        \"        self._state = init_state\",\n\n        \"    @property\",\n        \"    def state(self):\",\n        \"        return self._state\",\n\n        \"    # get behavior/properties from current state\",\n        \"    def __getattr__(self, attrname):\",\n        \"        attr = getattr(self._state, attrname)\",\n        \"        return attr\",\n\n        \"    def __str__(self):\",\n        \"       return '{0}: {1}'.format(self.__class__.__name__, self._state)\",\n        ])\n\n    return indent + (\"\\n\" + indent).join(statedef) + \"\\n\"", "response": "Parse action to convert statemachine to corresponding Python classes and methods"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse action to convert named transitions to corresponding Python statemachine classes and methods.", "response": "def expand_named_state_definition(source, loc, tokens):\n    \"\"\"\n    Parse action to convert statemachine with named transitions to corresponding Python\n    classes and methods\n    \"\"\"\n    indent = \" \" * (pp.col(loc, source) - 1)\n    statedef = []\n    # build list of states and transitions\n    states = set()\n    transitions = set()\n\n    baseStateClass = tokens.name\n\n    fromTo = {}\n    for tn in tokens.transitions:\n        states.add(tn.from_state)\n        states.add(tn.to_state)\n        transitions.add(tn.transition)\n        if tn.from_state in fromTo:\n            fromTo[tn.from_state][tn.transition] = tn.to_state\n        else:\n            fromTo[tn.from_state] = {tn.transition: tn.to_state}\n\n    # add entries for terminal states\n    for s in states:\n        if s not in fromTo:\n            fromTo[s] = {}\n\n    # define state transition class\n    statedef.extend([\n        \"class {baseStateClass}Transition:\".format(baseStateClass=baseStateClass),\n        \"    def __str__(self):\",\n        \"        return self.transitionName\",\n    ])\n    statedef.extend(\n        \"{tn_name} = {baseStateClass}Transition()\".format(tn_name=tn,\n                                                          baseStateClass=baseStateClass)\n        for tn in transitions)\n    statedef.extend(\"{tn_name}.transitionName = '{tn_name}'\".format(tn_name=tn)\n                    for tn in transitions)\n\n    statedef.extend([\n        \"from statemachine import InvalidTransitionException as BaseTransitionException\",\n        \"class InvalidTransitionException(BaseTransitionException): pass\",\n    ])\n\n    # define base class for state classes\n    statedef.extend([\n        \"class %s(object):\" % baseStateClass,\n        \"    def __str__(self):\",\n        \"        return self.__class__.__name__\",\n\n        \"    @classmethod\",\n        \"    def states(cls):\",\n        \"        return list(cls.__subclasses__())\",\n\n        \"    @classmethod\",\n        \"    def next_state(cls, name):\",\n        \"        try:\",\n        \"            return cls.tnmap[name]()\",\n        \"        except KeyError:\",\n        \"            raise InvalidTransitionException('%s does not support transition %r'% (cls.__name__, name))\",\n\n        \"    def __bad_tn(name):\",\n        \"        def _fn(cls):\",\n        \"            raise InvalidTransitionException('%s does not support transition %r'% (cls.__name__, name))\",\n        \"        _fn.__name__ = name\",\n        \"        return _fn\",\n    ])\n\n    # define default 'invalid transition' methods in base class, valid transitions will be implemented in subclasses\n    statedef.extend(\n        \"    {tn_name} = classmethod(__bad_tn({tn_name!r}))\".format(tn_name=tn)\n        for tn in transitions)\n\n    # define all state classes\n    statedef.extend(\"class %s(%s): pass\" % (s, baseStateClass)\n                    for s in states)\n\n    # define state transition methods for valid transitions from each state\n    for s in states:\n        trns = list(fromTo[s].items())\n        # statedef.append(\"%s.tnmap = {%s}\" % (s, \", \".join(\"%s:%s\" % tn for tn in trns)))\n        statedef.extend(\"%s.%s = classmethod(lambda cls: %s())\" % (s, tn_, to_)\n                        for tn_, to_ in trns)\n\n    statedef.extend([\n        \"{baseStateClass}.transitions = classmethod(lambda cls: [{transition_class_list}])\".format(\n            baseStateClass=baseStateClass,\n            transition_class_list = ', '.join(\"cls.{0}\".format(tn) for tn in transitions)\n        ),\n        \"{baseStateClass}.transition_names = [tn.__name__ for tn in {baseStateClass}.transitions()]\".format(\n            baseStateClass=baseStateClass\n        )\n    ])\n\n    # define <state>Mixin class for application classes that delegate to the state\n    statedef.extend([\n        \"class {baseStateClass}Mixin:\".format(baseStateClass=baseStateClass),\n        \"    def __init__(self):\",\n        \"        self._state = None\",\n\n        \"    def initialize_state(self, init_state):\",\n        \"        self._state = init_state\",\n\n        \"    @property\",\n        \"    def state(self):\",\n        \"        return self._state\",\n\n        \"    def __str__(self):\",\n        \"       return '{0}: {1}'.format(self.__class__.__name__, self._state)\"\n    ])\n\n    # define transition methods to be delegated to the _state instance variable\n    statedef.extend(\n        \"    def {tn_name}(self): self._state = self._state.{tn_name}()\".format(tn_name=tn)\n        for tn in transitions\n    )\n    return indent + (\"\\n\" + indent).join(statedef) + \"\\n\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fatal(ftn, txt):\r\n    msg = \"{0}.{1}:FATAL:{2}\\n\".format(modname, ftn, txt)\r\n    raise SystemExit(msg)", "response": "If can t continue."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main(pargs):\r\n    input_file = sys.argv[1]\r\n    fp = ParseFileLineByLine(input_file)\r\n    for i in fp:\r\n        print(i)", "response": "This is only used for testing."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef readline(self):\r\n        line = self.file.readline()\r\n        if self.grammar and line:\r\n            try:\r\n                return self.grammar.parseString(line).asDict()\r\n            except ParseException:\r\n                return self.readline()\r\n        else:\r\n            return line", "response": "Reads and parses a single line."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef readlines(self):\r\n        if self.grammar:\r\n            tot = []\r\n            # Used this way instead of a 'for' loop against\r\n            # self.file.readlines() so that there wasn't two copies of the file\r\n            # in memory.\r\n            while 1:\r\n                line = self.file.readline()\r\n                if not line:\r\n                    break\r\n                tot.append(line)\r\n            return tot\r\n        return self.file.readlines()", "response": "Returns a list of all lines in the file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new dictionary of all the rooms in the given string.", "response": "def createRooms( rm ):\r\n    \"\"\"\r\n    create rooms, using multiline string showing map layout\r\n    string contains symbols for the following:\r\n     A-Z, a-z indicate rooms, and rooms will be stored in a dictionary by\r\n               reference letter\r\n     -, | symbols indicate connection between rooms\r\n     <, >, ^, . symbols indicate one-way connection between rooms\r\n    \"\"\"\r\n    # start with empty dictionary of rooms\r\n    ret = {}\r\n\r\n    # look for room symbols, and initialize dictionary\r\n    # - exit room is always marked 'Z'\r\n    for c in rm:\r\n        if c in string.ascii_letters:\r\n            if c != \"Z\":\r\n                ret[c] = Room(c)\r\n            else:\r\n                ret[c] = Exit()\r\n\r\n    # scan through input string looking for connections between rooms\r\n    rows = rm.split(\"\\n\")\r\n    for row,line in enumerate(rows):\r\n        for col,c in enumerate(line):\r\n            if c in string.ascii_letters:\r\n                room = ret[c]\r\n                n = None\r\n                s = None\r\n                e = None\r\n                w = None\r\n\r\n                # look in neighboring cells for connection symbols (must take\r\n                # care to guard that neighboring cells exist before testing\r\n                # contents)\r\n                if col > 0 and line[col-1] in \"<-\":\r\n                    other = line[col-2]\r\n                    w = ret[other]\r\n                if col < len(line)-1 and line[col+1] in \"->\":\r\n                    other = line[col+2]\r\n                    e = ret[other]\r\n                if row > 1 and col < len(rows[row-1]) and rows[row-1][col] in '|^':\r\n                    other = rows[row-2][col]\r\n                    n = ret[other]\r\n                if row < len(rows)-1 and col < len(rows[row+1]) and rows[row+1][col] in '|.':\r\n                    other = rows[row+2][col]\r\n                    s = ret[other]\r\n\r\n                # set connections to neighboring rooms\r\n                room.doors=[n,s,e,w]\r\n\r\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a parser for the grammar", "response": "def parser(self):\r\n        \"\"\"\r\n        This function returns a parser.\r\n        The grammar should be like most full text search engines (Google, Tsearch, Lucene).\r\n\r\n        Grammar:\r\n        - a query consists of alphanumeric words, with an optional '*' wildcard\r\n          at the end of a word\r\n        - a sequence of words between quotes is a literal string\r\n        - words can be used together by using operators ('and' or 'or')\r\n        - words with operators can be grouped with parenthesis\r\n        - a word or group of words can be preceded by a 'not' operator\r\n        - the 'and' operator precedes an 'or' operator\r\n        - if an operator is missing, use an 'and' operator\r\n        \"\"\"\r\n        operatorOr = Forward()\r\n\r\n        operatorWord = Group(Combine(Word(alphanums) + Suppress('*'))).setResultsName('wordwildcard') | \\\r\n                            Group(Word(alphanums)).setResultsName('word')\r\n\r\n        operatorQuotesContent = Forward()\r\n        operatorQuotesContent << (\r\n            (operatorWord + operatorQuotesContent) | operatorWord\r\n        )\r\n\r\n        operatorQuotes = Group(\r\n            Suppress('\"') + operatorQuotesContent + Suppress('\"')\r\n        ).setResultsName(\"quotes\") | operatorWord\r\n\r\n        operatorParenthesis = Group(\r\n            (Suppress(\"(\") + operatorOr + Suppress(\")\"))\r\n        ).setResultsName(\"parenthesis\") | operatorQuotes\r\n\r\n        operatorNot = Forward()\r\n        operatorNot << (Group(\r\n            Suppress(Keyword(\"not\", caseless=True)) + operatorNot\r\n        ).setResultsName(\"not\") | operatorParenthesis)\r\n\r\n        operatorAnd = Forward()\r\n        operatorAnd << (Group(\r\n            operatorNot + Suppress(Keyword(\"and\", caseless=True)) + operatorAnd\r\n        ).setResultsName(\"and\") | Group(\r\n            operatorNot + OneOrMore(~oneOf(\"and or\") + operatorAnd)\r\n        ).setResultsName(\"and\") | operatorNot)\r\n\r\n        operatorOr << (Group(\r\n            operatorAnd + Suppress(Keyword(\"or\", caseless=True)) + operatorOr\r\n        ).setResultsName(\"or\") | operatorAnd)\r\n\r\n        return operatorOr.parseString"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nevaluate quoted strings First is does an 'and' on the indidual search terms, then it asks the function GetQuoted to only return the subset of ID's that contain the literal string.", "response": "def evaluateQuotes(self, argument):\r\n        \"\"\"Evaluate quoted strings\r\n\r\n        First is does an 'and' on the indidual search terms, then it asks the\r\n        function GetQuoted to only return the subset of ID's that contain the\r\n        literal string.\r\n        \"\"\"\r\n        r = set()\r\n        search_terms = []\r\n        for item in argument:\r\n            search_terms.append(item[0])\r\n            if len(r) == 0:\r\n                r = self.evaluate(item)\r\n            else:\r\n                r = r.intersection(self.evaluate(item))\r\n        return self.GetQuotes(' '.join(search_terms), r)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the name and value of the attribute", "response": "def set_attribute(self, name, value):\r\n        \"\"\"Sets attribute's name and value\"\"\"\r\n        self.attribute_name = name\r\n        self.attribute = value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef display(self):\r\n        #Finding the maximum length for each column\r\n        sym_name = \"Symbol name\"\r\n        sym_len = max(max(len(i.name) for i in self.table),len(sym_name))\r\n        kind_name = \"Kind\"\r\n        kind_len = max(max(len(SharedData.KINDS[i.kind]) for i in self.table),len(kind_name))\r\n        type_name = \"Type\"\r\n        type_len = max(max(len(SharedData.TYPES[i.type]) for i in self.table),len(type_name))\r\n        attr_name = \"Attribute\"\r\n        attr_len = max(max(len(i.attribute_str()) for i in self.table),len(attr_name))\r\n        #print table header\r\n        print(\"{0:3s} | {1:^{2}s} | {3:^{4}s} | {5:^{6}s} | {7:^{8}} | {9:s}\".format(\" No\", sym_name, sym_len, kind_name, kind_len, type_name, type_len, attr_name, attr_len, \"Parameters\"))\r\n        print(\"-----------------------------\" + \"-\" * (sym_len + kind_len + type_len + attr_len))\r\n        #print symbol table\r\n        for i,sym in enumerate(self.table):\r\n            parameters = \"\"\r\n            for p in sym.param_types:\r\n                if parameters == \"\":\r\n                    parameters = \"{0}\".format(SharedData.TYPES[p])\r\n                else:\r\n                    parameters += \", {0}\".format(SharedData.TYPES[p])\r\n            print(\"{0:3d} | {1:^{2}s} | {3:^{4}s} | {5:^{6}s} | {7:^{8}} | ({9})\".format(i, sym.name, sym_len, SharedData.KINDS[sym.kind], kind_len, SharedData.TYPES[sym.type], type_len, sym.attribute_str(), attr_len, parameters))", "response": "Displays the symbol table content"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninserts a new symbol at the end of the symbol table. Returns symbol index", "response": "def insert_symbol(self, sname, skind, stype):\r\n        \"\"\"Inserts new symbol at the end of the symbol table.\r\n           Returns symbol index\r\n           sname - symbol name\r\n           skind - symbol kind\r\n           stype - symbol type\r\n        \"\"\"\r\n        self.table.append(SymbolTableEntry(sname, skind, stype))\r\n        self.table_len = len(self.table)\r\n        return self.table_len-1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclear all symbols beginning with the given index from the table.", "response": "def clear_symbols(self, index):\r\n        \"\"\"Clears all symbols begining with the index to the end of table\"\"\"\r\n        try:\r\n            del self.table[index:]\r\n        except Exception:\r\n            self.error()\r\n        self.table_len = len(self.table)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsearch for a symbol in the table from the end to the beginning. Returns symbol index or None if no such symbol exists.", "response": "def lookup_symbol(self, sname, skind=list(SharedData.KINDS.keys()), stype=list(SharedData.TYPES.keys())):\r\n        \"\"\"Searches for symbol, from the end to the begining.\r\n           Returns symbol index or None\r\n           sname - symbol name\r\n           skind - symbol kind (one kind, list of kinds, or None) deafult: any kind\r\n           stype - symbol type (or None) default: any type\r\n        \"\"\"\r\n        skind = skind if isinstance(skind, list) else [skind]\r\n        stype = stype if isinstance(stype, list) else [stype]\r\n        for i, sym in [[x, self.table[x]] for x in range(len(self.table) - 1, SharedData.LAST_WORKING_REGISTER, -1)]:\r\n            if (sym.name == sname) and (sym.kind in skind) and (sym.type in stype):\r\n                return i\r\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef insert_id(self, sname, skind, skinds, stype):\r\n        index = self.lookup_symbol(sname, skinds)\r\n        if index == None:\r\n            index = self.insert_symbol(sname, skind, stype)\r\n            return index\r\n        else:\r\n            raise SemanticException(\"Redefinition of '%s'\" % sname)", "response": "Inserts a new identifier at the end of the symbol table if possible. Returns symbol index or raises an exception if the symbol already exists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninsert a new global variable", "response": "def insert_global_var(self, vname, vtype):\r\n        \"Inserts a new global variable\"\r\n        return self.insert_id(vname, SharedData.KINDS.GLOBAL_VAR, [SharedData.KINDS.GLOBAL_VAR, SharedData.KINDS.FUNCTION], vtype)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninsert a new local variable", "response": "def insert_local_var(self, vname, vtype, position):\r\n        \"Inserts a new local variable\"\r\n        index = self.insert_id(vname, SharedData.KINDS.LOCAL_VAR, [SharedData.KINDS.LOCAL_VAR, SharedData.KINDS.PARAMETER], vtype)\r\n        self.table[index].attribute = position"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninserting a new parameter", "response": "def insert_parameter(self, pname, ptype):\r\n        \"Inserts a new parameter\"\r\n        index = self.insert_id(pname, SharedData.KINDS.PARAMETER, SharedData.KINDS.PARAMETER, ptype)\r\n        #set parameter's attribute to it's ordinal number\r\n        self.table[index].set_attribute(\"Index\", self.shared.function_params)\r\n        #set parameter's type in param_types list of a function\r\n        self.table[self.shared.function_index].param_types.append(ptype)\r\n        return index"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef insert_function(self, fname, ftype):\r\n        \"Inserts a new function\"\r\n        index = self.insert_id(fname, SharedData.KINDS.FUNCTION, [SharedData.KINDS.GLOBAL_VAR, SharedData.KINDS.FUNCTION], ftype)\r\n        self.table[index].set_attribute(\"Params\",0)\r\n        return index", "response": "Inserts a new function"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef insert_constant(self, cname, ctype):\r\n        index = self.lookup_symbol(cname, stype=ctype)\r\n        if index == None:\r\n            num = int(cname)\r\n            if ctype == SharedData.TYPES.INT:\r\n                if (num < SharedData.MIN_INT) or (num > SharedData.MAX_INT):\r\n                    raise SemanticException(\"Integer constant '%s' out of range\" % cname)\r\n            elif ctype == SharedData.TYPES.UNSIGNED:\r\n                if (num < 0) or (num > SharedData.MAX_UNSIGNED):\r\n                    raise SemanticException(\"Unsigned constant '%s' out of range\" % cname)\r\n            index = self.insert_symbol(cname, SharedData.KINDS.CONSTANT, ctype)\r\n        return index", "response": "Inserts a constant into the table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef same_types(self, index1, index2):\r\n        try:\r\n            same = self.table[index1].type == self.table[index2].type != SharedData.TYPES.NO_TYPE\r\n        except Exception:\r\n            self.error()\r\n        return same", "response": "Returns True if both symbol table elements are of the same type"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if index and function s argument are of the same type as the argument number.", "response": "def same_type_as_argument(self, index, function_index, argument_number):\r\n        \"\"\"Returns True if index and function's argument are of the same type\r\n           index - index in symbol table\r\n           function_index - function's index in symbol table\r\n           argument_number - # of function's argument\r\n        \"\"\"\r\n        try:\r\n            same = self.table[function_index].param_types[argument_number] == self.table[index].type\r\n        except Exception:\r\n            self.error()\r\n        return same"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef take_register(self, rtype = SharedData.TYPES.NO_TYPE):\r\n        if len(self.free_registers) == 0:\r\n            self.error(\"no more free registers\")\r\n        reg = self.free_registers.pop()\r\n        self.used_registers.append(reg)\r\n        self.symtab.set_type(reg, rtype)\r\n        return reg", "response": "Reserves one working register and sets its type"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef take_function_register(self, rtype = SharedData.TYPES.NO_TYPE):\r\n        reg = SharedData.FUNCTION_REGISTER\r\n        if reg not in self.free_registers:\r\n            self.error(\"function register already taken\")\r\n        self.free_registers.remove(reg)\r\n        self.used_registers.append(reg)\r\n        self.symtab.set_type(reg, rtype)\r\n        return reg", "response": "Reserves register for function return value and sets its type"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreleases a working register", "response": "def free_register(self, reg):\r\n        \"\"\"Releases working register\"\"\"\r\n        if reg not in self.used_registers:\r\n            self.error(\"register %s is not taken\" % self.REGISTERS[reg])\r\n        self.used_registers.remove(reg)\r\n        self.free_registers.append(reg)\r\n        self.free_registers.sort(reverse = True)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef free_if_register(self, index):\r\n        if (index < 0) or (index > SharedData.FUNCTION_REGISTER):\r\n            return\r\n        else:\r\n            self.free_register(index)", "response": "Free the specified entry if it is a working register."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef label(self, name, internal=False, definition=False):\r\n        return \"{0}{1}{2}\".format(self.internal if internal else \"\", name, self.definition if definition else \"\")", "response": "Generates a label name for the current object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef symbol(self, index):\r\n        #if index is actually a string, just return it\r\n        if isinstance(index, str):\r\n            return index\r\n        elif (index < 0) or (index >= self.symtab.table_len):\r\n            self.error(\"symbol table index out of range\")\r\n        sym = self.symtab.table[index]\r\n        #local variables are located at negative offset from frame pointer register\r\n        if sym.kind == SharedData.KINDS.LOCAL_VAR:\r\n            return \"-{0}(1:%14)\".format(sym.attribute * 4 + 4)\r\n        #parameters are located at positive offset from frame pointer register\r\n        elif sym.kind == SharedData.KINDS.PARAMETER:\r\n            return \"{0}(1:%14)\".format(8 + sym.attribute * 4)\r\n        elif sym.kind == SharedData.KINDS.CONSTANT:\r\n            return \"${0}\".format(sym.name)\r\n        else:\r\n            return \"{0}\".format(sym.name)", "response": "Generates a symbol name from the given index"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npushes all used working registers before function call", "response": "def save_used_registers(self):\r\n        \"\"\"Pushes all used working registers before function call\"\"\"\r\n        used = self.used_registers[:]\r\n        del self.used_registers[:]\r\n        self.used_registers_stack.append(used[:])\r\n        used.sort()\r\n        for reg in used:\r\n            self.newline_text(\"PUSH\\t%s\" % SharedData.REGISTERS[reg], True)\r\n        self.free_registers.extend(used)\r\n        self.free_registers.sort(reverse = True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef restore_used_registers(self):\r\n        used = self.used_registers_stack.pop()\r\n        self.used_registers = used[:]\r\n        used.sort(reverse = True)\r\n        for reg in used:\r\n            self.newline_text(\"POP \\t%s\" % SharedData.REGISTERS[reg], True)\r\n            self.free_registers.remove(reg)", "response": "Pops all used working registers after function call"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninserting a newline and text optionally with indentation", "response": "def newline_text(self, text, indent = False):\r\n        \"\"\"Inserts a newline and text, optionally with indentation (helper function)\"\"\"\r\n        self.newline(indent)\r\n        self.text(text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninserts a newline and a label", "response": "def newline_label(self, name, internal=False, definition=False):\r\n        \"\"\"Inserts a newline and a label (helper function)\r\n           name - label name\r\n           internal - boolean value, adds \"@\" prefix to label\r\n           definition - boolean value, adds \":\" suffix to label\r\n        \"\"\"\r\n        self.newline_text(self.label(\"{0}{1}{2}\".format(\"@\" if internal else \"\", name, \":\" if definition else \"\")))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef global_var(self, name):\r\n        self.newline_label(name, False, True)\r\n        self.newline_text(\"WORD\\t1\", True)", "response": "Inserts a new static ( global ) variable definition"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef arithmetic_mnemonic(self, op_name, op_type):\r\n        return self.OPERATIONS[op_name] + self.OPSIGNS[op_type]", "response": "Generates an arithmetic instruction mnemonic"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef arithmetic(self, operation, operand1, operand2, operand3 = None):\r\n        if isinstance(operand1, int):\r\n            output_type = self.symtab.get_type(operand1)\r\n            self.free_if_register(operand1)\r\n        else:\r\n            output_type = None\r\n        if isinstance(operand2, int):\r\n            output_type = self.symtab.get_type(operand2) if output_type == None else output_type\r\n            self.free_if_register(operand2)\r\n        else:\r\n            output_type = SharedData.TYPES.NO_TYPE if output_type == None else output_type\r\n        #if operand3 is not defined, reserve one free register for it\r\n        output = self.take_register(output_type) if operand3 == None else operand3\r\n        mnemonic = self.arithmetic_mnemonic(operation, output_type)\r\n        self.newline_text(\"{0}\\t{1},{2},{3}\".format(mnemonic, self.symbol(operand1), self.symbol(operand2), self.symbol(output)), True)\r\n        return output", "response": "Generates an arithmetic instruction"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef relop_code(self, relop, operands_type):\r\n        code = self.RELATIONAL_DICT[relop]\r\n        offset = 0 if operands_type == SharedData.TYPES.INT else len(SharedData.RELATIONAL_OPERATORS)\r\n        return code + offset", "response": "Returns code for relational operator\r\n          "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef jump(self, relcode, opposite, label):\r\n        jump = self.OPPOSITE_JUMPS[relcode] if opposite else self.CONDITIONAL_JUMPS[relcode]\r\n        self.newline_text(\"{0}\\t{1}\".format(jump, label), True)", "response": "Generates a jump instruction\r\n           relcode opposite label"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a move instruction", "response": "def move(self,operand1, operand2):\r\n        \"\"\"Generates a move instruction\r\n           If the output operand (opernad2) is a working register, sets it's type\r\n           operandX - index in symbol table or text representation of operand\r\n        \"\"\"\r\n        if isinstance(operand1, int):\r\n            output_type = self.symtab.get_type(operand1)\r\n            self.free_if_register(operand1)\r\n        else:\r\n            output_type = SharedData.TYPES.NO_TYPE\r\n        self.newline_text(\"MOV \\t{0},{1}\".format(self.symbol(operand1), self.symbol(operand2)), True)\r\n        if isinstance(operand2, int):\r\n            if self.symtab.get_kind(operand2) == SharedData.KINDS.WORKING_REGISTER:\r\n                self.symtab.set_type(operand2, output_type)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef function_begin(self):\r\n        self.newline_label(self.shared.function_name, False, True)\r\n        self.push(\"%14\")\r\n        self.move(\"%15\", \"%14\")", "response": "Inserts function name label and function frame initialization"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninserts a local variable initialization and body label", "response": "def function_body(self):\r\n        \"\"\"Inserts a local variable initialization and body label\"\"\"\r\n        if self.shared.function_vars > 0:\r\n            const = self.symtab.insert_constant(\"0{}\".format(self.shared.function_vars * 4), SharedData.TYPES.UNSIGNED)\r\n            self.arithmetic(\"-\", \"%15\", const, \"%15\")\r\n        self.newline_label(self.shared.function_name + \"_body\", True, True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninsert an exit label and function return instructions", "response": "def function_end(self):\r\n        \"\"\"Inserts an exit label and function return instructions\"\"\"\r\n        self.newline_label(self.shared.function_name + \"_exit\", True, True)\r\n        self.move(\"%14\", \"%15\")\r\n        self.pop(\"%14\")\r\n        self.newline_text(\"RET\", True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating code for a function call", "response": "def function_call(self, function, arguments):\r\n        \"\"\"Generates code for a function call\r\n           function - function index in symbol table\r\n           arguments - list of arguments (indexes in symbol table)\r\n        \"\"\"\r\n        #push each argument to stack\r\n        for arg in arguments:\r\n            self.push(self.symbol(arg))\r\n            self.free_if_register(arg)\r\n        self.newline_text(\"CALL\\t\"+self.symtab.get_name(function), True)\r\n        args = self.symtab.get_attribute(function)\r\n        #generates stack cleanup if function has arguments\r\n        if args > 0:\r\n            args_space = self.symtab.insert_constant(\"{0}\".format(args * 4), SharedData.TYPES.UNSIGNED)\r\n            self.arithmetic(\"+\", \"%15\", args_space, \"%15\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndisplay a warning message. Uses exshared for current location of parsing", "response": "def warning(self, message, print_location=True):\r\n        \"\"\"Displays warning message. Uses exshared for current location of parsing\"\"\"\r\n        msg = \"Warning\"\r\n        if print_location and (exshared.location != None):\r\n            wline = lineno(exshared.location, exshared.text)\r\n            wcol = col(exshared.location, exshared.text)\r\n            wtext = line(exshared.location, exshared.text)\r\n            msg += \" at line %d, col %d\" % (wline, wcol)\r\n        msg += \": %s\" % message\r\n        if print_location and (exshared.location != None):\r\n            msg += \"\\n%s\" % wtext\r\n        print(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef global_variable_action(self, text, loc, var):\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"GLOBAL_VAR:\",var)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        index = self.symtab.insert_global_var(var.name, var.type)\r\n        self.codegen.global_var(var.name)\r\n        return index", "response": "Code executed after recognising a global variable"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncode executed after recognising a local variable", "response": "def local_variable_action(self, text, loc, var):\r\n        \"\"\"Code executed after recognising a local variable\"\"\"\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"LOCAL_VAR:\",var, var.name, var.type)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        index = self.symtab.insert_local_var(var.name, var.type, self.shared.function_vars)\r\n        self.shared.function_vars += 1\r\n        return index"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parameter_action(self, text, loc, par):\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"PARAM:\",par)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        index = self.symtab.insert_parameter(par.name, par.type)\r\n        self.shared.function_params += 1\r\n        return index", "response": "Code executed after recognising a parameter"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncoding executed after recognising a constant", "response": "def constant_action(self, text, loc, const):\r\n        \"\"\"Code executed after recognising a constant\"\"\"\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"CONST:\",const)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        return self.symtab.insert_constant(const[0], const[1])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef function_begin_action(self, text, loc, fun):\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"FUN_BEGIN:\",fun)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        self.shared.function_index = self.symtab.insert_function(fun.name, fun.type)\r\n        self.shared.function_name = fun.name\r\n        self.shared.function_params = 0\r\n        self.shared.function_vars = 0\r\n        self.codegen.function_begin();", "response": "Code executed after recognising a function definition"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncoding executed after recognising the beginning of function s body", "response": "def function_body_action(self, text, loc, fun):\r\n        \"\"\"Code executed after recognising the beginning of function's body\"\"\"\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"FUN_BODY:\",fun)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        self.codegen.function_body()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncode executed at the end of function definition", "response": "def function_end_action(self, text, loc, fun):\r\n        \"\"\"Code executed at the end of function definition\"\"\"\r\n        if DEBUG > 0:\r\n            print(\"FUN_END:\",fun)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        #set function's attribute to number of function parameters\r\n        self.symtab.set_attribute(self.shared.function_index, self.shared.function_params)\r\n        #clear local function symbols (but leave function name)\r\n        self.symtab.clear_symbols(self.shared.function_index + 1)\r\n        self.codegen.function_end()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef return_action(self, text, loc, ret):\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"RETURN:\",ret)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        if not self.symtab.same_types(self.shared.function_index, ret.exp[0]):\r\n            raise SemanticException(\"Incompatible type in return\")\r\n        #set register for function's return value to expression value\r\n        reg = self.codegen.take_function_register()\r\n        self.codegen.move(ret.exp[0], reg)\r\n        #after return statement, register for function's return value is available again\r\n        self.codegen.free_register(reg)\r\n        #jump to function's exit\r\n        self.codegen.unconditional_jump(self.codegen.label(self.shared.function_name+\"_exit\", True))", "response": "Code executed after recognising a return statement"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lookup_id_action(self, text, loc, var):\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"EXP_VAR:\",var)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        var_index = self.symtab.lookup_symbol(var.name, [SharedData.KINDS.GLOBAL_VAR, SharedData.KINDS.PARAMETER, SharedData.KINDS.LOCAL_VAR])\r\n        if var_index == None:\r\n            raise SemanticException(\"'%s' undefined\" % var.name)\r\n        return var_index", "response": "Code executed after recognising an identificator in expression"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncode executed after recognising an assignment statement", "response": "def assignment_action(self, text, loc, assign):\r\n        \"\"\"Code executed after recognising an assignment statement\"\"\"\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"ASSIGN:\",assign)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        var_index = self.symtab.lookup_symbol(assign.var, [SharedData.KINDS.GLOBAL_VAR, SharedData.KINDS.PARAMETER, SharedData.KINDS.LOCAL_VAR])\r\n        if var_index == None:\r\n            raise SemanticException(\"Undefined lvalue '%s' in assignment\" % assign.var)\r\n        if not self.symtab.same_types(var_index, assign.exp[0]):\r\n            raise SemanticException(\"Incompatible types in assignment\")\r\n        self.codegen.move(assign.exp[0], var_index)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mulexp_action(self, text, loc, mul):\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"MUL_EXP:\",mul)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        #iterate through all multiplications/divisions\r\n        m = list(mul)\r\n        while len(m) > 1:\r\n            if not self.symtab.same_types(m[0], m[2]):\r\n                raise SemanticException(\"Invalid opernads to binary '%s'\" % m[1])\r\n            reg = self.codegen.arithmetic(m[1], m[0], m[2])\r\n            #replace first calculation with it's result\r\n            m[0:3] = [reg]\r\n        return m[0]", "response": "Code executed after recognising a mulexp expression"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncoding executed after recognising a numexp expression", "response": "def numexp_action(self, text, loc, num):\r\n        \"\"\"Code executed after recognising a numexp expression (something +|- something)\"\"\"\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"NUM_EXP:\",num)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        #iterate through all additions/substractions\r\n        n = list(num)\r\n        while len(n) > 1:\r\n            if not self.symtab.same_types(n[0], n[2]):\r\n                raise SemanticException(\"Invalid opernads to binary '%s'\" % n[1])\r\n            reg = self.codegen.arithmetic(n[1], n[0], n[2])\r\n            #replace first calculation with it's result\r\n            n[0:3] = [reg]\r\n        return n[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef function_call_prepare_action(self, text, loc, fun):\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"FUN_PREP:\",fun)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        index = self.symtab.lookup_symbol(fun.name, SharedData.KINDS.FUNCTION)\r\n        if index == None:\r\n            raise SemanticException(\"'%s' is not a function\" % fun.name)\r\n        #save any previous function call data (for nested function calls)\r\n        self.function_call_stack.append(self.function_call_index)\r\n        self.function_call_index = index\r\n        self.function_arguments_stack.append(self.function_arguments[:])\r\n        del self.function_arguments[:]\r\n        self.codegen.save_used_registers()", "response": "Code executed after recognising a function call."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef argument_action(self, text, loc, arg):\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"ARGUMENT:\",arg.exp)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        arg_ordinal = len(self.function_arguments)\r\n        #check argument's type\r\n        if not self.symtab.same_type_as_argument(arg.exp, self.function_call_index, arg_ordinal):\r\n            raise SemanticException(\"Incompatible type for argument %d in '%s'\" % (arg_ordinal + 1, self.symtab.get_name(self.function_call_index)))\r\n        self.function_arguments.append(arg.exp)", "response": "Code executed after recognising each of the function s arguments"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncoding executed after recognising the whole function call", "response": "def function_call_action(self, text, loc, fun):\r\n        \"\"\"Code executed after recognising the whole function call\"\"\"\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"FUN_CALL:\",fun)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        #check number of arguments\r\n        if len(self.function_arguments) != self.symtab.get_attribute(self.function_call_index):\r\n            raise SemanticException(\"Wrong number of arguments for function '%s'\" % fun.name)\r\n        #arguments should be pushed to stack in reverse order\r\n        self.function_arguments.reverse()\r\n        self.codegen.function_call(self.function_call_index, self.function_arguments)\r\n        self.codegen.restore_used_registers()\r\n        return_type = self.symtab.get_type(self.function_call_index)\r\n        #restore previous function call data\r\n        self.function_call_index = self.function_call_stack.pop()\r\n        self.function_arguments = self.function_arguments_stack.pop()\r\n        register = self.codegen.take_register(return_type)\r\n        #move result to a new free register, to allow the next function call\r\n        self.codegen.move(self.codegen.take_function_register(return_type), register)\r\n        return register"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef relexp_action(self, text, loc, arg):\r\n        if DEBUG > 0:\r\n            print(\"REL_EXP:\",arg)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        exshared.setpos(loc, text)\r\n        if not self.symtab.same_types(arg[0], arg[2]):\r\n            raise SemanticException(\"Invalid operands for operator '{0}'\".format(arg[1]))\r\n        self.codegen.compare(arg[0], arg[2])\r\n        #return relational operator's code\r\n        self.relexp_code = self.codegen.relop_code(arg[1], self.symtab.get_type(arg[0]))\r\n        return self.relexp_code", "response": "Code executed after recognising a relational expression"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef andexp_action(self, text, loc, arg):\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"AND+EXP:\",arg)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        label = self.codegen.label(\"false{0}\".format(self.false_label_number), True, False)\r\n        self.codegen.jump(self.relexp_code, True, label)\r\n        self.andexp_code = self.relexp_code\r\n        return self.andexp_code", "response": "Code executed after recognising an andexp expression"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncoding executed after recognising an if statement", "response": "def if_begin_action(self, text, loc, arg):\r\n        \"\"\"Code executed after recognising an if statement (if keyword)\"\"\"\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"IF_BEGIN:\",arg)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        self.false_label_number += 1\r\n        self.label_number = self.false_label_number\r\n        self.codegen.newline_label(\"if{0}\".format(self.label_number), True, True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef if_body_action(self, text, loc, arg):\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"IF_BODY:\",arg)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        #generate conditional jump (based on last compare)\r\n        label = self.codegen.label(\"false{0}\".format(self.false_label_number), True, False)\r\n        self.codegen.jump(self.relexp_code, True, label)\r\n        #generate 'true' label (executes if condition is satisfied)\r\n        self.codegen.newline_label(\"true{0}\".format(self.label_number), True, True)\r\n        #save label numbers (needed for nested if/while statements)\r\n        self.label_stack.append(self.false_label_number)\r\n        self.label_stack.append(self.label_number)", "response": "Code executed after recognising if statement s body"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef if_else_action(self, text, loc, arg):\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"IF_ELSE:\",arg)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        #jump to exit after all statements for true condition are executed\r\n        self.label_number = self.label_stack.pop()\r\n        label = self.codegen.label(\"exit{0}\".format(self.label_number), True, False)\r\n        self.codegen.unconditional_jump(label)\r\n        #generate final 'false' label (executes if condition isn't satisfied)\r\n        self.codegen.newline_label(\"false{0}\".format(self.label_stack.pop()), True, True)\r\n        self.label_stack.append(self.label_number)", "response": "Code executed after recognising if statement s else body"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncodes executed after recognising a whole if statement", "response": "def if_end_action(self, text, loc, arg):\r\n        \"\"\"Code executed after recognising a whole if statement\"\"\"\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"IF_END:\",arg)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        self.codegen.newline_label(\"exit{0}\".format(self.label_stack.pop()), True, True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if there is a main function and the type of main function", "response": "def program_end_action(self, text, loc, arg):\r\n        \"\"\"Checks if there is a 'main' function and the type of 'main' function\"\"\"\r\n        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"PROGRAM_END:\",arg)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        index = self.symtab.lookup_symbol(\"main\",SharedData.KINDS.FUNCTION)\r\n        if index == None:\r\n            raise SemanticException(\"Undefined reference to 'main'\", False)\r\n        elif self.symtab.get_type(index) != SharedData.TYPES.INT:\r\n            self.warning(\"Return type of 'main' is not int\", False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_text(self,text):\r\n        try:\r\n            return self.rProgram.ignore(cStyleComment).parseString(text, parseAll=True)\r\n        except SemanticException as err:\r\n            print(err)\r\n            exit(3)\r\n        except ParseException as err:\r\n            print(err)\r\n            exit(3)", "response": "Parse the text of the tag."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a file and return a dict of the attributes.", "response": "def parse_file(self,filename):\r\n        \"\"\"Parse file (helper function)\"\"\"\r\n        try:\r\n            return self.rProgram.ignore(cStyleComment).parseFile(filename, parseAll=True)\r\n        except SemanticException as err:\r\n            print(err)\r\n            exit(3)\r\n        except ParseException as err:\r\n            print(err)\r\n            exit(3)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef BNF():\r\n    global bnf\r\n    if not bnf:\r\n        point = Literal( \".\" )\r\n        # use CaselessKeyword for e and pi, to avoid accidentally matching\r\n        # functions that start with 'e' or 'pi' (such as 'exp'); Keyword\r\n        # and CaselessKeyword only match whole words\r\n        e     = CaselessKeyword( \"E\" )\r\n        pi    = CaselessKeyword( \"PI\" )\r\n        #~ fnumber = Combine( Word( \"+-\"+nums, nums ) +\r\n                           #~ Optional( point + Optional( Word( nums ) ) ) +\r\n                           #~ Optional( e + Word( \"+-\"+nums, nums ) ) )\r\n        fnumber = Regex(r\"[+-]?\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?\")\r\n        ident = Word(alphas, alphanums+\"_$\")\r\n\r\n        plus, minus, mult, div = map(Literal, \"+-*/\")\r\n        lpar, rpar = map(Suppress, \"()\")\r\n        addop  = plus | minus\r\n        multop = mult | div\r\n        expop = Literal( \"^\" )\r\n\r\n        expr = Forward()\r\n        atom = ((0,None)*minus + ( pi | e | fnumber | ident + lpar + expr + rpar | ident ).setParseAction( pushFirst ) |\r\n                Group( lpar + expr + rpar )).setParseAction(pushUMinus)\r\n\r\n        # by defining exponentiation as \"atom [ ^ factor ]...\" instead of \"atom [ ^ atom ]...\", we get right-to-left exponents, instead of left-to-righ\r\n        # that is, 2^3^2 = 2^(3^2), not (2^3)^2.\r\n        factor = Forward()\r\n        factor << atom + ZeroOrMore( ( expop + factor ).setParseAction( pushFirst ) )\r\n\r\n        term = factor + ZeroOrMore( ( multop + factor ).setParseAction( pushFirst ) )\r\n        expr << term + ZeroOrMore( ( addop + term ).setParseAction( pushFirst ) )\r\n        bnf = expr\r\n    return bnf", "response": "Returns a new grammar for the most common words in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nencodes a unicode value into a cookie and sign it with the app s secret key.", "response": "def encode_cookie(payload, key=None):\n    '''\n    This will encode a ``unicode`` value into a cookie, and sign that cookie\n    with the app's secret key.\n\n    :param payload: The value to encode, as `unicode`.\n    :type payload: unicode\n\n    :param key: The key to use when creating the cookie digest. If not\n                specified, the SECRET_KEY value from app config will be used.\n    :type key: str\n    '''\n    return u'{0}|{1}'.format(payload, _cookie_digest(payload, key=key))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode_cookie(cookie, key=None):\n    '''\n    This decodes a cookie given by `encode_cookie`. If verification of the\n    cookie fails, ``None`` will be implicitly returned.\n\n    :param cookie: An encoded cookie.\n    :type cookie: str\n\n    :param key: The key to use when creating the cookie digest. If not\n                specified, the SECRET_KEY value from app config will be used.\n    :type key: str\n    '''\n    try:\n        payload, digest = cookie.rsplit(u'|', 1)\n        if hasattr(digest, 'decode'):\n            digest = digest.decode('ascii')  # pragma: no cover\n    except ValueError:\n        return\n\n    if safe_str_cmp(_cookie_digest(payload, key=key), digest):\n        return payload", "response": "This function decodes a cookie given by encode_cookie."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreducing the scheme and host from a given login URL so it can be passed to the given login URL more efficiently.", "response": "def make_next_param(login_url, current_url):\n    '''\n    Reduces the scheme and host from a given URL so it can be passed to\n    the given `login` URL more efficiently.\n\n    :param login_url: The login URL being redirected to.\n    :type login_url: str\n    :param current_url: The URL to reduce.\n    :type current_url: str\n    '''\n    l = urlparse(login_url)\n    c = urlparse(current_url)\n\n    if (not l.scheme or l.scheme == c.scheme) and \\\n            (not l.netloc or l.netloc == c.netloc):\n        return urlunparse(('', '', c.path, c.params, c.query, ''))\n    return current_url"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a URL for redirecting to a login page.", "response": "def login_url(login_view, next_url=None, next_field='next'):\n    '''\n    Creates a URL for redirecting to a login page. If only `login_view` is\n    provided, this will just return the URL for it. If `next_url` is provided,\n    however, this will append a ``next=URL`` parameter to the query string\n    so that the login view can redirect back to that URL. Flask-Login's default\n    unauthorized handler uses this function when redirecting to your login url.\n    To force the host name used, set `FORCE_HOST_FOR_REDIRECTS` to a host. This\n    prevents from redirecting to external sites if request headers Host or\n    X-Forwarded-For are present.\n\n    :param login_view: The name of the login view. (Alternately, the actual\n                       URL to the login view.)\n    :type login_view: str\n    :param next_url: The URL to give the login view for redirection.\n    :type next_url: str\n    :param next_field: What field to store the next URL in. (It defaults to\n                       ``next``.)\n    :type next_field: str\n    '''\n    base = expand_login_view(login_view)\n\n    if next_url is None:\n        return base\n\n    parsed_result = urlparse(base)\n    md = url_decode(parsed_result.query)\n    md[next_field] = make_next_param(base, next_url)\n    netloc = current_app.config.get('FORCE_HOST_FOR_REDIRECTS') or \\\n        parsed_result.netloc\n    parsed_result = parsed_result._replace(netloc=netloc,\n                                           query=url_encode(md, sort=True))\n    return urlunparse(parsed_result)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlogging a user in the base language.", "response": "def login_user(user, remember=False, duration=None, force=False, fresh=True):\n    '''\n    Logs a user in. You should pass the actual user object to this. If the\n    user's `is_active` property is ``False``, they will not be logged in\n    unless `force` is ``True``.\n\n    This will return ``True`` if the log in attempt succeeds, and ``False`` if\n    it fails (i.e. because the user is inactive).\n\n    :param user: The user object to log in.\n    :type user: object\n    :param remember: Whether to remember the user after their session expires.\n        Defaults to ``False``.\n    :type remember: bool\n    :param duration: The amount of time before the remember cookie expires. If\n        ``None`` the value set in the settings is used. Defaults to ``None``.\n    :type duration: :class:`datetime.timedelta`\n    :param force: If the user is inactive, setting this to ``True`` will log\n        them in regardless. Defaults to ``False``.\n    :type force: bool\n    :param fresh: setting this to ``False`` will log in the user with a session\n        marked as not \"fresh\". Defaults to ``True``.\n    :type fresh: bool\n    '''\n    if not force and not user.is_active:\n        return False\n\n    user_id = getattr(user, current_app.login_manager.id_attribute)()\n    session['user_id'] = user_id\n    session['_fresh'] = fresh\n    session['_id'] = current_app.login_manager._session_identifier_generator()\n\n    if remember:\n        session['remember'] = 'set'\n        if duration is not None:\n            try:\n                # equal to timedelta.total_seconds() but works with Python 2.6\n                session['remember_seconds'] = (duration.microseconds +\n                                               (duration.seconds +\n                                                duration.days * 24 * 3600) *\n                                               10**6) / 10.0**6\n            except AttributeError:\n                raise Exception('duration must be a datetime.timedelta, '\n                                'instead got: {0}'.format(duration))\n\n    current_app.login_manager._update_request_context_with_user(user)\n    user_logged_in.send(current_app._get_current_object(), user=_get_user())\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlog a user out.", "response": "def logout_user():\n    '''\n    Logs a user out. (You do not need to pass the actual user.) This will\n    also clean up the remember me cookie if it exists.\n    '''\n\n    user = _get_user()\n\n    if 'user_id' in session:\n        session.pop('user_id')\n\n    if '_fresh' in session:\n        session.pop('_fresh')\n\n    if '_id' in session:\n        session.pop('_id')\n\n    cookie_name = current_app.config.get('REMEMBER_COOKIE_NAME', COOKIE_NAME)\n    if cookie_name in request.cookies:\n        session['remember'] = 'clear'\n        if 'remember_seconds' in session:\n            session.pop('remember_seconds')\n\n    user_logged_out.send(current_app._get_current_object(), user=user)\n\n    current_app.login_manager._update_request_context_with_user()\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fresh_login_required(func):\n    '''\n    If you decorate a view with this, it will ensure that the current user's\n    login is fresh - i.e. their session was not restored from a 'remember me'\n    cookie. Sensitive operations, like changing a password or e-mail, should\n    be protected with this, to impede the efforts of cookie thieves.\n\n    If the user is not authenticated, :meth:`LoginManager.unauthorized` is\n    called as normal. If they are authenticated, but their session is not\n    fresh, it will call :meth:`LoginManager.needs_refresh` instead. (In that\n    case, you will need to provide a :attr:`LoginManager.refresh_view`.)\n\n    Behaves identically to the :func:`login_required` decorator with respect\n    to configutation variables.\n\n    .. Note ::\n\n        Per `W3 guidelines for CORS preflight requests\n        <http://www.w3.org/TR/cors/#cross-origin-request-with-preflight-0>`_,\n        HTTP ``OPTIONS`` requests are exempt from login checks.\n\n    :param func: The view function to decorate.\n    :type func: function\n    '''\n    @wraps(func)\n    def decorated_view(*args, **kwargs):\n        if request.method in EXEMPT_METHODS:\n            return func(*args, **kwargs)\n        elif current_app.config.get('LOGIN_DISABLED'):\n            return func(*args, **kwargs)\n        elif not current_user.is_authenticated:\n            return current_app.login_manager.unauthorized()\n        elif not login_fresh():\n            return current_app.login_manager.needs_refresh()\n        return func(*args, **kwargs)\n    return decorated_view", "response": "Decorator that ensures that the current user s login is fresh - i. e. their session was not restored from a remember me cookie."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the login view for the app or blueprint.", "response": "def set_login_view(login_view, blueprint=None):\n    '''\n    Sets the login view for the app or blueprint. If a blueprint is passed,\n    the login view is set for this blueprint on ``blueprint_login_views``.\n\n    :param login_view: The user object to log in.\n    :type login_view: str\n    :param blueprint: The blueprint which this login view should be set on.\n        Defaults to ``None``.\n    :type blueprint: object\n    '''\n\n    num_login_views = len(current_app.login_manager.blueprint_login_views)\n    if blueprint is not None or num_login_views != 0:\n\n        (current_app.login_manager\n            .blueprint_login_views[blueprint.name]) = login_view\n\n        if (current_app.login_manager.login_view is not None and\n                None not in current_app.login_manager.blueprint_login_views):\n\n            (current_app.login_manager\n                .blueprint_login_views[None]) = (current_app.login_manager\n                                                 .login_view)\n\n        current_app.login_manager.login_view = None\n    else:\n        current_app.login_manager.login_view = login_view"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize an application. This registers an after_request call and attaches this LoginManager to it as app. login_manager.", "response": "def init_app(self, app, add_context_processor=True):\n        '''\n        Configures an application. This registers an `after_request` call, and\n        attaches this `LoginManager` to it as `app.login_manager`.\n\n        :param app: The :class:`flask.Flask` object to configure.\n        :type app: :class:`flask.Flask`\n        :param add_context_processor: Whether to add a context processor to\n            the app that adds a `current_user` variable to the template.\n            Defaults to ``True``.\n        :type add_context_processor: bool\n        '''\n        app.login_manager = self\n        app.after_request(self._update_remember_cookie)\n\n        if add_context_processor:\n            app.context_processor(_user_context_processor)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unauthorized(self):\n        '''\n        This is called when the user is required to log in. If you register a\n        callback with :meth:`LoginManager.unauthorized_handler`, then it will\n        be called. Otherwise, it will take the following actions:\n\n            - Flash :attr:`LoginManager.login_message` to the user.\n\n            - If the app is using blueprints find the login view for\n              the current blueprint using `blueprint_login_views`. If the app\n              is not using blueprints or the login view for the current\n              blueprint is not specified use the value of `login_view`.\n\n            - Redirect the user to the login view. (The page they were\n              attempting to access will be passed in the ``next`` query\n              string variable, so you can redirect there if present instead\n              of the homepage. Alternatively, it will be added to the session\n              as ``next`` if USE_SESSION_FOR_NEXT is set.)\n\n        If :attr:`LoginManager.login_view` is not defined, then it will simply\n        raise a HTTP 401 (Unauthorized) error instead.\n\n        This should be returned from a view or before/after_request function,\n        otherwise the redirect will have no effect.\n        '''\n        user_unauthorized.send(current_app._get_current_object())\n\n        if self.unauthorized_callback:\n            return self.unauthorized_callback()\n\n        if request.blueprint in self.blueprint_login_views:\n            login_view = self.blueprint_login_views[request.blueprint]\n        else:\n            login_view = self.login_view\n\n        if not login_view:\n            abort(401)\n\n        if self.login_message:\n            if self.localize_callback is not None:\n                flash(self.localize_callback(self.login_message),\n                      category=self.login_message_category)\n            else:\n                flash(self.login_message, category=self.login_message_category)\n\n        config = current_app.config\n        if config.get('USE_SESSION_FOR_NEXT', USE_SESSION_FOR_NEXT):\n            login_url = expand_login_view(login_view)\n            session['_id'] = self._session_identifier_generator()\n            session['next'] = make_next_param(login_url, request.url)\n            redirect_url = make_login_url(login_view)\n        else:\n            redirect_url = make_login_url(login_view, next_url=request.url)\n\n        return redirect(redirect_url)", "response": "This method is called when the user is not authorized to log in. It will return a HTTP 401 error if the user is not authorized to log in."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef needs_refresh(self):\n        '''\n        This is called when the user is logged in, but they need to be\n        reauthenticated because their session is stale. If you register a\n        callback with `needs_refresh_handler`, then it will be called.\n        Otherwise, it will take the following actions:\n\n            - Flash :attr:`LoginManager.needs_refresh_message` to the user.\n\n            - Redirect the user to :attr:`LoginManager.refresh_view`. (The page\n              they were attempting to access will be passed in the ``next``\n              query string variable, so you can redirect there if present\n              instead of the homepage.)\n\n        If :attr:`LoginManager.refresh_view` is not defined, then it will\n        simply raise a HTTP 401 (Unauthorized) error instead.\n\n        This should be returned from a view or before/after_request function,\n        otherwise the redirect will have no effect.\n        '''\n        user_needs_refresh.send(current_app._get_current_object())\n\n        if self.needs_refresh_callback:\n            return self.needs_refresh_callback()\n\n        if not self.refresh_view:\n            abort(401)\n\n        if self.localize_callback is not None:\n            flash(self.localize_callback(self.needs_refresh_message),\n                  category=self.needs_refresh_message_category)\n        else:\n            flash(self.needs_refresh_message,\n                  category=self.needs_refresh_message_category)\n\n        config = current_app.config\n        if config.get('USE_SESSION_FOR_NEXT', USE_SESSION_FOR_NEXT):\n            login_url = expand_login_view(self.refresh_view)\n            session['_id'] = self._session_identifier_generator()\n            session['next'] = make_next_param(login_url, request.url)\n            redirect_url = make_login_url(self.refresh_view)\n        else:\n            login_url = self.refresh_view\n            redirect_url = make_login_url(login_url, next_url=request.url)\n\n        return redirect(redirect_url)", "response": "This method is called when the user is trying to refresh the user s session."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _update_request_context_with_user(self, user=None):\n        '''Store the given user as ctx.user.'''\n\n        ctx = _request_ctx_stack.top\n        ctx.user = self.anonymous_user() if user is None else user", "response": "Store the given user as ctx. user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload user from session or remember_me cookie as applicable", "response": "def _load_user(self):\n        '''Loads user from session or remember_me cookie as applicable'''\n\n        if self._user_callback is None and self._request_callback is None:\n            raise Exception(\n                \"Missing user_loader or request_loader. Refer to \"\n                \"http://flask-login.readthedocs.io/#how-it-works \"\n                \"for more info.\")\n\n        user_accessed.send(current_app._get_current_object())\n\n        # Check SESSION_PROTECTION\n        if self._session_protection_failed():\n            return self._update_request_context_with_user()\n\n        user = None\n\n        # Load user from Flask Session\n        user_id = session.get('user_id')\n        if user_id is not None and self._user_callback is not None:\n            user = self._user_callback(user_id)\n\n        # Load user from Remember Me Cookie or Request Loader\n        if user is None:\n            config = current_app.config\n            cookie_name = config.get('REMEMBER_COOKIE_NAME', COOKIE_NAME)\n            header_name = config.get('AUTH_HEADER_NAME', AUTH_HEADER_NAME)\n            has_cookie = (cookie_name in request.cookies and\n                          session.get('remember') != 'clear')\n            if has_cookie:\n                cookie = request.cookies[cookie_name]\n                user = self._load_user_from_remember_cookie(cookie)\n            elif self._request_callback:\n                user = self._load_user_from_request(request)\n            elif header_name in request.headers:\n                header = request.headers[header_name]\n                user = self._load_user_from_header(header)\n\n        return self._update_request_context_with_user(user)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a pretrained tree and cluster size into a tuple of labels and probabilities.", "response": "def _tree_to_labels(X, single_linkage_tree, min_cluster_size=10,\n                    cluster_selection_method='eom',\n                    allow_single_cluster=False,\n                    match_reference_implementation=False):\n    \"\"\"Converts a pretrained tree and cluster size into a\n    set of labels and probabilities.\n    \"\"\"\n    condensed_tree = condense_tree(single_linkage_tree,\n                                   min_cluster_size)\n    stability_dict = compute_stability(condensed_tree)\n    labels, probabilities, stabilities = get_clusters(condensed_tree,\n                                                      stability_dict,\n                                                      cluster_selection_method,\n                                                      allow_single_cluster,\n                                                      match_reference_implementation)\n\n    return (labels, probabilities, stabilities, condensed_tree,\n            single_linkage_tree)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming check_array after removing infinite values from the given distance matrix.", "response": "def check_precomputed_distance_matrix(X):\n    \"\"\"Perform check_array(X) after removing infinite values (numpy.inf) from the given distance matrix.\n    \"\"\"\n    tmp = X.copy()\n    tmp[np.isinf(tmp)] = 1\n    check_array(tmp)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming HDBSCAN clustering from a vector array or distance matrix.", "response": "def hdbscan(X, min_cluster_size=5, min_samples=None, alpha=1.0,\n            metric='minkowski', p=2, leaf_size=40,\n            algorithm='best', memory=Memory(cachedir=None, verbose=0),\n            approx_min_span_tree=True, gen_min_span_tree=False,\n            core_dist_n_jobs=4,\n            cluster_selection_method='eom', allow_single_cluster=False,\n            match_reference_implementation=False, **kwargs):\n    \"\"\"Perform HDBSCAN clustering from a vector array or distance matrix.\n\n    Parameters\n    ----------\n    X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \\\n            array of shape (n_samples, n_samples)\n        A feature array, or array of distances between samples if\n        ``metric='precomputed'``.\n\n    min_cluster_size : int, optional (default=5)\n        The minimum number of samples in a group for that group to be\n        considered a cluster; groupings smaller than this size will be left\n        as noise.\n\n    min_samples : int, optional (default=None)\n        The number of samples in a neighborhood for a point\n        to be considered as a core point. This includes the point itself.\n        defaults to the min_cluster_size.\n\n    alpha : float, optional (default=1.0)\n        A distance scaling parameter as used in robust single linkage.\n        See [2]_ for more information.\n\n    metric : string or callable, optional (default='minkowski')\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by metrics.pairwise.pairwise_distances for its\n        metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square.\n\n    p : int, optional (default=2)\n        p value to use if using the minkowski metric.\n\n    leaf_size : int, optional (default=40)\n        Leaf size for trees responsible for fast nearest\n        neighbour queries.\n\n    algorithm : string, optional (default='best')\n        Exactly which algorithm to use; hdbscan has variants specialised\n        for different characteristics of the data. By default this is set\n        to ``best`` which chooses the \"best\" algorithm given the nature of\n        the data. You can force other options if you believe you know\n        better. Options are:\n            * ``best``\n            * ``generic``\n            * ``prims_kdtree``\n            * ``prims_balltree``\n            * ``boruvka_kdtree``\n            * ``boruvka_balltree``\n\n    memory : instance of joblib.Memory or string, optional\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n\n    approx_min_span_tree : bool, optional (default=True)\n        Whether to accept an only approximate minimum spanning tree.\n        For some algorithms this can provide a significant speedup, but\n        the resulting clustering may be of marginally lower quality.\n        If you are willing to sacrifice speed for correctness you may want\n        to explore this; in general this should be left at the default True.\n\n    gen_min_span_tree : bool, optional (default=False)\n        Whether to generate the minimum spanning tree for later analysis.\n\n    core_dist_n_jobs : int, optional (default=4)\n        Number of parallel jobs to run in core distance computations (if\n        supported by the specific algorithm). For ``core_dist_n_jobs``\n        below -1, (n_cpus + 1 + core_dist_n_jobs) are used.\n\n    cluster_selection_method : string, optional (default='eom')\n        The method used to select clusters from the condensed tree. The\n        standard approach for HDBSCAN* is to use an Excess of Mass algorithm\n        to find the most persistent clusters. Alternatively you can instead\n        select the clusters at the leaves of the tree -- this provides the\n        most fine grained and homogeneous clusters. Options are:\n            * ``eom``\n            * ``leaf``\n\n    allow_single_cluster : bool, optional (default=False)\n        By default HDBSCAN* will not produce a single cluster, setting this\n        to t=True will override this and allow single cluster results in\n        the case that you feel this is a valid result for your dataset.\n        (default False)\n\n    match_reference_implementation : bool, optional (default=False)\n        There exist some interpretational differences between this\n        HDBSCAN* implementation and the original authors reference\n        implementation in Java. This can result in very minor differences\n        in clustering results. Setting this flag to True will, at a some\n        performance cost, ensure that the clustering results match the\n        reference implementation.\n\n    **kwargs : optional\n        Arguments passed to the distance metric\n\n    Returns\n    -------\n    labels : ndarray, shape (n_samples, )\n        Cluster labels for each point.  Noisy samples are given the label -1.\n\n    probabilities : ndarray, shape (n_samples, )\n        Cluster membership strengths for each point. Noisy samples are assigned\n        0.\n\n    cluster_persistence : array, shape  (n_clusters, )\n        A score of how persistent each cluster is. A score of 1.0 represents\n        a perfectly stable cluster that persists over all distance scales,\n        while a score of 0.0 represents a perfectly ephemeral cluster. These\n        scores can be guage the relative coherence of the clusters output\n        by the algorithm.\n\n    condensed_tree : record array\n        The condensed cluster hierarchy used to generate clusters.\n\n    single_linkage_tree : ndarray, shape (n_samples - 1, 4)\n        The single linkage tree produced during clustering in scipy\n        hierarchical clustering format\n        (see http://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html).\n\n    min_spanning_tree : ndarray, shape (n_samples - 1, 3)\n        The minimum spanning as an edgelist. If gen_min_span_tree was False\n        this will be None.\n\n    References\n    ----------\n\n    .. [1] Campello, R. J., Moulavi, D., & Sander, J. (2013, April).\n       Density-based clustering based on hierarchical density estimates.\n       In Pacific-Asia Conference on Knowledge Discovery and Data Mining\n       (pp. 160-172). Springer Berlin Heidelberg.\n\n    .. [2] Chaudhuri, K., & Dasgupta, S. (2010). Rates of convergence for the\n       cluster tree. In Advances in Neural Information Processing Systems\n       (pp. 343-351).\n\n    \"\"\"\n    if min_samples is None:\n        min_samples = min_cluster_size\n\n    if type(min_samples) is not int or type(min_cluster_size) is not int:\n        raise ValueError('Min samples and min cluster size must be integers!')\n\n    if min_samples <= 0 or min_cluster_size <= 0:\n        raise ValueError('Min samples and Min cluster size must be positive'\n                         ' integers')\n\n    if min_cluster_size == 1:\n        raise ValueError('Min cluster size must be greater than one')\n\n    if not isinstance(alpha, float) or alpha <= 0.0:\n        raise ValueError('Alpha must be a positive float value greater than'\n                         ' 0!')\n\n    if leaf_size < 1:\n        raise ValueError('Leaf size must be greater than 0!')\n\n    if metric == 'minkowski':\n        if p is None:\n            raise TypeError('Minkowski metric given but no p value supplied!')\n        if p < 0:\n            raise ValueError('Minkowski metric with negative p value is not'\n                             ' defined!')\n\n    if match_reference_implementation:\n        min_samples = min_samples - 1\n        min_cluster_size = min_cluster_size + 1\n        approx_min_span_tree = False\n\n    if cluster_selection_method not in ('eom', 'leaf'):\n        raise ValueError('Invalid Cluster Selection Method: %s\\n'\n                         'Should be one of: \"eom\", \"leaf\"\\n')\n\n    # Checks input and converts to an nd-array where possible\n    if metric != 'precomputed' or issparse(X):\n        X = check_array(X, accept_sparse='csr')\n    else:\n        # Only non-sparse, precomputed distance matrices are handled here\n        #   and thereby allowed to contain numpy.inf for missing distances\n        check_precomputed_distance_matrix(X)\n\n    # Python 2 and 3 compliant string_type checking\n    if isinstance(memory, six.string_types):\n        memory = Memory(cachedir=memory, verbose=0)\n\n    size = X.shape[0]\n    min_samples = min(size - 1, min_samples)\n    if min_samples == 0:\n        min_samples = 1\n\n    if algorithm != 'best':\n        if metric != 'precomputed' and issparse(X) and metric != 'generic':\n            raise ValueError(\"Sparse data matrices only support algorithm 'generic'.\")\n                  \n        if algorithm == 'generic':\n            (single_linkage_tree,\n             result_min_span_tree) = memory.cache(\n                _hdbscan_generic)(X, min_samples, alpha, metric,\n                                  p, leaf_size, gen_min_span_tree, **kwargs)\n        elif algorithm == 'prims_kdtree':\n            if metric not in KDTree.valid_metrics:\n                raise ValueError(\"Cannot use Prim's with KDTree for this\"\n                                 \" metric!\")\n            (single_linkage_tree, result_min_span_tree) = memory.cache(\n                _hdbscan_prims_kdtree)(X, min_samples, alpha,\n                                       metric, p, leaf_size,\n                                       gen_min_span_tree, **kwargs)\n        elif algorithm == 'prims_balltree':\n            if metric not in BallTree.valid_metrics:\n                raise ValueError(\"Cannot use Prim's with BallTree for this\"\n                                 \" metric!\")\n            (single_linkage_tree, result_min_span_tree) = memory.cache(\n                _hdbscan_prims_balltree)(X, min_samples, alpha,\n                                         metric, p, leaf_size,\n                                         gen_min_span_tree, **kwargs)\n        elif algorithm == 'boruvka_kdtree':\n            if metric not in BallTree.valid_metrics:\n                raise ValueError(\"Cannot use Boruvka with KDTree for this\"\n                                 \" metric!\")\n            (single_linkage_tree, result_min_span_tree) = memory.cache(\n                _hdbscan_boruvka_kdtree)(X, min_samples, alpha,\n                                         metric, p, leaf_size,\n                                         approx_min_span_tree,\n                                         gen_min_span_tree,\n                                         core_dist_n_jobs, **kwargs)\n        elif algorithm == 'boruvka_balltree':\n            if metric not in BallTree.valid_metrics:\n                raise ValueError(\"Cannot use Boruvka with BallTree for this\"\n                                 \" metric!\")\n            (single_linkage_tree, result_min_span_tree) = memory.cache(\n                _hdbscan_boruvka_balltree)(X, min_samples, alpha,\n                                           metric, p, leaf_size,\n                                           approx_min_span_tree,\n                                           gen_min_span_tree,\n                                           core_dist_n_jobs, **kwargs)\n        else:\n            raise TypeError('Unknown algorithm type %s specified' % algorithm)\n    else:\n\n        if issparse(X) or metric not in FAST_METRICS:\n            # We can't do much with sparse matrices ...\n            (single_linkage_tree, result_min_span_tree) = memory.cache(\n                _hdbscan_generic)(X, min_samples,\n                                  alpha, metric, p, leaf_size,\n                                  gen_min_span_tree, **kwargs)\n        elif metric in KDTree.valid_metrics:\n            # TO DO: Need heuristic to decide when to go to boruvka;\n            # still debugging for now\n            if X.shape[1] > 60:\n                (single_linkage_tree, result_min_span_tree) = memory.cache(\n                    _hdbscan_prims_kdtree)(X, min_samples, alpha,\n                                           metric, p, leaf_size,\n                                           gen_min_span_tree, **kwargs)\n            else:\n                (single_linkage_tree, result_min_span_tree) = memory.cache(\n                    _hdbscan_boruvka_kdtree)(X, min_samples, alpha,\n                                             metric, p, leaf_size,\n                                             approx_min_span_tree,\n                                             gen_min_span_tree,\n                                             core_dist_n_jobs, **kwargs)\n        else:  # Metric is a valid BallTree metric\n            # TO DO: Need heuristic to decide when to go to boruvka;\n            # still debugging for now\n            if X.shape[1] > 60:\n                (single_linkage_tree, result_min_span_tree) = memory.cache(\n                    _hdbscan_prims_balltree)(X, min_samples, alpha,\n                                             metric, p, leaf_size,\n                                             gen_min_span_tree, **kwargs)\n            else:\n                (single_linkage_tree, result_min_span_tree) = memory.cache(\n                    _hdbscan_boruvka_balltree)(X, min_samples, alpha,\n                                               metric, p, leaf_size,\n                                               approx_min_span_tree,\n                                               gen_min_span_tree,\n                                               core_dist_n_jobs, **kwargs)\n\n    return _tree_to_labels(X,\n                           single_linkage_tree,\n                           min_cluster_size,\n                           cluster_selection_method,\n                           allow_single_cluster,\n                           match_reference_implementation) + \\\n            (result_min_span_tree,)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fit(self, X, y=None):\n        if self.metric != 'precomputed':\n            X = check_array(X, accept_sparse='csr')\n            self._raw_data = X\n        elif issparse(X):\n            # Handle sparse precomputed distance matrices separately\n            X = check_array(X, accept_sparse='csr')\n        else:\n            # Only non-sparse, precomputed distance matrices are allowed\n            #   to have numpy.inf values indicating missing distances\n            check_precomputed_distance_matrix(X)\n\n        kwargs = self.get_params()\n        # prediction data only applies to the persistent model, so remove\n        # it from the keyword args we pass on the the function\n        kwargs.pop('prediction_data', None)\n        kwargs.update(self._metric_kwargs)\n\n        (self.labels_,\n         self.probabilities_,\n         self.cluster_persistence_,\n         self._condensed_tree,\n         self._single_linkage_tree,\n         self._min_spanning_tree) = hdbscan(X, **kwargs)\n\n        if self.prediction_data:\n            self.generate_prediction_data()\n\n        return self", "response": "Perform HDBSCAN clustering from features or distance matrix X."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate prediction data for the given object.", "response": "def generate_prediction_data(self):\n        \"\"\"\n        Create data that caches intermediate results used for predicting\n        the label of new/unseen points. This data is only useful if\n        you are intending to use functions from ``hdbscan.prediction``.\n        \"\"\"\n\n        if self.metric in FAST_METRICS:\n            min_samples = self.min_samples or self.min_cluster_size\n            if self.metric in KDTree.valid_metrics:\n                tree_type = 'kdtree'\n            elif self.metric in BallTree.valid_metrics:\n                tree_type = 'balltree'\n            else:\n                warn('Metric {} not supported for prediction data!'.format(self.metric))\n                return\n\n            self._prediction_data = PredictionData(\n                self._raw_data, self.condensed_tree_, min_samples,\n                tree_type=tree_type, metric=self.metric,\n                **self._metric_kwargs\n            )\n        else:\n            warn('Cannot generate prediction data for non-vector'\n                 'space inputs -- access to the source data rather'\n                 'than mere distances is required!')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _bfs_from_cluster_tree(tree, bfs_root):\n\n    result = []\n    to_process = [bfs_root]\n\n    while to_process:\n        result.extend(to_process)\n        to_process = tree['child'][np.in1d(tree['parent'], to_process)].tolist()\n\n    return result", "response": "Perform a breadth first search on a tree in condensed tree format bfs_root"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates data for use in plotting the iCal plot of the condensed tree or iCal plot.", "response": "def get_plot_data(self,\n                      leaf_separation=1,\n                      log_size=False,\n                      max_rectangle_per_icicle=20):\n        \"\"\"Generates data for use in plotting the 'icicle plot' or dendrogram\n        plot of the condensed tree generated by HDBSCAN.\n\n        Parameters\n        ----------\n        leaf_separation : float, optional\n                          How far apart to space the final leaves of the\n                          dendrogram. (default 1)\n\n        log_size : boolean, optional\n                   Use log scale for the 'size' of clusters (i.e. number of\n                   points in the cluster at a given lambda value).\n                   (default False)\n\n        max_rectangles_per_icicle : int, optional\n            To simplify the plot this method will only emit\n            ``max_rectangles_per_icicle`` bars per branch of the dendrogram.\n            This ensures that we don't suffer from massive overplotting in\n            cases with a lot of data points.\n\n        Returns\n        -------\n        plot_data : dict\n                    Data associated to bars in a bar plot:\n                        `bar_centers` x coordinate centers for bars\n                        `bar_tops` heights of bars in lambda scale\n                        `bar_bottoms` y coordinate of bottoms of bars\n                        `bar_widths` widths of the bars (in x coord scale)\n                        `bar_bounds` a 4-tuple of [left, right, bottom, top]\n                                     giving the bounds on a full set of\n                                     cluster bars\n                    Data associates with cluster splits:\n                        `line_xs` x coordinates for horizontal dendrogram lines\n                        `line_ys` y coordinates for horizontal dendrogram lines\n        \"\"\"\n        leaves = _get_leaves(self._raw_tree)\n        last_leaf = self._raw_tree['parent'].max()\n        root = self._raw_tree['parent'].min()\n\n        # We want to get the x and y coordinates for the start of each cluster\n        # Initialize the leaves, since we know where they go, the iterate\n        # through everything from the leaves back, setting coords as we go\n        if isinstance(leaves, np.int64):\n            cluster_x_coords = {leaves: leaf_separation}\n        else:\n            cluster_x_coords = dict(zip(leaves, [leaf_separation * x\n                                                 for x in range(len(leaves))]))\n        cluster_y_coords = {root: 0.0}\n\n        for cluster in range(last_leaf, root - 1, -1):\n            split = self._raw_tree[['child', 'lambda_val']]\n            split = split[(self._raw_tree['parent'] == cluster) &\n                          (self._raw_tree['child_size'] > 1)]\n            if len(split['child']) > 1:\n                left_child, right_child = split['child']\n                cluster_x_coords[cluster] = np.mean([cluster_x_coords[left_child],\n                                                     cluster_x_coords[right_child]])\n                cluster_y_coords[left_child] = split['lambda_val'][0]\n                cluster_y_coords[right_child] = split['lambda_val'][1]\n\n        # We use bars to plot the 'icicles', so we need to generate centers, tops,\n        # bottoms and widths for each rectangle. We can go through each cluster\n        # and do this for each in turn.\n        bar_centers = []\n        bar_tops = []\n        bar_bottoms = []\n        bar_widths = []\n\n        cluster_bounds = {}\n\n        scaling = np.sum(self._raw_tree[self._raw_tree['parent'] == root]['child_size'])\n\n        if log_size:\n            scaling = np.log(scaling)\n\n        for c in range(last_leaf, root - 1, -1):\n\n            cluster_bounds[c] = [0, 0, 0, 0]\n\n            c_children = self._raw_tree[self._raw_tree['parent'] == c]\n            current_size = np.sum(c_children['child_size'])\n            current_lambda = cluster_y_coords[c]\n            cluster_max_size = current_size\n            cluster_max_lambda = c_children['lambda_val'].max()\n            cluster_min_size = np.sum(\n                c_children[c_children['lambda_val'] ==\n                           cluster_max_lambda]['child_size'])\n\n            if log_size:\n                current_size = np.log(current_size)\n                cluster_max_size = np.log(cluster_max_size)\n                cluster_min_size = np.log(cluster_min_size)\n\n            total_size_change = float(cluster_max_size - cluster_min_size)\n            step_size_change = total_size_change / max_rectangle_per_icicle\n\n            cluster_bounds[c][CB_LEFT] = cluster_x_coords[c] * scaling - (current_size / 2.0)\n            cluster_bounds[c][CB_RIGHT] = cluster_x_coords[c] * scaling + (current_size / 2.0)\n            cluster_bounds[c][CB_BOTTOM] = cluster_y_coords[c]\n            cluster_bounds[c][CB_TOP] = np.max(c_children['lambda_val'])\n\n            last_step_size = current_size\n            last_step_lambda = current_lambda\n\n            for i in np.argsort(c_children['lambda_val']):\n                row = c_children[i]\n                if row['lambda_val'] != current_lambda and \\\n                        (last_step_size - current_size > step_size_change\n                        or row['lambda_val'] == cluster_max_lambda):\n                    bar_centers.append(cluster_x_coords[c] * scaling)\n                    bar_tops.append(row['lambda_val'] - last_step_lambda)\n                    bar_bottoms.append(last_step_lambda)\n                    bar_widths.append(last_step_size)\n                    last_step_size = current_size\n                    last_step_lambda = current_lambda\n                if log_size:\n                    exp_size = np.exp(current_size) - row['child_size']\n                    # Ensure we don't try to take log of zero\n                    if exp_size > 0.01:\n                        current_size = np.log(np.exp(current_size) - row['child_size'])\n                    else:\n                        current_size = 0.0\n                else:\n                    current_size -= row['child_size']\n                current_lambda = row['lambda_val']\n\n        # Finally we need the horizontal lines that occur at cluster splits.\n        line_xs = []\n        line_ys = []\n\n        for row in self._raw_tree[self._raw_tree['child_size'] > 1]:\n            parent = row['parent']\n            child = row['child']\n            child_size = row['child_size']\n            if log_size:\n                child_size = np.log(child_size)\n            sign = np.sign(cluster_x_coords[child] - cluster_x_coords[parent])\n            line_xs.append([\n                cluster_x_coords[parent] * scaling,\n                cluster_x_coords[child] * scaling + sign * (child_size / 2.0)\n            ])\n            line_ys.append([\n                cluster_y_coords[child],\n                cluster_y_coords[child]\n            ])\n\n        return {\n            'bar_centers': bar_centers,\n            'bar_tops': bar_tops,\n            'bar_bottoms': bar_bottoms,\n            'bar_widths': bar_widths,\n            'line_xs': line_xs,\n            'line_ys': line_ys,\n            'cluster_bounds': cluster_bounds\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nusing matplotlib to plot the condensed tree.", "response": "def plot(self, leaf_separation=1, cmap='viridis', select_clusters=False,\n             label_clusters=False, selection_palette=None,\n             axis=None, colorbar=True, log_size=False,\n             max_rectangles_per_icicle=20):\n        \"\"\"Use matplotlib to plot an 'icicle plot' dendrogram of the condensed tree.\n\n        Effectively this is a dendrogram where the width of each cluster bar is\n        equal to the number of points (or log of the number of points) in the cluster\n        at the given lambda value. Thus bars narrow as points progressively drop\n        out of clusters. The make the effect more apparent the bars are also colored\n        according the the number of points (or log of the number of points).\n\n        Parameters\n        ----------\n        leaf_separation : float, optional (default 1)\n                          How far apart to space the final leaves of the\n                          dendrogram.\n\n        cmap : string or matplotlib colormap, optional (default viridis)\n               The matplotlib colormap to use to color the cluster bars.\n\n\n        select_clusters : boolean, optional (default False)\n                          Whether to draw ovals highlighting which cluster\n                          bar represent the clusters that were selected by\n                          HDBSCAN as the final clusters.\n\n        label_clusters : boolean, optional (default False)\n                         If select_clusters is True then this determines\n                         whether to draw text labels on the clusters.\n\n        selection_palette : list of colors, optional (default None)\n                            If not None, and at least as long as\n                            the number of clusters, draw ovals\n                            in colors iterating through this palette.\n                            This can aid in cluster identification\n                            when plotting.\n\n        axis : matplotlib axis or None, optional (default None)\n               The matplotlib axis to render to. If None then a new axis\n               will be generated. The rendered axis will be returned.\n\n\n        colorbar : boolean, optional (default True)\n                   Whether to draw a matplotlib colorbar displaying the range\n                   of cluster sizes as per the colormap.\n\n        log_size : boolean, optional (default False)\n                   Use log scale for the 'size' of clusters (i.e. number of\n                   points in the cluster at a given lambda value).\n\n\n        max_rectangles_per_icicle : int, optional (default 20)\n            To simplify the plot this method will only emit\n            ``max_rectangles_per_icicle`` bars per branch of the dendrogram.\n            This ensures that we don't suffer from massive overplotting in\n            cases with a lot of data points.\n\n         Returns\n        -------\n        axis : matplotlib axis\n               The axis on which the 'icicle plot' has been rendered.\n        \"\"\"\n        try:\n            import matplotlib.pyplot as plt\n        except ImportError:\n            raise ImportError(\n                'You must install the matplotlib library to plot the condensed tree.'\n                'Use get_plot_data to calculate the relevant data without plotting.')\n\n        plot_data = self.get_plot_data(leaf_separation=leaf_separation,\n                                       log_size=log_size,\n                                       max_rectangle_per_icicle=max_rectangles_per_icicle)\n\n        if cmap != 'none':\n            sm = plt.cm.ScalarMappable(cmap=cmap,\n                                       norm=plt.Normalize(0, max(plot_data['bar_widths'])))\n            sm.set_array(plot_data['bar_widths'])\n            bar_colors = [sm.to_rgba(x) for x in plot_data['bar_widths']]\n        else:\n            bar_colors = 'black'\n\n        if axis is None:\n            axis = plt.gca()\n\n        axis.bar(\n            plot_data['bar_centers'],\n            plot_data['bar_tops'],\n            bottom=plot_data['bar_bottoms'],\n            width=plot_data['bar_widths'],\n            color=bar_colors,\n            align='center',\n            linewidth=0\n        )\n\n        drawlines = []\n        for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):\n            drawlines.append(xs)\n            drawlines.append(ys)\n        axis.plot(*drawlines, color='black', linewidth=1)\n        # for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):\n        #     axis.plot(xs, ys, color='black', linewidth=1)\n\n        if select_clusters:\n            try:\n                from matplotlib.patches import Ellipse\n            except ImportError:\n                raise ImportError('You must have matplotlib.patches available to plot selected clusters.')\n\n            chosen_clusters = self._select_clusters()\n            \n            # Extract the chosen cluster bounds. If enough duplicate data points exist in the\n            # data the lambda value might be infinite. This breaks labeling and highlighting\n            # the chosen clusters.\n            cluster_bounds = np.array([ plot_data['cluster_bounds'][c] for c in chosen_clusters ])\n            if not np.isfinite(cluster_bounds).all():\n                warn('Infinite lambda values encountered in chosen clusters.'\n                     ' This might be due to duplicates in the data.')\n\n            # Extract the plot range of the y-axis and set default center and height values for ellipses.\n            # Extremly dense clusters might result in near infinite lambda values. Setting max_height\n            # based on the percentile should alleviate the impact on plotting.\n            plot_range = np.hstack([plot_data['bar_tops'], plot_data['bar_bottoms']])\n            plot_range = plot_range[np.isfinite(plot_range)]\n            mean_y_center = np.mean([np.max(plot_range), np.min(plot_range)])\n            max_height = np.diff(np.percentile(plot_range, q=[10,90]))\n\n            for i, c in enumerate(chosen_clusters):\n                c_bounds = plot_data['cluster_bounds'][c]\n                width = (c_bounds[CB_RIGHT] - c_bounds[CB_LEFT])\n                height = (c_bounds[CB_TOP] - c_bounds[CB_BOTTOM])\n                center = (\n                    np.mean([c_bounds[CB_LEFT], c_bounds[CB_RIGHT]]),\n                    np.mean([c_bounds[CB_TOP], c_bounds[CB_BOTTOM]]),\n                )\n                \n                # Set center and height to default values if necessary\n                if not np.isfinite(center[1]):\n                    center = (center[0], mean_y_center)\n                if not np.isfinite(height):\n                    height = max_height\n\n                # Ensure the ellipse is visible\n                min_height = 0.1*max_height\n                if height < min_height:\n                    height = min_height\n\n                if selection_palette is not None and \\\n                        len(selection_palette) >= len(chosen_clusters):\n                    oval_color = selection_palette[i]\n                else:\n                    oval_color = 'r'\n\n                box = Ellipse(\n                    center,\n                    2.0 * width,\n                    1.2 * height,\n                    facecolor='none',\n                    edgecolor=oval_color,\n                    linewidth=2\n                )\n\n                if label_clusters:\n                    axis.annotate(str(i), xy=center,\n                                  xytext=(center[0] - 4.0 * width, center[1] + 0.65 * height),\n                                  horizontalalignment='left',\n                                  verticalalignment='bottom')\n\n                axis.add_artist(box)\n\n        if colorbar:\n            cb = plt.colorbar(sm)\n            if log_size:\n                cb.ax.set_ylabel('log(Number of points)')\n            else:\n                cb.ax.set_ylabel('Number of points')\n\n        axis.set_xticks([])\n        for side in ('right', 'top', 'bottom'):\n            axis.spines[side].set_visible(False)\n        axis.invert_yaxis()\n        axis.set_ylabel('$\\lambda$ value')\n\n        return axis"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a pandas dataframe representation of the condensed tree.", "response": "def to_pandas(self):\n        \"\"\"Return a pandas dataframe representation of the condensed tree.\n\n        Each row of the dataframe corresponds to an edge in the tree.\n        The columns of the dataframe are `parent`, `child`, `lambda_val`\n        and `child_size`.\n\n        The `parent` and `child` are the ids of the\n        parent and child nodes in the tree. Node ids less than the number\n        of points in the original dataset represent individual points, while\n        ids greater than the number of points are clusters.\n\n        The `lambda_val` value is the value (1/distance) at which the `child`\n        node leaves the cluster.\n\n        The `child_size` is the number of points in the `child` node.\n        \"\"\"\n        try:\n            from pandas import DataFrame, Series\n        except ImportError:\n            raise ImportError('You must have pandas installed to export pandas DataFrames')\n\n        result = DataFrame(self._raw_tree)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_networkx(self):\n        try:\n            from networkx import DiGraph, set_node_attributes\n        except ImportError:\n            raise ImportError('You must have networkx installed to export networkx graphs')\n\n        result = DiGraph()\n        for row in self._raw_tree:\n            result.add_edge(row['parent'], row['child'], weight=row['lambda_val'])\n\n        set_node_attributes(result, dict(self._raw_tree[['child', 'child_size']]), 'size')\n\n        return result", "response": "Return a NetworkX DiGraph object representing the condensed tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots a dendrogram of the single linkage tree.", "response": "def plot(self, axis=None, truncate_mode=None, p=0, vary_line_width=True,\n             cmap='viridis', colorbar=True):\n        \"\"\"Plot a dendrogram of the single linkage tree.\n\n        Parameters\n        ----------\n        truncate_mode : str, optional\n                        The dendrogram can be hard to read when the original\n                        observation matrix from which the linkage is derived\n                        is large. Truncation is used to condense the dendrogram.\n                        There are several modes:\n\n        ``None/'none'``\n                No truncation is performed (Default).\n\n        ``'lastp'``\n                The last p non-singleton formed in the linkage are the only\n                non-leaf nodes in the linkage; they correspond to rows\n                Z[n-p-2:end] in Z. All other non-singleton clusters are\n                contracted into leaf nodes.\n\n        ``'level'/'mtica'``\n                No more than p levels of the dendrogram tree are displayed.\n                This corresponds to Mathematica(TM) behavior.\n\n        p : int, optional\n            The ``p`` parameter for ``truncate_mode``.\n\n        vary_line_width : boolean, optional\n            Draw downward branches of the dendrogram with line thickness that\n            varies depending on the size of the cluster.\n\n        cmap : string or matplotlib colormap, optional\n               The matplotlib colormap to use to color the cluster bars.\n               A value of 'none' will result in black bars.\n               (default 'viridis')\n\n        colorbar : boolean, optional\n                   Whether to draw a matplotlib colorbar displaying the range\n                   of cluster sizes as per the colormap. (default True)\n\n        Returns\n        -------\n        axis : matplotlib axis\n               The axis on which the dendrogram plot has been rendered.\n\n        \"\"\"\n        dendrogram_data = dendrogram(self._linkage, p=p, truncate_mode=truncate_mode, no_plot=True)\n        X = dendrogram_data['icoord']\n        Y = dendrogram_data['dcoord']\n\n        try:\n            import matplotlib.pyplot as plt\n        except ImportError:\n            raise ImportError('You must install the matplotlib library to plot the single linkage tree.')\n\n        if axis is None:\n            axis = plt.gca()\n\n        if vary_line_width:\n            linewidths = [(_line_width(y[0], self._linkage),\n                           _line_width(y[1], self._linkage))\n                          for y in Y]\n        else:\n            linewidths = [(1.0, 1.0)] * len(Y)\n\n        if cmap != 'none':\n            color_array = np.log2(np.array(linewidths).flatten())\n            sm = plt.cm.ScalarMappable(cmap=cmap,\n                                       norm=plt.Normalize(0, color_array.max()))\n            sm.set_array(color_array)\n\n        for x, y, lw in zip(X, Y, linewidths):\n            left_x = x[:2]\n            right_x = x[2:]\n            left_y = y[:2]\n            right_y = y[2:]\n            horizontal_x = x[1:3]\n            horizontal_y = y[1:3]\n\n            if cmap != 'none':\n                axis.plot(left_x, left_y, color=sm.to_rgba(np.log2(lw[0])),\n                          linewidth=np.log2(1 + lw[0]),\n                          solid_joinstyle='miter', solid_capstyle='butt')\n                axis.plot(right_x, right_y, color=sm.to_rgba(np.log2(lw[1])),\n                          linewidth=np.log2(1 + lw[1]),\n                          solid_joinstyle='miter', solid_capstyle='butt')\n            else:\n                axis.plot(left_x, left_y, color='k',\n                          linewidth=np.log2(1 + lw[0]),\n                          solid_joinstyle='miter', solid_capstyle='butt')\n                axis.plot(right_x, right_y, color='k',\n                          linewidth=np.log2(1 + lw[1]),\n                          solid_joinstyle='miter', solid_capstyle='butt')\n\n            axis.plot(horizontal_x, horizontal_y, color='k', linewidth=1.0,\n                      solid_joinstyle='miter', solid_capstyle='butt')\n\n        if colorbar:\n            cb = plt.colorbar(sm)\n            cb.ax.set_ylabel('log(Number of points)')\n\n        axis.set_xticks([])\n        for side in ('right', 'top', 'bottom'):\n            axis.spines[side].set_visible(False)\n        axis.set_ylabel('distance')\n\n        return axis"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a pandas dataframe representation of the single linkage tree.", "response": "def to_pandas(self):\n        \"\"\"Return a pandas dataframe representation of the single linkage tree.\n\n        Each row of the dataframe corresponds to an edge in the tree.\n        The columns of the dataframe are `parent`, `left_child`,\n        `right_child`, `distance` and `size`.\n\n        The `parent`, `left_child` and `right_child` are the ids of the\n        parent and child nodes in the tree. Node ids less than the number\n        of points in the original dataset represent individual points, while\n        ids greater than the number of points are clusters.\n\n        The `distance` value is the at which the child nodes merge to form\n        the parent node.\n\n        The `size` is the number of points in the `parent` node.\n        \"\"\"\n        try:\n            from pandas import DataFrame, Series\n        except ImportError:\n            raise ImportError('You must have pandas installed to export pandas DataFrames')\n\n        max_node = 2 * self._linkage.shape[0]\n        num_points = max_node - (self._linkage.shape[0] - 1)\n\n        parent_array = np.arange(num_points, max_node + 1)\n\n        result = DataFrame({\n            'parent': parent_array,\n            'left_child': self._linkage.T[0],\n            'right_child': self._linkage.T[1],\n            'distance': self._linkage.T[2],\n            'size': self._linkage.T[3]\n        })[['parent', 'left_child', 'right_child', 'distance', 'size']]\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_networkx(self):\n        try:\n            from networkx import DiGraph, set_node_attributes\n        except ImportError:\n            raise ImportError('You must have networkx installed to export networkx graphs')\n\n        max_node = 2 * self._linkage.shape[0]\n        num_points = max_node - (self._linkage.shape[0] - 1)\n\n        result = DiGraph()\n        for parent, row in enumerate(self._linkage, num_points):\n            result.add_edge(parent, row[0], weight=row[2])\n            result.add_edge(parent, row[1], weight=row[2])\n\n        size_dict = {parent: row[3] for parent, row in enumerate(self._linkage, num_points)}\n        set_node_attributes(result, size_dict, 'size')\n\n        return result", "response": "Return a NetworkX DiGraph representing the single linkage tree."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot(self, axis=None, node_size=40, node_color='k',\n             node_alpha=0.8, edge_alpha=0.5, edge_cmap='viridis_r',\n             edge_linewidth=2, vary_line_width=True, colorbar=True):\n        \"\"\"Plot the minimum spanning tree (as projected into 2D by t-SNE if required).\n\n        Parameters\n        ----------\n\n        axis : matplotlib axis, optional\n               The axis to render the plot to\n\n        node_size : int, optional\n                The size of nodes in the plot (default 40).\n\n        node_color : matplotlib color spec, optional\n                The color to render nodes (default black).\n\n        node_alpha : float, optional\n                The alpha value (between 0 and 1) to render nodes with\n                (default 0.8).\n\n        edge_cmap : matplotlib colormap, optional\n                The colormap to color edges by (varying color by edge\n                    weight/distance). Can be a cmap object or a string\n                    recognised by matplotlib. (default `viridis_r`)\n\n        edge_alpha : float, optional\n                The alpha value (between 0 and 1) to render edges with\n                (default 0.5).\n\n        edge_linewidth : float, optional\n                The linewidth to use for rendering edges (default 2).\n\n        vary_line_width : bool, optional\n                Edge width is proportional to (log of) the inverse of the\n                mutual reachability distance. (default True)\n\n        colorbar : bool, optional\n                Whether to draw a colorbar. (default True)\n\n        Returns\n        -------\n\n        axis : matplotlib axis\n                The axis used the render the plot.\n        \"\"\"\n        try:\n            import matplotlib.pyplot as plt\n            from matplotlib.collections import LineCollection\n        except ImportError:\n            raise ImportError('You must install the matplotlib library to plot the minimum spanning tree.')\n\n        if self._data.shape[0] > 32767:\n            warn('Too many data points for safe rendering of an minimal spanning tree!')\n            return None\n\n        if axis is None:\n            axis = plt.gca()\n\n        if self._data.shape[1] > 2:\n            # Get a 2D projection; if we have a lot of dimensions use PCA first\n            if self._data.shape[1] > 32:\n                # Use PCA to get down to 32 dimension\n                data_for_projection = PCA(n_components=32).fit_transform(self._data)\n            else:\n                data_for_projection = self._data.copy()\n\n            projection = TSNE().fit_transform(data_for_projection)\n        else:\n            projection = self._data.copy()\n\n        if vary_line_width:\n            line_width = edge_linewidth * (np.log(self._mst.T[2].max() / self._mst.T[2]) + 1.0)\n        else:\n            line_width = edge_linewidth\n\n        line_coords = projection[self._mst[:, :2].astype(int)]\n        line_collection = LineCollection(line_coords, linewidth=line_width,\n                                         cmap=edge_cmap, alpha=edge_alpha)\n        line_collection.set_array(self._mst[:, 2].T)\n\n        axis.add_artist(line_collection)\n        axis.scatter(projection.T[0], projection.T[1], c=node_color, alpha=node_alpha, s=node_size)\n        axis.set_xticks([])\n        axis.set_yticks([])\n\n        if colorbar:\n            cb = plt.colorbar(line_collection)\n            cb.ax.set_ylabel('Mutual reachability distance')\n\n        return axis", "response": "Plot the minimum spanning tree."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_pandas(self):\n        try:\n            from pandas import DataFrame\n        except ImportError:\n            raise ImportError('You must have pandas installed to export pandas DataFrames')\n\n        result = DataFrame({'from': self._mst.T[0].astype(int),\n                            'to': self._mst.T[1].astype(int),\n                            'distance': self._mst.T[2]})\n        return result", "response": "Return a Pandas dataframe of the minimum spanning tree."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a NetworkX Graph object representing the minimum spanning tree.", "response": "def to_networkx(self):\n        \"\"\"Return a NetworkX Graph object representing the minimum spanning tree.\n\n        Edge weights in the graph are the distance between the nodes they connect.\n\n        Nodes have a `data` attribute attached giving the data vector of the\n        associated point.\n        \"\"\"\n        try:\n            from networkx import Graph, set_node_attributes\n        except ImportError:\n            raise ImportError('You must have networkx installed to export networkx graphs')\n\n        result = Graph()\n        for row in self._mst:\n            result.add_edge(row[0], row[1], weight=row[2])\n\n        data_dict = {index: tuple(row) for index, row in enumerate(self._data)}\n        set_node_attributes(result, data_dict, 'data')\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the all - points - core - distance for all the points in a cluster.", "response": "def all_points_core_distance(distance_matrix, d=2.0):\n    \"\"\"\n    Compute the all-points-core-distance for all the points of a cluster.\n\n    Parameters\n    ----------\n    distance_matrix : array (cluster_size, cluster_size)\n        The pairwise distance matrix between points in the cluster.\n\n    d : integer\n        The dimension of the data set, which is used in the computation\n        of the all-point-core-distance as per the paper.\n\n    Returns\n    -------\n    core_distances : array (cluster_size,)\n        The all-points-core-distance of each point in the cluster\n\n    References\n    ----------\n    Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and Sander, J.,\n    2014. Density-Based Clustering Validation. In SDM (pp. 839-847).\n    \"\"\"\n    distance_matrix[distance_matrix != 0] = (1.0 / distance_matrix[\n        distance_matrix != 0]) ** d\n    result = distance_matrix.sum(axis=1)\n    result /= distance_matrix.shape[0] - 1\n    result **= (-1.0 / d)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef all_points_mutual_reachability(X, labels, cluster_id,\n                                   metric='euclidean', d=None, **kwd_args):\n    \"\"\"\n    Compute the all-points-mutual-reachability distances for all the points of\n    a cluster.\n\n    If metric is 'precomputed' then assume X is a distance matrix for the full\n    dataset. Note that in this case you must pass in 'd' the dimension of the\n    dataset.\n\n    Parameters\n    ----------\n    X : array (n_samples, n_features) or (n_samples, n_samples)\n        The input data of the clustering. This can be the data, or, if\n        metric is set to `precomputed` the pairwise distance matrix used\n        for the clustering.\n\n    labels : array (n_samples)\n        The label array output by the clustering, providing an integral\n        cluster label to each data point, with -1 for noise points.\n\n    cluster_id : integer\n        The cluster label for which to compute the all-points\n        mutual-reachability (which should be done on a cluster\n        by cluster basis).\n\n    metric : string\n        The metric used to compute distances for the clustering (and\n        to be re-used in computing distances for mr distance). If\n        set to `precomputed` then X is assumed to be the precomputed\n        distance matrix between samples.\n\n    d : integer (or None)\n        The number of features (dimension) of the dataset. This need only\n        be set in the case of metric being set to `precomputed`, where\n        the ambient dimension of the data is unknown to the function.\n\n    **kwd_args :\n        Extra arguments to pass to the distance computation for other\n        metrics, such as minkowski, Mahanalobis etc.\n\n    Returns\n    -------\n\n    mutual_reachaibility : array (n_samples, n_samples)\n        The pairwise mutual reachability distances between all points in `X`\n        with `label` equal to `cluster_id`.\n\n    core_distances : array (n_samples,)\n        The all-points-core_distance of all points in `X` with `label` equal\n        to `cluster_id`.\n\n    References\n    ----------\n    Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and Sander, J.,\n    2014. Density-Based Clustering Validation. In SDM (pp. 839-847).\n    \"\"\"\n    if metric == 'precomputed':\n        if d is None:\n            raise ValueError('If metric is precomputed a '\n                             'd value must be provided!')\n        distance_matrix = X[labels == cluster_id, :][:, labels == cluster_id]\n    else:\n        subset_X = X[labels == cluster_id, :]\n        distance_matrix = pairwise_distances(subset_X, metric=metric,\n                                             **kwd_args)\n        d = X.shape[1]\n\n    core_distances = all_points_core_distance(distance_matrix.copy(), d=d)\n    core_dist_matrix = np.tile(core_distances, (core_distances.shape[0], 1))\n\n    result = np.dstack(\n        [distance_matrix, core_dist_matrix, core_dist_matrix.T]).max(axis=-1)\n\n    return result, core_distances", "response": "Compute the pairwise mutual reachability distances between all the points in X and cluster_id."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the internal minimum spanning tree given a matrix of mutual reachability distances.", "response": "def internal_minimum_spanning_tree(mr_distances):\n    \"\"\"\n    Compute the 'internal' minimum spanning tree given a matrix of mutual\n    reachability distances. Given a minimum spanning tree the 'internal'\n    graph is the subgraph induced by vertices of degree greater than one.\n\n    Parameters\n    ----------\n    mr_distances : array (cluster_size, cluster_size)\n        The pairwise mutual reachability distances, inferred to be the edge\n        weights of a complete graph. Since MSTs are computed per cluster\n        this is the all-points-mutual-reacability for points within a single\n        cluster.\n\n    Returns\n    -------\n    internal_nodes : array\n        An array listing the indices of the internal nodes of the MST\n\n    internal_edges : array (?, 3)\n        An array of internal edges in weighted edge list format; that is\n        an edge is an array of length three listing the two vertices\n        forming the edge and weight of the edge.\n\n    References\n    ----------\n    Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and Sander, J.,\n    2014. Density-Based Clustering Validation. In SDM (pp. 839-847).\n    \"\"\"\n    single_linkage_data = mst_linkage_core(mr_distances)\n    min_span_tree = single_linkage_data.copy()\n    for index, row in enumerate(min_span_tree[1:], 1):\n        candidates = np.where(isclose(mr_distances[int(row[1])], row[2]))[0]\n        candidates = np.intersect1d(candidates,\n                                    single_linkage_data[:index, :2].astype(\n                                        int))\n        candidates = candidates[candidates != row[1]]\n        assert len(candidates) > 0\n        row[0] = candidates[0]\n\n    vertices = np.arange(mr_distances.shape[0])[\n        np.bincount(min_span_tree.T[:2].flatten().astype(np.intp)) > 1]\n    # A little \"fancy\" we select from the flattened array reshape back\n    # (Fortran format to get indexing right) and take the product to do an and\n    # then convert back to boolean type.\n    edge_selection = np.prod(np.in1d(min_span_tree.T[:2], vertices).reshape(\n        (min_span_tree.shape[0], 2), order='F'), axis=1).astype(bool)\n\n    # Density sparseness is not well defined if there are no\n    # internal edges (as per the referenced paper). However\n    # MATLAB code from the original authors simply selects the\n    # largest of *all* the edges in the case that there are\n    # no internal edges, so we do the same here\n    if np.any(edge_selection):\n        # If there are any internal edges, then subselect them out\n        edges = min_span_tree[edge_selection]\n    else:\n        # If there are no internal edges then we want to take the\n        # max over all the edges that exist in the MST, so we simply\n        # do nothing and return all the edges in the MST.\n        edges = min_span_tree.copy()\n\n    return vertices, edges"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef density_separation(X, labels, cluster_id1, cluster_id2,\n                       internal_nodes1, internal_nodes2,\n                       core_distances1, core_distances2,\n                       metric='euclidean', **kwd_args):\n    \"\"\"\n    Compute the density separation between two clusters. This is the minimum\n    all-points mutual reachability distance between pairs of points, one from\n    internal nodes of MSTs of each cluster.\n\n    Parameters\n    ----------\n    X : array (n_samples, n_features) or (n_samples, n_samples)\n        The input data of the clustering. This can be the data, or, if\n        metric is set to `precomputed` the pairwise distance matrix used\n        for the clustering.\n\n    labels : array (n_samples)\n        The label array output by the clustering, providing an integral\n        cluster label to each data point, with -1 for noise points.\n\n    cluster_id1 : integer\n        The first cluster label to compute separation between.\n\n    cluster_id2 : integer\n        The second cluster label to compute separation between.\n\n    internal_nodes1 : array\n        The vertices of the MST for `cluster_id1` that were internal vertices.\n\n    internal_nodes2 : array\n        The vertices of the MST for `cluster_id2` that were internal vertices.\n\n    core_distances1 : array (size of cluster_id1,)\n        The all-points-core_distances of all points in the cluster\n        specified by cluster_id1.\n\n    core_distances2 : array (size of cluster_id2,)\n        The all-points-core_distances of all points in the cluster\n        specified by cluster_id2.\n\n    metric : string\n        The metric used to compute distances for the clustering (and\n        to be re-used in computing distances for mr distance). If\n        set to `precomputed` then X is assumed to be the precomputed\n        distance matrix between samples.\n\n    **kwd_args :\n        Extra arguments to pass to the distance computation for other\n        metrics, such as minkowski, Mahanalobis etc.\n\n    Returns\n    -------\n    The 'density separation' between the clusters specified by\n    `cluster_id1` and `cluster_id2`.\n\n    References\n    ----------\n    Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and Sander, J.,\n    2014. Density-Based Clustering Validation. In SDM (pp. 839-847).\n    \"\"\"\n    if metric == 'precomputed':\n        sub_select = X[labels == cluster_id1, :][:, labels == cluster_id2]\n        distance_matrix = sub_select[internal_nodes1, :][:, internal_nodes2]\n    else:\n        cluster1 = X[labels == cluster_id1][internal_nodes1]\n        cluster2 = X[labels == cluster_id2][internal_nodes2]\n        distance_matrix = cdist(cluster1, cluster2, metric, **kwd_args)\n\n    core_dist_matrix1 = np.tile(core_distances1[internal_nodes1],\n                                (distance_matrix.shape[1], 1)).T\n    core_dist_matrix2 = np.tile(core_distances2[internal_nodes2],\n                                (distance_matrix.shape[0], 1))\n\n    mr_dist_matrix = np.dstack([distance_matrix,\n                                core_dist_matrix1,\n                                core_dist_matrix2]).max(axis=-1)\n\n    return mr_dist_matrix.min()", "response": "Compute the density separation between two clusters."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the density based cluster validity index for the specified cluster labels and for each cluster in labels.", "response": "def validity_index(X, labels, metric='euclidean',\n                   d=None, per_cluster_scores=False, **kwd_args):\n    \"\"\"\n    Compute the density based cluster validity index for the\n    clustering specified by `labels` and for each cluster in `labels`.\n\n    Parameters\n    ----------\n    X : array (n_samples, n_features) or (n_samples, n_samples)\n        The input data of the clustering. This can be the data, or, if\n        metric is set to `precomputed` the pairwise distance matrix used\n        for the clustering.\n\n    labels : array (n_samples)\n        The label array output by the clustering, providing an integral\n        cluster label to each data point, with -1 for noise points.\n\n    metric : optional, string (default 'euclidean')\n        The metric used to compute distances for the clustering (and\n        to be re-used in computing distances for mr distance). If\n        set to `precomputed` then X is assumed to be the precomputed\n        distance matrix between samples.\n\n    d : optional, integer (or None) (default None)\n        The number of features (dimension) of the dataset. This need only\n        be set in the case of metric being set to `precomputed`, where\n        the ambient dimension of the data is unknown to the function.\n\n    per_cluster_scores : optional, boolean (default False)\n        Whether to return the validity index for individual clusters.\n        Defaults to False with the function returning a single float\n        value for the whole clustering.\n\n    **kwd_args :\n        Extra arguments to pass to the distance computation for other\n        metrics, such as minkowski, Mahanalobis etc.\n\n    Returns\n    -------\n    validity_index : float\n        The density based cluster validity index for the clustering. This\n        is a numeric value between -1 and 1, with higher values indicating\n        a 'better' clustering.\n\n    per_cluster_validity_index : array (n_clusters,)\n        The cluster validity index of each individual cluster as an array.\n        The overall validity index is the weighted average of these values.\n        Only returned if per_cluster_scores is set to True.\n\n    References\n    ----------\n    Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and Sander, J.,\n    2014. Density-Based Clustering Validation. In SDM (pp. 839-847).\n    \"\"\"\n    core_distances = {}\n    density_sparseness = {}\n    mst_nodes = {}\n    mst_edges = {}\n\n    max_cluster_id = labels.max() + 1\n    density_sep = np.inf * np.ones((max_cluster_id, max_cluster_id),\n                                   dtype=np.float64)\n    cluster_validity_indices = np.empty(max_cluster_id, dtype=np.float64)\n\n    for cluster_id in range(max_cluster_id):\n\n        if np.sum(labels == cluster_id) == 0:\n            continue\n\n        mr_distances, core_distances[\n            cluster_id] = all_points_mutual_reachability(\n            X,\n            labels,\n            cluster_id,\n            metric,\n            d,\n            **kwd_args\n        )\n\n        mst_nodes[cluster_id], mst_edges[cluster_id] = \\\n            internal_minimum_spanning_tree(mr_distances)\n        density_sparseness[cluster_id] = mst_edges[cluster_id].T[2].max()\n\n    for i in range(max_cluster_id):\n\n        if np.sum(labels == i) == 0:\n            continue\n\n        internal_nodes_i = mst_nodes[i]\n        for j in range(i + 1, max_cluster_id):\n\n            if np.sum(labels == j) == 0:\n                continue\n\n            internal_nodes_j = mst_nodes[j]\n            density_sep[i, j] = density_separation(\n                X, labels, i, j,\n                internal_nodes_i, internal_nodes_j,\n                core_distances[i], core_distances[j],\n                metric=metric, **kwd_args\n            )\n            density_sep[j, i] = density_sep[i, j]\n\n    n_samples = float(X.shape[0])\n    result = 0\n\n    for i in range(max_cluster_id):\n\n        if np.sum(labels == i) == 0:\n            continue\n\n        min_density_sep = density_sep[i].min()\n        cluster_validity_indices[i] = (\n            (min_density_sep - density_sparseness[i]) /\n            max(min_density_sep, density_sparseness[i])\n        )\n        cluster_size = np.sum(labels == i)\n        result += (cluster_size / n_samples) * cluster_validity_indices[i]\n\n    if per_cluster_scores:\n        return result, cluster_validity_indices\n    else:\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nroughing single linkage clustering from a vector array or distance matrix.", "response": "def robust_single_linkage(X, cut, k=5, alpha=1.4142135623730951,\n                          gamma=5, metric='euclidean', algorithm='best',\n                          memory=Memory(cachedir=None, verbose=0), leaf_size=40,\n                          core_dist_n_jobs=4, **kwargs):\n    \"\"\"Perform robust single linkage clustering from a vector array\n    or distance matrix.\n\n    Parameters\n    ----------\n    X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \\\n            array of shape (n_samples, n_samples)\n        A feature array, or array of distances between samples if\n        ``metric='precomputed'``.\n\n    cut : float\n        The reachability distance value to cut the cluster heirarchy at\n        to derive a flat cluster labelling.\n\n    k : int, optional (default=5)\n        Reachability distances will be computed with regard to the `k`\n        nearest neighbors.\n\n    alpha : float, optional (default=np.sqrt(2))\n        Distance scaling for reachability distance computation. Reachability\n        distance is computed as\n        $max \\{ core_k(a), core_k(b), 1/\\alpha d(a,b) \\}$.\n\n    gamma : int, optional (default=5)\n        Ignore any clusters in the flat clustering with size less than gamma,\n        and declare points in such clusters as noise points.\n\n    metric : string, or callable, optional (default='euclidean')\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by metrics.pairwise.pairwise_distances for its\n        metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square.\n\n    algorithm : string, optional (default='best')\n        Exactly which algorithm to use; hdbscan has variants specialised\n        for different characteristics of the data. By default this is set\n        to ``best`` which chooses the \"best\" algorithm given the nature of\n        the data. You can force other options if you believe you know\n        better. Options are:\n            * ``generic``\n            * ``best``\n            * ``prims_kdtree``\n            * ``prims_balltree``\n            * ``boruvka_kdtree``\n            * ``boruvka_balltree``\n\n    memory : Instance of joblib.Memory or string (optional)\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n\n    leaf_size : int, optional (default=40)\n        Leaf size for trees responsible for fast nearest\n        neighbour queries.\n\n    core_dist_n_jobs : int, optional\n        Number of parallel jobs to run in core distance computations (if\n        supported by the specific algorithm). For ``core_dist_n_jobs``\n        below -1, (n_cpus + 1 + core_dist_n_jobs) are used.\n        (default 4)\n\n    Returns\n    -------\n    labels : ndarray, shape (n_samples, )\n        Cluster labels for each point.  Noisy samples are given the label -1.\n\n    single_linkage_tree : ndarray, shape (n_samples - 1, 4)\n        The single linkage tree produced during clustering in scipy\n        hierarchical clustering format\n        (see http://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html).\n\n    References\n    ----------\n    .. [1] Chaudhuri, K., & Dasgupta, S. (2010). Rates of convergence for the\n       cluster tree. In Advances in Neural Information Processing Systems\n       (pp. 343-351).\n\n    \"\"\"\n\n    if not isinstance(k, int) or k < 1:\n        raise ValueError('k must be an integer greater than zero!')\n\n    if not isinstance(alpha, float) or alpha < 1.0:\n        raise ValueError('alpha must be a float greater than or equal to 1.0!')\n\n    if not isinstance(gamma, int) or gamma < 1:\n        raise ValueError('gamma must be an integer greater than zero!')\n\n    if not isinstance(leaf_size, int) or leaf_size < 1:\n        raise ValueError('Leaf size must be at least one!')\n\n    if metric == 'minkowski':\n        if 'p' not in kwargs or kwargs['p'] is None:\n            raise TypeError('Minkowski metric given but no p value supplied!')\n        if kwargs['p'] < 0:\n            raise ValueError('Minkowski metric with negative p value is not'\n                             ' defined!')\n\n    X = check_array(X, accept_sparse='csr')\n    if isinstance(memory, six.string_types):\n        memory = Memory(cachedir=memory, verbose=0)\n\n    if algorithm != 'best':\n        if algorithm == 'generic':\n            single_linkage_tree = memory.cache(_rsl_generic)(\n                X, k, alpha, metric, **kwargs)\n        elif algorithm == 'prims_kdtree':\n            single_linkage_tree = memory.cache(_rsl_prims_kdtree)(\n                X, k, alpha, metric, **kwargs)\n        elif algorithm == 'prims_balltree':\n            single_linkage_tree = memory.cache(_rsl_prims_balltree)(\n                X, k, alpha, metric, **kwargs)\n        elif algorithm == 'boruvka_kdtree':\n            single_linkage_tree = \\\n                memory.cache(_rsl_boruvka_kdtree)(X, k, alpha, metric, leaf_size,\n                                                  core_dist_n_jobs, **kwargs)\n        elif algorithm == 'boruvka_balltree':\n            single_linkage_tree = \\\n                memory.cache(_rsl_boruvka_balltree)(X, k, alpha, metric, leaf_size,\n                                                    core_dist_n_jobs, **kwargs)\n        else:\n            raise TypeError('Unknown algorithm type %s specified' % algorithm)\n    else:\n        if issparse(X) or metric not in FAST_METRICS:\n            # We can't do much with sparse matrices ...\n            single_linkage_tree = memory.cache(_rsl_generic)(\n                X, k, alpha, metric, **kwargs)\n        elif metric in KDTree.valid_metrics:\n            # Need heuristic to decide when to go to boruvka;\n            # still debugging for now\n            if X.shape[1] > 128:\n                single_linkage_tree = memory.cache(_rsl_prims_kdtree)(\n                    X, k, alpha, metric, **kwargs)\n            else:\n                single_linkage_tree = \\\n                    memory.cache(_rsl_boruvka_kdtree)(X, k, alpha, metric,\n                                                        leaf_size,\n                                                        core_dist_n_jobs,\n                                                        **kwargs)\n        else:  # Metric is a valid BallTree metric\n            # Need heuristic to decide when to go to boruvka;\n            # still debugging for now\n            if X.shape[1] > 128:\n                single_linkage_tree = memory.cache(_rsl_prims_kdtree)(\n                    X, k, alpha, metric, **kwargs)\n            else:\n                single_linkage_tree = \\\n                    memory.cache(_rsl_boruvka_balltree)(X, k, alpha, metric,\n                                                        leaf_size,\n                                                        core_dist_n_jobs,\n                                                        **kwargs)\n\n    labels = single_linkage_tree.get_clusters(cut, gamma)\n\n    return labels, single_linkage_tree.to_numpy()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming robust single linkage clustering from features or sparse matrix X.", "response": "def fit(self, X, y=None):\n        \"\"\"Perform robust single linkage clustering from features or\n        distance matrix.\n\n        Parameters\n        ----------\n        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \\\n                array of shape (n_samples, n_samples)\n            A feature array, or array of distances between samples if\n            ``metric='precomputed'``.\n\n        Returns\n        -------\n        self : object\n            Returns self\n        \"\"\"\n        X = check_array(X, accept_sparse='csr')\n\n        kwargs = self.get_params()\n        del kwargs['metric_params']\n        kwargs.update(self.metric_params)\n\n        self.labels_, self._cluster_hierarchy = robust_single_linkage(\n            X, **kwargs)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_neighbor_and_lambda(neighbor_indices, neighbor_distances,\n                              core_distances, min_samples):\n    \"\"\"\n    Find the nearest mutual reachability neighbor of a point, and  compute\n    the associated lambda value for the point, given the mutual reachability\n    distance to a nearest neighbor.\n\n    Parameters\n    ----------\n    neighbor_indices : array (2 * min_samples, )\n        An array of raw distance based nearest neighbor indices.\n\n    neighbor_distances : array (2 * min_samples, )\n        An array of raw distances to the nearest neighbors.\n\n    core_distances : array (n_samples, )\n        An array of core distances for all points\n\n    min_samples : int\n        The min_samples value used to generate core distances.\n\n    Returns\n    -------\n    neighbor : int\n        The index into the full raw data set of the nearest mutual reachability\n        distance neighbor of the point.\n\n    lambda_ : float\n        The lambda value at which this point joins/merges with `neighbor`.\n    \"\"\"\n    neighbor_core_distances = core_distances[neighbor_indices]\n    point_core_distances = neighbor_distances[min_samples] * np.ones(\n        neighbor_indices.shape[0])\n    mr_distances = np.vstack((\n        neighbor_core_distances,\n        point_core_distances,\n        neighbor_distances\n    )).max(axis=0)\n\n    nn_index = mr_distances.argmin()\n\n    nearest_neighbor = neighbor_indices[nn_index]\n    if mr_distances[nn_index] > 0.0:\n        lambda_ = 1. / mr_distances[nn_index]\n    else:\n        lambda_ = np.finfo(np.double).max\n\n    return nearest_neighbor, lambda_", "response": "Find the nearest mutual reachability neighbor of a point and compute the associated lambda value for the point."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextending the condensed tree with an additional point added.", "response": "def _extend_condensed_tree(tree, neighbor_indices, neighbor_distances,\n                           core_distances, min_samples):\n    \"\"\"\n    Create a new condensed tree with an additional point added, allowing for\n    computations as if this point had been part of the original tree. Note\n    that this makes as little change to the tree as possible, with no\n    re-optimizing/re-condensing so that the selected clusters remain\n    effectively unchanged.\n\n    Parameters\n    ----------\n    tree : structured array\n        The raw format condensed tree to update.\n\n    neighbor_indices : array (2 * min_samples, )\n        An array of raw distance based nearest neighbor indices.\n\n    neighbor_distances : array (2 * min_samples, )\n        An array of raw distances to the nearest neighbors.\n\n    core_distances : array (n_samples, )\n        An array of core distances for all points\n\n    min_samples : int\n        The min_samples value used to generate core distances.\n\n    Returns\n    -------\n    new_tree : structured array\n        The original tree with an extra row providing the parent cluster\n        and lambda information for a new point given index -1.\n    \"\"\"\n    tree_root = tree['parent'].min()\n\n    nearest_neighbor, lambda_ = _find_neighbor_and_lambda(neighbor_indices,\n                                                          neighbor_distances,\n                                                          core_distances,\n                                                          min_samples\n                                                          )\n\n    neighbor_tree_row = get_tree_row_with_child(tree, nearest_neighbor)\n    potential_cluster = neighbor_tree_row['parent']\n\n    if neighbor_tree_row['lambda_val'] <= lambda_:\n        # New point departs with the old\n        new_tree_row = (potential_cluster, -1, 1,\n                        neighbor_tree_row['lambda_val'])\n    else:\n        # Find appropriate cluster based on lambda of new point\n        while potential_cluster > tree_root and \\\n                        tree[tree['child'] ==\n                                potential_cluster]['lambda_val'] >= lambda_:\n            potential_cluster = tree['parent'][tree['child']\n                                               == potential_cluster][0]\n\n        new_tree_row = (potential_cluster, -1, 1, lambda_)\n\n    return np.append(tree, new_tree_row)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the cluster label and membership of a new data point.", "response": "def _find_cluster_and_probability(tree, cluster_tree, neighbor_indices,\n                                  neighbor_distances, core_distances,\n                                  cluster_map, max_lambdas,\n                                  min_samples):\n    \"\"\"\n    Return the cluster label (of the original clustering) and membership\n    probability of a new data point.\n\n    Parameters\n    ----------\n    tree : CondensedTree\n        The condensed tree associated with the clustering.\n\n    cluster_tree : structured_array\n        The raw form of the condensed tree with only cluster information (no\n        data on individual points). This is significantly more compact.\n\n    neighbor_indices : array (2 * min_samples, )\n        An array of raw distance based nearest neighbor indices.\n\n    neighbor_distances : array (2 * min_samples, )\n        An array of raw distances to the nearest neighbors.\n\n    core_distances : array (n_samples, )\n        An array of core distances for all points\n\n    cluster_map : dict\n        A dictionary mapping cluster numbers in the condensed tree to labels\n        in the final selected clustering.\n\n    max_lambdas : dict\n        A dictionary mapping cluster numbers in the condensed tree to the\n        maximum lambda value seen in that cluster.\n\n    min_samples : int\n        The min_samples value used to generate core distances.\n    \"\"\"\n    raw_tree = tree._raw_tree\n    tree_root = cluster_tree['parent'].min()\n\n    nearest_neighbor, lambda_ = _find_neighbor_and_lambda(neighbor_indices,\n                                                          neighbor_distances,\n                                                          core_distances,\n                                                          min_samples\n                                                          )\n\n    neighbor_tree_row = get_tree_row_with_child(raw_tree, nearest_neighbor)\n    potential_cluster = neighbor_tree_row['parent']\n\n    if neighbor_tree_row['lambda_val'] > lambda_:\n        # Find appropriate cluster based on lambda of new point\n        while potential_cluster > tree_root and \\\n                        cluster_tree['lambda_val'][cluster_tree['child']\n                                == potential_cluster] >= lambda_:\n            potential_cluster = cluster_tree['parent'][cluster_tree['child']\n                                                       == potential_cluster][0]\n\n    if potential_cluster in cluster_map:\n        cluster_label = cluster_map[potential_cluster]\n    else:\n        cluster_label = -1\n\n    if cluster_label >= 0:\n        max_lambda = max_lambdas[potential_cluster]\n\n        if max_lambda > 0.0:\n            lambda_ = min(max_lambda, lambda_)\n            prob = (lambda_ / max_lambda)\n        else:\n            prob = 1.0\n    else:\n        prob = 0.0\n\n    return cluster_label, prob"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef approximate_predict(clusterer, points_to_predict):\n    if clusterer.prediction_data_ is None:\n        raise ValueError('Clusterer does not have prediction data!'\n                         ' Try fitting with prediction_data=True set,'\n                         ' or run generate_prediction_data on the clusterer')\n\n    points_to_predict = np.asarray(points_to_predict)\n\n    if points_to_predict.shape[1] != \\\n            clusterer.prediction_data_.raw_data.shape[1]:\n        raise ValueError('New points dimension does not match fit data!')\n\n    if clusterer.prediction_data_.cluster_tree.shape[0] == 0:\n        warn('Clusterer does not have any defined clusters, new data'\n             ' will be automatically predicted as noise.')\n        labels = -1 * np.ones(points_to_predict.shape[0], dtype=np.int32)\n        probabilities = np.zeros(points_to_predict.shape[0], dtype=np.float32)\n        return labels, probabilities\n\n    labels = np.empty(points_to_predict.shape[0], dtype=np.int)\n    probabilities = np.empty(points_to_predict.shape[0], dtype=np.float64)\n\n    min_samples = clusterer.min_samples or clusterer.min_cluster_size\n    neighbor_distances, neighbor_indices = \\\n        clusterer.prediction_data_.tree.query(points_to_predict,\n                                              k=2 * min_samples)\n\n    for i in range(points_to_predict.shape[0]):\n        label, prob = _find_cluster_and_probability(\n            clusterer.condensed_tree_,\n            clusterer.prediction_data_.cluster_tree,\n            neighbor_indices[i],\n            neighbor_distances[i],\n            clusterer.prediction_data_.core_distances,\n            clusterer.prediction_data_.cluster_map,\n            clusterer.prediction_data_.max_lambdas,\n            min_samples\n        )\n        labels[i] = label\n        probabilities[i] = prob\n\n    return labels, probabilities", "response": "Predict the cluster label of new points."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npredict soft cluster membership.", "response": "def membership_vector(clusterer, points_to_predict):\n    \"\"\"Predict soft cluster membership. The result produces a vector\n    for each point in ``points_to_predict`` that gives a probability that\n    the given point is a member of a cluster for each of the selected clusters\n    of the ``clusterer``.\n\n    Parameters\n    ----------\n    clusterer : HDBSCAN\n        A clustering object that has been fit to the data and\n        either had ``prediction_data=True`` set, or called the\n        ``generate_prediction_data`` method after the fact.\n\n    points_to_predict : array, or array-like (n_samples, n_features)\n        The new data points to predict cluster labels for. They should\n        have the same dimensionality as the original dataset over which\n        clusterer was fit.\n\n    Returns\n    -------\n    membership_vectors : array (n_samples, n_clusters)\n        The probability that point ``i`` is a member of cluster ``j`` is\n        in ``membership_vectors[i, j]``.\n\n    See Also\n    --------\n    :py:func:`hdbscan.predict.predict`\n    :py:func:`hdbscan.predict.all_points_membership_vectors`\n\"\"\"\n\n    points_to_predict = points_to_predict.astype(np.float64)\n    clusters = np.array(\n        sorted(list(clusterer.condensed_tree_._select_clusters()))).astype(np.intp)\n\n    result = np.empty((points_to_predict.shape[0], clusters.shape[0]),\n                      dtype=np.float64)\n\n    min_samples = clusterer.min_samples or clusterer.min_cluster_size\n    neighbor_distances, neighbor_indices = \\\n        clusterer.prediction_data_.tree.query(points_to_predict,\n                                              k=2 * min_samples)\n\n    for i in range(points_to_predict.shape[0]):\n\n        # We need to find where in the tree the new point would go\n        # for the purposes of outlier membership approximation\n        nearest_neighbor, lambda_ = \\\n            _find_neighbor_and_lambda(\n                neighbor_indices[i],\n                neighbor_distances[i],\n                clusterer.prediction_data_.core_distances,\n                min_samples)\n\n        neighbor_tree_row = get_tree_row_with_child(\n            clusterer.condensed_tree_._raw_tree, nearest_neighbor)\n\n        if neighbor_tree_row['lambda_val'] <= lambda_:\n            lambda_ = neighbor_tree_row['lambda_val']\n\n        distance_vec = dist_membership_vector(\n            points_to_predict[i],\n            clusterer.prediction_data_.exemplars,\n            clusterer.prediction_data_.dist_metric)\n        outlier_vec = outlier_membership_vector(\n            nearest_neighbor,\n            lambda_,\n            clusters,\n            clusterer.condensed_tree_._raw_tree,\n            clusterer.prediction_data_.leaf_max_lambdas,\n            clusterer.prediction_data_.cluster_tree)\n\n        result[i] = distance_vec ** 0.5 * outlier_vec ** 2.0\n        result[i] /= result[i].sum()\n\n        result[i] *= prob_in_some_cluster(\n            nearest_neighbor,\n            lambda_,\n            clusters,\n            clusterer.condensed_tree_._raw_tree,\n            clusterer.prediction_data_.leaf_max_lambdas,\n            clusterer.prediction_data_.cluster_tree)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npredicting soft cluster membership vectors for all points in the original dataset.", "response": "def all_points_membership_vectors(clusterer):\n    \"\"\"Predict soft cluster membership vectors for all points in the\n    original dataset the clusterer was trained on. This function is more\n    efficient by making use of the fact that all points are already in the\n    condensed tree, and processing in bulk.\n\n    Parameters\n    ----------\n    clusterer : HDBSCAN\n         A clustering object that has been fit to the data and\n        either had ``prediction_data=True`` set, or called the\n        ``generate_prediction_data`` method after the fact.\n        This method does not work if the clusterer was trained\n        with ``metric='precomputed'``.\n\n    Returns\n    -------\n    membership_vectors : array (n_samples, n_clusters)\n        The probability that point ``i`` of the original dataset is a member of\n        cluster ``j`` is in ``membership_vectors[i, j]``.\n\n    See Also\n    --------\n    :py:func:`hdbscan.predict.predict`\n    :py:func:`hdbscan.predict.all_points_membership_vectors`\n    \"\"\"\n    clusters = np.array(sorted(list(clusterer.condensed_tree_._select_clusters()))).astype(np.intp)\n    all_points = clusterer.prediction_data_.raw_data\n\n    # When no clusters found, return array of 0's\n    if clusters.size == 0:\n        return np.zeros(all_points.shape[0])\n\n    distance_vecs = all_points_dist_membership_vector(\n        all_points,\n        clusterer.prediction_data_.exemplars,\n        clusterer.prediction_data_.dist_metric)\n    outlier_vecs = all_points_outlier_membership_vector(\n        clusters,\n        clusterer.condensed_tree_._raw_tree,\n        clusterer.prediction_data_.leaf_max_lambdas,\n        clusterer.prediction_data_.cluster_tree)\n    in_cluster_probs = all_points_prob_in_some_cluster(\n        clusters,\n        clusterer.condensed_tree_._raw_tree,\n        clusterer.prediction_data_.leaf_max_lambdas,\n        clusterer.prediction_data_.cluster_tree)\n\n    result = distance_vecs * outlier_vecs\n    row_sums = result.sum(axis=1)\n    result = result / row_sums[:, np.newaxis]\n    result *= in_cluster_probs[:, np.newaxis]\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_browsers(verify_ssl=True):\n    html = get(settings.BROWSERS_STATS_PAGE, verify_ssl=verify_ssl)\n    html = html.decode('utf-8')\n    html = html.split('<table class=\"w3-table-all notranslate\">')[1]\n    html = html.split('</table>')[0]\n\n    pattern = r'\\.asp\">(.+?)<'\n    browsers = re.findall(pattern, html, re.UNICODE)\n\n    browsers = [\n        settings.OVERRIDES.get(browser, browser)\n        for browser in browsers\n    ]\n\n    pattern = r'td\\sclass=\"right\">(.+?)\\s'\n    browsers_statistics = re.findall(pattern, html, re.UNICODE)\n\n    return list(zip(browsers, browsers_statistics))", "response": "get the list of all available browsers"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the browser versions for a given browser.", "response": "def get_browser_versions(browser, verify_ssl=True):\n    \"\"\"\n    very very hardcoded/dirty re/split stuff, but no dependencies\n    \"\"\"\n    html = get(\n        settings.BROWSER_BASE_PAGE.format(browser=quote_plus(browser)),\n        verify_ssl=verify_ssl,\n    )\n    html = html.decode('iso-8859-1')\n    html = html.split('<div id=\\'liste\\'>')[1]\n    html = html.split('</div>')[0]\n\n    pattern = r'\\?id=\\d+\\'>(.+?)</a'\n    browsers_iter = re.finditer(pattern, html, re.UNICODE)\n\n    browsers = []\n\n    for browser in browsers_iter:\n        if 'more' in browser.group(1).lower():\n            continue\n\n        browsers.append(browser.group(1))\n\n        if len(browsers) == settings.BROWSERS_COUNT_LIMIT:\n            break\n\n    if not browsers:\n        raise FakeUserAgentError(\n            'No browsers version found for {browser}'.format(browser=browser))\n\n    return browsers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfilter out cells based on counts and number of genes expressed.", "response": "def filter_cells(\n    data: AnnData,\n    min_counts: Optional[int] = None,\n    min_genes:  Optional[int] = None,\n    max_counts: Optional[int] = None,\n    max_genes:  Optional[int] = None,\n    inplace: bool = True,\n    copy: bool = False,\n) -> Optional[Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"Filter cell outliers based on counts and numbers of genes expressed.\n\n    For instance, only keep cells with at least `min_counts` counts or\n    `min_genes` genes expressed. This is to filter measurement outliers,\n    i.e. \u201cunreliable\u201d observations.\n\n    Only provide one of the optional parameters ``min_counts``, ``min_genes``,\n    ``max_counts``, ``max_genes`` per call.\n\n    Parameters\n    ----------\n    data\n        The (annotated) data matrix of shape ``n_obs`` \u00d7 ``n_vars``.\n        Rows correspond to cells and columns to genes.\n    min_counts\n        Minimum number of counts required for a cell to pass filtering.\n    min_genes\n        Minimum number of genes expressed required for a cell to pass filtering.\n    max_counts\n        Maximum number of counts required for a cell to pass filtering.\n    max_genes\n        Maximum number of genes expressed required for a cell to pass filtering.\n    inplace\n        Perform computation inplace or return result.\n\n    Returns\n    -------\n    Depending on ``inplace``, returns the following arrays or directly subsets\n    and annotates the data matrix:\n\n    cells_subset : numpy.ndarray\n        Boolean index mask that does filtering. ``True`` means that the\n        cell is kept. ``False`` means the cell is removed.\n    number_per_cell : numpy.ndarray\n        Depending on what was tresholded (``counts`` or ``genes``), the array stores\n        ``n_counts`` or ``n_cells`` per gene.\n\n    Examples\n    --------\n    >>> adata = sc.datasets.krumsiek11()\n    >>> adata.n_obs\n    640\n    >>> adata.var_names\n    ['Gata2' 'Gata1' 'Fog1' 'EKLF' 'Fli1' 'SCL' 'Cebpa'\n     'Pu.1' 'cJun' 'EgrNab' 'Gfi1']\n    >>> # add some true zeros\n    >>> adata.X[adata.X < 0.3] = 0\n    >>> # simply compute the number of genes per cell\n    >>> sc.pp.filter_cells(adata, min_genes=0)\n    >>> adata.n_obs\n    640\n    >>> adata.obs['n_genes'].min()\n    1\n    >>> # filter manually\n    >>> adata_copy = adata[adata.obs['n_genes'] >= 3]\n    >>> adata_copy.obs['n_genes'].min()\n    >>> adata.n_obs\n    554\n    >>> adata.obs['n_genes'].min()\n    3\n    >>> # actually do some filtering\n    >>> sc.pp.filter_cells(adata, min_genes=3)\n    >>> adata.n_obs\n    554\n    >>> adata.obs['n_genes'].min()\n    3\n    \"\"\"\n    if copy:\n       logg.warn('`copy` is deprecated, use `inplace` instead.')\n    n_given_options = sum(\n        option is not None for option in\n        [min_genes, min_counts, max_genes, max_counts])\n    if n_given_options != 1:\n        raise ValueError(\n            'Only provide one of the optional parameters `min_counts`,'\n            '`min_genes`, `max_counts`, `max_genes` per call.')\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        cell_subset, number = materialize_as_ndarray(filter_cells(adata.X, min_counts, min_genes, max_counts, max_genes))\n        if not inplace:\n            return cell_subset, number\n        if min_genes is None and max_genes is None: adata.obs['n_counts'] = number\n        else: adata.obs['n_genes'] = number\n        adata._inplace_subset_obs(cell_subset)\n        return adata if copy else None\n    X = data  # proceed with processing the data matrix\n    min_number = min_counts if min_genes is None else min_genes\n    max_number = max_counts if max_genes is None else max_genes\n    number_per_cell = np.sum(X if min_genes is None and max_genes is None\n                             else X > 0, axis=1)\n    if issparse(X): number_per_cell = number_per_cell.A1\n    if min_number is not None:\n        cell_subset = number_per_cell >= min_number\n    if max_number is not None:\n        cell_subset = number_per_cell <= max_number\n\n    s = np.sum(~cell_subset)\n    if s > 0:\n        logg.info('filtered out {} cells that have'.format(s), end=' ')\n        if min_genes is not None or min_counts is not None:\n            logg.info('less than',\n                   str(min_genes) + ' genes expressed'\n                   if min_counts is None else str(min_counts) + ' counts', no_indent=True)\n        if max_genes is not None or max_counts is not None:\n            logg.info('more than ',\n                   str(max_genes) + ' genes expressed'\n                   if max_counts is None else str(max_counts) + ' counts', no_indent=True)\n    return cell_subset, number_per_cell"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfilter genes based on number of cells or counts.", "response": "def filter_genes(\n    data: AnnData,\n    min_counts: Optional[int] = None,\n    min_cells:  Optional[int] = None,\n    max_counts: Optional[int] = None,\n    max_cells:  Optional[int] = None,\n    inplace: bool = True,\n    copy: bool = False,\n) -> Union[AnnData, None, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"Filter genes based on number of cells or counts.\n\n    Keep genes that have at least ``min_counts`` counts or are expressed in at\n    least ``min_cells`` cells or have at most ``max_counts`` counts or are expressed\n    in at most ``max_cells`` cells.\n\n    Only provide one of the optional parameters ``min_counts``, ``min_cells``,\n    ``max_counts``, ``max_cells`` per call.\n\n    Parameters\n    ----------\n    data\n        An annotated data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes.\n    min_counts\n        Minimum number of counts required for a gene to pass filtering.\n    min_cells\n        Minimum number of cells expressed required for a gene to pass filtering.\n    max_counts\n        Maximum number of counts required for a gene to pass filtering.\n    max_cells\n        Maximum number of cells expressed required for a gene to pass filtering.\n    inplace\n        Perform computation inplace or return result.\n\n    Returns\n    -------\n    Depending on `inplace`, returns the following arrays or directly subsets\n    and annotates the data matrix\n\n    gene_subset : numpy.ndarray\n        Boolean index mask that does filtering. `True` means that the\n        gene is kept. `False` means the gene is removed.\n    number_per_gene : numpy.ndarray\n        Depending on what was tresholded (`counts` or `cells`), the array stores\n        `n_counts` or `n_cells` per gene.\n    \"\"\"\n    if copy:\n       logg.warn('`copy` is deprecated, use `inplace` instead.')\n    n_given_options = sum(\n        option is not None for option in\n        [min_cells, min_counts, max_cells, max_counts])\n    if n_given_options != 1:\n        raise ValueError(\n            'Only provide one of the optional parameters `min_counts`,'\n            '`min_cells`, `max_counts`, `max_cells` per call.')\n\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        gene_subset, number = materialize_as_ndarray(\n            filter_genes(adata.X, min_cells=min_cells,\n                         min_counts=min_counts, max_cells=max_cells,\n                         max_counts=max_counts))\n        if not inplace:\n            return gene_subset, number\n        if min_cells is None and max_cells is None:\n            adata.var['n_counts'] = number\n        else:\n            adata.var['n_cells'] = number\n        adata._inplace_subset_var(gene_subset)\n        return adata if copy else None\n\n    X = data  # proceed with processing the data matrix\n    min_number = min_counts if min_cells is None else min_cells\n    max_number = max_counts if max_cells is None else max_cells\n    number_per_gene = np.sum(X if min_cells is None and max_cells is None\n                             else X > 0, axis=0)\n    if issparse(X):\n        number_per_gene = number_per_gene.A1\n    if min_number is not None:\n        gene_subset = number_per_gene >= min_number\n    if max_number is not None:\n        gene_subset = number_per_gene <= max_number\n\n    s = np.sum(~gene_subset)\n    if s > 0:\n        logg.info('filtered out {} genes that are detected'.format(s), end=' ')\n        if min_cells is not None or min_counts is not None:\n            logg.info('in less than',\n                   str(min_cells) + ' cells'\n                   if min_counts is None else str(min_counts) + ' counts', no_indent=True)\n        if max_cells is not None or max_counts is not None:\n            logg.info('in more than ',\n                   str(max_cells) + ' cells'\n                   if max_counts is None else str(max_counts) + ' counts', no_indent=True)\n    return gene_subset, number_per_gene"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sqrt(\n    data: AnnData,\n    copy: bool = False,\n    chunked: bool = False,\n    chunk_size: Optional[int] = None,\n) -> Optional[AnnData]:\n    \"\"\"Square root the data matrix.\n\n    Computes :math:`X = \\\\sqrt(X)`.\n\n    Parameters\n    ----------\n    data\n        The (annotated) data matrix of shape ``n_obs`` \u00d7 ``n_vars``.\n        Rows correspond to cells and columns to genes.\n    copy\n        If an :class:`~scanpy.api.AnnData` is passed,\n        determines whether a copy is returned.\n    chunked\n        Process the data matrix in chunks, which will save memory.\n        Applies only to :class:`~anndata.AnnData`.\n    chunk_size\n        ``n_obs`` of the chunks to process the data in.\n\n    Returns\n    -------\n    Returns or updates `data`, depending on `copy`.\n    \"\"\"\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        if chunked:\n            for chunk, start, end in adata.chunked_X(chunk_size):\n                adata.X[start:end] = sqrt(chunk)\n        else:\n            adata.X = sqrt(data.X)\n        return adata if copy else None\n    X = data  # proceed with data matrix\n    if not issparse(X):\n        return np.sqrt(X)\n    else:\n        return X.sqrt()", "response": "Square root the data matrix."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the principal components of the given data matrix.", "response": "def pca(\n    data: Union[AnnData, np.ndarray, spmatrix],\n    n_comps: int = N_PCS,\n    zero_center: Optional[bool] = True,\n    svd_solver: str = 'auto',\n    random_state: int = 0,\n    return_info: bool = False,\n    use_highly_variable: Optional[bool] = None,\n    dtype: str = 'float32',\n    copy: bool = False,\n    chunked: bool = False,\n    chunk_size: Optional[int] = None,\n) -> Union[AnnData, np.ndarray, spmatrix]:\n    \"\"\"Principal component analysis [Pedregosa11]_.\n\n    Computes PCA coordinates, loadings and variance decomposition. Uses the\n    implementation of *scikit-learn* [Pedregosa11]_.\n\n    Parameters\n    ----------\n    data\n        The (annotated) data matrix of shape ``n_obs`` \u00d7 ``n_vars``.\n        Rows correspond to cells and columns to genes.\n    n_comps\n        Number of principal components to compute.\n    zero_center\n        If `True`, compute standard PCA from covariance matrix.\n        If ``False``, omit zero-centering variables\n        (uses :class:`~sklearn.decomposition.TruncatedSVD`),\n        which allows to handle sparse input efficiently.\n        Passing ``None`` decides automatically based on sparseness of the data.\n    svd_solver\n        SVD solver to use:\n\n        ``'arpack'``\n          for the ARPACK wrapper in SciPy (:func:`~scipy.sparse.linalg.svds`)\n\n        ``'randomized'``\n          for the randomized algorithm due to Halko (2009).\n\n        ``'auto'`` (the default)\n          chooses automatically depending on the size of the problem.\n\n    random_state\n        Change to use different initial states for the optimization.\n    return_info\n        Only relevant when not passing an :class:`~anndata.AnnData`:\n        see \u201c**Returns**\u201d.\n    use_highly_variable\n        Whether to use highly variable genes only, stored in\n        ``.var['highly_variable']``.\n        By default uses them if they have been determined beforehand.\n    dtype\n        Numpy data type string to which to convert the result.\n    copy\n        If an :class:`~anndata.AnnData` is passed, determines whether a copy\n        is returned. Is ignored otherwise.\n    chunked\n        If ``True``, perform an incremental PCA on segments of ``chunk_size``.\n        The incremental PCA automatically zero centers and ignores settings of\n        ``random_seed`` and ``svd_solver``. If ``False``, perform a full PCA.\n    chunk_size\n        Number of observations to include in each chunk.\n        Required if ``chunked=True`` was passed.\n\n    Returns\n    -------\n    X_pca : :class:`scipy.sparse.spmatrix` or :class:`numpy.ndarray`\n        If `data` is array-like and ``return_info=False`` was passed,\n        this function only returns `X_pca`\u2026\n    adata : anndata.AnnData\n        \u2026otherwise if ``copy=True`` it returns or else adds fields to ``adata``:\n\n        ``.obsm['X_pca']``\n             PCA representation of data.\n\n        ``.varm['PCs']``\n             The principal components containing the loadings.\n\n        ``.uns['pca']['variance_ratio']``)\n             Ratio of explained variance.\n\n        ``.uns['pca']['variance']``\n             Explained variance, equivalent to the eigenvalues of the covariance matrix.\n    \"\"\"\n    # chunked calculation is not randomized, anyways\n    if svd_solver in {'auto', 'randomized'} and not chunked:\n        logg.info(\n            'Note that scikit-learn\\'s randomized PCA might not be exactly '\n            'reproducible across different computational platforms. For exact '\n            'reproducibility, choose `svd_solver=\\'arpack\\'.` This will likely '\n            'become the Scanpy default in the future.')\n\n    data_is_AnnData = isinstance(data, AnnData)\n    if data_is_AnnData:\n        adata = data.copy() if copy else data\n    else:\n        adata = AnnData(data)\n\n    logg.info('computing PCA with n_comps =', n_comps, r=True)\n\n    if adata.n_vars < n_comps:\n        n_comps = adata.n_vars - 1\n        logg.msg('reducing number of computed PCs to',\n               n_comps, 'as dim of data is only', adata.n_vars, v=4)\n\n    if use_highly_variable is True and 'highly_variable' not in adata.var.keys():\n        raise ValueError('Did not find adata.var[\\'highly_variable\\']. '\n                         'Either your data already only consists of highly-variable genes '\n                         'or consider running `pp.filter_genes_dispersion` first.')\n    if use_highly_variable is None:\n        use_highly_variable = True if 'highly_variable' in adata.var.keys() else False\n    if use_highly_variable:\n        logg.info('computing PCA on highly variable genes')\n    adata_comp = adata[:, adata.var['highly_variable']] if use_highly_variable else adata\n\n    if chunked:\n        if not zero_center or random_state or svd_solver != 'auto':\n            logg.msg('Ignoring zero_center, random_state, svd_solver', v=4)\n\n        from sklearn.decomposition import IncrementalPCA\n\n        X_pca = np.zeros((adata_comp.X.shape[0], n_comps), adata_comp.X.dtype)\n\n        pca_ = IncrementalPCA(n_components=n_comps)\n\n        for chunk, _, _ in adata_comp.chunked_X(chunk_size):\n            chunk = chunk.toarray() if issparse(chunk) else chunk\n            pca_.partial_fit(chunk)\n\n        for chunk, start, end in adata_comp.chunked_X(chunk_size):\n            chunk = chunk.toarray() if issparse(chunk) else chunk\n            X_pca[start:end] = pca_.transform(chunk)\n    else:\n        if zero_center is None:\n            zero_center = not issparse(adata_comp.X)\n        if zero_center:\n            from sklearn.decomposition import PCA\n            if issparse(adata_comp.X):\n                logg.msg('    as `zero_center=True`, '\n                       'sparse input is densified and may '\n                       'lead to huge memory consumption', v=4)\n                X = adata_comp.X.toarray()  # Copying the whole adata_comp.X here, could cause memory problems\n            else:\n                X = adata_comp.X\n            pca_ = PCA(n_components=n_comps, svd_solver=svd_solver, random_state=random_state)\n        else:\n            from sklearn.decomposition import TruncatedSVD\n            logg.msg('    without zero-centering: \\n'\n                   '    the explained variance does not correspond to the exact statistical defintion\\n'\n                   '    the first component, e.g., might be heavily influenced by different means\\n'\n                   '    the following components often resemble the exact PCA very closely', v=4)\n            pca_ = TruncatedSVD(n_components=n_comps, random_state=random_state)\n            X = adata_comp.X\n        X_pca = pca_.fit_transform(X)\n\n    if X_pca.dtype.descr != np.dtype(dtype).descr: X_pca = X_pca.astype(dtype)\n\n    if data_is_AnnData:\n        adata.obsm['X_pca'] = X_pca\n        if use_highly_variable:\n            adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps))\n            adata.varm['PCs'][adata.var['highly_variable']] = pca_.components_.T\n        else:\n            adata.varm['PCs'] = pca_.components_.T\n        adata.uns['pca'] = {}\n        adata.uns['pca']['variance'] = pca_.explained_variance_\n        adata.uns['pca']['variance_ratio'] = pca_.explained_variance_ratio_\n        logg.info('    finished', t=True)\n        logg.msg('and added\\n'\n                 '    \\'X_pca\\', the PCA coordinates (adata.obs)\\n'\n                 '    \\'PC1\\', \\'PC2\\', ..., the loadings (adata.var)\\n'\n                 '    \\'pca_variance\\', the variance / eigenvalues (adata.uns)\\n'\n                 '    \\'pca_variance_ratio\\', the variance ratio (adata.uns)', v=4)\n        return adata if copy else None\n    else:\n        logg.info('    finished', t=True)\n        if return_info:\n            return X_pca, pca_.components_, pca_.explained_variance_ratio_, pca_.explained_variance_\n        else:\n            return X_pca"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nnormalizes total counts per cell.", "response": "def normalize_per_cell(\n    data,\n    counts_per_cell_after=None,\n    counts_per_cell=None,\n    key_n_counts=None,\n    copy=False,\n    layers=[],\n    use_rep=None,\n    min_counts=1,\n) -> Optional[AnnData]:\n    \"\"\"Normalize total counts per cell.\n\n    .. warning::\n        .. deprecated:: 1.3.7\n            Use :func:`~scanpy.api.pp.normalize_total` instead.\n            The new function is equivalent to the present\n            function, except that\n\n            * the new function doesn't filter cells based on `min_counts`,\n              use :func:`~scanpy.api.pp.filter_cells` if filtering is needed.\n            * some arguments were renamed\n            * `copy` is replaced by `inplace`\n\n    Normalize each cell by total counts over all genes, so that every cell has\n    the same total count after normalization.\n\n    Similar functions are used, for example, by Seurat [Satija15]_, Cell Ranger\n    [Zheng17]_ or SPRING [Weinreb17]_.\n\n    Parameters\n    ----------\n    data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes.\n    counts_per_cell_after : `float` or `None`, optional (default: `None`)\n        If `None`, after normalization, each cell has a total count equal\n        to the median of the *counts_per_cell* before normalization.\n    counts_per_cell : `np.array`, optional (default: `None`)\n        Precomputed counts per cell.\n    key_n_counts : `str`, optional (default: `'n_counts'`)\n        Name of the field in `adata.obs` where the total counts per cell are\n        stored.\n    copy : `bool`, optional (default: `False`)\n        If an :class:`~anndata.AnnData` is passed, determines whether a copy\n        is returned.\n    min_counts : `int`, optional (default: 1)\n        Cells with counts less than `min_counts` are filtered out during\n        normalization.\n\n    Returns\n    -------\n    Returns or updates `adata` with normalized version of the original\n    `adata.X`, depending on `copy`.\n\n    Examples\n    --------\n    >>> adata = AnnData(\n    >>>     data=np.array([[1, 0], [3, 0], [5, 6]]))\n    >>> print(adata.X.sum(axis=1))\n    [  1.   3.  11.]\n    >>> sc.pp.normalize_per_cell(adata)\n    >>> print(adata.obs)\n    >>> print(adata.X.sum(axis=1))\n       n_counts\n    0       1.0\n    1       3.0\n    2      11.0\n    [ 3.  3.  3.]\n    >>> sc.pp.normalize_per_cell(adata, counts_per_cell_after=1,\n    >>>                          key_n_counts='n_counts2')\n    >>> print(adata.obs)\n    >>> print(adata.X.sum(axis=1))\n       n_counts  n_counts2\n    0       1.0        3.0\n    1       3.0        3.0\n    2      11.0        3.0\n    [ 1.  1.  1.]\n    \"\"\"\n    if key_n_counts is None: key_n_counts = 'n_counts'\n    if isinstance(data, AnnData):\n        logg.msg('normalizing by total count per cell', r=True)\n        adata = data.copy() if copy else data\n        if counts_per_cell is None:\n            cell_subset, counts_per_cell = materialize_as_ndarray(\n                        filter_cells(adata.X, min_counts=min_counts))\n            adata.obs[key_n_counts] = counts_per_cell\n            adata._inplace_subset_obs(cell_subset)\n            counts_per_cell=counts_per_cell[cell_subset]\n        normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell)\n\n        layers = adata.layers.keys() if layers == 'all' else layers\n        if use_rep == 'after':\n            after = counts_per_cell_after\n        elif use_rep == 'X':\n            after = np.median(counts_per_cell[cell_subset])\n        elif use_rep is None:\n            after = None\n        else: raise ValueError('use_rep should be \"after\", \"X\" or None')\n        for layer in layers:\n            subset, counts = filter_cells(adata.layers[layer],\n                    min_counts=min_counts)\n            temp = normalize_per_cell(adata.layers[layer], after, counts, copy=True)\n            adata.layers[layer] = temp\n\n        logg.msg('    finished', t=True, end=': ')\n        logg.msg('normalized adata.X and added', no_indent=True)\n        logg.msg('    \\'{}\\', counts per cell before normalization (adata.obs)'\n            .format(key_n_counts))\n        return adata if copy else None\n    # proceed with data matrix\n    X = data.copy() if copy else data\n    if counts_per_cell is None:\n        if copy == False:\n            raise ValueError('Can only be run with copy=True')\n        cell_subset, counts_per_cell = filter_cells(X, min_counts=min_counts)\n        X = X[cell_subset]\n        counts_per_cell = counts_per_cell[cell_subset]\n    if counts_per_cell_after is None:\n        counts_per_cell_after = np.median(counts_per_cell)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        counts_per_cell += counts_per_cell == 0\n        counts_per_cell /= counts_per_cell_after\n        if not issparse(X): X /= materialize_as_ndarray(counts_per_cell[:, np.newaxis])\n        else: sparsefuncs.inplace_row_scale(X, 1/counts_per_cell)\n    return X if copy else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef normalize_per_cell_weinreb16_deprecated(\n    X,\n    max_fraction=1,\n    mult_with_mean=False,\n) -> np.ndarray:\n    \"\"\"Normalize each cell [Weinreb17]_.\n\n    This is a deprecated version. See `normalize_per_cell` instead.\n\n    Normalize each cell by UMI count, so that every cell has the same total\n    count.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Expression matrix. Rows correspond to cells and columns to genes.\n    max_fraction : float, optional\n        Only use genes that make up more than max_fraction of the total\n        reads in every cell.\n    mult_with_mean: bool, optional\n        Multiply the result with the mean of total counts.\n\n    Returns\n    -------\n    Normalized version of the original expression matrix.\n    \"\"\"\n    if max_fraction < 0 or max_fraction > 1:\n        raise ValueError('Choose max_fraction between 0 and 1.')\n\n    counts_per_cell = X.sum(1).A1 if issparse(X) else X.sum(1)\n    gene_subset = np.all(X <= counts_per_cell[:, None] * max_fraction, axis=0)\n    if issparse(X): gene_subset = gene_subset.A1\n    tc_include = X[:, gene_subset].sum(1).A1 if issparse(X) else X[:, gene_subset].sum(1)\n\n    X_norm = X.multiply(csr_matrix(1/tc_include[:, None])) if issparse(X) else X / tc_include[:, None]\n    if mult_with_mean:\n        X_norm *= np.mean(counts_per_cell)\n\n    return X_norm", "response": "This is a deprecated version. See normalize_per_cell."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregressing out unwanted sources of variation.", "response": "def regress_out(adata, keys, n_jobs=None, copy=False) -> Optional[AnnData]:\n    \"\"\"Regress out unwanted sources of variation.\n\n    Uses simple linear regression. This is inspired by Seurat's `regressOut`\n    function in R [Satija15].\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        The annotated data matrix.\n    keys : `str` or list of `str`\n        Keys for observation annotation on which to regress on.\n    n_jobs : `int` or `None`, optional. If None is given, then the n_jobs seting is used (default: `None`)\n        Number of jobs for parallel computation.\n    copy : `bool`, optional (default: `False`)\n        If an :class:`~anndata.AnnData` is passed, determines whether a copy\n        is returned.\n\n    Returns\n    -------\n    Depending on `copy` returns or updates `adata` with the corrected data matrix.\n    \"\"\"\n    logg.info('regressing out', keys, r=True)\n    if issparse(adata.X):\n        logg.info('    sparse input is densified and may '\n                  'lead to high memory use')\n    adata = adata.copy() if copy else adata\n    if isinstance(keys, str):\n        keys = [keys]\n\n    if issparse(adata.X):\n        adata.X = adata.X.toarray()\n\n    n_jobs = sett.n_jobs if n_jobs is None else n_jobs\n\n    # regress on a single categorical variable\n    sanitize_anndata(adata)\n    variable_is_categorical = False\n    if keys[0] in adata.obs_keys() and is_categorical_dtype(adata.obs[keys[0]]):\n        if len(keys) > 1:\n            raise ValueError(\n                'If providing categorical variable, '\n                'only a single one is allowed. For this one '\n                'we regress on the mean for each category.')\n        logg.msg('... regressing on per-gene means within categories')\n        regressors = np.zeros(adata.X.shape, dtype='float32')\n        for category in adata.obs[keys[0]].cat.categories:\n            mask = (category == adata.obs[keys[0]]).values\n            for ix, x in enumerate(adata.X.T):\n                regressors[mask, ix] = x[mask].mean()\n        variable_is_categorical = True\n    # regress on one or several ordinal variables\n    else:\n        # create data frame with selected keys (if given)\n        if keys:\n            regressors = adata.obs[keys]\n        else:\n            regressors = adata.obs.copy()\n\n        # add column of ones at index 0 (first column)\n        regressors.insert(0, 'ones', 1.0)\n\n    len_chunk = np.ceil(min(1000, adata.X.shape[1]) / n_jobs).astype(int)\n    n_chunks = np.ceil(adata.X.shape[1] / len_chunk).astype(int)\n\n    tasks = []\n    # split the adata.X matrix by columns in chunks of size n_chunk (the last chunk could be of smaller\n    # size than the others)\n    chunk_list = np.array_split(adata.X, n_chunks, axis=1)\n    if variable_is_categorical:\n        regressors_chunk = np.array_split(regressors, n_chunks, axis=1)\n    for idx, data_chunk in enumerate(chunk_list):\n        # each task is a tuple of a data_chunk eg. (adata.X[:,0:100]) and\n        # the regressors. This data will be passed to each of the jobs.\n        if variable_is_categorical:\n            regres = regressors_chunk[idx]\n        else:\n            regres = regressors\n        tasks.append(tuple((data_chunk, regres, variable_is_categorical)))\n\n    if n_jobs > 1 and n_chunks > 1:\n        import multiprocessing\n        pool = multiprocessing.Pool(n_jobs)\n        res = pool.map_async(_regress_out_chunk, tasks).get(9999999)\n        pool.close()\n\n    else:\n        res = list(map(_regress_out_chunk, tasks))\n\n    # res is a list of vectors (each corresponding to a regressed gene column).\n    # The transpose is needed to get the matrix in the shape needed\n    adata.X = np.vstack(res).T.astype(adata.X.dtype)\n    logg.info('    finished', t=True)\n    return adata if copy else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef scale(data, zero_center=True, max_value=None, copy=False) -> Optional[AnnData]:\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        # need to add the following here to make inplace logic work\n        if zero_center and issparse(adata.X):\n            logg.msg(\n                '... scale_data: as `zero_center=True`, sparse input is '\n                'densified and may lead to large memory consumption')\n            adata.X = adata.X.toarray()\n        scale(adata.X, zero_center=zero_center, max_value=max_value, copy=False)\n        return adata if copy else None\n    X = data.copy() if copy else data  # proceed with the data matrix\n    zero_center = zero_center if zero_center is not None else False if issparse(X) else True\n    if not zero_center and max_value is not None:\n        logg.msg(\n            '... scale_data: be careful when using `max_value` without `zero_center`',\n            v=4)\n    if max_value is not None:\n        logg.msg('... clipping at max_value', max_value)\n    if zero_center and issparse(X):\n        logg.msg('... scale_data: as `zero_center=True`, sparse input is '\n                 'densified and may lead to large memory consumption, returning copy',\n                 v=4)\n        X = X.toarray()\n        copy = True\n    _scale(X, zero_center)\n    if max_value is not None: X[X > max_value] = max_value\n    return X if copy else None", "response": "Scale data to unit variance and zero mean."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef subsample(data, fraction=None, n_obs=None, random_state=0, copy=False) -> Optional[AnnData]:\n    np.random.seed(random_state)\n    old_n_obs = data.n_obs if isinstance(data, AnnData) else data.shape[0]\n    if n_obs is not None:\n        new_n_obs = n_obs\n    elif fraction is not None:\n        if fraction > 1 or fraction < 0:\n            raise ValueError('`fraction` needs to be within [0, 1], not {}'\n                             .format(fraction))\n        new_n_obs = int(fraction * old_n_obs)\n        logg.msg('... subsampled to {} data points'.format(new_n_obs))\n    else:\n        raise ValueError('Either pass `n_obs` or `fraction`.')\n    obs_indices = np.random.choice(old_n_obs, size=new_n_obs, replace=False)\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        adata._inplace_subset_obs(obs_indices)\n        return adata if copy else None\n    else:\n        X = data\n        return X[obs_indices], obs_indices", "response": "Subsample the data matrix to a fraction of the number of observations."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats time in seconds.", "response": "def _sec_to_str(t):\n    \"\"\"Format time in seconds.\n\n    Parameters\n    ----------\n    t : int\n        Time in seconds.\n    \"\"\"\n    from functools import reduce\n    return \"%d:%02d:%02d.%02d\" % \\\n        reduce(lambda ll, b: divmod(ll[0], b) + ll[1:],\n               [(t*100,), 100, 60, 60])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef umap(\n    adata,\n    min_dist=0.5,\n    spread=1.0,\n    n_components=2,\n    maxiter=None,\n    alpha=1.0,\n    gamma=1.0,\n    negative_sample_rate=5,\n    init_pos='spectral',\n    random_state=0,\n    a=None,\n    b=None,\n    copy=False,\n):\n    \"\"\"Embed the neighborhood graph using UMAP [McInnes18]_.\n\n    UMAP (Uniform Manifold Approximation and Projection) is a manifold learning\n    technique suitable for visualizing high-dimensional data. Besides tending to\n    be faster than tSNE, it optimizes the embedding such that it best reflects\n    the topology of the data, which we represent throughout Scanpy using a\n    neighborhood graph. tSNE, by contrast, optimizes the distribution of\n    nearest-neighbor distances in the embedding such that these best match the\n    distribution of distances in the high-dimensional space.  We use the\n    implementation of `umap-learn <https://github.com/lmcinnes/umap>`__\n    [McInnes18]_. For a few comparisons of UMAP with tSNE, see this `preprint\n    <https://doi.org/10.1101/298430>`__.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    min_dist : `float`, optional (default: 0.5)\n        The effective minimum distance between embedded points. Smaller values\n        will result in a more clustered/clumped embedding where nearby points on\n        the manifold are drawn closer together, while larger values will result\n        on a more even dispersal of points. The value should be set relative to\n        the ``spread`` value, which determines the scale at which embedded\n        points will be spread out. The default of in the `umap-learn` package is\n        0.1.\n    spread : `float` (optional, default 1.0)\n        The effective scale of embedded points. In combination with `min_dist`\n        this determines how clustered/clumped the embedded points are.\n    n_components : `int`, optional (default: 2)\n        The number of dimensions of the embedding.\n    maxiter : `int`, optional (default: `None`)\n        The number of iterations (epochs) of the optimization. Called `n_epochs`\n        in the original UMAP.\n    alpha : `float`, optional (default: 1.0)\n        The initial learning rate for the embedding optimization.\n    gamma : `float` (optional, default 1.0)\n        Weighting applied to negative samples in low dimensional embedding\n        optimization. Values higher than one will result in greater weight\n        being given to negative samples.\n    negative_sample_rate : `int` (optional, default 5)\n        The number of negative edge/1-simplex samples to use per positive\n        edge/1-simplex sample in optimizing the low dimensional embedding.\n    init_pos : `string` or `np.array`, optional (default: 'spectral')\n        How to initialize the low dimensional embedding. Called `init` in the\n        original UMAP.\n        Options are:\n\n        * Any key for `adata.obsm`.\n        * 'paga': positions from :func:`~scanpy.api.pl.paga`.\n        * 'spectral': use a spectral embedding of the graph.\n        * 'random': assign initial embedding positions at random.\n        * A numpy array of initial embedding positions.\n    random_state : `int`, `RandomState` or `None`, optional (default: 0)\n        If `int`, `random_state` is the seed used by the random number generator;\n        If `RandomState`, `random_state` is the random number generator;\n        If `None`, the random number generator is the `RandomState` instance used\n        by `np.random`.\n    a : `float` (optional, default `None`)\n        More specific parameters controlling the embedding. If `None` these\n        values are set automatically as determined by `min_dist` and\n        `spread`.\n    b : `float` (optional, default `None`)\n        More specific parameters controlling the embedding. If `None` these\n        values are set automatically as determined by `min_dist` and\n        `spread`.\n    copy : `bool` (default: `False`)\n        Return a copy instead of writing to adata.\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n\n    **X_umap** : `adata.obsm` field\n        UMAP coordinates of data.\n    \"\"\"\n    adata = adata.copy() if copy else adata\n    if 'neighbors' not in adata.uns:\n        raise ValueError(\n            'Did not find \\'neighbors/connectivities\\'. Run `sc.pp.neighbors` first.')\n    logg.info('computing UMAP', r=True)\n    if ('params' not in adata.uns['neighbors']\n        or adata.uns['neighbors']['params']['method'] != 'umap'):\n        logg.warn('neighbors/connectivities have not been computed using umap')\n    from umap.umap_ import find_ab_params, simplicial_set_embedding\n    if a is None or b is None:\n        a, b = find_ab_params(spread, min_dist)\n    else:\n        a = a\n        b = b\n    if init_pos in adata.obsm.keys():\n        init_coords = adata.obsm[init_pos]\n    elif init_pos == 'paga':\n        init_coords = get_init_pos_from_paga(adata, random_state=random_state)\n    else:\n        init_coords = init_pos\n    from sklearn.utils import check_random_state\n    random_state = check_random_state(random_state)\n    n_epochs = 0 if maxiter is None else maxiter\n    verbosity = _VERBOSITY_LEVELS_FROM_STRINGS.get(settings.verbosity, settings.verbosity)\n    neigh_params = adata.uns['neighbors']['params']\n    X = choose_representation(adata, neigh_params.get('use_rep', None), neigh_params.get('n_pcs', None))\n    X_umap = simplicial_set_embedding(\n        X,\n        adata.uns['neighbors']['connectivities'].tocoo(),\n        n_components,\n        alpha,\n        a,\n        b,\n        gamma,\n        negative_sample_rate,\n        n_epochs,\n        init_coords,\n        random_state,\n        neigh_params.get('metric', 'euclidean'),\n        neigh_params.get('metric_kwds', {}),\n        verbose=max(0, verbosity-3))\n    adata.obsm['X_umap'] = X_umap  # annotate samples with UMAP coordinates\n    logg.info('    finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\\n')\n    logg.hint('added\\n'\n              '    \\'X_umap\\', UMAP coordinates (adata.obsm)')\n    return adata if copy else None", "response": "Embed the neighborhood graph using UMAP."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sandbag(\n        adata,\n        annotation,\n        gene_names,\n        sample_names,\n        fraction=0.65,\n        filter_genes=None,\n        filter_samples=None):\n    \"\"\"Generate pairs of genes [Scialdone15]_ [Fechtner18]_.\n\n    Calculates the pairs of genes serving as marker pairs for each phase,\n    based on a matrix of gene counts and an annotation of known phases.\n\n    This reproduces the approach of [Scialdone15]_ in the implementation of\n    [Fechtner18]_.\n\n    More information and bug reports `here\n    <https://github.com/rfechtner/pypairs>`__.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        The annotated data matrix.\n    categories : `dict`\n        Dictionary of lists, i.e. {phase: [sample, ...]},\n        containing annotation of samples to their phase\n    gene_names: `list`\n        List of genes.\n    sample_names: `list`\n        List of samples.\n    fraction : `float`, optional (default: 0.5)\n        Fraction to be used as threshold.\n    filter_genes : `list` or `None`, optional (default: `None`)\n        Genes for sampling the reference set. Default is all genes.\n    filter_samples : `list` or `None`, optional (default: `None`)\n        Cells for sampling the reference set. Default is all samples.\n\n    Returns\n    -------\n    `dict` of `list` of `tuple`, i.e.\n    {phase: [(Gene1, Gene2), ...]},\n    containing marker pairs per phase\n    \"\"\"\n    try:\n        from pypairs import __version__ as pypairsversion\n        from distutils.version import LooseVersion\n\n        if LooseVersion(pypairsversion) < LooseVersion(\"v3.0.9\"):\n            raise ImportError('Please only use `pypairs` >= v3.0.9 ')\n    except ImportError:\n        raise ImportError('You need to install the package `pypairs`.')\n\n\n    from pypairs.pairs import sandbag\n    from . import settings\n    from pypairs import settings as pp_settings\n\n    pp_settings.verbosity = settings.verbosity\n    pp_settings.n_jobs = settings.n_jobs\n    pp_settings.writedir = settings.writedir\n    pp_settings.cachedir = settings.cachedir\n    pp_settings.logfile = settings.logfile\n\n    return sandbag(\n        data = adata,\n        annotation = annotation,\n        gene_names = gene_names,\n        sample_names = sample_names,\n        fraction = fraction,\n        filter_genes = filter_genes,\n        filter_samples = filter_samples\n    )", "response": "Generate a sequence of genes serving as marker pairs per phase."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cyclone(\n        adata,\n        marker_pairs,\n        gene_names,\n        sample_names,\n        iterations=1000,\n        min_iter=100,\n        min_pairs=50):\n    \"\"\"Assigns scores and predicted class to observations [Scialdone15]_ [Fechtner18]_.\n\n    Calculates scores for each observation and each phase and assigns prediction\n    based on marker pairs indentified by sandbag.\n\n    This reproduces the approach of [Scialdone15]_ in the implementation of\n    [Fechtner18]_.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        The annotated data matrix.\n    marker_pairs : `dict`\n        Dictionary of marker pairs. See :func:`~scanpy.api.sandbag` output.\n    gene_names: `list`\n        List of genes.\n    sample_names: `list`\n        List of samples.\n    iterations : `int`, optional (default: 1000)\n        An integer scalar specifying the number of\n        iterations for random sampling to obtain a cycle score.\n    min_iter : `int`, optional (default: 100)\n        An integer scalar specifying the minimum number of iterations\n        for score estimation\n    min_pairs : `int`, optional (default: 50)\n        An integer scalar specifying the minimum number of iterations\n        for score estimation\n\n    Returns\n    -------\n    A :class:`~pandas.DataFrame` with samples as index and categories as columns with scores for each category for each\n    sample and a additional column with the name of the max scoring category for each sample.\n\n    If marker pairs contain only the cell cycle categories G1, S and G2M an additional column\n    ``pypairs_cc_prediction`` will be added. Where category S is assigned to samples where G1 and G2M score are\n    below 0.5.\n    \"\"\"\n    try:\n        from pypairs import __version__ as pypairsversion\n        from distutils.version import LooseVersion\n\n        if LooseVersion(pypairsversion) < LooseVersion(\"v3.0.9\"):\n            raise ImportError('Please only use `pypairs` >= v3.0.9 ')\n    except ImportError:\n        raise ImportError('You need to install the package `pypairs`.')\n\n\n    from pypairs.pairs import cyclone\n    from . import settings\n    from pypairs import settings as pp_settings\n\n    pp_settings.verbosity = settings.verbosity\n    pp_settings.n_jobs = settings.n_jobs\n    pp_settings.writedir = settings.writedir\n    pp_settings.cachedir = settings.cachedir\n    pp_settings.logfile = settings.logfile\n\n    return cyclone(\n        data = adata,\n        marker_pairs = marker_pairs,\n        gene_names = gene_names,\n        sample_names = sample_names,\n        iterations = iterations,\n        min_iter = min_iter,\n        min_pairs = min_pairs\n    )", "response": "This function creates a new object that can be used to clone the data from a set of genes and samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dca(adata,\n        mode='denoise',\n        ae_type='zinb-conddisp',\n        normalize_per_cell=True,\n        scale=True,\n        log1p=True,\n        # network args\n        hidden_size=(64, 32, 64),\n        hidden_dropout=0.,\n        batchnorm=True,\n        activation='relu',\n        init='glorot_uniform',\n        network_kwds={},\n        # training args\n        epochs=300,\n        reduce_lr=10,\n        early_stop=15,\n        batch_size=32,\n        optimizer='rmsprop',\n        random_state=0,\n        threads=None,\n        verbose=False,\n        training_kwds={},\n        return_model=False,\n        return_info=False,\n        copy=False\n        ):\n    \"\"\"Deep count autoencoder [Eraslan18]_.\n\n    Fits a count autoencoder to the raw count data given in the anndata object\n    in order to denoise the data and to capture hidden representation of\n    cells in low dimensions. Type of the autoencoder and return values are\n    determined by the parameters.\n    \n    .. note::\n        More information and bug reports `here <https://github.com/theislab/dca>`__.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        An anndata file with `.raw` attribute representing raw counts.\n    mode : `str`, optional. `denoise`(default), or `latent`.\n        `denoise` overwrites `adata.X` with denoised expression values.\n        In `latent` mode DCA adds `adata.obsm['X_dca']` to given adata\n        object. This matrix represent latent representation of cells via DCA.\n    ae_type : `str`, optional. `zinb-conddisp`(default), `zinb`, `nb-conddisp` or `nb`.\n        Type of the autoencoder. Return values and the architecture is\n        determined by the type e.g. `nb` does not provide dropout\n        probabilities. Types that end with \"-conddisp\", assumes that dispersion is mean dependant.\n    normalize_per_cell : `bool`, optional. Default: `True`.\n        If true, library size normalization is performed using\n        the `sc.pp.normalize_per_cell` function in Scanpy and saved into adata\n        object. Mean layer is re-introduces library size differences by\n        scaling the mean value of each cell in the output layer. See the\n        manuscript for more details.\n    scale : `bool`, optional. Default: `True`.\n        If true, the input of the autoencoder is centered using\n        `sc.pp.scale` function of Scanpy. Note that the output is kept as raw\n        counts as loss functions are designed for the count data.\n    log1p : `bool`, optional. Default: `True`.\n        If true, the input of the autoencoder is log transformed with a\n        pseudocount of one using `sc.pp.log1p` function of Scanpy.\n    hidden_size : `tuple` or `list`, optional. Default: (64, 32, 64).\n        Width of hidden layers.\n    hidden_dropout : `float`, `tuple` or `list`, optional. Default: 0.0.\n        Probability of weight dropout in the autoencoder (per layer if list\n        or tuple).\n    batchnorm : `bool`, optional. Default: `True`.\n        If true, batch normalization is performed.\n    activation : `str`, optional. Default: `relu`.\n        Activation function of hidden layers.\n    init : `str`, optional. Default: `glorot_uniform`.\n        Initialization method used to initialize weights.\n    network_kwds : `dict`, optional.\n        Additional keyword arguments for the autoencoder.\n    epochs : `int`, optional. Default: 300.\n        Number of total epochs in training.\n    reduce_lr : `int`, optional. Default: 10.\n        Reduces learning rate if validation loss does not improve in given number of epochs.\n    early_stop : `int`, optional. Default: 15.\n        Stops training if validation loss does not improve in given number of epochs.\n    batch_size : `int`, optional. Default: 32.\n        Number of samples in the batch used for SGD.\n    optimizer : `str`, optional. Default: \"rmsprop\".\n        Type of optimization method used for training.\n    random_state : `int`, optional. Default: 0.\n        Seed for python, numpy and tensorflow.\n    threads : `int` or None, optional. Default: None\n        Number of threads to use in training. All cores are used by default.\n    verbose : `bool`, optional. Default: `False`.\n        If true, prints additional information about training and architecture.\n    training_kwds : `dict`, optional.\n        Additional keyword arguments for the training process.\n    return_model : `bool`, optional. Default: `False`.\n        If true, trained autoencoder object is returned. See \"Returns\".\n    return_info : `bool`, optional. Default: `False`.\n        If true, all additional parameters of DCA are stored in `adata.obsm` such as dropout\n        probabilities (obsm['X_dca_dropout']) and estimated dispersion values\n        (obsm['X_dca_dispersion']), in case that autoencoder is of type\n        zinb or zinb-conddisp.\n    copy : `bool`, optional. Default: `False`.\n        If true, a copy of anndata is returned.\n\n    Returns\n    -------\n    If `copy` is true and `return_model` is false, AnnData object is returned.\n\n    In \"denoise\" mode, `adata.X` is overwritten with the denoised values. In \"latent\" mode, latent\\\n    low dimensional representation of cells are stored in `adata.obsm['X_dca']` and `adata.X`\\\n    is not modified. Note that these values are not corrected for library size effects.\n\n    If `return_info` is true, all estimated distribution parameters are stored in AnnData such as:\n\n    - `.obsm[\"X_dca_dropout\"]` which is the mixture coefficient (pi) of the zero component\\\n    in ZINB, i.e. dropout probability (only if `ae_type` is `zinb` or `zinb-conddisp`).\n\n    - `.obsm[\"X_dca_dispersion\"]` which is the dispersion parameter of NB.\n\n    - `.uns[\"dca_loss_history\"]` which stores the loss history of the training. See `.history`\\\n    attribute of Keras History class for mode details.\n\n    Finally, the raw counts are stored in `.raw` attribute of AnnData object.\n\n    If `return_model` is given, trained model is returned. When both `copy` and `return_model`\\\n    are true, a tuple of anndata and model is returned in that order.\n    \"\"\"\n\n    try:\n        from dca.api import dca\n    except ImportError:\n        raise ImportError('Please install dca package (>= 0.2.1) via `pip install dca`')\n\n    return dca(adata,\n        mode=mode,\n        ae_type=ae_type,\n        normalize_per_cell=normalize_per_cell,\n        scale=scale,\n        log1p=log1p,\n        hidden_size=hidden_size,\n        hidden_dropout=hidden_dropout,\n        batchnorm=batchnorm,\n        activation=activation,\n        init=init,\n        network_kwds=network_kwds,\n        epochs=epochs,\n        reduce_lr=reduce_lr,\n        early_stop=early_stop,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        random_state=random_state,\n        threads=threads,\n        verbose=verbose,\n        training_kwds=training_kwds,\n        return_model=return_model)", "response": "DCA algorithm for the count autoencoder."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef score_genes(\n        adata,\n        gene_list,\n        ctrl_size=50,\n        gene_pool=None,\n        n_bins=25,\n        score_name='score',\n        random_state=0,\n        copy=False,\n        use_raw=False):  # we use the scikit-learn convention of calling the seed \"random_state\"\n    \"\"\"Score a set of genes [Satija15]_.\n\n    The score is the average expression of a set of genes subtracted with the\n    average expression of a reference set of genes. The reference set is\n    randomly sampled from the `gene_pool` for each binned expression value.\n\n    This reproduces the approach in Seurat [Satija15]_ and has been implemented\n    for Scanpy by Davide Cittaro.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        The annotated data matrix.\n    gene_list : iterable\n        The list of gene names used for score calculation.\n    ctrl_size : `int`, optional (default: 50)\n        Number of reference genes to be sampled. If `len(gene_list)` is not too\n        low, you can set `ctrl_size=len(gene_list)`.\n    gene_pool : `list` or `None`, optional (default: `None`)\n        Genes for sampling the reference set. Default is all genes.\n    n_bins : `int`, optional (default: 25)\n        Number of expression level bins for sampling.\n    score_name : `str`, optional (default: `'score'`)\n        Name of the field to be added in `.obs`.\n    random_state : `int`, optional (default: 0)\n        The random seed for sampling.\n    copy : `bool`, optional (default: `False`)\n        Copy `adata` or modify it inplace.\n    use_raw : `bool`, optional (default: `False`)\n        Use `raw` attribute of `adata` if present.\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with an additional field\n    `score_name`.\n\n    Examples\n    --------\n    See this `notebook <https://github.com/theislab/scanpy_usage/tree/master/180209_cell_cycle>`__.\n    \"\"\"\n    logg.info('computing score \\'{}\\''.format(score_name), r=True)\n    adata = adata.copy() if copy else adata\n\n    if random_state:\n        np.random.seed(random_state)\n\n    gene_list_in_var = []\n    var_names = adata.raw.var_names if use_raw else adata.var_names\n    for gene in gene_list:\n        if gene in var_names:\n            gene_list_in_var.append(gene)\n        else:\n            logg.warn('gene: {} is not in adata.var_names and will be ignored'.format(gene))\n    gene_list = set(gene_list_in_var[:])\n\n    if not gene_pool:\n        gene_pool = list(var_names)\n    else:\n        gene_pool = [x for x in gene_pool if x in var_names]\n\n    # Trying here to match the Seurat approach in scoring cells.\n    # Basically we need to compare genes against random genes in a matched\n    # interval of expression.\n\n    _adata = adata.raw if use_raw else adata\n    # TODO: this densifies the whole data matrix for `gene_pool`\n    if issparse(_adata.X):\n        obs_avg = pd.Series(\n            np.nanmean(\n                _adata[:, gene_pool].X.toarray(), axis=0), index=gene_pool)  # average expression of genes\n    else:\n        obs_avg = pd.Series(\n            np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool)  # average expression of genes\n\n    obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor\n\n    n_items = int(np.round(len(obs_avg) / (n_bins - 1)))\n    obs_cut = obs_avg.rank(method='min') // n_items\n    control_genes = set()\n\n    # now pick `ctrl_size` genes from every cut\n    for cut in np.unique(obs_cut.loc[gene_list]):\n        r_genes = np.array(obs_cut[obs_cut == cut].index)\n        np.random.shuffle(r_genes)\n        control_genes.update(set(r_genes[:ctrl_size]))  # uses full r_genes if ctrl_size > len(r_genes)\n\n    # To index, we need a list - indexing implies an order.\n    control_genes = list(control_genes - gene_list)\n    gene_list = list(gene_list)\n\n\n    X_list = _adata[:, gene_list].X\n    if issparse(X_list): X_list = X_list.toarray()\n    X_control = _adata[:, control_genes].X\n    if issparse(X_control): X_control = X_control.toarray()\n    X_control = np.nanmean(X_control, axis=1)\n\n    if len(gene_list) == 0:\n        # We shouldn't even get here, but just in case\n        logg.hint(\n            'could not add \\n'\n            '    \\'{}\\', score of gene set (adata.obs)'.format(score_name))\n        return adata if copy else None\n    elif len(gene_list) == 1:\n        score = _adata[:, gene_list].X - X_control\n    else:\n        score = np.nanmean(X_list, axis=1) - X_control\n\n    adata.obs[score_name] = pd.Series(np.array(score).ravel(), index=adata.obs_names)\n\n    logg.info('    finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\\n')\n    logg.hint('added\\n'\n              '    \\'{}\\', score of gene set (adata.obs)'.format(score_name))\n    return adata if copy else None", "response": "Score a set of genes in the current language."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef score_genes_cell_cycle(\n        adata,\n        s_genes,\n        g2m_genes,\n        copy=False,\n        **kwargs):\n    \"\"\"Score cell cycle genes [Satija15]_.\n\n    Given two lists of genes associated to S phase and G2M phase, calculates\n    scores and assigns a cell cycle phase (G1, S or G2M). See\n    :func:`~scanpy.api.score_genes` for more explanation.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        The annotated data matrix.\n    s_genes : `list`\n        List of genes associated with S phase.\n    g2m_genes : `list`\n        List of genes associated with G2M phase.\n    copy : `bool`, optional (default: `False`)\n        Copy `adata` or modify it inplace.\n    **kwargs : optional keyword arguments\n        Are passed to :func:`~scanpy.api.score_genes`. `ctrl_size` is not\n        possible, as it's set as `min(len(s_genes), len(g2m_genes))`.\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n\n    **S_score** : `adata.obs`, dtype `object`\n        The score for S phase for each cell.\n    **G2M_score** : `adata.obs`, dtype `object`\n        The score for G2M phase for each cell.\n    **phase** : `adata.obs`, dtype `object`\n        The cell cycle phase (`S`, `G2M` or `G1`) for each cell.\n\n    See also\n    --------\n    score_genes\n\n    Examples\n    --------\n    See this `notebook <https://github.com/theislab/scanpy_usage/tree/master/180209_cell_cycle>`__.\n    \"\"\"\n    logg.info('calculating cell cycle phase')\n\n    adata = adata.copy() if copy else adata\n    ctrl_size = min(len(s_genes), len(g2m_genes))\n    # add s-score\n    score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs)\n    # add g2m-score\n    score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs)\n    scores = adata.obs[['S_score', 'G2M_score']]\n\n    # default phase is S\n    phase = pd.Series('S', index=scores.index)\n\n    # if G2M is higher than S, it's G2M\n    phase[scores.G2M_score > scores.S_score] = 'G2M'\n\n    # if all scores are negative, it's G1...\n    phase[np.all(scores < 0, axis=1)] = 'G1'\n\n    adata.obs['phase'] = phase\n    logg.hint('    \\'phase\\', cell cycle phase (adata.obs)')\n    return adata if copy else None", "response": "Score cell cycle genes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef magic(adata,\n          name_list=None,\n          k=10,\n          a=15,\n          t='auto',\n          n_pca=100,\n          knn_dist='euclidean',\n          random_state=None,\n          n_jobs=None,\n          verbose=False,\n          copy=None,\n          **kwargs):\n    \"\"\"Markov Affinity-based Graph Imputation of Cells (MAGIC) API [vanDijk18]_.\n\n    MAGIC is an algorithm for denoising and transcript recover of single cells\n    applied to single-cell sequencing data. MAGIC builds a graph from the data\n    and uses diffusion to smooth out noise and recover the data manifold.\n\n    More information and bug reports\n    `here <https://github.com/KrishnaswamyLab/MAGIC>`__. For help, visit\n    <https://krishnaswamylab.org/get-help>.\n\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        An anndata file with `.raw` attribute representing raw counts.\n    name_list : `list`, `'all_genes'`, or `'pca_only'`, optional (default: `'all_genes'`)\n        Denoised genes to return. Default is all genes, but this\n        may require a large amount of memory if the input data is sparse.\n    k : int, optional, default: 10\n        number of nearest neighbors on which to build kernel\n    a : int, optional, default: 15\n        sets decay rate of kernel tails.\n        If None, alpha decaying kernel is not used\n    t : int, optional, default: 'auto'\n        power to which the diffusion operator is powered.\n        This sets the level of diffusion. If 'auto', t is selected\n        according to the Procrustes disparity of the diffused data\n    n_pca : int, optional, default: 100\n        Number of principal components to use for calculating\n        neighborhoods. For extremely large datasets, using\n        n_pca < 20 allows neighborhoods to be calculated in\n        roughly log(n_samples) time.\n    knn_dist : string, optional, default: 'euclidean'\n        recommended values: 'euclidean', 'cosine', 'precomputed'\n        Any metric from `scipy.spatial.distance` can be used\n        distance metric for building kNN graph. If 'precomputed',\n        `data` should be an n_samples x n_samples distance or\n        affinity matrix\n    random_state : `int`, `numpy.RandomState` or `None`, optional (default: `None`)\n        Random seed. Defaults to the global `numpy` random number generator\n    n_jobs : `int` or None, optional. Default: None\n        Number of threads to use in training. All cores are used by default.\n    verbose : `bool`, `int` or `None`, optional (default: `sc.settings.verbosity`)\n        If `True` or an integer `>= 2`, print status messages.\n        If `None`, `sc.settings.verbosity` is used.\n    copy : `bool` or `None`, optional. Default: `None`.\n        If true, a copy of anndata is returned. If `None`, `copy` is True if\n        `genes` is not `'all_genes'` or `'pca_only'`. `copy` may only be False\n        if `genes` is `'all_genes'` or `'pca_only'`, as the resultant data\n        will otherwise have different column names from the input data.\n    kwargs : additional arguments to `magic.MAGIC`\n\n    Returns\n    -------\n    If `copy` is True, AnnData object is returned.\n\n    If `subset_genes` is not `all_genes`, PCA on MAGIC values of cells are stored in\n    `adata.obsm['X_magic']` and `adata.X` is not modified.\n\n    The raw counts are stored in `.raw` attribute of AnnData object.\n\n    Examples\n    --------\n    >>> import scanpy.api as sc\n    >>> import magic\n    >>> adata = sc.datasets.paul15()\n    >>> sc.pp.normalize_per_cell(adata)\n    >>> sc.pp.sqrt(adata)  # or sc.pp.log1p(adata)\n    >>> adata_magic = sc.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5)\n    >>> adata_magic.shape\n    (2730, 3)\n    >>> sc.pp.magic(adata, name_list='pca_only', k=5)\n    >>> adata.obsm['X_magic'].shape\n    (2730, 100)\n    >>> sc.pp.magic(adata, name_list='all_genes', k=5)\n    >>> adata.X.shape\n    (2730, 3451)\n    \"\"\"\n\n    try:\n        from magic import MAGIC\n    except ImportError:\n        raise ImportError(\n            'Please install magic package via `pip install --user '\n            'git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python`')\n\n    logg.info('computing PHATE', r=True)\n    needs_copy = not (name_list is None or\n                      (isinstance(name_list, str) and\n                       name_list in [\"all_genes\", \"pca_only\"]))\n    if copy is None:\n        copy = needs_copy\n    elif needs_copy and not copy:\n        raise ValueError(\n            \"Can only perform MAGIC in-place with `name_list=='all_genes' or \"\n            \"`name_list=='pca_only'` (got {}). Consider setting \"\n            \"`copy=True`\".format(name_list))\n    adata = adata.copy() if copy else adata\n    verbose = settings.verbosity if verbose is None else verbose\n    if isinstance(verbose, (str, int)):\n        verbose = _settings_verbosity_greater_or_equal_than(2)\n    n_jobs = settings.n_jobs if n_jobs is None else n_jobs\n\n    X_magic = MAGIC(k=k,\n                    a=a,\n                    t=t,\n                    n_pca=n_pca,\n                    knn_dist=knn_dist,\n                    random_state=random_state,\n                    n_jobs=n_jobs,\n                    verbose=verbose,\n                    **kwargs).fit_transform(adata,\n                                            genes=name_list)\n    logg.info('    finished', time=True,\n              end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\\n')\n    # update AnnData instance\n    if name_list == \"pca_only\":\n        # special case - update adata.obsm with smoothed values\n        adata.obsm[\"X_magic\"] = X_magic.X\n        logg.hint('added\\n'\n                  '    \\'X_magic\\', PCA on MAGIC coordinates (adata.obsm)')\n    elif copy:\n        # just return X_magic\n        X_magic.raw = adata\n        adata = X_magic\n    else:\n        # replace data with smoothed data\n        adata.raw = adata\n        adata.X = X_magic.X\n\n    if copy:\n        return adata", "response": "Markov Affinity - based Graph Imputation of Cells."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a single - cell graph that is coarse - grained for the given set of groups and clusters.", "response": "def paga(\n        adata,\n        groups='louvain',\n        use_rna_velocity=False,\n        model='v1.2',\n        copy=False):\n    \"\"\"Mapping out the coarse-grained connectivity structures of complex manifolds [Wolf19]_.\n\n    By quantifying the connectivity of partitions (groups, clusters) of the\n    single-cell graph, partition-based graph abstraction (PAGA) generates a much\n    simpler abstracted graph (*PAGA graph*) of partitions, in which edge weights\n    represent confidence in the presence of connections. By tresholding this\n    confidence in :func:`~scanpy.pl.paga`, a much simpler representation of the\n    manifold data is obtained, which is nonetheless faithful to the topology of\n    the manifold.\n\n    The confidence should be interpreted as the ratio of the actual versus the\n    expected value of connetions under the null model of randomly connecting\n    partitions. We do not provide a p-value as this null model does not\n    precisely capture what one would consider \"connected\" in real data, hence it\n    strongly overestimates the expected value. See an extensive discussion of\n    this in [Wolf19]_.\n\n    .. note::\n        Note that you can use the result of :func:`~scanpy.pl.paga` in\n        :func:`~scanpy.tl.umap` and :func:`~scanpy.tl.draw_graph` via\n        `init_pos='paga'` to get single-cell embeddings that are typically more\n        faithful to the global topology.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        An annotated data matrix.\n    groups : key for categorical in `adata.obs`, optional (default: 'louvain')\n        You can pass your predefined groups by choosing any categorical\n        annotation of observations (`adata.obs`).\n    use_rna_velocity : `bool` (default: `False`)\n        Use RNA velocity to orient edges in the abstracted graph and estimate\n        transitions. Requires that `adata.uns` contains a directed single-cell\n        graph with key `['velocity_graph']`. This feature might be subject\n        to change in the future.\n    model : {'v1.2', 'v1.0'}, optional (default: 'v1.2')\n        The PAGA connectivity model.\n    copy : `bool`, optional (default: `False`)\n        Copy `adata` before computation and return a copy. Otherwise, perform\n        computation inplace and return `None`.\n\n    Returns\n    -------\n    **connectivities** : :class:`numpy.ndarray` (adata.uns['connectivities'])\n        The full adjacency matrix of the abstracted graph, weights correspond to\n        confidence in the connectivities of partitions.\n    **connectivities_tree** : :class:`scipy.sparse.csr_matrix` (adata.uns['connectivities_tree'])\n        The adjacency matrix of the tree-like subgraph that best explains\n        the topology.\n\n    Notes\n    -----\n    Together with a random walk-based distance measure\n    (e.g. :func:`scanpy.tl.dpt`) this generates a partial coordinatization of\n    data useful for exploring and explaining its variation.\n\n    See Also\n    --------\n    pl.paga\n    pl.paga_path\n    pl.paga_compare\n    \"\"\"\n    if 'neighbors' not in adata.uns:\n        raise ValueError(\n            'You need to run `pp.neighbors` first to compute a neighborhood graph.')\n    adata = adata.copy() if copy else adata\n    utils.sanitize_anndata(adata)\n    logg.info('running PAGA', reset=True)\n    paga = PAGA(adata, groups, model=model)\n    # only add if not present\n    if 'paga' not in adata.uns:\n        adata.uns['paga'] = {}\n    if not use_rna_velocity:\n        paga.compute_connectivities()\n        adata.uns['paga']['connectivities'] = paga.connectivities\n        adata.uns['paga']['connectivities_tree'] = paga.connectivities_tree\n        # adata.uns['paga']['expected_n_edges_random'] = paga.expected_n_edges_random\n        adata.uns[groups + '_sizes'] = np.array(paga.ns)\n    else:\n        paga.compute_transitions()\n        adata.uns['paga']['transitions_confidence'] = paga.transitions_confidence\n        # adata.uns['paga']['transitions_ttest'] = paga.transitions_ttest\n    adata.uns['paga']['groups'] = groups\n    logg.info('    finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\\n')\n    if use_rna_velocity:\n        logg.hint(\n            'added\\n'\n            '    \\'paga/transitions_confidence\\', connectivities adjacency (adata.uns)')\n        #    '    \\'paga/transitions_ttest\\', t-test on transitions (adata.uns)')\n    else:\n        logg.hint(\n            'added\\n'\n            '    \\'paga/connectivities\\', connectivities adjacency (adata.uns)\\n'\n            '    \\'paga/connectivities_tree\\', connectivities subtree (adata.uns)')\n    return adata if copy else None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the degree of each node in the abstracted graph.", "response": "def paga_degrees(adata) -> List[int]:\n    \"\"\"Compute the degree of each node in the abstracted graph.\n\n    Parameters\n    ----------\n    adata : AnnData\n        Annotated data matrix.\n\n    Returns\n    -------\n    List of degrees for each node.\n    \"\"\"\n    import networkx as nx\n    g = nx.Graph(adata.uns['paga']['connectivities'])\n    degrees = [d for _, d in g.degree(weight='weight')]\n    return degrees"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef paga_expression_entropies(adata) -> List[float]:\n    from scipy.stats import entropy\n    groups_order, groups_masks = utils.select_groups(\n        adata, key=adata.uns['paga']['groups'])\n    entropies = []\n    for mask in groups_masks:\n        X_mask = adata.X[mask].todense()\n        x_median = np.nanmedian(X_mask, axis=1,overwrite_input=True)\n        x_probs = (x_median - np.nanmin(x_median)) / (np.nanmax(x_median) - np.nanmin(x_median))\n        entropies.append(entropy(x_probs))\n    return entropies", "response": "Compute the median expression entropy for each node - group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompare paths in abstracted graphs in two datasets.", "response": "def paga_compare_paths(adata1, adata2,\n                       adjacency_key='connectivities', adjacency_key2=None):\n    \"\"\"Compare paths in abstracted graphs in two datasets.\n\n    Compute the fraction of consistent paths between leafs, a measure for the\n    topological similarity between graphs.\n\n    By increasing the verbosity to level 4 and 5, the paths that do not agree\n    and the paths that agree are written to the output, respectively.\n\n    The PAGA \"groups key\" needs to be the same in both objects.\n\n    Parameters\n    ----------\n    adata1, adata2 : AnnData\n        Annotated data matrices to compare.\n    adjacency_key : str\n        Key for indexing the adjacency matrices in `.uns['paga']` to be used in\n        adata1 and adata2.\n    adjacency_key2 : str, None\n        If provided, used for adata2.\n\n\n    Returns\n    -------\n    OrderedTuple with attributes ``n_steps`` (total number of steps in paths)\n    and ``frac_steps`` (fraction of consistent steps), ``n_paths`` and\n    ``frac_paths``.\n    \"\"\"\n    import networkx as nx\n    g1 = nx.Graph(adata1.uns['paga'][adjacency_key])\n    g2 = nx.Graph(adata2.uns['paga'][adjacency_key2 if adjacency_key2 is not None else adjacency_key])\n    leaf_nodes1 = [str(x) for x in g1.nodes() if g1.degree(x) == 1]\n    logg.msg('leaf nodes in graph 1: {}'.format(leaf_nodes1), v=5, no_indent=True)\n    paga_groups = adata1.uns['paga']['groups']\n    asso_groups1 = utils.identify_groups(adata1.obs[paga_groups].values,\n                                         adata2.obs[paga_groups].values)\n    asso_groups2 = utils.identify_groups(adata2.obs[paga_groups].values,\n                                         adata1.obs[paga_groups].values)\n    orig_names1 = adata1.obs[paga_groups].cat.categories\n    orig_names2 = adata2.obs[paga_groups].cat.categories\n\n    import itertools\n    n_steps = 0\n    n_agreeing_steps = 0\n    n_paths = 0\n    n_agreeing_paths = 0\n    # loop over all pairs of leaf nodes in the reference adata1\n    for (r, s) in itertools.combinations(leaf_nodes1, r=2):\n        r2, s2 = asso_groups1[r][0], asso_groups1[s][0]\n        orig_names = [orig_names1[int(i)] for i in [r, s]]\n        orig_names += [orig_names2[int(i)] for i in [r2, s2]]\n        logg.msg('compare shortest paths between leafs ({}, {}) in graph1 and ({}, {}) in graph2:'\n               .format(*orig_names), v=4, no_indent=True)\n        no_path1 = False\n        try:\n            path1 = [str(x) for x in nx.shortest_path(g1, int(r), int(s))]\n        except nx.NetworkXNoPath:\n            no_path1 = True\n        no_path2 = False\n        try:\n            path2 = [str(x) for x in nx.shortest_path(g2, int(r2), int(s2))]\n        except nx.NetworkXNoPath:\n            no_path2 = True\n        if no_path1 and no_path2:\n            # consistent behavior\n            n_paths += 1\n            n_agreeing_paths += 1\n            n_steps += 1\n            n_agreeing_steps += 1\n            logg.msg('there are no connecting paths in both graphs', v=5, no_indent=True)\n            continue\n        elif no_path1 or no_path2:\n            # non-consistent result\n            n_paths += 1\n            n_steps += 1\n            continue\n        if len(path1) >= len(path2):\n            path_mapped = [asso_groups1[l] for l in path1]\n            path_compare = path2\n            path_compare_id = 2\n            path_compare_orig_names = [[orig_names2[int(s)] for s in l] for l in path_compare]\n            path_mapped_orig_names = [[orig_names2[int(s)] for s in l] for l in path_mapped]\n        else:\n            path_mapped = [asso_groups2[l] for l in path2]\n            path_compare = path1\n            path_compare_id = 1\n            path_compare_orig_names = [[orig_names1[int(s)] for s in l] for l in path_compare]\n            path_mapped_orig_names = [[orig_names1[int(s)] for s in l] for l in path_mapped]\n        n_agreeing_steps_path = 0\n        ip_progress = 0\n        for il, l in enumerate(path_compare[:-1]):\n            for ip, p in enumerate(path_mapped):\n                if ip >= ip_progress and l in p:\n                    # check whether we can find the step forward of path_compare in path_mapped\n                    if (ip + 1 < len(path_mapped)\n                        and\n                        path_compare[il + 1] in path_mapped[ip + 1]):\n                        # make sure that a step backward leads us to the same value of l\n                        # in case we \"jumped\"\n                        logg.msg('found matching step ({} -> {}) at position {} in path{} and position {} in path_mapped'\n                               .format(l, path_compare_orig_names[il + 1], il, path_compare_id, ip), v=6)\n                        consistent_history = True\n                        for iip in range(ip, ip_progress, -1):\n                            if l not in path_mapped[iip - 1]:\n                                consistent_history = False\n                        if consistent_history:\n                            # here, we take one step further back (ip_progress - 1); it's implied that this\n                            # was ok in the previous step\n                            logg.msg('    step(s) backward to position(s) {} in path_mapped are fine, too: valid step'\n                                   .format(list(range(ip - 1, ip_progress - 2, -1))), v=6)\n                            n_agreeing_steps_path += 1\n                            ip_progress = ip + 1\n                            break\n        n_steps_path = len(path_compare) - 1\n        n_agreeing_steps += n_agreeing_steps_path\n        n_steps += n_steps_path\n        n_paths += 1\n        if n_agreeing_steps_path == n_steps_path: n_agreeing_paths += 1\n\n        # only for the output, use original names\n        path1_orig_names = [orig_names1[int(s)] for s in path1]\n        path2_orig_names = [orig_names2[int(s)] for s in path2]\n        logg.msg('      path1 = {},\\n'\n               'path_mapped = {},\\n'\n               '      path2 = {},\\n'\n               '-> n_agreeing_steps = {} / n_steps = {}.'\n               .format(path1_orig_names,\n                       [list(p) for p in path_mapped_orig_names],\n                       path2_orig_names,\n                       n_agreeing_steps_path, n_steps_path), v=5, no_indent=True)\n    Result = namedtuple('paga_compare_paths_result',\n                        ['frac_steps', 'n_steps', 'frac_paths', 'n_paths'])\n    return Result(frac_steps=n_agreeing_steps/n_steps if n_steps > 0 else np.nan,\n                  n_steps=n_steps if n_steps > 0 else np.nan,\n                  frac_paths=n_agreeing_paths/n_paths if n_steps > 0 else np.nan,\n                  n_paths=n_paths if n_steps > 0 else np.nan)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _calc_density(\n    x: np.ndarray,\n    y: np.ndarray,\n):\n    \"\"\"\n    Function to calculate the density of cells in an embedding.\n    \"\"\"    \n\n    # Calculate the point density\n    xy = np.vstack([x,y])\n    z = gaussian_kde(xy)(xy)\n\n    min_z = np.min(z)\n    max_z = np.max(z)\n\n    # Scale between 0 and 1\n    scaled_z = (z-min_z)/(max_z-min_z)\n\n    return(scaled_z)", "response": "Function to calculate the density of cells in an embedding."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the density of cells in an embedded space.", "response": "def embedding_density(\n    adata: AnnData,\n    basis: str,\n    *,\n    groupby: Optional[str] = None,\n    key_added: Optional[str] = None,\n    components: Union[str, Sequence[str]] = None\n):\n    \"\"\"Calculate the density of cells in an embedding (per condition)\n\n    Gaussian kernel density estimation is used to calculate the density of\n    cells in an embedded space. This can be performed per category over a\n    categorical cell annotation. The cell density can be plotted using the \n    `sc.pl.embedding_density()` function.\n\n    Note that density values are scaled to be between 0 and 1. Thus, the\n    density value at each cell is only comparable to other densities in \n    the same condition category.\n\n    This function was written by Sophie Tritschler and implemented into\n    Scanpy by Malte Luecken.\n    \n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    basis\n        The embedding over which the density will be calculated. This embedded\n        representation should be found in `adata.obsm['X_[basis]']``.\n    groupby\n        Keys for categorical observation/cell annotation for which densities\n        are calculated per category. Columns with up to ten categories are\n        accepted.\n    key_added\n        Name of the `.obs` covariate that will be added with the density\n        estimates.\n    components\n        The embedding dimensions over which the density should be calculated.\n        This is limited to two components.\n\n    Returns\n    -------\n    Updates `adata.obs` with an additional field specified by the `key_added`\n    parameter. This parameter defaults to `[basis]_density_[groupby]`, where\n    where `[basis]` is one of `umap`, `diffmap`, `pca`, `tsne`, or `draw_graph_fa`\n    and `[groupby]` denotes the parameter input.\n    Updates `adata.uns` with an additional field `[key_added]_params`.\n\n    Examples\n    --------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.umap(adata)\n    >>> sc.tl.embedding_density(adata, basis='umap', groupby='phase')\n    >>> sc.pl.embedding_density(adata, basis='umap', key='umap_density_phase', \n    ...                         group='G1')\n    >>> sc.pl.embedding_density(adata, basis='umap', key='umap_density_phase', \n    ...                         group='S')\n    \"\"\"\n    sanitize_anndata(adata) # to ensure that newly created covariates are categorical to test for categoy numbers\n\n    logg.info('computing density on \\'{}\\''.format(basis), r=True)\n\n    # Test user inputs\n    basis = basis.lower()\n    \n    if basis == 'fa':\n        basis = 'draw_graph_fa'\n\n    if 'X_'+basis not in adata.obsm_keys():\n        raise ValueError('Cannot find the embedded representation `adata.obsm[X_{!r}]`. '\n                         'Compute the embedding first.'.format(basis))\n\n    if components is None: components = '1,2'\n    if isinstance(components, str): components = components.split(',')\n    components = np.array(components).astype(int) - 1\n\n    if len(components) != 2:\n        raise ValueError('Please specify exactly 2 components, or `None`.')\n\n    if basis == 'diffmap': components += 1\n\n    if groupby is not None:\n        if groupby not in adata.obs:\n            raise ValueError('Could not find {!r} `.obs` column.'.format(groupby))\n\n        if adata.obs[groupby].dtype.name != 'category':\n            raise ValueError('{!r} column does not contain Categorical data'.format(groupby))\n    \n        if len(adata.obs[groupby].cat.categories) > 10:\n            raise ValueError('More than 10 categories in {!r} column.'.format(groupby))\n\n    # Define new covariate name\n    if key_added is not None:\n        density_covariate = key_added\n    elif groupby is not None:\n        density_covariate = basis+'_density_'+groupby\n    else:\n        density_covariate = basis+'_density'\n\n    # Calculate the densities over each category in the groupby column\n    if groupby is not None:\n        categories = adata.obs[groupby].cat.categories\n\n        density_values = np.zeros(adata.n_obs)\n        \n        for cat in categories:\n            cat_mask = adata.obs[groupby] == cat\n            embed_x = adata.obsm['X_'+basis][cat_mask, components[0]]\n            embed_y = adata.obsm['X_'+basis][cat_mask, components[1]]\n\n            dens_embed = _calc_density(embed_x, embed_y)\n            density_values[cat_mask] = dens_embed\n\n        adata.obs[density_covariate] = density_values\n        \n    # Calculate the density over the whole embedding without subsetting\n    else: #if groupby is None\n        embed_x = adata.obsm['X_'+basis][:, components[0]]\n        embed_y = adata.obsm['X_'+basis][:, components[1]]\n\n        adata.obs[density_covariate] = _calc_density(embed_x, embed_y)\n\n\n    # Reduce diffmap components for labeling\n    # Note: plot_scatter takes care of correcting diffmap components for plotting automatically\n    if basis != 'diffmap': components += 1\n\n    adata.uns[density_covariate+'_params'] = {'covariate':groupby, 'components':components.tolist()}\n    \n\n    logg.hint('added\\n'\n              '    \\'{}\\', densities (adata.obs)\\n'\n              '    \\'{}_params\\', parameter (adata.uns)'.format(density_covariate, density_covariate))\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mnn_correct(*datas, var_index=None, var_subset=None, batch_key='batch', index_unique='-',\n                batch_categories=None, k=20, sigma=1., cos_norm_in=True, cos_norm_out=True,\n                svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd',\n                do_concatenate=True, save_raw=False, n_jobs=None, **kwargs):\n    \"\"\"Correct batch effects by matching mutual nearest neighbors [Haghverdi18]_ [Kang18]_.\n\n    This uses the implementation of `mnnpy\n    <https://github.com/chriscainx/mnnpy>`__ [Kang18]_.\n\n    Depending on `do_concatenate`, returns matrices or `AnnData` objects in the\n    original order containing corrected expression values or a concatenated\n    matrix or AnnData object.\n\n    Be reminded that it is not advised to use the corrected data matrices for\n    differential expression testing.\n\n    More information and bug reports `here <https://github.com/chriscainx/mnnpy>`__.\n\n    Parameters\n    ----------\n    datas : `numpy.ndarray` or :class:`~anndata.AnnData`\n        Expression matrices or AnnData objects. Matrices should be shaped like\n        n_obs * n_vars (n_cell * n_gene) and have consistent number of\n        columns. AnnData objects should have same number of variables.\n    var_index : `list` or `None`, optional (default: None)\n        The index (list of str) of vars (genes). Necessary when using only a\n        subset of vars to perform MNN correction, and should be supplied with\n        var_subset. When datas are AnnData objects, var_index is ignored.\n    var_subset : `list` or `None`, optional (default: None)\n        The subset of vars (list of str) to be used when performing MNN\n        correction. Typically, a list of highly variable genes (HVGs). When set\n        to None, uses all vars.\n    batch_key : `str`, optional (default: 'batch')\n        The batch_key for AnnData.concatenate. Only valid when do_concatenate\n        and supplying AnnData objects.\n    index_unique : `str`, optional (default: '-')\n        The index_unique for AnnData.concatenate. Only valid when do_concatenate\n        and supplying AnnData objects.\n    batch_categories : `list` or `None`, optional (default: None)\n        The batch_categories for AnnData.concatenate. Only valid when\n        do_concatenate and supplying AnnData objects.\n    k : `int`, optional (default: 20)\n        Number of mutual nearest neighbors.\n    sigma : `float`, optional (default: 1)\n        The bandwidth of the Gaussian smoothing kernel used to compute the\n        correction vectors. Default is 1.\n    cos_norm_in : `bool`, optional (default: True)\n        Whether cosine normalization should be performed on the input data prior\n        to calculating distances between cells.\n    cos_norm_out : `bool`, optional (default: True)\n        Whether cosine normalization should be performed prior to computing corrected expression values.\n    svd_dim : `int` or `None`, optional (default: None)\n        The number of dimensions to use for summarizing biological substructure\n        within each batch. If None, biological components will not be removed\n        from the correction vectors.\n    var_adj : `bool`, optional (default: True)\n        Whether to adjust variance of the correction vectors. Note this step\n        takes most computing time.\n    compute_angle : `bool`, optional (default: False)\n        Whether to compute the angle between each cell\u2019s correction vector and\n        the biological subspace of the reference batch.\n    mnn_order : `list` or `None`, optional (default: None)\n        The order in which batches are to be corrected. When set to None, datas\n        are corrected sequentially.\n    svd_mode : `str`, optional (default: 'rsvd')\n        One of 'svd', 'rsvd', and 'irlb'. 'svd' computes SVD using a\n        non-randomized SVD-via-ID algorithm, while 'rsvd' uses a randomized\n        version. 'irlb' performes truncated SVD by implicitly restarted Lanczos\n        bidiagonalization (forked from https://github.com/airysen/irlbpy).\n    do_concatenate : `bool`, optional (default: True)\n        Whether to concatenate the corrected matrices or AnnData objects. Default is True.\n    save_raw : `bool`, optional (default: False)\n        Whether to save the original expression data in the .raw attribute of AnnData objects.\n    n_jobs : `int` or `None`, optional (default: None)\n        The number of jobs. When set to None, automatically uses the number of cores.\n    kwargs : `dict` or `None`, optional (default: None)\n        optional keyword arguments for irlb.\n\n    Returns\n    -------\n    **datas** : :class:`~numpy.ndarray` or :class:`~anndata.AnnData`\n        Corrected matrix/matrices or AnnData object/objects, depending on the\n        input type and `do_concatenate`.\n    **mnn_list** : ``List[pandas.DataFrame]``\n        A list containing MNN pairing information as DataFrames in each iteration step.\n    **angle_list** : ``List[Tuple[Optional[float], int]]`` or ``None``\n        A list containing angles of each batch.\n    \"\"\"\n    try:\n        from mnnpy import mnn_correct as mnn_cor\n        n_jobs = settings.n_jobs if n_jobs is None else n_jobs\n        datas, mnn_list, angle_list = mnn_cor(\n            *datas, var_index=var_index, var_subset=var_subset, batch_key=batch_key, index_unique=index_unique,\n            batch_categories=batch_categories, k=k, sigma=sigma, cos_norm_in=cos_norm_in, cos_norm_out=cos_norm_out,\n            svd_dim=svd_dim, var_adj=var_adj, compute_angle=compute_angle, mnn_order=mnn_order, svd_mode=svd_mode,\n            do_concatenate=do_concatenate, save_raw=save_raw, n_jobs=n_jobs, **kwargs)\n        return datas, mnn_list, angle_list\n    except ImportError:\n        raise ImportError(\n            'Please install the package mnnpy '\n            '(https://github.com/chriscainx/mnnpy). ')", "response": "Correct batch effects by matching mutual nearest neighbors."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a. h5ad file and return a new object.", "response": "def read(filename, backed=False, sheet=None, ext=None, delimiter=None,\n         first_column_names=False, backup_url=None, cache=False, **kwargs) -> AnnData:\n    \"\"\"Read file and return :class:`~anndata.AnnData` object.\n\n    To speed up reading, consider passing `cache=True`, which creates an hdf5\n    cache file.\n\n    Parameters\n    ----------\n    filename : `str`\n        If the filename has no file extension, it is interpreted as a key for\n        generating a filename via `sc.settings.writedir + filename +\n        sc.settings.file_format_data`.  This is the same behavior as in\n        `sc.read(filename, ...)`.\n    backed : {`False`, `True`, 'r', 'r+'}, optional (default: `False`)\n        Load :class:`~anndata.AnnData` in `backed` mode instead of fully\n        loading it into memory (`memory` mode). Only applies to `.h5ad` files.\n        `True` and 'r' are equivalent. If you want to modify backed attributes\n        of the AnnData object, you need to choose 'r+'.\n    sheet : `str`, optional (default: `None`)\n        Name of sheet/table in hdf5 or Excel file.\n    cache : `bool`, optional (default: `False`)\n        If `False`, read from source, if `True`, read from fast 'h5ad' cache.\n    ext : `str`, optional (default: `None`)\n        Extension that indicates the file type. If `None`, uses extension of\n        filename.\n    delimiter : `str`, optional (default: `None`)\n        Delimiter that separates data within text file. If `None`, will split at\n        arbitrary number of white spaces, which is different from enforcing\n        splitting at any single white space ' '.\n    first_column_names : `bool`, optional (default: `False`)\n        Assume the first column stores row names. This is only necessary if\n        these are not strings: strings in the first column are automatically\n        assumed to be row names.\n    backup_url : `str`, optional (default: `None`)\n        Retrieve the file from an URL if not present on disk.\n\n    Returns\n    -------\n    An :class:`~anndata.AnnData` object\n    \"\"\"\n    filename = str(filename)  # allow passing pathlib.Path objects\n    if is_valid_filename(filename):\n        return _read(filename, backed=backed, sheet=sheet, ext=ext,\n                     delimiter=delimiter, first_column_names=first_column_names,\n                     backup_url=backup_url, cache=cache, **kwargs)\n    # generate filename and read to dict\n    filekey = filename\n    filename = settings.writedir + filekey + '.' + settings.file_format_data\n    if not Path(filename).exists():\n        raise ValueError('Reading with filekey \"{}\" failed, the '\n                         'inferred filename \"{}\" does not exist. '\n                         'If you intended to provide a filename, either '\n                         'use a filename ending on one of the available extensions {} '\n                         'or pass the parameter `ext`.'\n                         .format(filekey, filename, avail_exts))\n    return read_h5ad(filename, backed=backed)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_10x_h5(filename, genome=None, gex_only=True) -> AnnData:\n    logg.info('reading', filename, r=True, end=' ')\n    with tables.open_file(str(filename), 'r') as f:\n        v3 = '/matrix' in f\n    if v3:\n        adata = _read_v3_10x_h5(filename)\n        if genome:\n            if genome not in adata.var['genome'].values:\n                raise ValueError(\n                    \"Could not find data corresponding to genome '{genome}' in '{filename}'. \"\n                    \"Available genomes are: {avail}.\"\n                    .format(\n                        genome=genome, filename=filename,\n                        avail=list(adata.var[\"genome\"].unique()),\n                    )\n                )\n            adata = adata[:, list(map(lambda x: x == str(genome), adata.var['genome']))]\n        if gex_only:\n            adata = adata[:, list(map(lambda x: x == 'Gene Expression', adata.var['feature_types']))]\n        return adata\n    else:\n        return _read_legacy_10x_h5(filename, genome=genome)", "response": "Read 10x - Genomics - formatted hdf5 file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_legacy_10x_h5(filename, genome=None):\n    with tables.open_file(str(filename), 'r') as f:\n        try:\n            children = [x._v_name for x in f.list_nodes(f.root)]\n            if not genome:\n                if len(children) > 1:\n                    raise ValueError(\n                        \"'{filename}' contains more than one genome. For legacy 10x h5 \"\n                        \"files you must specify the genome if more than one is present. \"\n                        \"Available genomes are: {avail}\"\n                        .format(filename=filename, avail=children)\n                    )\n                genome = children[0]\n            elif genome not in children:\n                raise ValueError(\n                    \"Could not find genome '{genome}' in '{filename}'. \"\n                    \"Available genomes are: {avail}\"\n                    .format(\n                        genome=genome, filename=str(filename),\n                        avail=children,\n                    )\n                )\n            dsets = {}\n            for node in f.walk_nodes('/' + genome, 'Array'):\n                dsets[node.name] = node.read()\n            # AnnData works with csr matrices\n            # 10x stores the transposed data, so we do the transposition right away\n            from scipy.sparse import csr_matrix\n            M, N = dsets['shape']\n            data = dsets['data']\n            if dsets['data'].dtype == np.dtype('int32'):\n                data = dsets['data'].view('float32')\n                data[:] = dsets['data']\n            matrix = csr_matrix((data, dsets['indices'], dsets['indptr']),\n                                shape=(N, M))\n            # the csc matrix is automatically the transposed csr matrix\n            # as scanpy expects it, so, no need for a further transpostion\n            adata = AnnData(matrix,\n                            {'obs_names': dsets['barcodes'].astype(str)},\n                            {'var_names': dsets['gene_names'].astype(str),\n                             'gene_ids': dsets['genes'].astype(str)})\n            logg.info(t=True)\n            return adata\n        except KeyError:\n            raise Exception('File is missing one or more required datasets.')", "response": "Read a single cell ranger file from Cell Ranger v2 or earlier versions of the hdf5 file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read_v3_10x_h5(filename):\n    with tables.open_file(str(filename), 'r') as f:\n        try:\n            dsets = {}\n            for node in f.walk_nodes('/matrix', 'Array'):\n                dsets[node.name] = node.read()\n            from scipy.sparse import csr_matrix\n            M, N = dsets['shape']\n            data = dsets['data']\n            if dsets['data'].dtype == np.dtype('int32'):\n                data = dsets['data'].view('float32')\n                data[:] = dsets['data']\n            matrix = csr_matrix((data, dsets['indices'], dsets['indptr']),\n                                shape=(N, M))\n            adata = AnnData(matrix,\n                            {'obs_names': dsets['barcodes'].astype(str)},\n                            {'var_names': dsets['name'].astype(str),\n                             'gene_ids': dsets['id'].astype(str),\n                             'feature_types': dsets['feature_type'].astype(str),\n                             'genome': dsets['genome'].astype(str)})\n            logg.info(t=True)\n            return adata\n        except KeyError:\n            raise Exception('File is missing one or more required datasets.')", "response": "Read an hdf5 file from Cell Ranger v3 or later versions."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads 10x - Genomics - formatted mtx directory.", "response": "def read_10x_mtx(path, var_names='gene_symbols', make_unique=True, cache=False, gex_only=True) -> AnnData:\n    \"\"\"Read 10x-Genomics-formatted mtx directory.\n\n    Parameters\n    ----------\n    path : `str`\n        Path to directory for `.mtx` and `.tsv` files,\n        e.g. './filtered_gene_bc_matrices/hg19/'.\n    var_names : {'gene_symbols', 'gene_ids'}, optional (default: 'gene_symbols')\n        The variables index.\n    make_unique : `bool`, optional (default: `True`)\n        Whether to make the variables index unique by appending '-1',\n        '-2' etc. or not.\n    cache : `bool`, optional (default: `False`)\n        If `False`, read from source, if `True`, read from fast 'h5ad' cache.\n    gex_only : `bool`, optional (default: `True`)\n        Only keep 'Gene Expression' data and ignore other feature types,\n        e.g. 'Antibody Capture', 'CRISPR Guide Capture', or 'Custom'\n\n    Returns\n    -------\n    An :class:`~anndata.AnnData` object\n    \"\"\"\n    path = Path(path)\n    genefile_exists = (path / 'genes.tsv').is_file()\n    read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx\n    adata = read(\n        str(path),\n        var_names=var_names,\n        make_unique=make_unique,\n        cache=cache,\n    )\n    if genefile_exists or not gex_only:\n        return adata\n    else:\n        gex_rows = list(map(lambda x: x == 'Gene Expression', adata.var['feature_types']))\n        return adata[:, gex_rows]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads mex from output from Cell Ranger v2 or earlier versions of Mexfile.", "response": "def _read_legacy_10x_mtx(path, var_names='gene_symbols', make_unique=True, cache=False):\n    \"\"\"\n    Read mex from output from Cell Ranger v2 or earlier versions\n    \"\"\"\n    path = Path(path)\n    adata = read(path / 'matrix.mtx', cache=cache).T  # transpose the data\n    genes = pd.read_csv(path / 'genes.tsv', header=None, sep='\\t')\n    if var_names == 'gene_symbols':\n        var_names = genes[1]\n        if make_unique:\n            var_names = anndata.utils.make_index_unique(pd.Index(var_names))\n        adata.var_names = var_names\n        adata.var['gene_ids'] = genes[0].values\n    elif var_names == 'gene_ids':\n        adata.var_names = genes[0]\n        adata.var['gene_symbols'] = genes[1].values\n    else:\n        raise ValueError('`var_names` needs to be \\'gene_symbols\\' or \\'gene_ids\\'')\n    adata.obs_names = pd.read_csv(path / 'barcodes.tsv', header=None)[0]\n    return adata"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites the log entry to file.", "response": "def write(filename, adata, ext=None, compression='gzip', compression_opts=None):\n    \"\"\"Write :class:`~anndata.AnnData` objects to file.\n\n    Parameters\n    ----------\n    filename : `str`\n        If the filename has no file extension, it is interpreted as a key for\n        generating a filename via `sc.settings.writedir + filename +\n        sc.settings.file_format_data`.  This is the same behavior as in\n        :func:`~scanpy.api.read`.\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    ext : {`None`, `'h5'`, `'csv'`, `'txt'`, `'npz'`} (default: `None`)\n        File extension from wich to infer file format. If `None`, defaults to\n        `sc.settings.file_format_data`.\n    compression : {`None`, 'gzip', 'lzf'}, optional (default: `'gzip'`)\n        See http://docs.h5py.org/en/latest/high/dataset.html.\n    compression_opts : `int`, optional (default: `None`)\n        See http://docs.h5py.org/en/latest/high/dataset.html.\n    \"\"\"\n    filename = str(filename)  # allow passing pathlib.Path objects\n    if is_valid_filename(filename):\n        filename = filename\n        ext_ = is_valid_filename(filename, return_ext=True)\n        if ext is None:\n            ext = ext_\n        elif ext != ext_:\n            raise ValueError('It suffices to provide the file type by '\n                             'providing a proper extension to the filename.'\n                             'One of \"txt\", \"csv\", \"h5\" or \"npz\".')\n    else:\n        key = filename\n        ext = settings.file_format_data if ext is None else ext\n        filename = get_filename_from_key(key, ext)\n    if ext == 'csv':\n        adata.write_csvs(filename)\n    else:\n        adata.write(filename, compression=compression,\n                    compression_opts=compression_opts)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the parameters from a text file.", "response": "def read_params(filename, asheader=False, verbosity=0) -> Dict[str, Union[int, float, bool, str, None]]:\n    \"\"\"Read parameter dictionary from text file.\n\n    Assumes that parameters are specified in the format:\n        par1 = value1\n        par2 = value2\n\n    Comments that start with '#' are allowed.\n\n    Parameters\n    ----------\n    filename : str, Path\n        Filename of data file.\n    asheader : bool, optional\n        Read the dictionary from the header (comment section) of a file.\n\n    Returns\n    -------\n    Dictionary that stores parameters.\n    \"\"\"\n    filename = str(filename)  # allow passing pathlib.Path objects\n    from collections import OrderedDict\n    params = OrderedDict([])\n    for line in open(filename):\n        if '=' in line:\n            if not asheader or line.startswith('#'):\n                line = line[1:] if line.startswith('#') else line\n                key, val = line.split('=')\n                key = key.strip()\n                val = val.strip()\n                params[key] = convert_string(val)\n    return params"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites parameters to file.", "response": "def write_params(path, *args, **dicts):\n    \"\"\"Write parameters to file, so that it's readable by read_params.\n\n    Uses INI file format.\n    \"\"\"\n    path = Path(path)\n    if not path.parent.is_dir():\n        path.parent.mkdir(parents=True)\n    if len(args) == 1:\n        d = args[0]\n        with path.open('w') as f:\n            for key in d:\n                f.write(key + ' = ' + str(d[key]) + '\\n')\n    else:\n        with path.open('w') as f:\n            for k, d in dicts.items():\n                f.write('[' + k + ']\\n')\n                for key, val in d.items():\n                    f.write(key + ' = ' + str(val) + '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_params_from_list(params_list):\n    params = {}\n    for i in range(0, len(params_list)):\n        if '=' not in params_list[i]:\n            try:\n                if not isinstance(params[key], list): params[key] = [params[key]]\n                params[key] += [params_list[i]]\n            except KeyError:\n                raise ValueError('Pass parameters like `key1=a key2=b c d key3=...`.')\n        else:\n            key_val = params_list[i].split('=')\n            key, val = key_val\n            params[key] = convert_string(val)\n    return params", "response": "Transform params list to dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking a path into a filename.", "response": "def _slugify(path: Union[str, PurePath]) -> str:\n    \"\"\"Make a path into a filename.\"\"\"\n    if not isinstance(path, PurePath):\n        path = PurePath(path)\n    parts = list(path.parts)\n    if parts[0] == '/':\n        parts.pop(0)\n    elif len(parts[0]) == 3 and parts[0][1:] == ':\\\\':\n        parts[0] = parts[0][0]  # C:\\ \u2192 C\n    filename = '-'.join(parts)\n    assert '/' not in filename, filename\n    assert not filename[1:].startswith(':'), filename\n    return filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_softgz(filename) -> AnnData:\n    filename = str(filename)  # allow passing pathlib.Path objects\n    import gzip\n    with gzip.open(filename, mode='rt') as file:\n        # The header part of the file contains information about the\n        # samples. Read that information first.\n        samples_info = {}\n        for line in file:\n            if line.startswith(\"!dataset_table_begin\"):\n                break\n            elif line.startswith(\"!subset_description\"):\n                subset_description = line.split(\"=\")[1].strip()\n            elif line.startswith(\"!subset_sample_id\"):\n                subset_ids = line.split(\"=\")[1].split(\",\")\n                subset_ids = [x.strip() for x in subset_ids]\n                for k in subset_ids:\n                    samples_info[k] = subset_description\n        # Next line is the column headers (sample id's)\n        sample_names = file.readline().strip().split(\"\\t\")\n        # The column indices that contain gene expression data\n        I = [i for i, x in enumerate(sample_names) if x.startswith(\"GSM\")]\n        # Restrict the column headers to those that we keep\n        sample_names = [sample_names[i] for i in I]\n        # Get a list of sample labels\n        groups = [samples_info[k] for k in sample_names]\n        # Read the gene expression data as a list of lists, also get the gene\n        # identifiers\n        gene_names, X = [], []\n        for line in file:\n            # This is what signals the end of the gene expression data\n            # section in the file\n            if line.startswith(\"!dataset_table_end\"):\n                break\n            V = line.split(\"\\t\")\n            # Extract the values that correspond to gene expression measures\n            # and convert the strings to numbers\n            x = [float(V[i]) for i in I]\n            X.append(x)\n            gene_names.append(V[1])\n    # Convert the Python list of lists to a Numpy array and transpose to match\n    # the Scanpy convention of storing samples in rows and variables in colums.\n    X = np.array(X).T\n    obs = pd.DataFrame({\"groups\": groups}, index=sample_names)\n    var = pd.DataFrame(index=gene_names)\n    return AnnData(X=X, obs=obs, var=var)", "response": "Read a SOFT format data file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_bool(string):\n    if string == 'True':\n        return True, True\n    elif string == 'False':\n        return True, False\n    else:\n        return False, False", "response": "Check whether string is boolean."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts string to int float or bool.", "response": "def convert_string(string):\n    \"\"\"Convert string to int, float or bool.\n    \"\"\"\n    if is_int(string):\n        return int(string)\n    elif is_float(string):\n        return float(string)\n    elif convert_bool(string)[0]:\n        return convert_bool(string)[1]\n    elif string == 'None':\n        return None\n    else:\n        return string"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget files used by processes with name scanpy.", "response": "def get_used_files():\n    \"\"\"Get files used by processes with name scanpy.\"\"\"\n    import psutil\n    loop_over_scanpy_processes = (proc for proc in psutil.process_iter()\n                                  if proc.name() == 'scanpy')\n    filenames = []\n    for proc in loop_over_scanpy_processes:\n        try:\n            flist = proc.open_files()\n            for nt in flist:\n                filenames.append(nt.path)\n        # This catches a race condition where a process ends\n        # before we can examine its files\n        except psutil.NoSuchProcess as err:\n            pass\n    return set(filenames)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking whether the file is present and download if not.", "response": "def check_datafile_present_and_download(path, backup_url=None):\n    \"\"\"Check whether the file is present, otherwise download.\n    \"\"\"\n    path = Path(path)\n    if path.is_file(): return True\n    if backup_url is None: return False\n    logg.info('try downloading from url\\n' + backup_url + '\\n' +\n              '... this may take a while but only happens once')\n    if not path.parent.is_dir():\n        logg.info('creating directory', str(path.parent) + '/', 'for saving data')\n        path.parent.mkdir(parents=True)\n    from urllib.request import urlretrieve\n    urlretrieve(backup_url, str(path), reporthook=download_progress)\n    logg.info('')\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking whether the argument is a filename.", "response": "def is_valid_filename(filename, return_ext=False):\n    \"\"\"Check whether the argument is a filename.\"\"\"\n    ext = Path(filename).suffixes\n\n    if len(ext) > 2:\n        logg.warn('Your filename has more than two extensions: {}.\\n'\n                  'Only considering the two last: {}.'.format(ext, ext[-2:]))\n        ext = ext[-2:]\n\n    # cases for gzipped/bzipped text files\n    if len(ext) == 2 and ext[0][1:] in text_exts and ext[1][1:] in ('gz', 'bz2'):\n        return ext[0][1:] if return_ext else True\n    elif ext and ext[-1][1:] in avail_exts:\n        return ext[-1][1:] if return_ext else True\n    elif ''.join(ext) == '.soft.gz':\n        return 'soft.gz' if return_ext else True\n    elif ''.join(ext) == '.mtx.gz':\n        return 'mtx.gz' if return_ext else True\n    else:\n        if return_ext:\n            raise ValueError('\"{}\" does not end on a valid extension.\\n'\n                             'Please, provide one of the available extensions.\\n{}\\n'\n                             'Text files with .gz and .bz2 extensions are also supported.'\n                             .format(filename, avail_exts))\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots a correlation matrix for genes strored in sample annotation using rank_genes_groups. py .", "response": "def correlation_matrix(adata,groupby=None ,group=None, corr_matrix=None, annotation_key=None):\n    \"\"\"Plot correlation matrix.\n\n            Plot a correlation matrix for genes strored in sample annotation using rank_genes_groups.py\n\n            Parameters\n            ----------\n            adata : :class:`~anndata.AnnData`\n                Annotated data matrix.\n            groupby : `str`, optional (default: None)\n                If specified, searches data_annotation for correlation_matrix+groupby+str(group)\n            group : int\n                Identifier of the group (necessary if and only if groupby is also specified)\n            corr_matrix : DataFrame, optional (default: None)\n                Correlation matrix as a DataFrame (annotated axis) that can be transferred manually if wanted\n            annotation_key: `str`, optional (default: None)\n                If specified, looks in data annotation for this key.\n\n        \"\"\"\n\n    # TODO: At the moment, noly works for int identifiers\n\n    if corr_matrix is None:\n        # This will produce an error if he annotation doesn't exist, which is okay\n        if annotation_key is None:\n            if groupby is None:\n                corr_matrix = adata.uns['Correlation_matrix']\n            else:\n                corr_matrix= adata.uns['Correlation_matrix' + groupby+ str(group)]\n            # Throws error if does not exist\n        else:\n            corr_matrix = adata.uns[annotation_key]\n\n    # Set up mask\n    mask = np.zeros_like(corr_matrix, dtype=np.bool)\n    di = np.diag_indices(len(corr_matrix.axes[0]))\n    mask[di] = True\n\n    f, ax = plt.subplots(figsize=(11, 9))\n\n    cmap = sns.diverging_palette(240, 10, as_cmap=True)\n\n    sns.heatmap(corr_matrix, mask=mask, cmap=cmap,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n    if annotation_key is None:\n        if groupby is None:\n            plt.title('Correlation Matrix')\n        else:\n            plt.title('Correlation Matrix for Group ' + str(group) + \"in \" + groupby)\n    else:\n        plt.title('Correlation Matrix for' + annotation_key)\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef top_ranked_group_analysis(adata, groupby, groupid, n=100, special_markers=None,\n                              coloring='scores', annotate=False):\n    \"\"\"For one group, output a detailed chart analyzing highly ranked genes detailly.\n\n                This is a visualization tools that helps to find significant markers and get a better understanding of\n                the underlying data.\n\n                Parameters\n                ----------\n                adata : :class:`~anndata.AnnData`\n                    Annotated data matrix.\n                groupby : `str`\n                    The key of the sample grouping to consider.\n                groupid: int\n                    The group for which detailed analysis should be displayed.\n                x : 'str'\n                    x-axis labelling for plots\n                y : 'str'\n                    y-axis labelling for plots\n                groups : `str`, `list`, optional (default: `'all'`)\n                    Subset of groups, e.g. `['g1', 'g2', 'g3']`, to which comparison shall\n                    be restricted. If not passed, a ranking will be generated for all\n                    groups.\n                n : `int`, optional (default: 100)\n                    Number of datapoints in the scatterplot. If less are available, use all that are available\n                special_markers: 'dict', optional (default: None)\n                    If provided, this should be a dict containing a list of gene names for each group in groupby.\n                    Special marked genes are highlighted in the visualization\n                coloring : {'scores', 'absolute'}, optional (default: 'scores')\n                    Rank either according to Scores, or to absolute test-statistic value.\n                    In either case, results are scaled so as to guarantee sufficient contrast.\n                annotate : bool, optional (default: False)\n                    If True, annotate each datapoint in (each?) scatterplot. Helps identify specific genes. Only\n                    recommended for small n.\n            \"\"\"\n    groups = 'all'\n\n    groups_order, groups_masks = utils.select_groups(\n        adata, groups, groupby)\n\n\n    imask=groupid\n    mask=groups_masks[imask]\n\n    score_list = list()\n    name_list = list()\n    special_markers_indices = list()\n    # Note: No duplicates in each group\n    for j, k in enumerate(adata.uns['rank_genes_groups_gene_scores']):\n        # Make sure only first n datapoints are used\n        if j >= n:\n            break\n        score_list.append(k[imask])\n        name_list.append(adata.uns['rank_genes_groups_gene_names'][j][imask])\n        # Inefficient and not generalizable but works: Check added index if in list of specially_marked_genes\n        # TODO: Speed up if becomes a time issue\n        if special_markers is None:\n            pass\n        elif adata.uns['rank_genes_groups_gene_names'][j][imask] in special_markers[imask]:\n            special_markers_indices.append(len(name_list) - 1)\n        else:\n            pass\n\n    ### Get all the key figures\n    # make things faster by calculating only what is required for plot\n    mask_rest = ~mask\n\n    # Unlike in the above case, we need virtually all information, except for possibly CDR\n    rate_group = _zero_inflation_estimate(adata[:, name_list], mask)\n    rate_rest = _zero_inflation_estimate(adata[:, name_list], mask_rest)\n    rate_difference= rate_group-rate_rest\n    means_group = _tail_mean_estimate(adata[:, name_list], mask)\n    means_rest = _tail_mean_estimate(adata[:, name_list], mask_rest)\n    mean_difference=means_group-means_rest\n    var_group = _tail_var_estimate(adata[:, name_list], mask)\n    var_rest = _tail_var_estimate(adata[:, name_list], mask_rest)\n    CDR = _Avg_CDR(adata, mask, name_list, model='rough', n_genes=None)\n    adata_relevant = adata[:, name_list]\n    exp, full_var_group = simple._get_mean_var(adata_relevant.X[mask])\n    adata_relevant = adata[:, name_list]\n    exp_rest, full_var_rest = simple._get_mean_var(adata_relevant.X[mask_rest])\n\n    ### Prepare for coloring\n    # get colored scatterplot\n    # For coloring, get max score value, normalize (0,1)\n    # Depending on whether normalization should be scale-invariant or only rank-invariant, do the following\n    if coloring == 'scores':\n        score_list = score_list / max(score_list)\n        colors = cm.jet(score_list)\n    elif coloring == 'absolute':\n        color_list = rankdata(score_list)\n        max_values = max(color_list)\n        colors = cm.jet(color_list / max_values)\n        # Identify true markers distinctly by using different size.\n    else:\n        logg.error('coloring should be either <socres> or <absolute>')\n    s = 20 * np.ones(len(score_list))\n    # This works for numpy access (not for normal lists though)\n    s[special_markers_indices] = 100\n\n    # Now specifically say how each subplot should look like,loop over it\n\n    f, ((ax1, ax2, ax3), (ax4, ax5, ax6), (ax7, ax8, ax9)) = plt.subplots(3, 3, figsize=(12, 12))\n\n    ax1.scatter(mean_difference, score_list, s=s, color=colors)\n    ax1.set_xlabel(\"Expression Rate Group\")\n    ax1.set_ylabel(\"Expression Rate Rest\")\n    if annotate is True:\n        for i, txt in enumerate(name_list):\n            ax1.annotate(txt, (mean_difference[i], score_list[i]))\n\n    ax2.scatter(rate_group, score_list, s=s, color=colors)\n    ax2.set_xlabel(\"Expression Rate Group\")\n    ax2.set_ylabel(\"Expression Rate Rest\")\n    if annotate is True:\n        for i, txt in enumerate(name_list):\n            ax2.annotate(txt, (rate_group[i], score_list[i]))\n\n    ax3.scatter(rate_group, rate_rest, s=s, color=colors)\n    ax3.set_xlabel(\"Expression Rate Group\")\n    ax3.set_ylabel(\"Expression Rate Rest\")\n    if annotate is True:\n        for i, txt in enumerate(name_list):\n            ax3.annotate(txt, (rate_group[i], rate_rest[i]))\n\n    ax4.scatter(CDR, score_list, s=s, color=colors)\n    ax4.set_xlabel(\"Cellular Detection Rate\")\n    ax4.set_ylabel(\"Score\")\n    if annotate is True:\n        for i, txt in enumerate(name_list):\n            ax4.annotate(txt, (CDR[i], score_list[i]))\n\n    ax5.scatter(rate_difference, score_list, s=s, color=colors)\n    ax5.set_xlabel(\"Expression Rate Difference\")\n    ax5.set_ylabel(\"Score\")\n    if annotate is True:\n        for i, txt in enumerate(name_list):\n            ax5.annotate(txt, (rate_difference[i], score_list[i]))\n\n    ax6.scatter(rate_group, score_list, s=s, color=colors)\n    ax6.set_xlabel(\"Expression Rate Group\")\n    ax6.set_ylabel(\"Score\")\n    if annotate is True:\n        for i, txt in enumerate(name_list):\n            ax6.annotate(txt, (rate_group[i], score_list[i]))\n\n    ax7.scatter(rate_group, score_list, s=s, color=colors)\n    ax7.set_xlabel(\"Expression Rate Group\")\n    ax7.set_ylabel(\"Expression Rate Rest\")\n    if annotate is True:\n        for i, txt in enumerate(name_list):\n            ax7.annotate(txt, (rate_group[i], score_list[i]))\n\n    ax8.scatter(rate_rest,full_var_rest, s=s, color=colors)\n    ax8.set_xlabel(\"Expression Rate Rest\")\n    ax8.set_ylabel(\"Full Variance Rest\")\n    if annotate is True:\n        for i, txt in enumerate(name_list):\n            if i<20:\n                ax8.annotate(txt, (rate_rest[i], full_var_rest[i]))\n            else:\n                break\n    ax9.scatter(rate_group, full_var_group, s=s, color=colors)\n    ax9.set_xlabel(\"Expression Rate Group\")\n    ax9.set_ylabel(\"Full Variance Group\")\n    if annotate is True:\n        for i, txt in enumerate(name_list):\n            ax9.annotate(txt, (rate_group[i], full_var_group[i]))\n\n    # Here, write what we want in each subplot\n    plt.tight_layout()\n    plt.show()", "response": "Top Ranked Group Analysis"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef scatter(adata, groupby, groupid, x,y, n=100, special_markers=None,\n                              coloring='scores', size=12, annotate=True):\n    \"\"\"For one group, output a detailed chart analyzing highly ranked genes detailly.\n\n                This is a visualization tools that helps to find significant markers and get a better understanding of\n                the underlying data.\n\n                Parameters\n                ----------\n                adata : :class:`~anndata.AnnData`\n                    Annotated data matrix.\n                groupby : `str`\n                    The key of the sample grouping to consider.\n                groupid: int\n                    The group for which detailed analysis should be displayed.\n                x : 'str'\n                    x-axis labelling for plots\n                y : 'str'\n                    y-axis labelling for plots\n                n : `int`, optional (default: 100)\n                    Number of datapoints in the scatterplot. If less are available, use all that are available\n                special_markers: 'dict', optional (default: None)\n                    If provided, this should be a dict containing a list of gene names for each group in groupby.\n                    Special marked genes are highlighted in the visualization\n                coloring : {'scores', 'absolute'}, optional (default: 'scores')\n                    Rank either according to Scores, or to absolute test-statistic value.\n                    In either case, results are scaled so as to guarantee sufficient contrast.\n                size: int, optional (default: 12)\n                    Determines scatter plot size. Large scatter-plots make it easier to identify specific genes using\n                    annotate=True\n                annotate : bool, optional (default: False)\n                    If True, annotate each datapoint in (each?) scatterplot. Helps identify specific genes. Only\n                    recommended for small n.\n            \"\"\"\n    groups = 'all'\n\n    groups_order, groups_masks = utils.select_groups(\n        adata, groups, groupby)\n\n\n    imask=groupid\n    mask=groups_masks[imask]\n\n    score_list = list()\n    name_list = list()\n    special_markers_indices = list()\n    # Note: No duplicates in each group\n    for j, k in enumerate(adata.uns['rank_genes_groups_gene_scores']):\n        # Make sure only first n datapoints are used\n        if j >= n:\n            break\n        score_list.append(k[imask])\n        name_list.append(adata.uns['rank_genes_groups_gene_names'][j][imask])\n        # Inefficient and not generalizable but works: Check added index if in list of specially_marked_genes\n        # TODO: Speed up if becomes a time issue\n        if special_markers is None:\n            pass\n        elif adata.uns['rank_genes_groups_gene_names'][j][imask] in special_markers[imask]:\n            special_markers_indices.append(len(name_list) - 1)\n        else:\n            pass\n\n    ### Get all the key figures\n    # make things faster by calculating only what is required for plot\n    mask_rest = ~mask\n\n    # Get rate of expression\n    rate_group = _zero_inflation_estimate(adata[:, name_list], mask)\n    rate_rest = _zero_inflation_estimate(adata[:, name_list], mask_rest)\n    if (x in {'full_mean_group', 'tail_mean_group', 'full_mean_difference',\n              'tail_mean_difference'} or y in {'full_mean_group', 'tail_mean_group', 'full_mean_difference',\n                                               'tail_mean_difference'}):\n        means_group = _tail_mean_estimate(adata[:, name_list], mask)\n    if (x in {'full_mean_rest', 'tail_mean_rest', 'full_mean_difference',\n              'tail_mean_difference'} or y in {'full_mean_rest', 'tail_mean_rest', 'full_mean_difference',\n                                               'tail_mean_difference'}):\n        means_rest = _tail_mean_estimate(adata[:, name_list], mask_rest)\n\n    if (x == 'tail_var_group' or y == 'tail_var_group'):\n        # Get tail variance of expression\n        var_group = _tail_var_estimate(adata[:, name_list], mask)\n    if (x == 'tail_var_rest' or y == 'tail_var_rest'):\n        var_rest = _tail_var_estimate(adata[:, name_list], mask_rest)\n    if (x == 'CDR' or y == 'CDR'):\n        # Get CDR: Need to give full adata object, since we need to count everything\n        CDR = _Avg_CDR(adata, mask, name_list, model='rough', n_genes=None)\n    if (x == 'full_var_group' or y == 'full_var_group'):\n        # Slice first appropriately:\n        adata_relevant = adata[:, name_list]\n        exp, full_var_group = simple._get_mean_var(adata_relevant.X[mask])\n    if (x == 'full_var_rest' or y == 'full_var_rest'):\n        # Slice first appropriately:\n        adata_relevant = adata[:, name_list]\n        exp_rest, full_var_rest = simple._get_mean_var(adata_relevant.X[mask_rest])\n\n    ### Prepare for coloring\n    # get colored scatterplot\n    # For coloring, get max score value, normalize (0,1)\n    # Depending on whether normalization should be scale-invariant or only rank-invariant, do the following\n    if coloring == 'scores':\n        score_list = score_list / max(score_list)\n        colors = cm.jet(score_list)\n    elif coloring == 'absolute':\n        color_list = rankdata(score_list)\n        max_values = max(color_list)\n        colors = cm.jet(color_list / max_values)\n        # Identify true markers distinctly by using different size.\n    else:\n        logg.error('coloring should be either <socres> or <absolute>')\n    s = 20 * np.ones(len(score_list))\n    # This works for numpy access (not for normal lists though)\n    s[special_markers_indices] = 100\n    # In future, build method to mark top genes specially\n\n    ### Actually do the plotting: Looping is inefficient and lengthy, but clear style\n    # Potential values for x, y: 'mean' ('full' or 'tail'), 'tail_variance', 'inflation', 'CDR',\n    # tail_variance_rest, Score (Just the ranking as given by test-statistic), 'full_var', 'full_var_rest'\n\n    if x == 'expression_rate_difference':\n        x_plot = rate_group - rate_rest\n    elif x == 'expression_rate_group':\n        x_plot = rate_group\n    elif x == 'expression_rate_rest':\n        x_plot = rate_rest\n    elif x == 'Score':\n        x_plot = score_list\n    elif x == 'full_mean_difference':\n        x_plot = means_group * rate_group - means_rest * rate_rest\n    elif x == 'full_mean_group':\n        x_plot = means_group * rate_group\n    elif x == 'full_mean_rest':\n        x_plot = means_rest * rate_rest\n    elif x == 'tail_mean_difference':\n        x_plot = means_group - means_rest\n    elif x == 'tail_mean_group':\n        x_plot = means_group\n    elif x == 'tail_mean_rest':\n        x_plot = means_rest\n    elif x == 'tail_var_group':\n        x_plot = var_group\n    elif x == 'tail_var_rest':\n        x_plot = var_rest\n    elif x == 'full_var_group':\n        x_plot = full_var_group\n    elif x == 'full_var_rest':\n        x_plot = full_var_rest\n    elif x == 'CDR':\n        x_plot = CDR\n    else:\n        logg.error('No accepted input. Check function documentation to get an overview over all inputs')\n\n    if y == 'expression_rate_difference':\n        y_plot = rate_group - rate_rest\n    elif y == 'expression_rate_group':\n        y_plot = rate_group\n    elif y == 'expression_rate_rest':\n        y_plot = rate_rest\n    elif y == 'Score':\n        y_plot = score_list\n    elif y == 'full_mean_difference':\n        y_plot = means_group * rate_group - means_rest * rate_rest\n    elif y == 'full_mean_group':\n        y_plot = means_group * rate_group\n    elif y == 'full_mean_rest':\n        y_plot = means_rest * rate_rest\n    elif y == 'tail_mean_difference':\n        y_plot = means_group - means_rest\n    elif y == 'tail_mean_group':\n        y_plot = means_group\n    elif y == 'tail_mean_rest':\n        y_plot = means_rest\n    elif y == 'tail_var_group':\n        y_plot = var_group\n    elif y == 'tail_var_rest':\n        y_plot = var_rest\n    elif y == 'full_var_group':\n        y_plot = full_var_group\n    elif y == 'full_var_rest':\n        y_plot =     full_var_rest\n    elif y == 'CDR':\n        y_plot = CDR\n    else:\n        logg.error('No accepted input. Check function documentation to get an overview over all inputs')\n\n    # To make different scalings easier to compare, we set fixed limits for the case that x,y are e\n    # expression rates\n    if (x in {'expression_rate_difference', 'expression_rate_group', 'expression_rate_rest'} and\n                y in {'expression_rate_difference', 'expression_rate_group', 'expression_rate_rest'}):\n        plt.xlim(0, 1)\n        plt.ylim(0, 1)\n    fig, ax= plt.subplots(figsize=(size,size))\n    ax.scatter(x_plot, y_plot, color=colors, s=s)\n    plt.xlabel(x)\n    plt.ylabel(y)\n    if annotate is True:\n        for i, txt in enumerate(name_list):\n            plt.annotate(txt, (x_plot[i], y_plot[i]))\n    plt.show()", "response": "This function generates a scatter plot for highly ranked genes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates correlation matrix. Calculate a correlation matrix for genes strored in sample annotation using rank_genes_groups.py Parameters ---------- adata : :class:`~anndata.AnnData` Annotated data matrix. groupby : `str` The key of the sample grouping to consider. group : `str`, int, optional (default: None) Group name or index for which the correlation matrix for top_ranked genes should be calculated. If no parameter is passed, ROC/AUC is calculated for all groups n_genes : `int`, optional (default: 100) For how many genes to calculate ROC and AUC. If no parameter is passed, calculation is done for all stored top ranked genes. special_markers: 'dict', optional (default: None) If provided, this should be a dict containing a list of gene names for each group in groupby. Special marked genes are highlighted in the visualization coloring : {'scores', 'absolute'}, optional (default: 'scores') Rank either according to Scores, or to absolute test-statistic value. In either case, results are scaled so as to guarantee sufficient contrast. size: int, optional (default: 12) Determines scatter plot size. Large scatter-plots make it easier to identify specific genes using annotate=True annotate : bool, optional (default: False) If True, annotate each datapoint in (each?) scatterplot. Helps identify specific genes. Only recommended for small n.", "response": "def ROC_AUC_analysis(adata,groupby,group,n_genes=100, special_markers=None, coloring='scores', size=12, annotate=False):\n    \"\"\"Calculate correlation matrix.\n\n        Calculate a correlation matrix for genes strored in sample annotation using rank_genes_groups.py\n\n        Parameters\n        ----------\n        adata : :class:`~anndata.AnnData`\n            Annotated data matrix.\n        groupby : `str`\n            The key of the sample grouping to consider.\n        group : `str`, int, optional (default: None)\n            Group name or index for which the correlation matrix for top_ranked genes should be calculated.\n            If no parameter is passed, ROC/AUC is calculated for all groups\n        n_genes : `int`, optional (default: 100)\n            For how many genes to calculate ROC and AUC. If no parameter is passed, calculation is done for\n            all stored top ranked genes.\n        special_markers: 'dict', optional (default: None)\n            If provided, this should be a dict containing a list of gene names for each group in groupby.\n            Special marked genes are highlighted in the visualization\n        coloring : {'scores', 'absolute'}, optional (default: 'scores')\n            Rank either according to Scores, or to absolute test-statistic value.\n            In either case, results are scaled so as to guarantee sufficient contrast.\n        size: int, optional (default: 12)\n            Determines scatter plot size. Large scatter-plots make it easier to identify specific genes using\n            annotate=True\n        annotate : bool, optional (default: False)\n            If True, annotate each datapoint in (each?) scatterplot. Helps identify specific genes. Only\n            recommended for small n.\n\n    \"\"\"\n\n    fpr=adata.uns['ROCfpr' + groupby + str(group)]\n    tpr=adata.uns['ROCtpr' + groupby + str(group)]\n    # We dont need thresholds here\n\n    # TODO: ALlow for comparison with rest, weighting...\n    groups = 'all'\n\n    groups_order, groups_masks = utils.select_groups(\n        adata, groups, groupby)\n\n    imask = group\n    mask = groups_masks[imask]\n\n    score_list = list()\n    name_list = list()\n    special_markers_indices = list()\n    # Note: No duplicates in each group\n    for j, k in enumerate(adata.uns['rank_genes_groups_gene_scores']):\n        # Make sure only first n datapoints are used\n        if j >= n_genes:\n            break\n        score_list.append(k[imask])\n        name_list.append(adata.uns['rank_genes_groups_gene_names'][j][imask])\n        # Inefficient and not generalizable but works: Check added index if in list of specially_marked_genes\n        # TODO: Speed up if becomes a time issue\n        if special_markers is None:\n            pass\n        elif adata.uns['rank_genes_groups_gene_names'][j][imask] in special_markers[imask]:\n            special_markers_indices.append(len(name_list) - 1)\n        else:\n            pass\n\n    ### Get all the key figures\n    # make things faster by calculating only what is required for plot\n    mask_rest = ~mask\n\n    # Get rate of expression\n    rate_group = _zero_inflation_estimate(adata[:, name_list], mask)\n    rate_rest = _zero_inflation_estimate(adata[:, name_list], mask_rest)\n\n    if coloring == 'scores':\n        score_list = score_list / max(score_list)\n        colors = cm.jet(score_list)\n    elif coloring == 'absolute':\n        color_list = rankdata(score_list)\n        max_values = max(color_list)\n        colors = cm.jet(color_list / max_values)\n        # Identify true markers distinctly by using different size.\n    else:\n        logg.error('coloring should be either <socres> or <absolute>')\n    s = 20 * np.ones(len(score_list))\n    # This works for numpy access (not for normal lists though)\n    s[special_markers_indices] = 100\n\n    fig, ax = plt.subplots(figsize=(size, size))\n    ax.scatter(rate_rest, rate_group, color=colors, s=s)\n    plt.xlabel(\"False positive rate\")\n    plt.ylabel(\"True positive rate\")\n    if annotate is True:\n        for i, txt in enumerate(name_list):\n            plt.annotate(txt, (rate_rest[i], rate_group[i]))\n            # TODO: Add AUC\n\n    # Now annotate the lines for all genes:\n    # TODO: Until now, everything requires same number of n (i.e. all in name list). Shouldn't be the case. Resolve.\n    for i,j in enumerate(name_list):\n        plt.plot(fpr[name_list[i]], tpr[name_list[i]], color=colors[i])\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tqdm_hook(t):\n    last_b = [0]\n\n    def update_to(b=1, bsize=1, tsize=None):\n        \"\"\"\n        b  : int, optional\n            Number of blocks transferred so far [default: 1].\n        bsize  : int, optional\n            Size of each block (in tqdm units) [default: 1].\n        tsize  : int, optional\n            Total size (in tqdm units). If [default: None] remains unchanged.\n        \"\"\"\n        if tsize is not None:\n            t.total = tsize\n        t.update((b - last_b[0]) * bsize)\n        last_b[0] = b\n\n    return update_to", "response": "A function that returns a function that returns a new instance of the same type as the given tqdm instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ebi_expression_atlas(accession: str, *, filter_boring: bool = False):\n    experiment_dir = settings.datasetdir / accession\n    dataset_path = experiment_dir / \"{}.h5ad\".format(accession)\n    try:\n        adata = anndata.read(dataset_path)\n        if filter_boring:\n            adata.obs = _filter_boring(adata.obs)\n        return adata\n    except OSError:\n        # Dataset couldn't be read for whatever reason\n        pass\n\n    download_experiment(accession)\n\n    print(\"Downloaded {} to {}\".format(accession, experiment_dir.absolute()))\n\n    with ZipFile(experiment_dir / \"expression_archive.zip\", \"r\") as f:\n        adata = read_expression_from_archive(f)\n    obs = pd.read_csv(\n        experiment_dir / \"experimental_design.tsv\", sep=\"\\t\", index_col=0\n    )\n\n    adata.obs[obs.columns] = obs\n    adata.write(dataset_path, compression=\"gzip\")  # To be kind to disk space\n\n    if filter_boring:\n        adata.obs = _filter_boring(adata.obs)\n\n    return adata", "response": "Load a dataset from the EBI Single Cell Expression Atlas."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rna_velocity(adata, loomfile, copy=False):\n    # also see notebook \"scanpy_rna_velocity_all\"\n\n    # the following only works in theory...\n    \n    # this is n_genes x n_cells\n    ds = loompy.connect('./all_sgete_4GU75.loom')\n    row_attrs = dict(ds.row_attrs.items())\n    col_attrs = dict(ds.col_attrs.items())\n    gene_names = [gene for gene in row_attrs['Gene'] if gene in adata.var_names]\n    cell_names = [cell for cell in col_attrs['CellID'] if cell in adata.obs_names]\n\n    # subset the s and u matrices to the genes in adata\n    from anndata.base import _normalize_index\n    gene_index = _normalize_index(gene_names, adata.var_names)\n    cell_index = _normalize_index(cell_names, adata.obs_names)\n    if len(cell_index) == 0:\n        raise ValueError(\n            'Cell names in loom file do not match cell names in AnnData.')\n    # subset to cells and genes present in adata\n    ad_s = AnnData(ds.layer['spliced'].sparse(gene_index, cell_index).tocsr().T)\n    ad_u = AnnData(ds.layer['unspliced'].sparse(gene_index, cell_index).tocsr().T)\n    ds.close()\n\n    subset, _ = sc.pp.filter_genes(ad_u.X, min_cells=50)\n    print(np.sum(subset))\n    ad_s = ad_s[:, subset]\n    ad_u = ad_u[:, subset]\n    ad_s.var_names = np.array(gene_names)[subset]\n\n    # loop over genes\n    from scipy.sparse import dok_matrix\n    offset = np.zeros(ad_s.shape[1], dtype='float32')\n    gamma = np.zeros(ad_s.shape[1], dtype='float32')\n    X_du = dok_matrix(ad_s.shape, dtype='float32')\n    for i in range(ad_s.shape[1]):\n        x = ad_s.X[:, i].toarray()\n        y = ad_u.X[:, i].toarray()\n        subset = np.logical_and(x > 0, y > 0)\n        x = x[subset]\n        y = y[subset]\n        X = np.c_[np.ones(len(x)), x]\n        offset[i], gamma[i] = np.linalg.pinv(X).dot(y)\n        subset_indices = np.flatnonzero(subset)\n        index = subset_indices, np.array([i for dummy in subset_indices])\n        X_du[index] = y - gamma[i]*x - offset[i]\n        # pl.scatter(x, y)\n        # pl.scatter(x, gamma[i]*x + offset[i])\n        # pl.scatter(x, X_du[index].toarray()[0])\n        # pl.show()\n    X_du = X_du.tocoo().tocsr()\n\n    # sc.pp.neighbors(adata, n_neighbors=100)\n\n    graph = compute_velocity_graph(adata, ad_u, X_du)\n\n    return adata if copy else None", "response": "Estimate RNA velocities for each cell in the given LAMM file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a PhenoGraph clustering process for the given set of modules.", "response": "def phenograph( adata,\n                k=30,\n                directed=False,\n                prune=False,\n                min_cluster_size=10,\n                jaccard=True,\n                primary_metric='euclidean',\n                n_jobs=-1,\n                q_tol=1e-3,\n                louvain_time_limit=2000,\n                nn_method='kdtree'):\n    \"\"\"\n        PhenoGraph clustering [Levine15]_.\n\n\n        :param adata:    Numpy ndarray of data to cluster,\n                         or sparse matrix of k-nearest neighbor graph\n                         If ndarray, n-by-d array of n cells in d dimensions\n                         If sparse matrix, n-by-n adjacency matrix\n        :param k:        Number of nearest neighbors to use in first\n                         step of graph construction\n        :param directed: Whether to use a symmetric (default) or\n                         asymmetric (\"directed\") graph\n                         The graph construction process produces a directed graph,\n                         which is symmetrized by one of two methods (see below)\n        :param prune:    Whether to symmetrize by taking the average (prune=False) or\n                         product (prune=True) between the graph and its transpose\n        :param min_cluster_size:\n                         Cells that end up in a cluster smaller than min_cluster_size\n                         are considered outliers and are assigned to -1 in the cluster labels\n        :param jaccard:  If True, use Jaccard metric between k-neighborhoods to build graph\n                         If False, use a Gaussian kernel\n        :param primary_metric:\n                         Distance metric to define nearest neighbors\n                         Options include: {'euclidean','manhattan','correlation','cosine'}.\n                         Note that performance will be slower for correlation and cosine\n        :param n_jobs:   Nearest Neighbors and Jaccard coefficients will be computed\n                         in parallel using n_jobs. If n_jobs=-1, the number of jobs is\n                         determined automatically\n        :param q_tol:    Tolerance (i.e., precision) for monitoring modularity optimization\n        :param louvain_time_limit:\n                         Maximum number of seconds to run modularity optimization.\n                         If exceeded the best result so far is returned\n        :param nn_method:\n                         Whether to use brute force or kdtree for nearest neighbor\n                         search. For very large high-dimensional data sets, brute force\n                         (with parallel computation) performs faster than kdtree\n\n        :return communities:\n                        numpy integer array of community assignments for each\n                        row in data\n        :return graph:  numpy sparse array of the graph that was used for clustering\n        :return Q:      the modularity score for communities on graph\n\n\n        Example\n        -------\n        \n        >>> import scanpy.external as sce\n        >>> import scanpy.api as sc\n        >>> import numpy as np\n        >>> import pandas as pd\n        \n        Assume adata is your annotated data which has the normalized data.\n\n        Then do PCA:\n        \n        >>> sc.tl.pca(adata, n_comps = 100)\n        \n        Compute phenograph clusters:\n        \n        >>> result = sce.tl.phenograph(adata.obsm['X_pca'], k = 30)\n        \n        Embed the phenograph result into adata as a *categorical* variable (this helps in plotting):\n        \n        >>> adata.obs['pheno'] = pd.Categorical(result[0])\n        \n        Check by typing \"adata\" and you should see under obs a key called 'pheno'.\n        \n        Now to show phenograph on tSNE (for example):\n        \n        Compute tSNE:\n        \n        >>> sc.tl.tsne(adata, random_state = 7)\n        \n        Plot phenograph clusters on tSNE:\n        \n        >>> sc.pl.tsne(adata, color = ['pheno'], s = 100, palette = sc.pl.palettes.vega_20_scanpy, legend_fontsize = 10)\n        \n        Cluster and cluster centroids for input Numpy ndarray\n        \n        >>> df = np.random.rand(1000,40)\n        >>> df.shape\n        (1000, 40)\n        >>> result = sce.tl.phenograph(df, k=50)\n        Finding 50 nearest neighbors using minkowski metric and 'auto' algorithm\n        Neighbors computed in 0.16141605377197266 seconds\n        Jaccard graph constructed in 0.7866239547729492 seconds\n        Wrote graph to binary file in 0.42542195320129395 seconds\n        Running Louvain modularity optimization\n        After 1 runs, maximum modularity is Q = 0.223536\n        After 2 runs, maximum modularity is Q = 0.235874\n        Louvain completed 22 runs in 1.5609488487243652 seconds\n        PhenoGraph complete in 2.9466471672058105 seconds\n        \n        New results can be pushed into adata object:\n        \n        >>> dframe = pd.DataFrame(data=df, columns=range(df.shape[1]),index=range(df.shape[0]) )\n        >>> adata = sc.AnnData( X=dframe, obs=dframe, var=dframe)\n        >>> adata.obs['pheno'] = pd.Categorical(result[0])\n        \n\n    \"\"\"\n    logg.info('PhenoGraph clustering', r=True)\n\n    try:\n        import phenograph\n    except ImportError:\n        raise ImportError(\n            'please install phenograph: '\n            'pip3 install git+https://github.com/jacoblevine/phenograph.git')\n\n    communities, graph, Q = phenograph.cluster(\n                                        data=adata,\n                                        k=k,\n                                        directed=directed,\n                                        prune=prune,\n                                        min_cluster_size=min_cluster_size,\n                                        jaccard=jaccard,\n                                        primary_metric=primary_metric,\n                                        n_jobs=n_jobs,\n                                        q_tol=q_tol,\n                                        louvain_time_limit=louvain_time_limit,\n                                        nn_method=nn_method\n                                        )\n\n    logg.info('    finished', time=True)\n\n    return communities, graph, Q"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef timeseries(X, **kwargs):\n    pl.figure(figsize=(2*rcParams['figure.figsize'][0], rcParams['figure.figsize'][1]),\n              subplotpars=sppars(left=0.12, right=0.98, bottom=0.13))\n    timeseries_subplot(X, **kwargs)", "response": "Plot X. See timeseries_subplot."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting X in a subplot.", "response": "def timeseries_subplot(X,\n                       time=None,\n                       color=None,\n                       var_names=(),\n                       highlightsX=(),\n                       xlabel='',\n                       ylabel='gene expression',\n                       yticks=None,\n                       xlim=None,\n                       legend=True,\n                       palette=None,\n                       color_map='viridis'):\n    \"\"\"Plot X.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Call this with:\n        X with one column, color categorical.\n        X with one column, color continuous.\n        X with n columns, color is of length n.\n    \"\"\"\n\n    if color is not None:\n        use_color_map = isinstance(color[0], float) or isinstance(color[0], np.float32)\n    palette = default_palette(palette)\n    x_range = np.arange(X.shape[0]) if time is None else time\n    if X.ndim == 1: X = X[:, None]\n    if X.shape[1] > 1:\n        colors = palette[:X.shape[1]].by_key()['color']\n        subsets = [(x_range, X[:, i]) for i in range(X.shape[1])]\n    elif use_color_map:\n        colors = [color]\n        subsets = [(x_range, X[:, 0])]\n    else:\n        levels, _ = np.unique(color, return_inverse=True)\n        colors = np.array(palette[:len(levels)].by_key()['color'])\n        subsets = [(x_range[color == l], X[color == l, :]) for l in levels]\n\n    for i, (x, y) in enumerate(subsets):\n        pl.scatter(\n            x, y,\n            marker='.',\n            edgecolor='face',\n            s=rcParams['lines.markersize'],\n            c=colors[i],\n            label=var_names[i] if len(var_names) > 0 else '',\n            cmap=color_map,\n            rasterized=settings._vector_friendly)\n    ylim = pl.ylim()\n    for ih, h in enumerate(highlightsX):\n        pl.plot([h, h], [ylim[0], ylim[1]], '--', color='black')\n    pl.ylim(ylim)\n    if xlim is not None:\n        pl.xlim(xlim)\n    pl.xlabel(xlabel)\n    pl.ylabel(ylabel)\n    if yticks is not None:\n        pl.yticks(yticks)\n    if len(var_names) > 0 and legend:\n        pl.legend(frameon=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef timeseries_as_heatmap(X, var_names=None, highlightsX=None, color_map=None):\n    if highlightsX is None:\n        highlightsX = []\n    if var_names is None:\n        var_names = []\n    if len(var_names) == 0:\n        var_names = np.arange(X.shape[1])\n    if var_names.ndim == 2:\n        var_names = var_names[:, 0]\n\n    # transpose X\n    X = X.T\n    minX = np.min(X)\n\n    # insert space into X\n    if False:\n        # generate new array with highlightsX\n        space = 10  # integer\n        Xnew = np.zeros((X.shape[0], X.shape[1] + space*len(highlightsX)))\n        hold = 0\n        _hold = 0\n        space_sum = 0\n        for ih, h in enumerate(highlightsX):\n            _h = h + space_sum\n            Xnew[:, _hold:_h] = X[:, hold:h]\n            Xnew[:, _h:_h+space] = minX * np.ones((X.shape[0], space))\n            # update variables\n            space_sum += space\n            _hold = _h + space\n            hold = h\n        Xnew[:, _hold:] = X[:, hold:]\n\n    fig = pl.figure(figsize=(1.5*4, 2*4))\n    im = pl.imshow(np.array(X, dtype=np.float_), aspect='auto',\n                   interpolation='nearest', cmap=color_map)\n    pl.colorbar(shrink=0.5)\n    pl.yticks(range(X.shape[0]), var_names)\n    for ih, h in enumerate(highlightsX):\n        pl.plot([h, h], [0, X.shape[0]], '--', color='black')\n    pl.xlim([0, X.shape[1]-1])\n    pl.ylim([0, X.shape[0]-1])", "response": "Plot a timeseries as heatmap."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef savefig(writekey, dpi=None, ext=None):\n    if dpi is None:\n        # we need this as in notebooks, the internal figures are also influenced by 'savefig.dpi' this...\n        if not isinstance(rcParams['savefig.dpi'], str) and rcParams['savefig.dpi'] < 150:\n            if settings._low_resolution_warning:\n                logg.warn(\n                    'You are using a low resolution (dpi<150) for saving figures.\\n'\n                    'Consider running `set_figure_params(dpi_save=...)`, which will '\n                    'adjust `matplotlib.rcParams[\\'savefig.dpi\\']`')\n                settings._low_resolution_warning = False\n        else:\n            dpi = rcParams['savefig.dpi']\n    if not os.path.exists(settings.figdir): os.makedirs(settings.figdir)\n    if settings.figdir[-1] != '/': settings.figdir += '/'\n    if ext is None: ext = settings.file_format_figs\n    filename = settings.figdir + writekey + settings.plot_suffix + '.' + ext\n    # output the following msg at warning level; it's really important for the user\n    logg.msg('saving figure to file', filename, v=1)\n    pl.savefig(filename, dpi=dpi, bbox_inches='tight')", "response": "Save current figure to file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef scatter_group(ax, key, imask, adata, Y, projection='2d', size=3, alpha=None):\n    mask = adata.obs[key].cat.categories[imask] == adata.obs[key].values\n    color = adata.uns[key + '_colors'][imask]\n    if not isinstance(color[0], str):\n        from matplotlib.colors import rgb2hex\n        color = rgb2hex(adata.uns[key + '_colors'][imask])\n    if not is_color_like(color):\n        raise ValueError('\"{}\" is not a valid matplotlib color.'.format(color))\n    data = [Y[mask, 0], Y[mask, 1]]\n    if projection == '3d': data.append(Y[mask, 2])\n    ax.scatter(*data,\n               marker='.',\n               alpha=alpha,\n               c=color,\n               edgecolors='none',\n               s=size,\n               label=adata.obs[key].cat.categories[imask],\n               rasterized=settings._vector_friendly)\n    return mask", "response": "Scatter of group using representation of data Y."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setup_axes(\n        ax=None,\n        panels='blue',\n        colorbars=[False],\n        right_margin=None,\n        left_margin=None,\n        projection='2d',\n        show_ticks=False):\n    \"\"\"Grid of axes for plotting, legends and colorbars.\n    \"\"\"\n    if '3d' in projection: from mpl_toolkits.mplot3d import Axes3D\n    avail_projections = {'2d', '3d'}\n    if projection not in avail_projections:\n        raise ValueError('choose projection from', avail_projections)\n    if left_margin is not None:\n        raise ValueError('Currently not supporting to pass `left_margin`.')\n    if np.any(colorbars) and right_margin is None:\n        right_margin = 1 - rcParams['figure.subplot.right'] + 0.21  # 0.25\n    elif right_margin is None:\n        right_margin = 1 - rcParams['figure.subplot.right'] + 0.06  # 0.10\n    # make a list of right margins for each panel\n    if not isinstance(right_margin, list):\n        right_margin_list = [right_margin for i in range(len(panels))]\n    else:\n        right_margin_list = right_margin\n\n    # make a figure with len(panels) panels in a row side by side\n    top_offset = 1 - rcParams['figure.subplot.top']\n    bottom_offset = 0.15 if show_ticks else 0.08\n    left_offset = 1 if show_ticks else 0.3  # in units of base_height\n    base_height = rcParams['figure.figsize'][1]\n    height = base_height\n    base_width = rcParams['figure.figsize'][0]\n    if show_ticks: base_width *= 1.1\n\n    draw_region_width = base_width - left_offset - top_offset - 0.5  # this is kept constant throughout\n\n    right_margin_factor = sum([1 + right_margin for right_margin in right_margin_list])\n    width_without_offsets = right_margin_factor * draw_region_width  # this is the total width that keeps draw_region_width\n\n    right_offset = (len(panels) - 1) * left_offset\n    figure_width = width_without_offsets + left_offset + right_offset\n    draw_region_width_frac = draw_region_width / figure_width\n    left_offset_frac = left_offset / figure_width\n    right_offset_frac = 1 - (len(panels) - 1) * left_offset_frac\n\n    if ax is None:\n        pl.figure(figsize=(figure_width, height),\n                  subplotpars=sppars(left=0, right=1, bottom=bottom_offset))\n    left_positions = [left_offset_frac, left_offset_frac + draw_region_width_frac]\n    for i in range(1, len(panels)):\n        right_margin = right_margin_list[i-1]\n        left_positions.append(left_positions[-1] + right_margin * draw_region_width_frac)\n        left_positions.append(left_positions[-1] + draw_region_width_frac)\n    panel_pos = [[bottom_offset], [1-top_offset], left_positions]\n\n    axs = []\n    if ax is None:\n        for icolor, color in enumerate(panels):\n            left = panel_pos[2][2*icolor]\n            bottom = panel_pos[0][0]\n            width = draw_region_width / figure_width\n            height = panel_pos[1][0] - bottom\n            if projection == '2d': ax = pl.axes([left, bottom, width, height])\n            elif projection == '3d': ax = pl.axes([left, bottom, width, height], projection='3d')\n            axs.append(ax)\n    else:\n        axs = ax if isinstance(ax, list) else [ax]\n\n    return axs, panel_pos, draw_region_width, figure_width", "response": "Setup the axes for plotting legends and colorbars."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting scatter plot of data.", "response": "def scatter_base(\n    Y,\n    colors='blue',\n    sort_order=True,\n    alpha=None,\n    highlights=[],\n    right_margin=None,\n    left_margin=None,\n    projection='2d',\n    title=None,\n    component_name='DC',\n    component_indexnames=[1, 2, 3],\n    axis_labels=None,\n    colorbars=[False],\n    sizes=[1],\n    color_map='viridis',\n    show_ticks=True,\n    ax=None,\n) -> Union[Axes, List[Axes]]:\n    \"\"\"Plot scatter plot of data.\n\n    Parameters\n    ----------\n    Y : np.ndarray\n        Data array.\n    projection : {'2d', '3d'}\n\n    Returns\n    -------\n    Depending on whether supplying a single array or a list of arrays,\n    return a single axis or a list of axes.\n    \"\"\"\n    if isinstance(highlights, dict):\n        highlights_indices = sorted(highlights)\n        highlights_labels = [highlights[i] for i in highlights_indices]\n    else:\n        highlights_indices = highlights\n        highlights_labels = []\n    # if we have a single array, transform it into a list with a single array\n    if type(colors) == str: colors = [colors]\n    if len(sizes) != len(colors) and len(sizes) == 1:\n        sizes = [sizes[0] for i in range(len(colors))]\n    axs, panel_pos, draw_region_width, figure_width = setup_axes(\n        ax=ax, panels=colors, colorbars=colorbars, projection=projection,\n        right_margin=right_margin, left_margin=left_margin,\n        show_ticks=show_ticks)\n    for icolor, color in enumerate(colors):\n        ax = axs[icolor]\n        left = panel_pos[2][2*icolor]\n        bottom = panel_pos[0][0]\n        width = draw_region_width / figure_width\n        height = panel_pos[1][0] - bottom\n        Y_sort = Y\n        if not is_color_like(color) and sort_order:\n            sort = np.argsort(color)\n            color = color[sort]\n            Y_sort = Y[sort]\n        if projection == '2d': data = Y_sort[:, 0], Y_sort[:, 1]\n        elif projection == '3d': data = Y_sort[:, 0], Y_sort[:, 1], Y_sort[:, 2]\n        if not isinstance(color, str) or color != 'white':\n            sct = ax.scatter(*data,\n                             marker='.',\n                             c=color,\n                             alpha=alpha,\n                             edgecolors='none',  # 'face',\n                             s=sizes[icolor],\n                             cmap=color_map,\n                             rasterized=settings._vector_friendly)\n        if colorbars[icolor]:\n            width = 0.006 * draw_region_width / len(colors)\n            left = panel_pos[2][2*icolor+1] + (1.2 if projection == '3d' else 0.2) * width\n            rectangle = [left, bottom, width, height]\n            fig = pl.gcf()\n            ax_cb = fig.add_axes(rectangle)\n            cb = pl.colorbar(sct, format=ticker.FuncFormatter(ticks_formatter),\n                             cax=ax_cb)\n        # set the title\n        if title is not None: ax.set_title(title[icolor])\n        # output highlighted data points\n        for iihighlight, ihighlight in enumerate(highlights_indices):\n            ihighlight = ihighlight if isinstance(ihighlight, int) else int(ihighlight)\n            data = [Y[ihighlight, 0]], [Y[ihighlight, 1]]\n            if '3d' in projection:\n                data = [Y[ihighlight, 0]], [Y[ihighlight, 1]], [Y[ihighlight, 2]]\n            ax.scatter(*data, c='black',\n                       facecolors='black', edgecolors='black',\n                       marker='x', s=10, zorder=20)\n            highlight_text = (highlights_labels[iihighlight] if\n                              len(highlights_labels) > 0\n                              else str(ihighlight))\n            # the following is a Python 2 compatibility hack\n            ax.text(*([d[0] for d in data] + [highlight_text]),\n                    zorder=20,\n                    fontsize=10,\n                    color='black')\n        if not show_ticks:\n            ax.set_xticks([])\n            ax.set_yticks([])\n            if '3d' in projection: ax.set_zticks([])\n    # set default axis_labels\n    if axis_labels is None:\n        axis_labels = [[component_name + str(i) for i in idcs]\n                       for idcs in\n                       [component_indexnames for iax in range(len(axs))]]\n    else:\n        axis_labels = [[axis_labels[0], axis_labels[1]] for i in range(len(axs))]\n    for iax, ax in enumerate(axs):\n        ax.set_xlabel(axis_labels[iax][0])\n        ax.set_ylabel(axis_labels[iax][1])\n        if '3d' in projection:\n            # shift the label closer to the axis\n            ax.set_zlabel(axis_labels[iax][2], labelpad=-7)\n    for ax in axs:\n        # scale limits to match data\n        ax.autoscale_view()\n    return axs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting a scatter plot of data.", "response": "def scatter_single(ax, Y, *args, **kwargs):\n    \"\"\"Plot scatter plot of data.\n\n    Parameters\n    ----------\n    ax : matplotlib.axis\n        Axis to plot on.\n    Y : np.array\n        Data array, data to be plotted needs to be in the first two columns.\n    \"\"\"\n    if 's' not in kwargs:\n        kwargs['s'] = 2 if Y.shape[0] > 500 else 10\n    if 'edgecolors' not in kwargs:\n        kwargs['edgecolors'] = 'face'\n    ax.scatter(Y[:, 0], Y[:, 1], **kwargs, rasterized=settings._vector_friendly)\n    ax.set_xticks([])\n    ax.set_yticks([])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef arrows_transitions(ax, X, indices, weight=None):\n    step = 1\n    width = axis_to_data(ax, 0.001)\n    if X.shape[0] > 300:\n        step = 5\n        width = axis_to_data(ax, 0.0005)\n    if X.shape[0] > 500:\n        step = 30\n        width = axis_to_data(ax, 0.0001)\n    head_width = 10*width\n    for ix, x in enumerate(X):\n        if ix % step == 0:\n            X_step = X[indices[ix]] - x\n            # don't plot arrow of length 0\n            for itrans in range(X_step.shape[0]):\n                alphai = 1\n                widthi = width\n                head_widthi = head_width\n                if weight is not None:\n                    alphai *= weight[ix, itrans]\n                    widthi *= weight[ix, itrans]\n                if np.any(X_step[itrans, :1]):\n                    ax.arrow(x[0], x[1],\n                             X_step[itrans, 0], X_step[itrans, 1],\n                             length_includes_head=True,\n                             width=widthi,\n                             head_width=head_widthi,\n                             alpha=alphai,\n                             color='grey')", "response": "Plots arrows of transitions in data matrix X."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntake some 1d data and scale it so that min matches 0 and max 1.", "response": "def scale_to_zero_one(x):\n    \"\"\"Take some 1d data and scale it so that min matches 0 and max 1.\n    \"\"\"\n    xscaled = x - np.min(x)\n    xscaled /= np.max(xscaled)\n    return xscaled"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hierarchy_pos(G, root, levels=None, width=1., height=1.):\n    TOTAL = \"total\"\n    CURRENT = \"current\"\n\n    def make_levels(levels, node=root, currentLevel=0, parent=None):\n        \"\"\"Compute the number of nodes for each level\n        \"\"\"\n        if currentLevel not in levels:\n            levels[currentLevel] = {TOTAL: 0, CURRENT: 0}\n        levels[currentLevel][TOTAL] += 1\n        neighbors = list(G.neighbors(node))\n        if parent is not None:\n            neighbors.remove(parent)\n        for neighbor in neighbors:\n            levels = make_levels(levels, neighbor, currentLevel + 1, node)\n        return levels\n\n    def make_pos(pos, node=root, currentLevel=0, parent=None, vert_loc=0):\n        dx = 1/levels[currentLevel][TOTAL]\n        left = dx/2\n        pos[node] = ((left + dx*levels[currentLevel][CURRENT])*width,\n                     vert_loc)\n        levels[currentLevel][CURRENT] += 1\n        neighbors = list(G.neighbors(node))\n        if parent is not None:\n            neighbors.remove(parent)\n        for neighbor in neighbors:\n            pos = make_pos(pos, neighbor, currentLevel + 1, node, vert_loc-vert_gap)\n        return pos\n\n    if levels is None:\n        levels = make_levels({})\n    else:\n        levels = {l: {TOTAL: levels[l], CURRENT: 0} for l in levels}\n    vert_gap = height / (max([l for l in levels])+1)\n    return make_pos({})", "response": "Return the tree layout for a node in a hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nzooming into axis. Parameters ----------", "response": "def zoom(ax, xy='x', factor=1):\n    \"\"\"Zoom into axis.\n\n    Parameters\n    ----------\n    \"\"\"\n    limits = ax.get_xlim() if xy == 'x' else ax.get_ylim()\n    new_limits = (0.5*(limits[0] + limits[1])\n                  + 1./factor * np.array((-0.5, 0.5)) * (limits[1] - limits[0]))\n    if xy == 'x':\n        ax.set_xlim(new_limits)\n    else:\n        ax.set_ylim(new_limits)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_ax_size(ax, fig):\n    bbox = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n    width, height = bbox.width, bbox.height\n    width *= fig.dpi\n    height *= fig.dpi", "response": "Get axis size from matplotlib. axis. Axis object from matplotlib. Figure object from matplotlib. Axis object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef axis_to_data(ax, width):\n    xlim = ax.get_xlim()\n    widthx = width*(xlim[1] - xlim[0])\n    ylim = ax.get_ylim()\n    widthy = width*(ylim[1] - ylim[0])\n    return 0.5*(widthx + widthy)", "response": "Returns the corresponding in data\n    coordinates for a width in xaxis coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef axis_to_data_points(ax, points_axis):\n    axis_to_data = ax.transAxes + ax.transData.inverted()\n    return axis_to_data.transform(points_axis)", "response": "Maps points in axis coordinates to data coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nnormalize the total count of genes by fraction.", "response": "def normalize_total(adata, target_sum=None, fraction=1, key_added=None,\n                    layers=None, layer_norm=None, inplace=True):\n    \"\"\"\\\n    {norm_descr}\n\n    {params_bulk}\n\n    {norm_return}\n\n    {examples}\n    \"\"\"\n    if fraction < 0 or fraction > 1:\n        raise ValueError('Choose fraction between 0 and 1.')\n\n    X = adata.X\n    gene_subset = None\n    if not inplace:\n    # not recarray because need to support sparse\n        dat = {}\n\n    if fraction < 1:\n        logg.msg('normalizing by count per cell for \\\n                  genes that make up less than fraction * total count per cell', r=True)\n        X = adata.X\n\n        counts_per_cell = X.sum(1)\n        counts_per_cell = np.ravel(counts_per_cell)\n\n        gene_subset = (X>counts_per_cell[:, None]*fraction).sum(0)\n        gene_subset = (np.ravel(gene_subset) == 0)\n    else:\n        logg.msg('normalizing by total count per cell', r=True)\n\n    X = X if gene_subset is None else adata[:, gene_subset].X\n    counts_per_cell = X.sum(1)\n    # get rid of adata view\n    counts_per_cell = np.ravel(counts_per_cell).copy()\n    del X\n    del gene_subset\n\n    if key_added is not None:\n        adata.obs[key_added] = counts_per_cell\n\n    cell_subset = counts_per_cell>0\n    if not np.all(cell_subset):\n        logg.warn('Some cells have total count of genes equal to zero')\n\n    if layer_norm == 'after':\n        after = target_sum\n    elif layer_norm == 'X':\n        after = np.median(counts_per_cell[cell_subset])\n    elif layer_norm is None:\n        after = None\n    else:\n        raise ValueError('layer_norm should be \"after\", \"X\" or None')\n    del cell_subset\n\n    if inplace:\n        if hasattr(adata.X, '__itruediv__'):\n            _normalize_data(adata.X, counts_per_cell, target_sum)\n        else:\n            adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True)\n    else:\n        dat['X'] = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True)\n\n    layers = adata.layers.keys() if layers == 'all' else layers\n    if layers is not None:\n        for layer in layers:\n            L = adata.layers[layer]\n            counts = np.ravel(L.sum(1))\n            if inplace:\n                if hasattr(L, '__itruediv__'):\n                    _normalize_data(L, counts, after)\n                else:\n                    adata.layers[layer] = _normalize_data(L, counts, after, copy=True)\n            else:\n                dat[layer] = _normalize_data(L, counts, after, copy=True)\n\n    logg.msg('    finished', t=True, end=': ')\n    logg.msg('normalized adata.X')\n    if key_added is not None:\n        logg.msg('and added \\'{}\\', counts per cell before normalization (adata.obs)'\n            .format(key_added))\n\n    return dat if not inplace else None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting matplotlib. rcParams to Scanpy defaults.", "response": "def set_rcParams_scanpy(fontsize=14, color_map=None):\n    \"\"\"Set matplotlib.rcParams to Scanpy defaults.\n\n    Call this through `settings.set_figure_params`.\n    \"\"\"\n\n    # figure\n    rcParams['figure.figsize'] = (4, 4)\n    rcParams['figure.subplot.left'] = 0.18\n    rcParams['figure.subplot.right'] = 0.96\n    rcParams['figure.subplot.bottom'] = 0.15\n    rcParams['figure.subplot.top'] = 0.91\n\n    rcParams['lines.linewidth'] = 1.5  # the line width of the frame\n    rcParams['lines.markersize'] = 6\n    rcParams['lines.markeredgewidth'] = 1\n\n    # font\n    rcParams['font.sans-serif'] = [\n        'Arial',\n        'Helvetica',\n        'DejaVu Sans',\n        'Bitstream Vera Sans',\n        'sans-serif',\n    ]\n    fontsize = fontsize\n    rcParams['font.size'] = fontsize\n    rcParams['legend.fontsize'] = 0.92 * fontsize\n    rcParams['axes.titlesize'] = fontsize\n    rcParams['axes.labelsize'] = fontsize\n\n    # legend\n    rcParams['legend.numpoints'] = 1\n    rcParams['legend.scatterpoints'] = 1\n    rcParams['legend.handlelength'] = 0.5\n    rcParams['legend.handletextpad'] = 0.4\n\n    # color cycle\n    rcParams['axes.prop_cycle'] = cycler(color=palettes.default_20)\n\n    # lines\n    rcParams['axes.linewidth'] = 0.8\n    rcParams['axes.edgecolor'] = 'black'\n    rcParams['axes.facecolor'] = 'white'\n\n    # ticks\n    rcParams['xtick.color'] = 'k'\n    rcParams['ytick.color'] = 'k'\n    rcParams['xtick.labelsize'] = fontsize\n    rcParams['ytick.labelsize'] = fontsize\n\n    # axes grid\n    rcParams['axes.grid'] = True\n    rcParams['grid.color'] = '.8'\n\n    # color map\n    rcParams['image.cmap'] = rcParams['image.cmap'] if color_map is None else color_map"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef preprocess_with_pca(adata, n_pcs=None, random_state=0):\n    if n_pcs == 0:\n        logg.info('    using data matrix X directly (no PCA)')\n        return adata.X\n    elif n_pcs is None and 'X_pca' in adata.obsm_keys():\n        logg.info('    using \\'X_pca\\' with n_pcs = {}'\n                  .format(adata.obsm['X_pca'].shape[1]))\n        return adata.obsm['X_pca']\n    elif ('X_pca' in adata.obsm_keys()\n          and adata.obsm['X_pca'].shape[1] >= n_pcs):\n        logg.info('    using \\'X_pca\\' with n_pcs = {}'\n                  .format(n_pcs))\n        return adata.obsm['X_pca'][:, :n_pcs]\n    else:\n        n_pcs = N_PCS if n_pcs is None else n_pcs\n        if adata.X.shape[1] > n_pcs:\n            logg.info('    computing \\'X_pca\\' with n_pcs = {}'.format(n_pcs))\n            logg.hint('avoid this by setting n_pcs = 0')\n            X = pca(adata.X, n_comps=n_pcs, random_state=random_state)\n            adata.obsm['X_pca'] = X\n            return X\n        else:\n            logg.info('    using data matrix X directly (no PCA)')\n            return adata.X", "response": "Preprocess the data matrix X with PCA."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun a builtin scanpy command or a scanpy - * subcommand.", "response": "def main(argv: Optional[Sequence[str]] = None, *, check: bool = True, **runargs) -> Optional[CompletedProcess]:\n    \"\"\"Run a builtin scanpy command or a scanpy-* subcommand.\n\n    Uses :func:`subcommand.run` for the latter: ``~run(['scanpy', *argv], **runargs)``\n    \"\"\"\n    parser = ArgumentParser(description=\"There are a few packages providing commands. Try e.g. `pip install scanpy-scripts`!\")\n    parser.set_defaults(func=parser.print_help)\n    \n    subparsers: DelegatingSubparsersAction = parser.add_subparsers(\n        action=DelegatingSubparsersAction,\n        _command='scanpy',\n        _runargs={**runargs, 'check': check},\n    )\n    \n    parser_settings = subparsers.add_parser('settings')\n    parser_settings.set_defaults(func=cmd_settings)\n    \n    args = parser.parse_args(argv)\n    return args.func()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef console_main():\n    cmd = main(check=False)\n    if cmd is not None:\n        sys.exit(cmd.returncode)", "response": "This serves as CLI entry point and will not show a Python traceback if a called command fails"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef louvain(\n    adata: AnnData,\n    resolution: Optional[float] = None,\n    random_state: int = 0,\n    restrict_to: Optional[Tuple[str, Sequence[str]]] = None,\n    key_added: Optional[str] = 'louvain',\n    adjacency: Optional[spmatrix] = None,\n    flavor: str = 'vtraag',\n    directed: bool = True,\n    use_weights: bool = False,\n    partition_type: Optional[Type[MutableVertexPartition]] = None,\n    partition_kwargs: Optional[Mapping[str, Any]]=None,\n    copy: bool = False,\n) -> Optional[AnnData]:\n    \"\"\"Cluster cells into subgroups [Blondel08]_ [Levine15]_ [Traag17]_.\n\n    Cluster cells using the Louvain algorithm [Blondel08]_ in the implementation\n    of [Traag17]_. The Louvain algorithm has been proposed for single-cell\n    analysis by [Levine15]_.\n\n    This requires having ran :func:`~scanpy.pp.neighbors` or :func:`~scanpy.external.pp.bbknn` first,\n    or explicitly passing a ``adjacency`` matrix.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    resolution\n        For the default flavor (``'vtraag'``), you can provide a resolution\n        (higher resolution means finding more and smaller clusters),\n        which defaults to 1.0. See \u201cTime as a resolution parameter\u201d in [Lambiotte09]_.\n    random_state\n        Change the initialization of the optimization.\n    restrict_to\n        Restrict the clustering to the categories within the key for sample\n        annotation, tuple needs to contain ``(obs_key, list_of_categories)``.\n    key_added\n        Key under which to add the cluster labels. (default: ``'louvain'``)\n    adjacency\n        Sparse adjacency matrix of the graph, defaults to\n        ``adata.uns['neighbors']['connectivities']``.\n    flavor : {``'vtraag'``, ``'igraph'``}\n        Choose between to packages for computing the clustering.\n        ``'vtraag'`` is much more powerful, and the default.\n    directed\n        Interpret the ``adjacency`` matrix as directed graph?\n    use_weights\n        Use weights from knn graph.\n    partition_type\n        Type of partition to use.\n        Only a valid argument if ``flavor`` is ``'vtraag'``.\n    partition_kwargs\n        Key word arguments to pass to partitioning,\n        if ``vtraag`` method is being used.\n    copy\n        Copy adata or modify it inplace.\n\n    Returns\n    -------\n    :obj:`None`\n        By default (``copy=False``), updates ``adata`` with the following fields:\n\n        ``adata.obs['louvain']`` (:class:`pandas.Series`, dtype ``category``)\n            Array of dim (number of samples) that stores the subgroup id\n            (``'0'``, ``'1'``, ...) for each cell.\n\n    :class:`~anndata.AnnData`\n        When ``copy=True`` is set, a copy of ``adata`` with those fields is returned.\n    \"\"\"\n    logg.info('running Louvain clustering', r=True)\n    if (flavor != 'vtraag') and (partition_type is not None):\n        raise ValueError(\n            '`partition_type` is only a valid argument when `flavour` is \"vtraag\"')\n    adata = adata.copy() if copy else adata\n    if adjacency is None and 'neighbors' not in adata.uns:\n        raise ValueError(\n            'You need to run `pp.neighbors` first to compute a neighborhood graph.')\n    if adjacency is None:\n        adjacency = adata.uns['neighbors']['connectivities']\n    if restrict_to is not None:\n        restrict_key, restrict_categories = restrict_to\n        adjacency, restrict_indices = restrict_adjacency(\n            adata,\n            restrict_key,\n            restrict_categories,\n            adjacency\n        )\n    if flavor in {'vtraag', 'igraph'}:\n        if flavor == 'igraph' and resolution is not None:\n            logg.warn('`resolution` parameter has no effect for flavor \"igraph\"')\n        if directed and flavor == 'igraph':\n            directed = False\n        if not directed: logg.m('    using the undirected graph', v=4)\n        g = utils.get_igraph_from_adjacency(adjacency, directed=directed)\n        if use_weights:\n            weights = np.array(g.es[\"weight\"]).astype(np.float64)\n        else:\n            weights = None\n        if flavor == 'vtraag':\n            import louvain\n            if partition_kwargs is None:\n                partition_kwargs = {}\n            if partition_type is None:\n                partition_type = louvain.RBConfigurationVertexPartition\n            if resolution is not None:\n                partition_kwargs[\"resolution_parameter\"] = resolution\n            if use_weights:\n                partition_kwargs[\"weights\"] = weights\n            logg.info('    using the \"louvain\" package of Traag (2017)')\n            louvain.set_rng_seed(random_state)\n            part = louvain.find_partition(\n                g, partition_type,\n                **partition_kwargs,\n            )\n            # adata.uns['louvain_quality'] = part.quality()\n        else:\n            part = g.community_multilevel(weights=weights)\n        groups = np.array(part.membership)\n    elif flavor == 'taynaud':\n        # this is deprecated\n        import networkx as nx\n        import community\n        g = nx.Graph(adjacency)\n        partition = community.best_partition(g)\n        groups = np.zeros(len(partition), dtype=int)\n        for k, v in partition.items(): groups[k] = v\n    else:\n        raise ValueError('`flavor` needs to be \"vtraag\" or \"igraph\" or \"taynaud\".')\n    if restrict_to is not None:\n        groups = rename_groups(\n            adata,\n            key_added,\n            restrict_key,\n            restrict_categories,\n            restrict_indices,\n            groups\n        )\n    adata.obs[key_added] = pd.Categorical(\n        values=groups.astype('U'),\n        categories=natsorted(np.unique(groups).astype('U')),\n    )\n    adata.uns['louvain'] = {}\n    adata.uns['louvain']['params'] = {'resolution': resolution, 'random_state': random_state}\n    logg.info('    finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\\n')\n    logg.hint('found {} clusters and added\\n'\n              '    \\'{}\\', the cluster labels (adata.obs, categorical)'\n              .format(len(np.unique(groups)), key_added))\n    return adata if copy else None", "response": "Return an anndata object with the Louvain clustering algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nranking genes for characterizing groups.", "response": "def rank_genes_groups(\n    adata,\n    groupby,\n    use_raw=True,\n    groups: Union[str, Iterable[str]] = 'all',\n    reference='rest',\n    n_genes=100,\n    rankby_abs=False,\n    key_added=None,\n    copy=False,\n    method='t-test_overestim_var',\n    corr_method='benjamini-hochberg',\n    log_transformed=True,\n    **kwds\n):\n    \"\"\"Rank genes for characterizing groups.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    groupby : `str`\n        The key of the observations grouping to consider.\n    use_raw : `bool`, optional (default: `True`)\n        Use `raw` attribute of `adata` if present.\n    groups\n        Subset of groups, e.g. `['g1', 'g2', 'g3']`, to which comparison shall\n        be restricted, or `'all'` (default), for all groups.\n    reference : `str`, optional (default: `'rest'`)\n        If `'rest'`, compare each group to the union of the rest of the group.  If\n        a group identifier, compare with respect to this group.\n    n_genes : `int`, optional (default: 100)\n        The number of genes that appear in the returned tables.\n    method : `{'logreg', 't-test', 'wilcoxon', 't-test_overestim_var'}`, optional (default: 't-test_overestim_var')\n        If 't-test', uses t-test, if 'wilcoxon', uses Wilcoxon-Rank-Sum. If\n        't-test_overestim_var', overestimates variance of each group. If\n        'logreg' uses logistic regression, see [Ntranos18]_, `here\n        <https://github.com/theislab/scanpy/issues/95>`__ and `here\n        <http://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters>`__, for\n        why this is meaningful.\n    corr_method : `{'benjamini-hochberg', 'bonferroni'}`, optional (default: 'benjamini-hochberg')\n        p-value correction method. Used only for 't-test', 't-test_overestim_var',\n        and 'wilcoxon' methods.\n    rankby_abs : `bool`, optional (default: `False`)\n        Rank genes by the absolute value of the score, not by the\n        score. The returned scores are never the absolute values.\n    **kwds : keyword parameters\n        Are passed to test methods. Currently this affects only parameters that\n        are passed to `sklearn.linear_model.LogisticRegression\n        <http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html>`__.\n        For instance, you can pass `penalty='l1'` to try to come up with a\n        minimal set of genes that are good predictors (sparse solution meaning\n        few non-zero fitted coefficients).\n\n    Returns\n    -------\n    **names** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the gene\n        names. Ordered according to scores.\n    **scores** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the z-score\n        underlying the computation of a p-value for each gene for each\n        group. Ordered according to scores.\n    **logfoldchanges** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the log2\n        fold change for each gene for each group. Ordered according to\n        scores. Only provided if method is 't-test' like.\n        Note: this is an approximation calculated from mean-log values.\n    **pvals** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        p-values.\n    **pvals_adj** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Corrected p-values.\n\n    Notes\n    -----\n    There are slight inconsistencies depending on whether sparse\n    or dense data are passed. See `here <https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py>`__.\n\n    Examples\n    --------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon')\n\n    # to visualize the results\n    >>> sc.pl.rank_genes_groups(adata)\n    \"\"\"\n    if 'only_positive' in kwds:\n        rankby_abs = not kwds.pop('only_positive')  # backwards compat\n\n    logg.info('ranking genes', r=True)\n    avail_methods = {'t-test', 't-test_overestim_var', 'wilcoxon', 'logreg'}\n    if method not in avail_methods:\n        raise ValueError('Method must be one of {}.'.format(avail_methods))\n\n    avail_corr = {'benjamini-hochberg', 'bonferroni'}\n    if corr_method not in avail_corr:\n        raise ValueError('Correction method must be one of {}.'.format(avail_corr))\n\n\n    adata = adata.copy() if copy else adata\n    utils.sanitize_anndata(adata)\n    # for clarity, rename variable\n    groups_order = groups if isinstance(groups, str) else list(groups)\n    if isinstance(groups_order, list) and isinstance(groups_order[0], int):\n        groups_order = [str(n) for n in groups_order]\n    if reference != 'rest' and reference not in set(groups_order):\n        groups_order += [reference]\n    if (reference != 'rest'\n        and reference not in set(adata.obs[groupby].cat.categories)):\n        raise ValueError('reference = {} needs to be one of groupby = {}.'\n                         .format(reference,\n                                 adata.obs[groupby].cat.categories.tolist()))\n\n    groups_order, groups_masks = utils.select_groups(\n        adata, groups_order, groupby)\n\n    if key_added is None:\n        key_added = 'rank_genes_groups'\n    adata.uns[key_added] = {}\n    adata.uns[key_added]['params'] = {\n        'groupby': groupby,\n        'reference': reference,\n        'method': method,\n        'use_raw': use_raw,\n        'corr_method': corr_method,\n    }\n\n    # adata_comp mocks an AnnData object if use_raw is True\n    # otherwise it's just the AnnData object\n    adata_comp = adata\n    if adata.raw is not None and use_raw:\n        adata_comp = adata.raw\n    X = adata_comp.X\n\n    # for clarity, rename variable\n    n_genes_user = n_genes\n    # make sure indices are not OoB in case there are less genes than n_genes\n    if n_genes_user > X.shape[1]:\n        n_genes_user = X.shape[1]\n    # in the following, n_genes is simply another name for the total number of genes\n    n_genes = X.shape[1]\n\n    n_groups = groups_masks.shape[0]\n    ns = np.zeros(n_groups, dtype=int)\n    for imask, mask in enumerate(groups_masks):\n        ns[imask] = np.where(mask)[0].size\n    logg.msg('consider \\'{}\\' groups:'.format(groupby), groups_order, v=4)\n    logg.msg('with sizes:', ns, v=4)\n    if reference != 'rest':\n        ireference = np.where(groups_order == reference)[0][0]\n    reference_indices = np.arange(adata_comp.n_vars, dtype=int)\n\n    rankings_gene_scores = []\n    rankings_gene_names = []\n    rankings_gene_logfoldchanges = []\n    rankings_gene_pvals = []\n    rankings_gene_pvals_adj = []\n\n    if method in {'t-test', 't-test_overestim_var'}:\n        from scipy import stats\n        from statsmodels.stats.multitest import multipletests\n        # loop over all masks and compute means, variances and sample numbers\n        means = np.zeros((n_groups, n_genes))\n        vars = np.zeros((n_groups, n_genes))\n\n        for imask, mask in enumerate(groups_masks):\n            means[imask], vars[imask] = _get_mean_var(X[mask])\n\n        # test each either against the union of all other groups or against a\n        # specific group\n        for igroup in range(n_groups):\n            if reference == 'rest':\n                mask_rest = ~groups_masks[igroup]\n            else:\n                if igroup == ireference: continue\n                else: mask_rest = groups_masks[ireference]\n            mean_group, var_group = means[igroup], vars[igroup]\n            mean_rest, var_rest = _get_mean_var(X[mask_rest])\n\n            ns_group = ns[igroup]  # number of observations in group\n            if method == 't-test': ns_rest = np.where(mask_rest)[0].size\n            elif method == 't-test_overestim_var': ns_rest = ns[igroup]  # hack for overestimating the variance for small groups\n            else: raise ValueError('Method does not exist.')\n\n            scores, pvals = stats.ttest_ind_from_stats(\n                mean1=mean_group, std1=np.sqrt(var_group), nobs1=ns_group,\n                mean2=mean_rest,  std2=np.sqrt(var_rest),  nobs2=ns_rest,\n                equal_var=False  # Welch's\n            )\n\n            # Fold change\n            foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9)  # add small value to remove 0's\n\n            scores[np.isnan(scores)] = 0  # I think it's only nan when means are the same and vars are 0\n            pvals[np.isnan(pvals)] = 1  # This also has to happen for Benjamini Hochberg\n\n            if corr_method == 'benjamini-hochberg':\n                _, pvals_adj, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n            elif corr_method == 'bonferroni':\n                pvals_adj = np.minimum(pvals * n_genes, 1.0)\n\n            scores_sort = np.abs(scores) if rankby_abs else scores\n            partition = np.argpartition(scores_sort, -n_genes_user)[-n_genes_user:]\n            partial_indices = np.argsort(scores_sort[partition])[::-1]\n            global_indices = reference_indices[partition][partial_indices]\n            rankings_gene_scores.append(scores[global_indices])\n            rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices]))\n            rankings_gene_names.append(adata_comp.var_names[global_indices])\n            rankings_gene_pvals.append(pvals[global_indices])\n            rankings_gene_pvals_adj.append(pvals_adj[global_indices])\n\n    elif method == 'logreg':\n        # if reference is not set, then the groups listed will be compared to the rest\n        # if reference is set, then the groups listed will be compared only to the other groups listed\n        from sklearn.linear_model import LogisticRegression\n        reference = groups_order[0]\n        if len(groups) == 1:\n            raise Exception('Cannot perform logistic regression on a single cluster.')\n        adata_copy = adata[adata.obs[groupby].isin(groups_order)]\n        adata_comp = adata_copy\n        if adata.raw is not None and use_raw:\n            adata_comp = adata_copy.raw\n        X = adata_comp.X\n\n        clf = LogisticRegression(**kwds)\n        clf.fit(X, adata_copy.obs[groupby].cat.codes)\n        scores_all = clf.coef_\n        for igroup, group in enumerate(groups_order):\n            if len(groups_order) <= 2:  # binary logistic regression\n                scores = scores_all[0]\n            else:\n                scores = scores_all[igroup]\n            partition = np.argpartition(scores, -n_genes_user)[-n_genes_user:]\n            partial_indices = np.argsort(scores[partition])[::-1]\n            global_indices = reference_indices[partition][partial_indices]\n            rankings_gene_scores.append(scores[global_indices])\n            rankings_gene_names.append(adata_comp.var_names[global_indices])\n            if len(groups_order) <= 2:\n                break\n\n    elif method == 'wilcoxon':\n        from scipy import stats\n        from statsmodels.stats.multitest import multipletests\n        CONST_MAX_SIZE = 10000000\n        means = np.zeros((n_groups, n_genes))\n        vars = np.zeros((n_groups, n_genes))\n        # initialize space for z-scores\n        scores = np.zeros(n_genes)\n        # First loop: Loop over all genes\n        if reference != 'rest':\n            for imask, mask in enumerate(groups_masks):\n                means[imask], vars[imask] = _get_mean_var(X[mask])  # for fold-change only\n\n                if imask == ireference: continue\n\n                else: mask_rest = groups_masks[ireference]\n                ns_rest = np.where(mask_rest)[0].size\n                mean_rest, var_rest = _get_mean_var(X[mask_rest]) # for fold-change only\n\n                if ns_rest <= 25 or ns[imask] <= 25:\n                    logg.hint('Few observations in a group for '\n                              'normal approximation (<=25). Lower test accuracy.')\n                n_active = ns[imask]\n                m_active = ns_rest\n\n                # Now calculate gene expression ranking in chunkes:\n                chunk = []\n                # Calculate chunk frames\n                n_genes_max_chunk = floor(CONST_MAX_SIZE / (n_active + m_active))\n                if n_genes_max_chunk < n_genes:\n                    chunk_index = n_genes_max_chunk\n                    while chunk_index < n_genes:\n                        chunk.append(chunk_index)\n                        chunk_index = chunk_index + n_genes_max_chunk\n                    chunk.append(n_genes)\n                else:\n                    chunk.append(n_genes)\n\n                left = 0\n                # Calculate rank sums for each chunk for the current mask\n                for chunk_index, right in enumerate(chunk):\n                    # Check if issparse is true: AnnData objects are currently sparse.csr or ndarray.\n                    if issparse(X):\n                        df1 = pd.DataFrame(data=X[mask, left:right].todense())\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right].todense(),\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    else:\n                        df1 = pd.DataFrame(data=X[mask, left:right])\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right],\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    df1 = df1.append(df2)\n                    ranks = df1.rank()\n                    # sum up adjusted_ranks to calculate W_m,n\n                    scores[left:right] = np.sum(ranks.loc[0:n_active, :])\n                    left = right\n\n                scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(\n                    (n_active * m_active * (n_active + m_active + 1) / 12))\n                scores[np.isnan(scores)] = 0\n                pvals = 2 * stats.distributions.norm.sf(np.abs(scores))\n\n                if corr_method == 'benjamini-hochberg':\n                    pvals[np.isnan(pvals)] = 1  # set Nan values to 1 to properly convert using Benhjamini Hochberg\n                    _, pvals_adj, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n                elif corr_method == 'bonferroni':\n                    pvals_adj = np.minimum(pvals * n_genes, 1.0)\n\n                # Fold change\n                foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9)  # add small value to remove 0's\n                scores_sort = np.abs(scores) if rankby_abs else scores\n                partition = np.argpartition(scores_sort, -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(scores_sort[partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_scores.append(scores[global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n                rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices]))\n                rankings_gene_pvals.append(pvals[global_indices])\n                rankings_gene_pvals_adj.append(pvals_adj[global_indices])\n\n\n        # If no reference group exists, ranking needs only to be done once (full mask)\n        else:\n            scores = np.zeros((n_groups, n_genes))\n            chunk = []\n            n_cells = X.shape[0]\n            n_genes_max_chunk = floor(CONST_MAX_SIZE / n_cells)\n            if n_genes_max_chunk < n_genes:\n                chunk_index = n_genes_max_chunk\n                while chunk_index < n_genes:\n                    chunk.append(chunk_index)\n                    chunk_index = chunk_index + n_genes_max_chunk\n                chunk.append(n_genes)\n            else:\n                chunk.append(n_genes)\n            left = 0\n            for chunk_index, right in enumerate(chunk):\n                # Check if issparse is true\n                if issparse(X):\n                    df1 = pd.DataFrame(data=X[:, left:right].todense())\n                else:\n                    df1 = pd.DataFrame(data=X[:, left:right])\n                ranks = df1.rank()\n                # sum up adjusted_ranks to calculate W_m,n\n                for imask, mask in enumerate(groups_masks):\n                    scores[imask, left:right] = np.sum(ranks.loc[mask, :])\n                left = right\n\n            for imask, mask in enumerate(groups_masks):\n                mask_rest = ~groups_masks[imask]\n                means[imask], vars[imask] = _get_mean_var(X[mask]) #for fold-change\n                mean_rest, var_rest = _get_mean_var(X[mask_rest])  # for fold-change\n\n                scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(\n                    (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12))\n                scores[np.isnan(scores)] = 0\n                pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:]))\n\n                if corr_method == 'benjamini-hochberg':\n                    pvals[np.isnan(pvals)] = 1  # set Nan values to 1 to properly convert using Benhjamini Hochberg\n                    _, pvals_adj, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n                elif corr_method == 'bonferroni':\n                    pvals_adj = np.minimum(pvals * n_genes, 1.0)\n\n                # Fold change\n                foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9)  # add small value to remove 0's\n                scores_sort = np.abs(scores) if rankby_abs else scores\n                partition = np.argpartition(scores_sort[imask, :], -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(scores_sort[imask, partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_scores.append(scores[imask, global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n                rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices]))\n                rankings_gene_pvals.append(pvals[global_indices])\n                rankings_gene_pvals_adj.append(pvals_adj[global_indices])\n\n\n    groups_order_save = [str(g) for g in groups_order]\n    if (reference != 'rest' and method != 'logreg') or (method == 'logreg' and len(groups) == 2):\n        groups_order_save = [g for g in groups_order if g != reference]\n    adata.uns[key_added]['scores'] = np.rec.fromarrays(\n        [n for n in rankings_gene_scores],\n        dtype=[(rn, 'float32') for rn in groups_order_save])\n    adata.uns[key_added]['names'] = np.rec.fromarrays(\n        [n for n in rankings_gene_names],\n        dtype=[(rn, 'U50') for rn in groups_order_save])\n\n    if method in {'t-test', 't-test_overestim_var', 'wilcoxon'}:\n        adata.uns[key_added]['logfoldchanges'] = np.rec.fromarrays(\n            [n for n in rankings_gene_logfoldchanges],\n            dtype=[(rn, 'float32') for rn in groups_order_save])\n        adata.uns[key_added]['pvals'] = np.rec.fromarrays(\n            [n for n in rankings_gene_pvals],\n            dtype=[(rn, 'float64') for rn in groups_order_save])\n        adata.uns[key_added]['pvals_adj'] = np.rec.fromarrays(\n            [n for n in rankings_gene_pvals_adj],\n            dtype=[(rn, 'float64') for rn in groups_order_save])\n    logg.info('    finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\\n')\n    logg.hint(\n        'added to `.uns[\\'{}\\']`\\n'\n        '    \\'names\\', sorted np.recarray to be indexed by group ids\\n'\n        '    \\'scores\\', sorted np.recarray to be indexed by group ids\\n'\n        .format(key_added)\n        + ('    \\'logfoldchanges\\', sorted np.recarray to be indexed by group ids\\n'\n           '    \\'pvals\\', sorted np.recarray to be indexed by group ids\\n'\n           '    \\'pvals_adj\\', sorted np.recarray to be indexed by group ids'\n           if method in {'t-test', 't-test_overestim_var', 'wilcoxon'} else ''))\n    return adata if copy else None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter_rank_genes_groups(adata, key=None, groupby=None, use_raw=True, log=True,\n                             key_added='rank_genes_groups_filtered',\n                             min_in_group_fraction=0.25, min_fold_change=2,\n                             max_out_group_fraction=0.5):\n    \"\"\"Filters out genes based on fold change and fraction of genes expressing the gene within and outside the `groupby` categories.\n\n    See :func:`~scanpy.tl.rank_genes_groups`.\n\n    Results are stored in `adata.uns[key_added]` (default: 'rank_genes_groups_filtered').\n\n    To preserve the original structure of adata.uns['rank_genes_groups'], filtered genes\n    are set to `NaN`.\n\n    Parameters\n    ----------\n    adata: :class:`~anndata.AnnData`\n    key\n    groupby\n    use_raw\n    log : if true, it means that the values to work with are in log scale\n    key_added\n    min_in_group_fraction\n    min_fold_change\n    max_out_group_fraction\n\n    Returns\n    -------\n    Same output as :ref:`scanpy.tl.rank_genes_groups` but with filtered genes names set to\n    `nan`\n\n    Examples\n    --------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon')\n    >>> sc.tl.filter_rank_genes_groups(adata, min_fold_change=3)\n    >>> # visualize results\n    >>> sc.pl.rank_genes_groups(adata, key='rank_genes_groups_filtered')\n    >>> # visualize results using dotplot\n    >>> sc.pl.rank_genes_groups_dotplot(adata, key='rank_genes_groups_filtered')\n    \"\"\"\n    if key is None:\n        key = 'rank_genes_groups'\n\n    if groupby is None:\n        groupby = str(adata.uns[key]['params']['groupby'])\n\n    # convert structured numpy array into DataFrame\n    gene_names = pd.DataFrame(adata.uns[key]['names'])\n\n    fraction_in_cluster_matrix = pd.DataFrame(np.zeros(gene_names.shape), columns=gene_names.columns,\n                                              index=gene_names.index)\n    fold_change_matrix = pd.DataFrame(np.zeros(gene_names.shape), columns=gene_names.columns, index=gene_names.index)\n    fraction_out_cluster_matrix = pd.DataFrame(np.zeros(gene_names.shape), columns=gene_names.columns,\n                                               index=gene_names.index)\n    logg.info(\"Filtering genes using: min_in_group_fraction: {} \"\n              \"min_fold_change: {}, max_out_group_fraction: {}\".format(min_in_group_fraction, min_fold_change,\n                                                                       max_out_group_fraction))\n    from ..plotting._anndata import _prepare_dataframe\n    for cluster in gene_names.columns:\n        # iterate per column\n        var_names = gene_names[cluster].values\n\n        # add column to adata as __is_in_cluster__. This facilitates to measure fold change\n        # of each gene with respect to all other clusters\n        adata.obs['__is_in_cluster__'] = pd.Categorical(adata.obs[groupby] == cluster)\n\n        # obs_tidy has rows=groupby, columns=var_names\n        categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby='__is_in_cluster__', use_raw=use_raw)\n\n        # for if category defined by groupby (if any) compute for each var_name\n        # 1. the mean value over the category\n        # 2. the fraction of cells in the category having a value > 0\n\n        # 1. compute mean value\n        mean_obs = obs_tidy.groupby(level=0).mean()\n\n        # 2. compute fraction of cells having value >0\n        # transform obs_tidy into boolean matrix\n        obs_bool = obs_tidy.astype(bool)\n\n        # compute the sum per group which in the boolean matrix this is the number\n        # of values >0, and divide the result by the total number of values in the group\n        # (given by `count()`)\n        fraction_obs = obs_bool.groupby(level=0).sum() / obs_bool.groupby(level=0).count()\n\n        # Because the dataframe groupby is based on the '__is_in_cluster__' column,\n        # in this context, [True] means __is_in_cluster__.\n        # Also, in this context, fraction_obs.loc[True].values is the row of values\n        # that is assigned *as column* to fraction_in_cluster_matrix to follow the\n        # structure of the gene_names dataFrame\n        fraction_in_cluster_matrix.loc[:, cluster] = fraction_obs.loc[True].values\n        fraction_out_cluster_matrix.loc[:, cluster] = fraction_obs.loc[False].values\n\n        # compute fold change.\n        if log:\n            fold_change_matrix.loc[:, cluster] = (np.exp(mean_obs.loc[True]) / np.exp(mean_obs.loc[False])).values\n        else:\n            fold_change_matrix.loc[:, cluster] = (mean_obs.loc[True] / mean_obs.loc[False]).values\n\n    # remove temporary columns\n    adata.obs.drop(columns='__is_in_cluster__')\n    # filter original_matrix\n    gene_names = gene_names[(fraction_in_cluster_matrix > min_in_group_fraction) &\n                            (fraction_out_cluster_matrix < max_out_group_fraction) &\n                            (fold_change_matrix > min_fold_change)]\n    # create new structured array using 'key_added'.\n    adata.uns[key_added] = adata.uns[key].copy()\n    adata.uns[key_added]['names'] = gene_names.to_records(index=False)", "response": "Filter out genes based on fold change and fraction of genes expressing the gene within and outside the groupby categories."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef burczynski06() -> AnnData:\n    filename = settings.datasetdir / 'burczynski06/GDS1615_full.soft.gz'\n    url = 'ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS1nnn/GDS1615/soft/GDS1615_full.soft.gz'\n    adata = sc.read(filename, backup_url=url)\n    return adata", "response": "Bulk data with conditions ulcerative colitis and Crohn s disease and Crohn s disease."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsimulating myeloid progenitors [Krumsiek11]_. The literature-curated boolean network from [Krumsiek11]_ was used to simulate the data. It describes development to four cell fates: 'monocyte', 'erythrocyte', 'megakaryocyte' and 'neutrophil'. See also the discussion of this data in [Wolf19]_. Simulate via :func:`~scanpy.api.sim`. Returns ------- Annotated data matrix.", "response": "def krumsiek11() -> AnnData:\n    \"\"\"Simulated myeloid progenitors [Krumsiek11]_.\n\n    The literature-curated boolean network from [Krumsiek11]_ was used to\n    simulate the data. It describes development to four cell fates: 'monocyte',\n    'erythrocyte', 'megakaryocyte' and 'neutrophil'.\n\n    See also the discussion of this data in [Wolf19]_.\n\n    Simulate via :func:`~scanpy.api.sim`.\n\n    Returns\n    -------\n    Annotated data matrix.\n    \"\"\"\n    filename = os.path.dirname(__file__) + '/krumsiek11.txt'\n    verbosity_save = sc.settings.verbosity\n    sc.settings.verbosity = 'error'  # suppress output...\n    adata = sc.read(filename, first_column_names=True)\n    sc.settings.verbosity = verbosity_save\n    adata.uns['iroot'] = 0\n    fate_labels = {0: 'Stem', 159: 'Mo', 319: 'Ery',\n                   459: 'Mk', 619: 'Neu'}\n    adata.uns['highlights'] = fate_labels\n    cell_type = np.array(['progenitor' for i in range(adata.n_obs)])\n    cell_type[80:160] = 'Mo'\n    cell_type[240:320] = 'Ery'\n    cell_type[400:480] = 'Mk'\n    cell_type[560:640] = 'Neu'\n    adata.obs['cell_type'] = cell_type\n    sc.utils.sanitize_anndata(adata)\n    return adata"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef moignard15() -> AnnData:\n    filename = settings.datasetdir / 'moignard15/nbt.3154-S3.xlsx'\n    backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'\n    adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url)\n    # filter out 4 genes as in Haghverdi et al. (2016)\n    gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc'])\n    adata = adata[:, gene_subset]  # retain non-removed genes\n    # choose root cell for DPT analysis as in Haghverdi et al. (2016)\n    adata.uns['iroot'] = 532  # note that in Matlab/R, counting starts at 1\n    # annotate with Moignard et al. (2015) experimental cell groups\n    groups_order = ['HF', 'NP', 'PS', '4SG', '4SFG']\n    # annotate each observation/cell\n    adata.obs['exp_groups'] = [\n        next(gname for gname in groups_order if sname.startswith(gname))\n        for sname in adata.obs_names]\n    # fix the order and colors of names in \"groups\"\n    adata.obs['exp_groups'] = pd.Categorical(adata.obs['exp_groups'],\n                                             categories=groups_order)\n    adata.uns['exp_groups_colors'] = ['#D7A83E', '#7AAE5D', '#497ABC', '#AF353A', '#765099']\n    return adata", "response": "Hematopoiesis in early mouse embryos [ Moignard15 ]."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef paul15() -> AnnData:\n    logg.warn('In Scanpy 0.*, this returned logarithmized data. '\n              'Now it returns non-logarithmized data.')\n    import h5py\n    filename = settings.datasetdir / 'paul15/paul15.h5'\n    backup_url = 'http://falexwolf.de/data/paul15.h5'\n    sc.utils.check_presence_download(filename, backup_url)\n    with h5py.File(filename, 'r') as f:\n        X = f['data.debatched'][()]\n        gene_names = f['data.debatched_rownames'][()].astype(str)\n        cell_names = f['data.debatched_colnames'][()].astype(str)\n        clusters = f['cluster.id'][()].flatten()\n        infogenes_names = f['info.genes_strings'][()].astype(str)\n    # each row has to correspond to a observation, therefore transpose\n    adata = AnnData(X.transpose())\n    adata.var_names = gene_names\n    adata.row_names = cell_names\n    # names reflecting the cell type identifications from the paper\n    cell_type = {7: 'MEP', 8: 'Mk', 9: 'GMP', 10: 'GMP', 11: 'DC',\n                 12: 'Baso', 13: 'Baso', 14: 'Mo', 15: 'Mo',\n                 16: 'Neu', 17: 'Neu', 18: 'Eos', 19: 'Lymph'}\n    cell_type.update({i: 'Ery' for i in range(1, 7)})\n    adata.obs['paul15_clusters'] = [\n        str(i) + cell_type[i] for i in clusters.astype(int)]\n    # make string annotations categorical (optional)\n    sc.utils.sanitize_anndata(adata)\n    # just keep the first of the two equivalent names per gene\n    adata.var_names = [gn.split(';')[0] for gn in adata.var_names]\n    # remove 10 corrupted gene names\n    infogenes_names = np.intersect1d(infogenes_names, adata.var_names)\n    # restrict data array to the 3461 informative genes\n    adata = adata[:, infogenes_names]\n    # usually we'd set the root cell to an arbitrary cell in the MEP cluster\n    # adata.uns['iroot': np.flatnonzero(adata.obs['paul15_clusters']  == '7MEP')[0]\n    # here, set the root cell as in Haghverdi et al. (2016)\n    adata.uns['iroot'] = 840  # note that other than in Matlab/R, counting starts at 1\n    return adata", "response": "Load the raw data from the Myeloid Progenitors Paul 15 dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsimulates toggleswitch. Data obtained simulating a simple toggleswitch `Gardner *et al.*, Nature (2000) <https://doi.org/10.1038/35002131>`__. Simulate via :func:`~scanpy.api.sim`. Returns ------- Annotated data matrix.", "response": "def toggleswitch() -> AnnData:\n    \"\"\"Simulated toggleswitch.\n\n    Data obtained simulating a simple toggleswitch `Gardner *et al.*, Nature\n    (2000) <https://doi.org/10.1038/35002131>`__.\n\n    Simulate via :func:`~scanpy.api.sim`.\n\n    Returns\n    -------\n    Annotated data matrix.\n    \"\"\"\n    filename = os.path.dirname(__file__) + '/toggleswitch.txt'\n    adata = sc.read(filename, first_column_names=True)\n    adata.uns['iroot'] = 0\n    return adata"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsubstitute and processed 68k PBMCs.", "response": "def pbmc68k_reduced() -> AnnData:\n    \"\"\"Subsampled and processed 68k PBMCs.\n\n    10x PBMC 68k dataset from\n    https://support.10xgenomics.com/single-cell-gene-expression/datasets\n\n    The original PBMC 68k dataset was preprocessed using scanpy and was saved\n    keeping only 724 cells and 221 highly variable genes.\n\n    The saved file contains the annotation of cell types (key: 'bulk_labels'), UMAP coordinates,\n    louvain clustering and gene rankings based on the bulk_labels.\n\n    Returns\n    -------\n    Annotated data matrix.\n    \"\"\"\n\n    filename = os.path.dirname(__file__) + '/10x_pbmc68k_reduced.h5ad'\n    return sc.read(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing a neighborhood graph of observations.", "response": "def neighbors(\n    adata: AnnData,\n    n_neighbors: int = 15,\n    n_pcs: Optional[int] = None,\n    use_rep: Optional[str] = None,\n    knn: bool = True,\n    random_state: Optional[Union[int, RandomState]] = 0,\n    method: str = 'umap',\n    metric: Union[str, Callable[[np.ndarray, np.ndarray], float]] = 'euclidean',\n    metric_kwds: Mapping[str, Any] = {},\n    copy: bool = False\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Compute a neighborhood graph of observations [McInnes18]_.\n\n    The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,\n    which also provides a method for estimating connectivities of data points -\n    the connectivity of the manifold (`method=='umap'`). If `method=='gauss'`,\n    connectivities are computed according to [Coifman05]_, in the adaption of\n    [Haghverdi16]_.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    n_neighbors\n        The size of local neighborhood (in terms of number of neighboring data\n        points) used for manifold approximation. Larger values result in more\n        global views of the manifold, while smaller values result in more local\n        data being preserved. In general values should be in the range 2 to 100.\n        If `knn` is `True`, number of nearest neighbors to be searched. If `knn`\n        is `False`, a Gaussian kernel width is set to the distance of the\n        `n_neighbors` neighbor.\n    {n_pcs}\n    {use_rep}\n    knn\n        If `True`, use a hard threshold to restrict the number of neighbors to\n        `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian\n        Kernel to assign low weights to neighbors more distant than the\n        `n_neighbors` nearest neighbor.\n    random_state\n        A numpy random seed.\n    method : {{'umap', 'gauss', `None`}}  (default: `'umap'`)\n        Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_\n        with adaptive width [Haghverdi16]_) for computing connectivities.\n    metric\n        A known metric\u2019s name or a callable that returns a distance.\n    metric_kwds\n        Options for the metric.\n    copy\n        Return a copy instead of writing to adata.\n\n    Returns\n    -------\n    Depending on `copy`, updates or returns `adata` with the following:\n\n    **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`)\n        Weighted adjacency matrix of the neighborhood graph of data\n        points. Weights should be interpreted as connectivities.\n    **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`)\n        Instead of decaying weights, this stores distances for each pair of\n        neighbors.\n    \"\"\"\n    logg.info('computing neighbors', r=True)\n    adata = adata.copy() if copy else adata\n    if adata.isview:  # we shouldn't need this here...\n        adata._init_as_actual(adata.copy())\n    neighbors = Neighbors(adata)\n    neighbors.compute_neighbors(\n        n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,\n        method=method, metric=metric, metric_kwds=metric_kwds,\n        random_state=random_state)\n    adata.uns['neighbors'] = {}\n    adata.uns['neighbors']['params'] = {'n_neighbors': n_neighbors, 'method': method}\n    adata.uns['neighbors']['params']['metric'] = metric\n    if metric_kwds:\n        adata.uns['neighbors']['params']['metric_kwds'] = metric_kwds\n    if use_rep is not None:\n        adata.uns['neighbors']['params']['use_rep'] = use_rep\n    if n_pcs is not None:\n        adata.uns['neighbors']['params']['n_pcs'] = n_pcs\n    adata.uns['neighbors']['distances'] = neighbors.distances\n    adata.uns['neighbors']['connectivities'] = neighbors.connectivities\n    if neighbors.rp_forest is not None:\n        adata.uns['neighbors']['rp_forest'] = neighbors.rp_forest\n    logg.info('    finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\\n')\n    logg.hint(\n        'added to `.uns[\\'neighbors\\']`\\n'\n        '    \\'distances\\', distances for each pair of neighbors\\n'\n        '    \\'connectivities\\', weighted adjacency matrix')\n    return adata if copy else None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute_connectivities_umap(knn_indices, knn_dists,\n        n_obs, n_neighbors, set_op_mix_ratio=1.0,\n        local_connectivity=1.0):\n    \"\"\"This is from umap.fuzzy_simplicial_set [McInnes18]_.\n\n    Given a set of data X, a neighborhood size, and a measure of distance\n    compute the fuzzy simplicial set (here represented as a fuzzy graph in\n    the form of a sparse matrix) associated to the data. This is done by\n    locally approximating geodesic distance at each point, creating a fuzzy\n    simplicial set for each such point, and then combining all the local\n    fuzzy simplicial sets into a global one via a fuzzy union.\n    \"\"\"\n    from umap.umap_ import fuzzy_simplicial_set\n\n    X = coo_matrix(([], ([], [])), shape=(n_obs, 1))\n    connectivities = fuzzy_simplicial_set(X, n_neighbors, None, None,\n                                          knn_indices=knn_indices, knn_dists=knn_dists,\n                                          set_op_mix_ratio=set_op_mix_ratio,\n                                          local_connectivity=local_connectivity)\n    distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors)\n\n    return distances, connectivities.tocsr()", "response": "This function computes the fuzzy simplicial set for a set of data X and a measure of distance between each point and the local fuzzy cluster."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef restrict(self, index_array):\n        new_shape = index_array.shape[0], index_array.shape[0]\n        return OnFlySymMatrix(self.get_row, new_shape, DC_start=self.DC_start,\n                              DC_end=self.DC_end,\n                              rows=self.rows, restrict_array=index_array)", "response": "Generate a restricted view of the matrix."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transitions(self):\n        if issparse(self.Z):\n            Zinv = self.Z.power(-1)\n        else:\n            Zinv = np.diag(1./np.diag(self.Z))\n        return self.Z.dot(self.transitions_sym).dot(Zinv)", "response": "Return the transition matrix of the current state of the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_neighbors(\n        self,\n        n_neighbors: int = 30,\n        knn: bool = True,\n        n_pcs: Optional[int] = None,\n        use_rep: Optional[str] = None,\n        method: str = 'umap',\n        random_state: Optional[Union[RandomState, int]] = 0,\n        write_knn_indices: bool = False,\n        metric: str = 'euclidean',\n        metric_kwds: Mapping[str, Any] = {}\n    ) -> None:\n        \"\"\"\\\n        Compute distances and connectivities of neighbors.\n\n        Parameters\n        ----------\n        n_neighbors\n             Use this number of nearest neighbors.\n        knn\n             Restrict result to `n_neighbors` nearest neighbors.\n        {n_pcs}\n        {use_rep}\n\n        Returns\n        -------\n        Writes sparse graph attributes `.distances` and `.connectivities`.\n        Also writes `.knn_indices` and `.knn_distances` if\n        `write_knn_indices==True`.\n        \"\"\"\n        if n_neighbors > self._adata.shape[0]:  # very small datasets\n            n_neighbors = 1 + int(0.5*self._adata.shape[0])\n            logg.warn('n_obs too small: adjusting to `n_neighbors = {}`'\n                      .format(n_neighbors))\n        if method == 'umap' and not knn:\n            raise ValueError('`method = \\'umap\\' only with `knn = True`.')\n        if method not in {'umap', 'gauss'}:\n            raise ValueError('`method` needs to be \\'umap\\' or \\'gauss\\'.')\n        if self._adata.shape[0] >= 10000 and not knn:\n            logg.warn(\n                'Using high n_obs without `knn=True` takes a lot of memory...')\n        self.n_neighbors = n_neighbors\n        self.knn = knn\n        X = choose_representation(self._adata, use_rep=use_rep, n_pcs=n_pcs)\n        # neighbor search\n        use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False\n        if use_dense_distances:\n            _distances = pairwise_distances(X, metric=metric, **metric_kwds)\n            knn_indices, knn_distances = get_indices_distances_from_dense_matrix(\n                _distances, n_neighbors)\n            if knn:\n                self._distances = get_sparse_matrix_from_indices_distances_numpy(\n                    knn_indices, knn_distances, X.shape[0], n_neighbors)\n            else:\n                self._distances = _distances\n        else:\n            # non-euclidean case and approx nearest neighbors\n            if X.shape[0] < 4096:\n                X = pairwise_distances(X, metric=metric, **metric_kwds)\n                metric = 'precomputed'\n            knn_indices, knn_distances, _ = compute_neighbors_umap(\n                X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds)\n            #self._rp_forest = _make_forest_dict(forest)\n        # write indices as attributes\n        if write_knn_indices:\n            self.knn_indices = knn_indices\n            self.knn_distances = knn_distances\n        logg.msg('computed neighbors', t=True, v=4)\n        if not use_dense_distances or method == 'umap':\n            # we need self._distances also for method == 'gauss' if we didn't\n            # use dense distances\n            self._distances, self._connectivities = compute_connectivities_umap(\n                knn_indices, knn_distances, self._adata.shape[0], self.n_neighbors)\n        # overwrite the umap connectivities if method is 'gauss'\n        # self._distances is unaffected by this\n        if method == 'gauss':\n            self._compute_connectivities_diffmap()\n        logg.msg('computed connectivities', t=True, v=4)\n        self._number_connected_components = 1\n        if issparse(self._connectivities):\n            from scipy.sparse.csgraph import connected_components\n            self._connected_components = connected_components(self._connectivities)\n            self._number_connected_components = self._connected_components[0]", "response": "Compute nearest neighbors of a set of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute transition matrix. Parameters ---------- density_normalize : `bool` The density rescaling of Coifman and Lafon (2006): Then only the geometry of the data matters, not the sampled density. Returns ------- Makes attributes `.transitions_sym` and `.transitions` available.", "response": "def compute_transitions(self, density_normalize=True):\n        \"\"\"Compute transition matrix.\n\n        Parameters\n        ----------\n        density_normalize : `bool`\n            The density rescaling of Coifman and Lafon (2006): Then only the\n            geometry of the data matters, not the sampled density.\n\n        Returns\n        -------\n        Makes attributes `.transitions_sym` and `.transitions` available.\n        \"\"\"\n        W = self._connectivities\n        # density normalization as of Coifman et al. (2005)\n        # ensures that kernel matrix is independent of sampling density\n        if density_normalize:\n            # q[i] is an estimate for the sampling density at point i\n            # it's also the degree of the underlying graph\n            q = np.asarray(W.sum(axis=0))\n            if not issparse(W):\n                Q = np.diag(1.0/q)\n            else:\n                Q = scipy.sparse.spdiags(1.0/q, 0, W.shape[0], W.shape[0])\n            K = Q.dot(W).dot(Q)\n        else:\n            K = W\n\n        # z[i] is the square root of the row sum of K\n        z = np.sqrt(np.asarray(K.sum(axis=0)))\n        if not issparse(K):\n            self.Z = np.diag(1.0/z)\n        else:\n            self.Z = scipy.sparse.spdiags(1.0/z, 0, K.shape[0], K.shape[0])\n        self._transitions_sym = self.Z.dot(K).dot(self.Z)\n        logg.msg('computed transitions', v=4, time=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_eigen(self, n_comps=15, sym=None, sort='decrease'):\n        np.set_printoptions(precision=10)\n        if self._transitions_sym is None:\n            raise ValueError('Run `.compute_transitions` first.')\n        matrix = self._transitions_sym\n        # compute the spectrum\n        if n_comps == 0:\n            evals, evecs = scipy.linalg.eigh(matrix)\n        else:\n            n_comps = min(matrix.shape[0]-1, n_comps)\n            # ncv = max(2 * n_comps + 1, int(np.sqrt(matrix.shape[0])))\n            ncv = None\n            which = 'LM' if sort == 'decrease' else 'SM'\n            # it pays off to increase the stability with a bit more precision\n            matrix = matrix.astype(np.float64)\n            evals, evecs = scipy.sparse.linalg.eigsh(matrix, k=n_comps,\n                                                  which=which, ncv=ncv)\n            evals, evecs = evals.astype(np.float32), evecs.astype(np.float32)\n        if sort == 'decrease':\n            evals = evals[::-1]\n            evecs = evecs[:, ::-1]\n        logg.info('    eigenvalues of transition matrix\\n'\n                  '    {}'.format(str(evals).replace('\\n', '\\n    ')))\n        if self._number_connected_components > len(evals)/2:\n            logg.warn('Transition matrix has many disconnected components!')\n        self._eigen_values = evals\n        self._eigen_basis = evecs", "response": "Compute the eigendecomposition of the transition matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the Lp matrix for the current species.", "response": "def _compute_Lp_matrix(self):\n        \"\"\"See Fouss et al. (2006) and von Luxburg et al. (2007).\n\n        See Proposition 6 in von Luxburg (2007) and the inline equations\n        right in the text above.\n        \"\"\"\n        self.Lp = sum([1/self.eigen_values[i]\n                      * np.outer(self.eigen_basis[:, i], self.eigen_basis[:, i])\n                      for i in range(1, self.eigen_values.size)])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the commute time matrix for the current log - level and log - level entries.", "response": "def _compute_C_matrix(self):\n        \"\"\"See Fouss et al. (2006) and von Luxburg et al. (2007).\n\n        This is the commute-time matrix. It's a squared-euclidian distance\n        matrix in :math:`\\\\mathbb{R}^n`.\n        \"\"\"\n        self.C = np.repeat(np.diag(self.Lp)[:, np.newaxis],\n                           self.Lp.shape[0], axis=1)\n        self.C += np.repeat(np.diag(self.Lp)[np.newaxis, :],\n                            self.Lp.shape[0], axis=0)\n        self.C -= 2*self.Lp\n        # the following is much slower\n        # self.C = np.zeros(self.Lp.shape)\n        # for i in range(self.Lp.shape[0]):\n        #     for j in range(self.Lp.shape[1]):\n        #         self.C[i, j] = self.Lp[i, i] + self.Lp[j, j] - 2*self.Lp[i, j]\n        volG = np.sum(self.z)\n        self.C *= volG\n        settings.mt(0, 'computed commute distance matrix')\n        self.distances_dpt = self.C"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_MFP_matrix(self):\n        self.MFP = np.zeros(self.Lp.shape)\n        for i in range(self.Lp.shape[0]):\n            for k in range(self.Lp.shape[1]):\n                for j in range(self.Lp.shape[1]):\n                    self.MFP[i, k] += (self.Lp[i, j] - self.Lp[i, k]\n                                       - self.Lp[k, j] + self.Lp[k, k]) * self.z[j]\n        settings.mt(0, 'computed mean first passage time matrix')\n        self.distances_dpt = self.MFP", "response": "Compute the mean first passage time matrix."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns pseudotime with respect to root point.", "response": "def _set_pseudotime(self):\n        \"\"\"Return pseudotime with respect to root point.\n        \"\"\"\n        self.pseudotime = self.distances_dpt[self.iroot].copy()\n        self.pseudotime /= np.max(self.pseudotime[self.pseudotime < np.inf])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines the index of the root cell based on the expression vector.", "response": "def _set_iroot_via_xroot(self, xroot):\n        \"\"\"Determine the index of the root cell.\n\n        Given an expression vector, find the observation index that is closest\n        to this vector.\n\n        Parameters\n        ----------\n        xroot : np.ndarray\n            Vector that marks the root cell, the vector storing the initial\n            condition, only relevant for computing pseudotime.\n        \"\"\"\n        if self._adata.shape[1] != xroot.size:\n            raise ValueError(\n                'The root vector you provided does not have the '\n                'correct dimension.')\n        # this is the squared distance\n        dsqroot = 1e10\n        iroot = 0\n        for i in range(self._adata.shape[0]):\n            diff = self._adata.X[i, :] - xroot\n            dsq = diff.dot(diff)\n            if dsq < dsqroot:\n                dsqroot = dsq\n                iroot = i\n                if np.sqrt(dsqroot) < 1e-10: break\n        logg.msg('setting root index to', iroot, v=4)\n        if self.iroot is not None and iroot != self.iroot:\n            logg.warn('Changing index of iroot from {} to {}.'.format(self.iroot, iroot))\n        self.iroot = iroot"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nqueries the Mitochondrial gene symbols for specific organism through BioMart.", "response": "def mitochondrial_genes(host, org) -> pd.Index:\n    \"\"\"Mitochondrial gene symbols for specific organism through BioMart.\n\n    Parameters\n    ----------\n    host : {{'www.ensembl.org', ...}}\n        A valid BioMart host URL.\n    org : {{'hsapiens', 'mmusculus', 'drerio'}}\n        Organism to query. Currently available are human ('hsapiens'), mouse\n        ('mmusculus') and zebrafish ('drerio').\n\n    Returns\n    -------\n    A :class:`pandas.Index` containing mitochondrial gene symbols.\n    \"\"\"\n    try:\n        from bioservices import biomart\n    except ImportError:\n        raise ImportError(\n            'You need to install the `bioservices` module.')\n    from io import StringIO\n    s = biomart.BioMart(host=host)\n\n    # building query\n    s.new_query()\n    if org == 'hsapiens':\n        s.add_dataset_to_xml('hsapiens_gene_ensembl')\n        s.add_attribute_to_xml('hgnc_symbol')\n    elif org == 'mmusculus':\n        s.add_dataset_to_xml('mmusculus_gene_ensembl')\n        s.add_attribute_to_xml('mgi_symbol')\n    elif org == 'drerio':\n        s.add_dataset_to_xml('drerio_gene_ensembl')\n        s.add_attribute_to_xml('zfin_id_symbol')\n    else:\n        logg.msg('organism ', str(org), ' is unavailable', v=4, no_indent=True)\n        return None\n    s.add_attribute_to_xml('chromosome_name')\n    xml = s.get_xml()\n\n    # parsing mitochondrial gene symbols\n    res = pd.read_csv(StringIO(s.query(xml)), sep='\\t', header=None)\n    res.columns = ['symbol', 'chromosome_name']\n    res = res.dropna()\n    res = res[res['chromosome_name'] == 'MT']\n    res = res.set_index('symbol')\n    res = res[~res.index.duplicated(keep='first')]\n\n    return res.index"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gene_coordinates(host, org, gene, chr_exclude=[]) -> pd.DataFrame:\n    try:\n        from bioservices import biomart\n    except ImportError:\n        raise ImportError(\n            'You need to install the `bioservices` module.')\n    from io import StringIO\n    s = biomart.BioMart(host=host)\n\n    # building query\n    s.new_query()\n    if org == 'hsapiens':\n        s.add_dataset_to_xml('hsapiens_gene_ensembl')\n        s.add_attribute_to_xml('hgnc_symbol')\n    elif org == 'mmusculus':\n        s.add_dataset_to_xml('mmusculus_gene_ensembl')\n        s.add_attribute_to_xml('mgi_symbol')\n    elif org == 'drerio':\n        s.add_dataset_to_xml('drerio_gene_ensembl')\n        s.add_attribute_to_xml('zfin_id_symbol')\n    else:\n        logg.msg('organism ', str(org), ' is unavailable', v=4, no_indent=True)\n        return None\n    s.add_attribute_to_xml('chromosome_name')\n    s.add_attribute_to_xml('start_position')\n    s.add_attribute_to_xml('end_position')\n    xml = s.get_xml()\n\n    # parsing gene coordinates\n    res = pd.read_csv(StringIO(s.query(xml)), sep='\\t', header=None)\n    res.columns = ['symbol', 'chromosome_name', 'start', 'end']\n    res = res.dropna()\n    res = res[~res['chromosome_name'].isin(chr_exclude)]\n    res = res.set_index('symbol')\n\n    return res.loc[[gene], :]", "response": "Retrieve gene coordinates for specific organism through BioMart."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot the highest - expression genes of the given set of genes.", "response": "def highest_expr_genes(\n        adata, n_top=30, show=None, save=None,\n        ax=None, gene_symbols=None, **kwds\n    ):\n    \"\"\"\\\n    Fraction of counts assigned to each gene over all cells.\n\n    Computes, for each gene, the fraction of counts assigned to that gene within\n    a cell. The `n_top` genes with the highest mean fraction over all cells are\n    plotted as boxplots.\n\n    This plot is similar to the `scater` package function `plotHighestExprs(type\n    = \"highest-expression\")`, see `here\n    <https://bioconductor.org/packages/devel/bioc/vignettes/scater/inst/doc/vignette-qc.html>`__. Quoting\n    from there:\n\n        *We expect to see the \u201cusual suspects\u201d, i.e., mitochondrial genes, actin,\n        ribosomal protein, MALAT1. A few spike-in transcripts may also be\n        present here, though if all of the spike-ins are in the top 50, it\n        suggests that too much spike-in RNA was added. A large number of\n        pseudo-genes or predicted genes may indicate problems with alignment.*\n        -- Davis McCarthy and Aaron Lun\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    n_top : `int`, optional (default:30)\n        Number of top\n    {show_save_ax}\n    gene_symbols : `str`, optional (default:None)\n        Key for field in .var that stores gene symbols if you do not want to use .var_names.\n    **kwds : keyword arguments\n        Are passed to `seaborn.boxplot`.\n\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes`.\n    \"\"\"\n    from scipy.sparse import issparse\n\n    # compute the percentage of each gene per cell\n    dat = normalize_per_cell(adata, counts_per_cell_after=100, copy=True)\n\n    # identify the genes with the highest mean\n    if issparse(dat.X):\n        dat.var['mean_percent'] = dat.X.mean(axis=0).A1\n    else:\n        dat.var['mean_percent'] = dat.X.mean(axis=0)\n\n    top = dat.var.sort_values('mean_percent', ascending=False).index[:n_top]\n    dat = dat[:, top]\n    columns = dat.var_names if gene_symbols is None else dat.var[gene_symbols]\n    dat = pd.DataFrame(dat.X.toarray(), index=dat.obs_names, columns=columns)\n\n    if not ax:\n        # figsize is hardcoded to produce a tall image. To change the fig size,\n        # a matplotlib.axes.Axes object needs to be passed.\n        height = (n_top * 0.2) + 1.5\n        fig, ax = plt.subplots(figsize=(5, height))\n    sns.boxplot(data=dat, orient='h', ax=ax, fliersize=1, **kwds)\n    ax.set_xlabel('% of total counts')\n    utils.savefig_or_show('highest_expr_genes', show=show, save=save)\n    return ax if show == False else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_genes_dispersion(data,\n                            flavor='seurat',\n                            min_disp=None, max_disp=None,\n                            min_mean=None, max_mean=None,\n                            n_bins=20,\n                            n_top_genes=None,\n                            log=True,\n                            subset=True,\n                            copy=False):\n    \"\"\"Extract highly variable genes [Satija15]_ [Zheng17]_.\n\n    .. warning::\n        .. deprecated:: 1.3.6\n            Use :func:`~scanpy.api.pp.highly_variable_genes`\n            instead. The new function is equivalent to the present\n            function, except that\n\n            * the new function always expects logarithmized data\n            * `subset=False` in the new function, it suffices to\n              merely annotate the genes, tools like `pp.pca` will\n              detect the annotation\n            * you can now call: `sc.pl.highly_variable_genes(adata)`\n            * `copy` is replaced by `inplace`\n\n    If trying out parameters, pass the data matrix instead of AnnData.\n\n    Depending on `flavor`, this reproduces the R-implementations of Seurat\n    [Satija15]_ and Cell Ranger [Zheng17]_.\n\n    The normalized dispersion is obtained by scaling with the mean and standard\n    deviation of the dispersions for genes falling into a given bin for mean\n    expression of genes. This means that for each bin of mean expression, highly\n    variable genes are selected.\n\n    Use `flavor='cell_ranger'` with care and in the same way as in\n    :func:`~scanpy.api.pp.recipe_zheng17`.\n\n    Parameters\n    ----------\n    data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes.\n    flavor : {'seurat', 'cell_ranger'}, optional (default: 'seurat')\n        Choose the flavor for computing normalized dispersion. If choosing\n        'seurat', this expects non-logarithmized data - the logarithm of mean\n        and dispersion is taken internally when `log` is at its default value\n        `True`. For 'cell_ranger', this is usually called for logarithmized data\n        - in this case you should set `log` to `False`. In their default\n        workflows, Seurat passes the cutoffs whereas Cell Ranger passes\n        `n_top_genes`.\n    min_mean=0.0125, max_mean=3, min_disp=0.5, max_disp=`None` : `float`, optional\n        If `n_top_genes` unequals `None`, these cutoffs for the means and the\n        normalized dispersions are ignored.\n    n_bins : `int` (default: 20)\n        Number of bins for binning the mean gene expression. Normalization is\n        done with respect to each bin. If just a single gene falls into a bin,\n        the normalized dispersion is artificially set to 1. You'll be informed\n        about this if you set `settings.verbosity = 4`.\n    n_top_genes : `int` or `None` (default: `None`)\n        Number of highly-variable genes to keep.\n    log : `bool`, optional (default: `True`)\n        Use the logarithm of the mean to variance ratio.\n    subset : `bool`, optional (default: `True`)\n        Keep highly-variable genes only (if True) else write a bool array for h\n        ighly-variable genes while keeping all genes\n    copy : `bool`, optional (default: `False`)\n        If an :class:`~anndata.AnnData` is passed, determines whether a copy\n        is returned.\n\n    Returns\n    -------\n    If an AnnData `adata` is passed, returns or updates `adata` depending on\n    `copy`. It filters the `adata` and adds the annotations\n\n    **means** : adata.var\n        Means per gene. Logarithmized when `log` is `True`.\n    **dispersions** : adata.var\n        Dispersions per gene. Logarithmized when `log` is `True`.\n    **dispersions_norm** : adata.var\n        Normalized dispersions per gene. Logarithmized when `log` is `True`.\n\n    If a data matrix `X` is passed, the annotation is returned as `np.recarray`\n    with the same information stored in fields: `gene_subset`, `means`, `dispersions`, `dispersion_norm`.\n    \"\"\"\n    if n_top_genes is not None and not all([\n            min_disp is None, max_disp is None, min_mean is None, max_mean is None]):\n        logg.info('If you pass `n_top_genes`, all cutoffs are ignored.')\n    if min_disp is None: min_disp = 0.5\n    if min_mean is None: min_mean = 0.0125\n    if max_mean is None: max_mean = 3\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        result = filter_genes_dispersion(adata.X, log=log,\n                                         min_disp=min_disp, max_disp=max_disp,\n                                         min_mean=min_mean, max_mean=max_mean,\n                                         n_top_genes=n_top_genes,\n                                         flavor=flavor)\n        adata.var['means'] = result['means']\n        adata.var['dispersions'] = result['dispersions']\n        adata.var['dispersions_norm'] = result['dispersions_norm']\n        if subset:\n            adata._inplace_subset_var(result['gene_subset'])\n        else:\n            adata.var['highly_variable'] = result['gene_subset']\n        return adata if copy else None\n    logg.msg('extracting highly variable genes',\n              r=True, v=4)\n    X = data  # no copy necessary, X remains unchanged in the following\n    mean, var = materialize_as_ndarray(_get_mean_var(X))\n    # now actually compute the dispersion\n    mean[mean == 0] = 1e-12  # set entries equal to zero to small value\n    dispersion = var / mean\n    if log:  # logarithmized mean as in Seurat\n        dispersion[dispersion == 0] = np.nan\n        dispersion = np.log(dispersion)\n        mean = np.log1p(mean)\n    # all of the following quantities are \"per-gene\" here\n    import pandas as pd\n    df = pd.DataFrame()\n    df['mean'] = mean\n    df['dispersion'] = dispersion\n    if flavor == 'seurat':\n        df['mean_bin'] = pd.cut(df['mean'], bins=n_bins)\n        disp_grouped = df.groupby('mean_bin')['dispersion']\n        disp_mean_bin = disp_grouped.mean()\n        disp_std_bin = disp_grouped.std(ddof=1)\n        # retrieve those genes that have nan std, these are the ones where\n        # only a single gene fell in the bin and implicitly set them to have\n        # a normalized disperion of 1\n        one_gene_per_bin = disp_std_bin.isnull()\n        gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist()\n        if len(gen_indices) > 0:\n            logg.msg(\n                'Gene indices {} fell into a single bin: their '\n                'normalized dispersion was set to 1.\\n    '\n                'Decreasing `n_bins` will likely avoid this effect.'\n                .format(gen_indices), v=4)\n        # Circumvent pandas 0.23 bug. Both sides of the assignment have dtype==float32,\n        # but there\u2019s still a dtype error without \u201c.value\u201d.\n        disp_std_bin[one_gene_per_bin] = disp_mean_bin[one_gene_per_bin.values].values\n        disp_mean_bin[one_gene_per_bin] = 0\n        # actually do the normalization\n        df['dispersion_norm'] = (df['dispersion'].values  # use values here as index differs\n                                 - disp_mean_bin[df['mean_bin'].values].values) \\\n                                 / disp_std_bin[df['mean_bin'].values].values\n    elif flavor == 'cell_ranger':\n        from statsmodels import robust\n        df['mean_bin'] = pd.cut(df['mean'], np.r_[-np.inf,\n            np.percentile(df['mean'], np.arange(10, 105, 5)), np.inf])\n        disp_grouped = df.groupby('mean_bin')['dispersion']\n        disp_median_bin = disp_grouped.median()\n        # the next line raises the warning: \"Mean of empty slice\"\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            disp_mad_bin = disp_grouped.apply(robust.mad)\n        df['dispersion_norm'] = np.abs((df['dispersion'].values\n                                 - disp_median_bin[df['mean_bin'].values].values)) \\\n                                / disp_mad_bin[df['mean_bin'].values].values\n    else:\n        raise ValueError('`flavor` needs to be \"seurat\" or \"cell_ranger\"')\n    dispersion_norm = df['dispersion_norm'].values.astype('float32')\n    if n_top_genes is not None:\n        dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]\n        dispersion_norm[::-1].sort()  # interestingly, np.argpartition is slightly slower\n        disp_cut_off = dispersion_norm[n_top_genes-1]\n        gene_subset = df['dispersion_norm'].values >= disp_cut_off\n        logg.msg('the {} top genes correspond to a normalized dispersion cutoff of'\n                 .format(n_top_genes, disp_cut_off), v=5)\n    else:\n        max_disp = np.inf if max_disp is None else max_disp\n        dispersion_norm[np.isnan(dispersion_norm)] = 0  # similar to Seurat\n        gene_subset = np.logical_and.reduce((mean > min_mean, mean < max_mean,\n                                             dispersion_norm > min_disp,\n                                             dispersion_norm < max_disp))\n    logg.msg('    finished', time=True, v=4)\n    return np.rec.fromarrays((gene_subset,\n                              df['mean'].values,\n                              df['dispersion'].values,\n                              df['dispersion_norm'].values.astype('float32', copy=False)),\n                              dtype=[('gene_subset', bool),\n                                     ('means', 'float32'),\n                                     ('dispersions', 'float32'),\n                                     ('dispersions_norm', 'float32')])", "response": "Filter out the dispersions of the given highly variable genes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfilters genes by coefficient of variance and mean.", "response": "def filter_genes_cv_deprecated(X, Ecutoff, cvFilter):\n    \"\"\"Filter genes by coefficient of variance and mean.\n\n    See `filter_genes_dispersion`.\n\n    Reference: Weinreb et al. (2017).\n    \"\"\"\n    if issparse(X):\n        raise ValueError('Not defined for sparse input. See `filter_genes_dispersion`.')\n    mean_filter = np.mean(X, axis=0) > Ecutoff\n    var_filter = np.std(X, axis=0) / (np.mean(X, axis=0) + .0001) > cvFilter\n    gene_subset = np.nonzero(np.all([mean_filter, var_filter], axis=0))[0]\n    return gene_subset"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfilter genes by fano factor and mean.", "response": "def filter_genes_fano_deprecated(X, Ecutoff, Vcutoff):\n    \"\"\"Filter genes by fano factor and mean.\n\n    See `filter_genes_dispersion`.\n\n    Reference: Weinreb et al. (2017).\n    \"\"\"\n    if issparse(X):\n        raise ValueError('Not defined for sparse input. See `filter_genes_dispersion`.')\n    mean_filter = np.mean(X, axis=0) > Ecutoff\n    var_filter = np.var(X, axis=0) / (np.mean(X, axis=0) + .0001) > Vcutoff\n    gene_subset = np.nonzero(np.all([mean_filter, var_filter], axis=0))[0]\n    return gene_subset"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef materialize_as_ndarray(a):\n    if type(a) in (list, tuple):\n        if da is not None and any(isinstance(arr, da.Array) for arr in a):\n            return da.compute(*a, sync=True)\n        return tuple(np.asarray(arr) for arr in a)\n    return np.asarray(a)", "response": "Convert distributed arrays to ndarrays."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmerging AnnData objects and correct batch effects using the MNN method. Batch effect correction by matching mutual nearest neighbors [Haghverdi18]_ has been implemented as a function 'mnnCorrect' in the R package `scran <https://bioconductor.org/packages/release/bioc/html/scran.html>`__ This function provides a wrapper to use the mnnCorrect function when concatenating Anndata objects by using the Python-R interface `rpy2 <https://pypi.org/project/rpy2/>`__. Parameters ---------- adatas : :class:`~anndata.AnnData` AnnData matrices to concatenate with. Each dataset should generally be log-transformed, e.g., log-counts. Datasets should have the same number of genes, or at lease have all the genes in geneset. geneset : `list`, optional (default: `None`) A list specifying the genes with which distances between cells are calculated in mnnCorrect, typically the highly variable genes. All genes are used if no geneset provided. See the `scran manual <https://bioconductor.org/packages/release/bioc/html/scran.html>`__ for details. k : `int`, ptional (default: 20) See the `scran manual <https://bioconductor.org/packages/release/bioc/html/scran.html>`__ for details. sigma : `int`, ptional (default: 20) See the `scran manual <https://bioconductor.org/packages/release/bioc/html/scran.html>`__ for details. n_jobs : `int` or `None` (default: `sc.settings.n_jobs`) Number of jobs. kwds : Keyword arguments passed to Anndata.concatenate Returns ------- An :class:`~anndata.AnnData` object with MNN corrected data matrix X. Example ------- >>> adata1 AnnData object with n_obs \u00d7 n_vars = 223 \u00d7 33694 obs: 'n_genes', 'percent_mito', 'n_counts', 'Sample', 'Donor', 'Tissue' var: 'gene_ids', 'n_cells' >>> adata2 AnnData object with n_obs \u00d7 n_vars = 1457 \u00d7 33694 obs: 'n_genes', 'percent_mito', 'n_counts', 'Sample', 'Donor', 'Tissue' var: 'gene_ids', 'n_cells' >>> adata3 = sc.pp.mnnconcatenate(adata2, adata1, geneset = hvgs)", "response": "def mnn_concatenate(*adatas, geneset=None, k=20, sigma=1, n_jobs=None, **kwds):\n    \"\"\"Merge AnnData objects and correct batch effects using the MNN method.\n\n    Batch effect correction by matching mutual nearest neighbors [Haghverdi18]_\n    has been implemented as a function 'mnnCorrect' in the R package\n    `scran <https://bioconductor.org/packages/release/bioc/html/scran.html>`__\n    This function provides a wrapper to use the mnnCorrect function when\n    concatenating Anndata objects by using the Python-R interface `rpy2\n    <https://pypi.org/project/rpy2/>`__.\n\n    Parameters\n    ----------\n    adatas : :class:`~anndata.AnnData`\n        AnnData matrices to concatenate with. Each dataset should generally be\n        log-transformed, e.g., log-counts. Datasets should have the same number\n        of genes, or at lease have all the genes in geneset.\n    geneset : `list`, optional (default: `None`)\n        A list specifying the genes with which distances between cells are\n        calculated in mnnCorrect, typically the highly variable genes.\n        All genes are used if no geneset provided. See the `scran manual\n        <https://bioconductor.org/packages/release/bioc/html/scran.html>`__ for\n        details.\n    k : `int`, ptional (default: 20)\n        See the `scran manual <https://bioconductor.org/packages/release/bioc/html/scran.html>`__\n        for details.\n    sigma : `int`, ptional (default: 20)\n        See the `scran manual <https://bioconductor.org/packages/release/bioc/html/scran.html>`__\n        for details.\n    n_jobs : `int` or `None` (default: `sc.settings.n_jobs`)\n        Number of jobs.\n    kwds :\n        Keyword arguments passed to Anndata.concatenate\n\n    Returns\n    -------\n    An :class:`~anndata.AnnData` object with MNN corrected data matrix X.\n\n    Example\n    -------\n    >>> adata1\n    AnnData object with n_obs \u00d7 n_vars = 223 \u00d7 33694\n        obs: 'n_genes', 'percent_mito', 'n_counts', 'Sample', 'Donor', 'Tissue'\n        var: 'gene_ids', 'n_cells'\n    >>> adata2\n    AnnData object with n_obs \u00d7 n_vars = 1457 \u00d7 33694\n        obs: 'n_genes', 'percent_mito', 'n_counts', 'Sample', 'Donor', 'Tissue'\n        var: 'gene_ids', 'n_cells'\n    >>> adata3 = sc.pp.mnnconcatenate(adata2, adata1, geneset = hvgs)\n    \"\"\"\n    from rpy2.robjects.packages import importr\n    from rpy2.robjects import numpy2ri\n\n    adata = AnnData.concatenate(*adatas, **kwds)\n    if geneset is None:\n        datamats = tuple([adata.X.T for adata in adatas])\n    else:\n        datamats = tuple([adata[:, geneset].X.T for adata in adatas])\n    n_jobs = settings.n_jobs if n_jobs is None else n_jobs\n    numpy2ri.activate()\n    rbase      = importr('base')\n    rscran     = importr('scran')\n    bpparam    = importr('BiocParallel').MulticoreParam(\n        workers = n_jobs) if n_jobs > 1 else importr('BiocParallel').SerialParam()\n    mnn_result = rscran.mnnCorrect(*datamats, k=k, sigma=sigma, BPPARAM = bpparam)\n    corrected  = np.asarray(rbase.do_call(rbase.cbind, mnn_result[0])).T\n    numpy2ri.deactivate()\n    if geneset is None:\n        adata = adata[:, geneset]\n    adata.X = corrected\n    return adata"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new node with the Leiden clustering algorithm.", "response": "def leiden(\n    adata: AnnData,\n    resolution: float = 1,\n    *,\n    restrict_to: Optional[Tuple[str, Sequence[str]]] = None,\n    random_state: int = 0,\n    key_added: Optional[str] = 'leiden',\n    adjacency: Optional[sparse.spmatrix] = None,\n    directed: bool = True,\n    use_weights: bool = True,\n    n_iterations: int = -1,\n    partition_type: Optional[Type[MutableVertexPartition]] = None,\n    copy: bool = False,\n    **partition_kwargs\n):\n    \"\"\"Cluster cells into subgroups [Traag18]_.\n\n    Cluster cells using the Leiden algorithm [Traag18]_, an improved version of the\n    Louvain algorithm [Blondel08]_. The Louvain algorithm has been proposed for single-cell\n    analysis by [Levine15]_.\n\n    This requires having ran :func:`~scanpy.pp.neighbors` or :func:`~scanpy.external.pp.bbknn` first.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    resolution\n        A parameter value controlling the coarseness of the clustering.\n        Higher values lead to more clusters. Set to `None` if overriding `partition_type`\n        to one that doesn\u2019t accept a `resolution_parameter`.\n    random_state\n        Change the initialization of the optimization.\n    restrict_to\n        Restrict the clustering to the categories within the key for sample\n        annotation, tuple needs to contain `(obs_key, list_of_categories)`.\n    key_added\n        `adata.obs` key under which to add the cluster labels. (default: `'leiden'`)\n    adjacency\n        Sparse adjacency matrix of the graph, defaults to\n        `adata.uns['neighbors']['connectivities']`.\n    directed\n        Whether to treat the graph as directed or undirected.\n    use_weights\n        If `True`, edge weights from the graph are used in the computation\n        (placing more emphasis on stronger edges).\n    n_iterations\n        How many iterations of the Leiden clustering algorithm to perform.\n        Positive values above 2 define the total number of iterations to perform,\n        -1 has the algorithm run until it reaches its optimal clustering.\n    partition_type\n        Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`.\n        For the available options, consult the documentation for :func:`~leidenalg.find_partition`.\n    copy\n        Whether to copy `adata` or modify it inplace.\n    **partition_kwargs\n        Any further arguments to pass to `~leidenalg.find_partition`\n        (which in turn passes arguments to the `partition_type`).\n\n    Returns\n    -------\n    `adata.obs[key_added]`\n        Array of dim (number of samples) that stores the subgroup id (`'0'`, `'1'`, ...) for each cell.\n    `adata.uns['leiden']['params']`\n        A dict with the values for the parameters `resolution`, `random_state`, and `n_iterations`.\n    \"\"\"\n    try:\n        import leidenalg\n    except ImportError:\n        raise ImportError('Please install the leiden algorithm: `pip3 install leidenalg`.')\n\n    logg.info('running Leiden clustering', r=True)\n    adata = adata.copy() if copy else adata\n    # are we clustering a user-provided graph or the default AnnData one?\n    if adjacency is None:\n        if 'neighbors' not in adata.uns:\n            raise ValueError('You need to run `pp.neighbors` first to compute a neighborhood graph.')\n        adjacency = adata.uns['neighbors']['connectivities']\n    if restrict_to is not None:\n        restrict_key, restrict_categories = restrict_to\n        adjacency, restrict_indices = restrict_adjacency(\n            adata,\n            restrict_key,\n            restrict_categories,\n            adjacency\n        )\n    # convert it to igraph\n    g = utils.get_igraph_from_adjacency(adjacency, directed=directed)\n    # flip to the default partition type if not overriden by the user\n    if partition_type is None:\n        partition_type = leidenalg.RBConfigurationVertexPartition\n    # prepare find_partition arguments as a dictionary, appending to whatever the user provided\n    # it needs to be this way as this allows for the accounting of a None resolution\n    # (in the case of a partition variant that doesn't take it on input)\n    if partition_kwargs is None:\n        partition_kwargs = {}\n    if use_weights:\n        partition_kwargs['weights'] = np.array(g.es['weight']).astype(np.float64)\n    partition_kwargs['n_iterations'] = n_iterations\n    partition_kwargs['seed'] = random_state\n    if resolution is not None:\n        partition_kwargs['resolution_parameter'] = resolution\n    # clustering proper\n    part = leidenalg.find_partition(g, partition_type, **partition_kwargs)\n    # store output into adata.obs\n    groups = np.array(part.membership)\n    if restrict_to is not None:\n        groups = rename_groups(\n            adata,\n            key_added,\n            restrict_key,\n            restrict_categories,\n            restrict_indices,\n            groups\n        )\n    adata.obs[key_added] = pd.Categorical(\n        values=groups.astype('U'),\n        categories=natsorted(np.unique(groups).astype('U')),\n    )\n    # store information on the clustering parameters\n    adata.uns['leiden'] = {}\n    adata.uns['leiden']['params'] = dict(\n        resolution=resolution,\n        random_state=random_state,\n        n_iterations=n_iterations,\n    )\n    logg.info('    finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\\n')\n    logg.hint('found {} clusters and added\\n'\n              '    \\'{}\\', the cluster labels (adata.obs, categorical)'\n              .format(len(np.unique(groups)), key_added))\n    return adata if copy else None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _design_matrix(\n        model: pd.DataFrame,\n        batch_key: str,\n        batch_levels: Collection[str],\n) -> pd.DataFrame:\n    \"\"\"\n    Computes a simple design matrix.\n\n    Parameters\n    --------\n    model\n        Contains the batch annotation\n    batch_key\n        Name of the batch column\n    batch_levels\n        Levels of the batch annotation\n\n    Returns\n    --------\n    The design matrix for the regression problem\n    \"\"\"\n    import patsy\n\n    design = patsy.dmatrix(\n        \"~ 0 + C(Q('{}'), levels=batch_levels)\".format(batch_key),\n        model,\n        return_type=\"dataframe\",\n    )\n    model = model.drop([batch_key], axis=1)\n    numerical_covariates = model.select_dtypes('number').columns.values\n\n    logg.info(\"Found {} batches\\n\".format(design.shape[1]))\n    other_cols = [c for c in model.columns.values if c not in numerical_covariates]\n\n    if other_cols:\n        col_repr = \" + \".join(\"Q('{}')\".format(x) for x in other_cols)\n        factor_matrix = patsy.dmatrix(\"~ 0 + {}\".format(col_repr),\n                                      model[other_cols],\n                                      return_type=\"dataframe\")\n\n        design = pd.concat((design, factor_matrix), axis=1)\n        logg.info(\"Found {} categorical variables:\".format(len(other_cols)))\n        logg.info(\"\\t\" + \", \".join(other_cols) + '\\n')\n\n    if numerical_covariates is not None:\n        logg.info(\"Found {} numerical variables:\".format(len(numerical_covariates)))\n        logg.info(\"\\t\" + \", \".join(numerical_covariates) + '\\n')\n\n        for nC in numerical_covariates:\n            design[nC] = model[nC]\n\n    return design", "response": "Computes a simple design matrix for the regression problem."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstandardizes the data for a single - hot encoding cluster.", "response": "def _standardize_data(\n    model: pd.DataFrame,\n    data: pd.DataFrame,\n    batch_key: str,\n) -> Tuple[pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray]:\n    \"\"\"\n    Standardizes the data per gene.\n\n    The aim here is to make mean and variance be comparable across batches.\n\n    Parameters\n    --------\n    model\n        Contains the batch annotation\n    data\n        Contains the Data\n    batch_key\n        Name of the batch column in the model matrix\n\n    Returns\n    --------\n    s_data : pandas.DataFrame\n        Standardized Data\n    design : pandas.DataFrame\n        Batch assignment as one-hot encodings\n    var_pooled : numpy.ndarray\n        Pooled variance per gene\n    stand_mean : numpy.ndarray\n        Gene-wise mean\n    \"\"\"\n\n    # compute the design matrix\n    batch_items = model.groupby(batch_key).groups.items()\n    batch_levels, batch_info = zip(*batch_items)\n    n_batch = len(batch_info)\n    n_batches = np.array([len(v) for v in batch_info])\n    n_array = float(sum(n_batches))\n\n    design = _design_matrix(model, batch_key, batch_levels)\n\n    # compute pooled variance estimator\n    B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T)\n    grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :])\n    var_pooled = (data - np.dot(design, B_hat).T)**2\n    var_pooled = np.dot(var_pooled, np.ones((int(n_array), 1)) / int(n_array))\n\n    # Compute the means\n    if np.sum(var_pooled == 0) > 0:\n        print(\n            'Found {} genes with zero variance.'\n            .format(np.sum(var_pooled == 0))\n        )\n    stand_mean = np.dot(grand_mean.T.reshape((len(grand_mean), 1)), np.ones((1, int(n_array))))\n    tmp = np.array(design.copy())\n    tmp[:, :n_batch] = 0\n    stand_mean += np.dot(tmp, B_hat).T\n\n    # need to be a bit careful with the zero variance genes\n    # just set the zero variance genes to zero in the standardized data\n    s_data = np.where(var_pooled == 0, 0, (\n        (data - stand_mean) /\n        np.dot(np.sqrt(var_pooled), np.ones((1, int(n_array))))\n    ))\n    s_data = pd.DataFrame(s_data, index=data.index, columns=data.columns)\n\n    return s_data, design, var_pooled, stand_mean"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef combat(adata: AnnData, key: str = 'batch', covariates: Optional[Collection[str]] = None, inplace: bool = True):\n\n    # check the input\n    if key not in adata.obs_keys():\n        raise ValueError('Could not find the key {!r} in adata.obs'.format(key))\n\n    if covariates is not None:\n        cov_exist = np.isin(covariates, adata.obs_keys())\n        if np.any(~cov_exist):\n            missing_cov = np.array(covariates)[~cov_exist].tolist()\n            raise ValueError('Could not find the covariate(s) {!r} in adata.obs'.format(missing_cov))\n\n        if key in covariates:\n            raise ValueError('Batch key and covariates cannot overlap')\n\n        if len(covariates) != len(set(covariates)):\n            raise ValueError('Covariates must be unique')\n\n    # only works on dense matrices so far\n    if issparse(adata.X):\n        X = adata.X.A.T\n    else:\n        X = adata.X.T\n    data = pd.DataFrame(\n        data=X,\n        index=adata.var_names,\n        columns=adata.obs_names,\n    )\n\n    sanitize_anndata(adata)\n\n    # construct a pandas series of the batch annotation\n    model = adata.obs[[key] + (covariates if covariates else [])]\n    batch_info = model.groupby(key).groups.values()\n    n_batch = len(batch_info)\n    n_batches = np.array([len(v) for v in batch_info])\n    n_array = float(sum(n_batches))\n\n    # standardize across genes using a pooled variance estimator\n    logg.info(\"Standardizing Data across genes.\\n\")\n    s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key)\n\n    # fitting the parameters on the standardized data\n    logg.info(\"Fitting L/S model and finding priors\\n\")\n    batch_design = design[design.columns[:n_batch]]\n    # first estimate of the additive batch effect\n    gamma_hat = np.dot(np.dot(la.inv(np.dot(batch_design.T, batch_design)), batch_design.T), s_data.T)\n    delta_hat = []\n\n    # first estimate for the multiplicative batch effect\n    for i, batch_idxs in enumerate(batch_info):\n        delta_hat.append(s_data[batch_idxs].var(axis=1))\n\n    # empirically fix the prior hyperparameters\n    gamma_bar = gamma_hat.mean(axis=1)\n    t2 = gamma_hat.var(axis=1)\n    # a_prior and b_prior are the priors on lambda and theta from Johnson and Li (2006)\n    a_prior = list(map(_aprior, delta_hat))\n    b_prior = list(map(_bprior, delta_hat))\n\n    logg.info(\"Finding parametric adjustments\\n\")\n    # gamma star and delta star will be our empirical bayes (EB) estimators\n    # for the additive and multiplicative batch effect per batch and cell\n    gamma_star, delta_star = [], []\n    for i, batch_idxs in enumerate(batch_info):\n        # temp stores our estimates for the batch effect parameters.\n        # temp[0] is the additive batch effect\n        # temp[1] is the multiplicative batch effect\n        gamma, delta = _it_sol(\n            s_data[batch_idxs].values,\n            gamma_hat[i],\n            delta_hat[i].values,\n            gamma_bar[i],\n            t2[i],\n            a_prior[i],\n            b_prior[i],\n        )\n\n        gamma_star.append(gamma)\n        delta_star.append(delta)\n\n    logg.info(\"Adjusting data\\n\")\n    bayesdata = s_data\n    gamma_star = np.array(gamma_star)\n    delta_star = np.array(delta_star)\n\n    # we now apply the parametric adjustment to the standardized data from above\n    # loop over all batches in the data\n    for j, batch_idxs in enumerate(batch_info):\n\n        # we basically substract the additive batch effect, rescale by the ratio\n        # of multiplicative batch effect to pooled variance and add the overall gene\n        # wise mean\n        dsq = np.sqrt(delta_star[j,:])\n        dsq = dsq.reshape((len(dsq), 1))\n        denom =  np.dot(dsq, np.ones((1, n_batches[j])))\n        numer = np.array(bayesdata[batch_idxs] - np.dot(batch_design.loc[batch_idxs], gamma_star).T)\n        bayesdata[batch_idxs] = numer / denom\n\n    vpsq = np.sqrt(var_pooled).reshape((len(var_pooled), 1))\n    bayesdata = bayesdata * np.dot(vpsq, np.ones((1, int(n_array)))) + stand_mean\n\n    # put back into the adata object or return\n    if inplace:\n        adata.X = bayesdata.values.transpose()\n    else:\n        return bayesdata.values.transpose()", "response": "Combat function for batch effects."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:\n\n    n = (1 - np.isnan(s_data)).sum(axis=1)\n    g_old = g_hat.copy()\n    d_old = d_hat.copy()\n\n    change = 1\n    count = 0\n\n    # we place a normally distributed prior on gamma and and inverse gamma prior on delta\n    # in the loop, gamma and delta are updated together. they depend on each other. we iterate until convergence.\n    while change > conv:\n        g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old)\n        sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1]))\n        sum2 = sum2 ** 2\n        sum2 = sum2.sum(axis=1)\n        d_new = (0.5*sum2 + b) / (n/2.0 + a-1.0)\n\n        change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max())\n        g_old = g_new  # .copy()\n        d_old = d_new  # .copy()\n        count = count + 1\n\n    return g_new, d_new", "response": "Iteratively compute the conditional posterior means for gamma and delta."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tsne(\n    adata,\n    n_pcs=None,\n    use_rep=None,\n    perplexity=30,\n    early_exaggeration=12,\n    learning_rate=1000,\n    random_state=0,\n    use_fast_tsne=True,\n    n_jobs=None,\n    copy=False,\n):\n    \"\"\"\\\n    t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_.\n\n    t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been\n    proposed for visualizating single-cell data by [Amir13]_. Here, by default,\n    we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve\n    a huge speedup and better convergence if you install `Multicore-tSNE\n    <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which\n    will be automatically detected by Scanpy.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    {doc_n_pcs}\n    {use_rep}\n    perplexity : `float`, optional (default: 30)\n        The perplexity is related to the number of nearest neighbors that\n        is used in other manifold learning algorithms. Larger datasets\n        usually require a larger perplexity. Consider selecting a value\n        between 5 and 50. The choice is not extremely critical since t-SNE\n        is quite insensitive to this parameter.\n    early_exaggeration : `float`, optional (default: 12.0)\n        Controls how tight natural clusters in the original space are in the\n        embedded space and how much space will be between them. For larger\n        values, the space between natural clusters will be larger in the\n        embedded space. Again, the choice of this parameter is not very\n        critical. If the cost function increases during initial optimization,\n        the early exaggeration factor or the learning rate might be too high.\n    learning_rate : `float`, optional (default: 1000)\n        Note that the R-package \"Rtsne\" uses a default of 200.\n        The learning rate can be a critical parameter. It should be\n        between 100 and 1000. If the cost function increases during initial\n        optimization, the early exaggeration factor or the learning rate\n        might be too high. If the cost function gets stuck in a bad local\n        minimum increasing the learning rate helps sometimes.\n    random_state : `int` or `None`, optional (default: 0)\n        Change this to use different intial states for the optimization. If `None`,\n        the initial state is not reproducible.\n    use_fast_tsne : `bool`, optional (default: `True`)\n        Use the MulticoreTSNE package by D. Ulyanov if it is installed.\n    n_jobs : `int` or `None` (default: `sc.settings.n_jobs`)\n        Number of jobs.\n    copy : `bool` (default: `False`)\n        Return a copy instead of writing to adata.\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n\n    **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`)\n        tSNE coordinates of data.\n    \"\"\"\n    logg.info('computing tSNE', r=True)\n    adata = adata.copy() if copy else adata\n    X = choose_representation(adata, use_rep=use_rep, n_pcs=n_pcs)\n    # params for sklearn\n    verbosity = _VERBOSITY_LEVELS_FROM_STRINGS.get(settings.verbosity, settings.verbosity)\n    params_sklearn = dict(\n        perplexity=perplexity,\n        random_state=random_state,\n        verbose=max(0, verbosity-3),\n        early_exaggeration=early_exaggeration,\n        learning_rate=learning_rate,\n    )\n    n_jobs = settings.n_jobs if n_jobs is None else n_jobs\n    # deal with different tSNE implementations\n    multicore_failed = True\n    if n_jobs >= 1 and use_fast_tsne:\n        try:\n            from MulticoreTSNE import MulticoreTSNE as TSNE\n            tsne = TSNE(n_jobs=n_jobs, **params_sklearn)\n            logg.info('    using the \\'MulticoreTSNE\\' package by Ulyanov (2017)')\n            # need to transform to float64 for MulticoreTSNE...\n            X_tsne = tsne.fit_transform(X.astype('float64'))\n            multicore_failed = False\n        except ImportError:\n            logg.warn('Consider installing the package MulticoreTSNE '\n                      '(https://github.com/DmitryUlyanov/Multicore-TSNE). '\n                      'Even for n_jobs=1 this speeds up the computation considerably '\n                      'and might yield better converged results.')\n    if multicore_failed:\n        from sklearn.manifold import TSNE\n        from . import _tsne_fix   # fix by D. DeTomaso for sklearn < 0.19\n        # unfortunately, sklearn does not allow to set a minimum number of iterations for barnes-hut tSNE\n        tsne = TSNE(**params_sklearn)\n        logg.info('    using sklearn.manifold.TSNE with a fix by D. DeTomaso')\n        X_tsne = tsne.fit_transform(X)\n    # update AnnData instance\n    adata.obsm['X_tsne'] = X_tsne  # annotate samples with tSNE coordinates\n    logg.info('    finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\\n')\n    logg.hint('added\\n'\n              '    \\'X_tsne\\', tSNE coordinates (adata.obsm)')\n    return adata if copy else None", "response": "This function is used to visualize the T - SNE graph."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the quality control metrics for a given set of variables.", "response": "def calculate_qc_metrics(\n    adata,\n    expr_type=\"counts\",\n    var_type=\"genes\",\n    qc_vars=(),\n    percent_top=(50, 100, 200, 500),\n    inplace=False,\n) -> Optional[Tuple[pd.DataFrame, pd.DataFrame]]:\n    \"\"\"Calculate quality control metrics.\n\n    Calculates a number of qc metrics for an AnnData object, see section\n    `Returns` for specifics. Largely based on `calculateQCMetrics` from scater\n    [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    expr_type : `str`, optional (default: `\"counts\"`)\n        Name of kind of values in X.\n    var_type : `str`, optional (default: `\"genes\"`)\n        The kind of thing the variables are.\n    qc_vars : `Container`, optional (default: `()`)\n        Keys for boolean columns of `.var` which identify variables you could \n        want to control for (e.g. \"ERCC\" or \"mito\").\n    percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`)\n        Which proportions of top genes to cover. If empty or `None` don't\n        calculate. Values are considered 1-indexed, `percent_top=[50]` finds\n        cumulative proportion to the 50th most expressed gene.\n    inplace : bool, optional (default: `False`)\n        Whether to place calculated metrics in `.obs` and `.var`\n\n    Returns\n    -------\n    Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or\n    updates `adata`'s `obs` and `var`.\n\n    Observation level metrics include:\n\n    `total_{var_type}_by_{expr_type}`\n        E.g. \"total_genes_by_counts\". Number of genes with positive counts in a cell.\n    `total_{expr_type}`\n        E.g. \"total_counts\". Total number of counts for a cell.\n    `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`\n        E.g. \"pct_counts_in_top_50_genes\". Cumulative percentage of counts\n        for 50 most expressed genes in a cell.\n    `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`\n        E.g. \"total_counts_mito\". Total number of counts for variabes in\n        `qc_vars`.\n    `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`\n        E.g. \"pct_counts_mito\". Proportion of total counts for a cell which\n        are mitochondrial.\n\n    Variable level metrics include:\n\n    `total_{expr_type}`\n        E.g. \"total_counts\". Sum of counts for a gene.\n    `mean_{expr_type}`\n        E.g. \"mean counts\". Mean expression over all cells.\n    `n_cells_by_{expr_type}`\n        E.g. \"n_cells_by_counts\". Number of cells this expression is\n        measured in.\n    `pct_dropout_by_{expr_type}`\n        E.g. \"pct_dropout_by_counts\". Percentage of cells this feature does\n        not appear in.\n\n    Example\n    -------\n    Calculate qc metrics for visualization.\n\n    >>> adata = sc.datasets.pbmc3k()\n    >>> sc.pp.calculate_qc_metrics(adata, inplace=True)\n    >>> sns.jointplot(\"log1p_total_counts\", \"log1p_n_genes_by_counts\", data=adata.obs, kind=\"hex\")\n    \"\"\"\n    X = adata.X\n    obs_metrics = pd.DataFrame(index=adata.obs_names)\n    var_metrics = pd.DataFrame(index=adata.var_names)\n    if isspmatrix_coo(X):\n        X = csr_matrix(X)  # COO not subscriptable\n    if issparse(X):\n        X.eliminate_zeros()\n    # Calculate obs metrics\n    if issparse(X):\n        obs_metrics[\"n_{var_type}_by_{expr_type}\"] = X.getnnz(axis=1)\n    else:\n        obs_metrics[\"n_{var_type}_by_{expr_type}\"] = np.count_nonzero(X, axis=1)\n    obs_metrics[\"log1p_n_{var_type}_by_{expr_type}\"] = np.log1p(\n        obs_metrics[\"n_{var_type}_by_{expr_type}\"])\n    obs_metrics[\"total_{expr_type}\"] = X.sum(axis=1)\n    obs_metrics[\"log1p_total_{expr_type}\"] = np.log1p(\n        obs_metrics[\"total_{expr_type}\"])\n    if percent_top:\n        percent_top = sorted(percent_top)\n        proportions = top_segment_proportions(X, percent_top)\n        # Since there are local loop variables, formatting must occur in their scope\n        # Probably worth looking into a python3.5 compatable way to make this better\n        for i, n in enumerate(percent_top):\n            obs_metrics[\"pct_{expr_type}_in_top_{n}_{var_type}\".format(**locals())] = \\\n                proportions[:, i] * 100\n    for qc_var in qc_vars:\n        obs_metrics[\"total_{expr_type}_{qc_var}\".format(**locals())] = \\\n            X[:, adata.var[qc_var].values].sum(axis=1)\n        obs_metrics[\"log1p_total_{expr_type}_{qc_var}\".format(**locals())] = \\\n            np.log1p(\n                obs_metrics[\"total_{expr_type}_{qc_var}\".format(**locals())])\n        # \"total_{expr_type}\" not formatted yet\n        obs_metrics[\"pct_{expr_type}_{qc_var}\".format(**locals())] = \\\n            obs_metrics[\"total_{expr_type}_{qc_var}\".format(**locals())] / \\\n            obs_metrics[\"total_{expr_type}\"] * 100\n    # Calculate var metrics\n    if issparse(X):\n        # Current memory bottleneck for csr matrices:\n        var_metrics[\"n_cells_by_{expr_type}\"] = X.getnnz(axis=0)\n        var_metrics[\"mean_{expr_type}\"] = mean_variance_axis(X, axis=0)[0]\n    else:\n        var_metrics[\"n_cells_by_{expr_type}\"] = np.count_nonzero(X, axis=0)\n        var_metrics[\"mean_{expr_type}\"] = X.mean(axis=0)\n    var_metrics[\"log1p_mean_{expr_type}\"] = np.log1p(\n        var_metrics[\"mean_{expr_type}\"])\n    var_metrics[\"pct_dropout_by_{expr_type}\"] = \\\n        (1 - var_metrics[\"n_cells_by_{expr_type}\"] / X.shape[0]) * 100\n    var_metrics[\"total_{expr_type}\"] = np.ravel(X.sum(axis=0))\n    var_metrics[\"log1p_total_{expr_type}\"] = np.log1p(\n        var_metrics[\"total_{expr_type}\"])\n    # Format strings\n    for df in obs_metrics, var_metrics:\n        new_colnames = []\n        for col in df.columns:\n            new_colnames.append(col.format(**locals()))\n        df.columns = new_colnames\n    # Return\n    if inplace:\n        adata.obs[obs_metrics.columns] = obs_metrics\n        adata.var[var_metrics.columns] = var_metrics\n    else:\n        return obs_metrics, var_metrics"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef top_proportions(mtx, n):\n    if issparse(mtx):\n        if not isspmatrix_csr(mtx):\n            mtx = csr_matrix(mtx)\n        # Allowing numba to do more\n        return top_proportions_sparse_csr(mtx.data, mtx.indptr, np.array(n))\n    else:\n        return top_proportions_dense(mtx, n)", "response": "Calculates the cumulative proportions of top expressed genes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef top_segment_proportions(mtx, ns):\n    # Pretty much just does dispatch\n    if not (max(ns) <= mtx.shape[1] and min(ns) > 0):\n        raise IndexError(\"Positions outside range of features.\")\n    if issparse(mtx):\n        if not isspmatrix_csr(mtx):\n            mtx = csr_matrix(mtx)\n        return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr,\n                                                  np.array(ns, dtype=np.int))\n    else:\n        return top_segment_proportions_dense(mtx, ns)", "response": "Calculates total percentage of counts in top ns genes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sim(\n    model,\n    params_file=True,\n    tmax=None,\n    branching=None,\n    nrRealizations=None,\n    noiseObs=None,\n    noiseDyn=None,\n    step=None,\n    seed=None,\n    writedir=None,\n) -> AnnData:\n    \"\"\"Simulate dynamic gene expression data [Wittmann09]_ [Wolf18]_.\n\n    Sample from a stochastic differential equation model built from\n    literature-curated boolean gene regulatory networks, as suggested by\n    [Wittmann09]_. The Scanpy implementation is due to [Wolf18]_.\n\n    Parameters\n    ----------\n    model : {'krumsiek11', 'toggleswitch'}\n        Model file in 'sim_models' directory.\n    params_file : `bool`, (default: `True`)\n        Read default params from file.\n    tmax : `int`, optional (default: `None`)\n        Number of time steps per realization of time series.\n    branching : `bool`, optional (default: `None`)\n        Only write realizations that contain new branches.\n    nrRealizations : int, optional (default: `None`)\n        Number of realizations.\n    noiseObs : float, optional (default: `None`)\n        Observatory/Measurement noise.\n    noiseDyn : float, optional (default: `None`)\n        Dynamic noise.\n    step : int, optional (default: `None`)\n        Interval for saving state of system.\n    seed : int, optional (default: `None`)\n        Seed for generation of random numbers.\n    writedir: str, optional (default: `None`)\n        Path to directory for writing output files.\n\n    Returns\n    -------\n    Annotated data matrix.\n\n    Examples\n    --------\n    See this `use case <https://github.com/theislab/scanpy_usage/tree/master/170430_krumsiek11>`__\n    \"\"\"\n    params = locals()\n    if params_file:\n        model_key = os.path.basename(model).replace('.txt', '')\n        from .. import sim_models\n        pfile_sim = (os.path.dirname(sim_models.__file__)\n                     + '/' + model_key + '_params.txt')\n        default_params = readwrite.read_params(pfile_sim)\n        params = utils.update_params(default_params, params)\n    adata = sample_dynamic_data(params)\n    adata.uns['iroot'] = 0\n    return adata", "response": "Simulate dynamic gene expression data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_args(p):\n    # dictionary for adding arguments\n    dadd_args = {\n        '--opfile': {\n            'default': '',\n            'metavar': 'f',\n            'type': str,\n            'help': 'Specify a parameter file '\n                    '(default: \"sim/${exkey}_params.txt\")'}}\n    p = utils.add_args(p, dadd_args)\n    return p", "response": "Add command line arguments to the parser"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sample_dynamic_data(params):\n    model_key = os.path.basename(params['model']).replace('.txt', '')\n    if 'writedir' not in params or params['writedir'] is None:\n        params['writedir'] = settings.writedir + model_key + '_sim'\n    if not os.path.exists(params['writedir']): os.makedirs(params['writedir'])\n    readwrite.write_params(params['writedir'] + '/params.txt', params)\n    # init variables\n    writedir = params['writedir']\n    tmax = params['tmax']\n    branching = params['branching']\n    noiseObs = params['noiseObs']\n    noiseDyn = params['noiseDyn']\n    nrRealizations = params['nrRealizations']\n    step = params['step']  # step size for saving the figure\n\n    nrSamples = 1  # how many files?\n    maxRestarts = 1000\n    maxNrSamples = 1\n\n    # simple vector auto regressive process or\n    # hill kinetics process simulation\n    if 'krumsiek11' not in model_key:\n        # create instance, set seed\n        grnsim = GRNsim(model=model_key, params=params)\n        nrOffEdges_list = np.zeros(nrSamples)\n        for sample in range(nrSamples):\n            # random topology / for a given edge density\n            if 'hill' not in model_key:\n                Coupl = np.array(grnsim.Coupl)\n                for sampleCoupl in range(10):\n                    nrOffEdges = 0\n                    for gp in range(grnsim.dim):\n                        for g in range(grnsim.dim):\n                            # only consider off-diagonal edges\n                            if g != gp:\n                                Coupl[gp, g] = 0.7 if np.random.rand() < 0.4 else 0\n                                nrOffEdges += 1 if Coupl[gp, g] > 0 else 0\n                            else:\n                                Coupl[gp, g] = 0.7\n                    # check that the coupling matrix does not have eigenvalues\n                    # greater than 1, which would lead to an exploding var process\n                    if max(sp.linalg.eig(Coupl)[0]) < 1:\n                        break\n                nrOffEdges_list[sample] = nrOffEdges\n                grnsim.set_coupl(Coupl)\n            # init type\n            real = 0\n            X0 = np.random.rand(grnsim.dim)\n            Xsamples = []\n            for restart in range(nrRealizations+maxRestarts):\n                # slightly break symmetry in initial conditions\n                if 'toggleswitch' in model_key:\n                    X0 = (np.array([0.8 for i in range(grnsim.dim)])\n                          + 0.01*np.random.randn(grnsim.dim))\n                X = grnsim.sim_model(tmax=tmax, X0=X0,\n                                     noiseDyn=noiseDyn)\n                # check branching\n                check = True\n                if branching:\n                    check, Xsamples = _check_branching(X, Xsamples, restart)\n                if check:\n                    real += 1\n                    grnsim.write_data(X[::step], dir=writedir,\n                                      noiseObs=noiseObs,\n                                      append=(False if restart==0 else True),\n                                      branching=branching,\n                                      nrRealizations=nrRealizations)\n                # append some zeros\n                if 'zeros' in writedir and real == 2:\n                    grnsim.write_data(noiseDyn*np.random.randn(500,3), dir=writedir,\n                                      noiseObs=noiseObs,\n                                      append=(False if restart==0 else True),\n                                      branching=branching,\n                                      nrRealizations=nrRealizations)\n                if real >= nrRealizations:\n                    break\n        if False:\n            logg.info('mean nr of offdiagonal edges',nrOffEdges_list.mean(),\n                      'compared to total nr',grnsim.dim*(grnsim.dim-1)/2.)\n\n    # more complex models\n    else:\n        initType = 'random'\n\n        dim = 11\n        step = 5\n\n        grnsim = GRNsim(dim=dim,initType=initType,model=model_key,params=params)\n        curr_nrSamples = 0\n        Xsamples = []\n        for sample in range(maxNrSamples):\n            # choose initial conditions such that branchings result\n            if initType == 'branch':\n                X0mean = grnsim.branch_init_model1(tmax)\n                if X0mean is None:\n                    grnsim.set_coupl()\n                    continue\n            real = 0\n            for restart in range(nrRealizations+maxRestarts):\n                if initType == 'branch':\n                    # vary initial conditions around mean\n                    X0 = X0mean + (0.05*np.random.rand(dim) - 0.025*np.ones(dim))\n                else:\n                    # generate random initial conditions within [0.3,0.7]\n                    X0 = 0.4*np.random.rand(dim)+0.3\n                if model_key in [5,6]:\n                    X0 = np.array([0.3,0.3,0,0,0,0])\n                if model_key in [7,8,9,10]:\n                    X0 = 0.6*np.random.rand(dim)+0.2\n                    X0[2:] = np.zeros(4)\n                if 'krumsiek11' in model_key:\n                    X0 = np.zeros(dim)\n                    X0[grnsim.varNames['Gata2']] = 0.8\n                    X0[grnsim.varNames['Pu.1']] = 0.8\n                    X0[grnsim.varNames['Cebpa']] = 0.8\n                    X0 += 0.001*np.random.randn(dim)\n                    if False:\n                        switch_gene = restart - (nrRealizations - dim)\n                        if switch_gene >= dim:\n                            break\n                        X0[switch_gene] = 0 if X0[switch_gene] > 0.1 else 0.8\n                X = grnsim.sim_model(tmax,X0=X0,\n                                     noiseDyn=noiseDyn,\n                                     restart=restart)\n                # check branching\n                check = True\n                if branching:\n                    check, Xsamples = _check_branching(X,Xsamples,restart)\n                if check:\n                    real += 1\n                    grnsim.write_data(X[::step],dir=writedir,noiseObs=noiseObs,\n                                     append=(False if restart==0 else True),\n                                     branching=branching,\n                                     nrRealizations=nrRealizations)\n                if real >= nrRealizations:\n                    break\n    import glob\n    # load the last simulation file\n    filename = glob.glob(writedir + '/sim*.txt')[-1]\n    logg.info('reading simulation results', filename)\n    adata = readwrite._read(filename, first_column_names=True,\n                            suppress_cache_warning=True)\n    adata.uns['tmax_write'] = tmax/step\n    return adata", "response": "sample dynamic data from the current model"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_data(X, dir='sim/test', append=False, header='',\n               varNames={}, Adj=np.array([]), Coupl=np.array([]),\n               boolRules={}, model='', modelType='', invTimeStep=1):\n    \"\"\" Write simulated data.\n\n        Accounts for saving at the same time an ID\n        and a model file.\n    \"\"\"\n    # check if output directory exists\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n    # update file with sample ids\n    filename = dir+'/id.txt'\n    if os.path.exists(filename):\n        f = open(filename,'r')\n        id = int(f.read()) + (0 if append else 1)\n        f.close()\n    else:\n        id = 0\n    f = open(filename,'w')\n    id = '{:0>6}'.format(id)\n    f.write(str(id))\n    f.close()\n    # dimension\n    dim = X.shape[1]\n    # write files with adjacancy and coupling matrices\n    if not append:\n        if False:\n            if Adj.size > 0:\n                # due to 'update formulation' of model, there\n                # is always a diagonal dependence\n                Adj = np.copy(Adj)\n                if 'hill' in model:\n                    for i in range(Adj.shape[0]):\n                        Adj[i,i] = 1\n                np.savetxt(dir+'/adj_'+id+'.txt',Adj,\n                           header=header,\n                           fmt='%d')\n            if Coupl.size > 0:\n                np.savetxt(dir+'/coupl_'+id+'.txt',Coupl,\n                           header=header,\n                           fmt='%10.6f')\n        # write model file\n        if varNames and Coupl.size > 0:\n            f = open(dir+'/model_'+id+'.txt','w')\n            f.write('# For each \"variable = \", there must be a right hand side: \\n')\n            f.write('# either an empty string or a python-style logical expression \\n')\n            f.write('# involving variable names, \"or\", \"and\", \"(\", \")\". \\n')\n            f.write('# The order of equations matters! \\n')\n            f.write('# \\n')\n            f.write('# modelType = ' + modelType + '\\n')\n            f.write('# invTimeStep = '+ str(invTimeStep) + '\\n')\n            f.write('# \\n')\n            f.write('# boolean update rules: \\n')\n            for rule in boolRules.items():\n                f.write(rule[0] + ' = ' + rule[1] + '\\n')\n            # write coupling via names\n            f.write('# coupling list: \\n')\n            names = list(varNames.keys())\n            for gp in range(dim):\n                for g in range(dim):\n                    if np.abs(Coupl[gp,g]) > 1e-10:\n                        f.write('{:10} '.format(names[gp])\n                                + '{:10} '.format(names[g])\n                                + '{:10.3} '.format(Coupl[gp,g]) + '\\n')\n            f.close()\n    # write simulated data\n    # the binary mode option in the following line is a fix for python 3\n    # variable names\n    if varNames:\n        header += '{:>2} '.format('it')\n        for v in varNames.keys():\n            header += '{:>7} '.format(v)\n    f = open(dir+'/sim_'+id+'.txt','ab' if append else 'wb')\n    np.savetxt(f,np.c_[np.arange(0,X.shape[0]),X],header=('' if append else header),\n               fmt=['%4.f']+['%7.4f' for i in range(dim)])\n    f.close()", "response": "Write simulated data to a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_branching(X,Xsamples,restart,threshold=0.25):\n    check = True\n    if restart == 0:\n        Xsamples.append(X)\n    else:\n        for Xcompare in Xsamples:\n            Xtmax_diff = np.absolute(X[-1,:] - Xcompare[-1,:])\n            # If the second largest element is smaller than threshold\n            # set check to False, i.e. at least two elements\n            # need to change in order to have a branching.\n            # If we observe all parameters of the system,\n            # a new attractor state must involve changes in two\n            # variables.\n            if np.partition(Xtmax_diff,-2)[-2] < threshold:\n                check = False\n        if check:\n            Xsamples.append(X)\n    if not check:\n        logg.m('realization {}:'.format(restart), 'no new branch', v=4)\n    else:\n        logg.m('realization {}:'.format(restart), 'new branch', v=4)\n    return check, Xsamples", "response": "Checks whether the current time series data is branching."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_nocycles(Adj, verbosity=2):\n    dim = Adj.shape[0]\n    for g in range(dim):\n        v = np.zeros(dim)\n        v[g] = 1\n        for i in range(dim):\n            v = Adj.dot(v)\n            if v[g] > 1e-10:\n                if verbosity > 2:\n                    settings.m(0,Adj)\n                    settings.m(0,'contains a cycle of length',i+1,\n                          'starting from node',g,\n                          '-> reject')\n                return False\n    return True", "response": "Checks that there are no cycles in the graph described by the adjancancy matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsamples the coupling matrix.", "response": "def sample_coupling_matrix(dim=3,connectivity=0.5):\n    \"\"\"\\\n    Sample coupling matrix.\n\n    Checks that returned graphs contain no self-cycles.\n\n    Parameters\n    ----------\n    dim : int\n        dimension of coupling matrix.\n    connectivity : float\n        fraction of connectivity, fully connected means 1.,\n        not-connected means 0, in the case of fully connected, one has\n        dim*(dim-1)/2 edges in the graph.\n\n    Returns\n    -------\n    Tuple (Coupl,Adj,Adj_signed) of coupling matrix, adjancancy and\n    signed adjacancy matrix.\n    \"\"\"\n    max_trial = 10\n    check = False\n    for trial in range(max_trial):\n        # random topology for a given connectivity / edge density\n        Coupl = np.zeros((dim,dim))\n        n_edges = 0\n        for gp in range(dim):\n            for g in range(dim):\n                if gp != g:\n                    # need to have the factor 0.5, otherwise\n                    # connectivity=1 would lead to dim*(dim-1) edges\n                    if np.random.rand() < 0.5*connectivity:\n                        Coupl[gp,g] = 0.7\n                        n_edges += 1\n        # obtain adjacancy matrix\n        Adj_signed = np.zeros((dim,dim),dtype='int_')\n        Adj_signed = np.sign(Coupl)\n        Adj = np.abs(Adj_signed)\n        # check for cycles and whether there is at least one edge\n        if check_nocycles(Adj) and n_edges > 0:\n            check = True\n            break\n    if not check:\n        raise ValueError('did not find graph without cycles after',\n                         max_trial,'trials')\n    return Coupl, Adj, Adj_signed, n_edges"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding Xdiff from coefficients of boolean network and Xt.", "response": "def Xdiff_hill(self,Xt):\n        \"\"\" Build Xdiff from coefficients of boolean network,\n            that is, using self.boolCoeff. The employed functions\n            are Hill type activation and deactivation functions.\n\n            See Wittmann et al., BMC Syst. Biol. 3, 98 (2009),\n            doi:10.1186/1752-0509-3-98 for more details.\n        \"\"\"\n        verbosity = self.verbosity>0 and self.writeOutputOnce\n        self.writeOutputOnce = False\n        Xdiff = np.zeros(self.dim)\n        for ichild,child in enumerate(self.pas.keys()):\n            # check whether list of parents is non-empty,\n            # otherwise continue\n            if self.pas[child]:\n                Xdiff_syn = 0 # synthesize term\n                if verbosity > 0:\n                    Xdiff_syn_str = ''\n            else:\n                continue\n            # loop over all tuples for which the boolean update\n            # rule returns true, these are stored in self.boolCoeff\n            for ituple,tuple in enumerate(self.boolCoeff[child]):\n                Xdiff_syn_tuple = 1\n                Xdiff_syn_tuple_str = ''\n                for iv,v in enumerate(tuple):\n                    iparent = self.varNames[self.pas[child][iv]]\n                    x = Xt[iparent]\n                    threshold = 0.1/np.abs(self.Coupl[ichild,iparent])\n                    Xdiff_syn_tuple *= self.hill_a(x,threshold) if v else self.hill_i(x,threshold)\n                    if verbosity > 0:\n                        Xdiff_syn_tuple_str += (('a' if v else 'i')\n                                                +'('+self.pas[child][iv]+','+'{:.2}'.format(threshold)+')')\n                Xdiff_syn += Xdiff_syn_tuple\n                if verbosity > 0:\n                    Xdiff_syn_str += ('+' if ituple != 0 else '') + Xdiff_syn_tuple_str\n            # multiply with degradation term\n            Xdiff[ichild] = self.invTimeStep*(Xdiff_syn - Xt[ichild])\n            if verbosity > 0:\n                Xdiff_str = (child+'_{+1}-' + child + ' = ' + str(self.invTimeStep)\n                             + '*('+Xdiff_syn_str+'-'+child+')' )\n                settings.m(0,Xdiff_str)\n        return Xdiff"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninhibiting hill function. Is equivalent to 1-hill_a(self,x,power,threshold).", "response": "def hill_i(self,x,threshold=0.1,power=2):\n        \"\"\" Inhibiting hill function.\n\n            Is equivalent to 1-hill_a(self,x,power,threshold).\n        \"\"\"\n        x_pow = np.power(x,power)\n        threshold_pow = np.power(threshold,power)\n        return threshold_pow / (x_pow + threshold_pow)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nnormalize activating hill function.", "response": "def nhill_a(self,x,threshold=0.1,power=2,ichild=2):\n        \"\"\" Normalized activating hill function. \"\"\"\n        x_pow = np.power(x,power)\n        threshold_pow = np.power(threshold,power)\n        return x_pow / (x_pow + threshold_pow) * (1 + threshold_pow)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nnormalizes inhibiting hill function.", "response": "def nhill_i(self,x,threshold=0.1,power=2):\n        \"\"\" Normalized inhibiting hill function.\n\n            Is equivalent to 1-nhill_a(self,x,power,threshold).\n        \"\"\"\n        x_pow = np.power(x,power)\n        threshold_pow = np.power(threshold,power)\n        return threshold_pow / (x_pow + threshold_pow) * (1 - x_pow)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_model(self):\n        if self.verbosity > 0:\n            settings.m(0,'reading model',self.model)\n        # read model\n        boolRules = []\n        for line in open(self.model):\n            if line.startswith('#') and 'modelType =' in line:\n                keyval = line\n                if '|' in line:\n                    keyval, type = line.split('|')[:2]\n                self.modelType = keyval.split('=')[1].strip()\n            if line.startswith('#') and 'invTimeStep =' in line:\n                keyval = line\n                if '|' in line:\n                    keyval, type = line.split('|')[:2]\n                self.invTimeStep = float(keyval.split('=')[1].strip())\n            if not line.startswith('#'):\n                boolRules.append([s.strip() for s in line.split('=')])\n            if line.startswith('# coupling list:'):\n                break\n        self.dim = len(boolRules)\n        self.boolRules = collections.OrderedDict(boolRules)\n        self.varNames = collections.OrderedDict([(s, i)\n            for i, s in enumerate(self.boolRules.keys())])\n        names = self.varNames\n        # read couplings via names\n        self.Coupl = np.zeros((self.dim, self.dim))\n        boolContinue = True\n        for line in open(self.model):  # open(self.model.replace('/model','/couplList')):\n            if line.startswith('# coupling list:'):\n                boolContinue = False\n            if boolContinue:\n                continue\n            if not line.startswith('#'):\n                gps, gs, val = line.strip().split()\n                self.Coupl[int(names[gps]), int(names[gs])] = float(val)\n        # adjancecy matrices\n        self.Adj_signed = np.sign(self.Coupl)\n        self.Adj = np.abs(np.array(self.Adj_signed))\n        # build bool coefficients (necessary for odefy type\n        # version of the discrete model)\n        self.build_boolCoeff()", "response": "Read the model and the couplings from the model file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_coupl(self, Coupl=None):\n        self.varNames = collections.OrderedDict([(str(i), i) for i in range(self.dim)])\n        if (self.model not in self.availModels.keys()\n            and Coupl is None):\n            self.read_model()\n        elif 'var' in self.model:\n            # vector auto regressive process\n            self.Coupl = Coupl\n            self.boolRules = collections.OrderedDict(\n                              [(s, '') for s in self.varNames.keys()])\n            names = list(self.varNames.keys())\n            for gp in range(self.dim):\n                pas = []\n                for g in range(self.dim):\n                    if np.abs(self.Coupl[gp,g] > 1e-10):\n                        pas.append(names[g])\n                self.boolRules[\n                    names[gp]] = ''.join(pas[:1]\n                                         + [' or ' + pa for pa in pas[1:]])\n                self.Adj_signed = np.sign(Coupl)\n        elif self.model in ['6','7','8','9','10']:\n            self.Adj_signed = np.zeros((self.dim,self.dim))\n            n_sinknodes = 2\n#             sinknodes = np.random.choice(np.arange(0,self.dim),\n#                                              size=n_sinknodes,replace=False)\n            sinknodes = np.array([0,1])\n            # assume sinknodes have feeback\n            self.Adj_signed[sinknodes,sinknodes] = np.ones(n_sinknodes)\n#             # allow negative feedback\n#             if self.model == 10:\n#                 plus_minus = (np.random.randint(0,2,n_sinknodes) - 0.5)*2\n#                 self.Adj_signed[sinknodes,sinknodes] = plus_minus\n            leafnodes = np.array(sinknodes)\n            availnodes = np.array([i for i in range(self.dim) if i not in sinknodes])\n#             settings.m(0,leafnodes,availnodes)\n            while len(availnodes) != 0:\n                # parent\n                parent_idx = np.random.choice(np.arange(0,len(leafnodes)),\n                                                    size=1,replace=False)\n                parent = leafnodes[parent_idx]\n                # children\n                children_ids = np.random.choice(np.arange(0,len(availnodes)),\n                                                       size=2,replace=False)\n                children = availnodes[children_ids]\n                settings.m(0,parent,children)\n                self.Adj_signed[children,parent] = np.ones(2)\n                if self.model == 8:\n                    self.Adj_signed[children[0],children[1]] = -1\n                if self.model in [9,10]:\n                    self.Adj_signed[children[0],children[1]] = -1\n                    self.Adj_signed[children[1],children[0]] = -1\n                # update leafnodes\n                leafnodes = np.delete(leafnodes,parent_idx)\n                leafnodes = np.append(leafnodes,children)\n                # update availnodes\n                availnodes = np.delete(availnodes,children_ids)\n#                 settings.m(0,availnodes)\n#                 settings.m(0,leafnodes)\n#                 settings.m(0,self.Adj)\n#                 settings.m(0,'-')\n        else:\n            self.Adj = np.zeros((self.dim,self.dim))\n            for i in range(self.dim):\n                indep = np.random.binomial(1,self.p_indep)\n                if indep == 0:\n                    # this number includes parents (other variables)\n                    # and the variable itself, therefore its\n                    # self.maxnpar+2 in the following line\n                    nr = np.random.randint(1,self.maxnpar+2)\n                    j_par = np.random.choice(np.arange(0,self.dim),\n                                             size=nr,replace=False)\n                    self.Adj[i,j_par] = 1\n                else:\n                    self.Adj[i,i] = 1\n        #\n        self.Adj = np.abs(np.array(self.Adj_signed))", "response": "Construct the coupling matrix from the predefined models and the adjacancy matrix."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_coupl_old(self):\n        if self.model == 'krumsiek11' or self.model == 'var':\n            # we already built the coupling matrix in set_coupl20()\n            return\n        self.Coupl = np.zeros((self.dim,self.dim))\n        for i in range(self.Adj.shape[0]):\n            for j,a in enumerate(self.Adj[i]):\n                # if there is a 1 in Adj, specify co and antiregulation\n                # and strength of regulation\n                if a != 0:\n                    co_anti = np.random.randint(2)\n                    # set a lower bound for the coupling parameters\n                    # they ought not to be smaller than 0.1\n                    # and not be larger than 0.4\n                    self.Coupl[i,j] = 0.0*np.random.rand() + 0.1\n                    # set sign for coupling\n                    if co_anti == 1:\n                        self.Coupl[i,j] *= -1\n        # enforce certain requirements on models\n        if self.model == 1:\n            self.coupl_model1()\n        elif self.model == 5:\n            self.coupl_model5()\n        elif self.model in [6,7]:\n            self.coupl_model6()\n        elif self.model in [8,9,10]:\n            self.coupl_model8()\n        # output\n        if self.verbosity > 1:\n           settings.m(0,self.Coupl)", "response": "Create a coupling matrix from the adjacency matrix."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef coupl_model1(self):\n        self.Coupl[0,0] = np.abs(self.Coupl[0,0])\n        self.Coupl[0,1] = -np.abs(self.Coupl[0,1])\n        self.Coupl[1,1] = np.abs(self.Coupl[1,1])", "response": "In model 1 we want enforce the following signs\n            on the couplings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsimulating the model backwards in time.", "response": "def sim_model_backwards(self,tmax,X0):\n        \"\"\" Simulate the model backwards in time.\n        \"\"\"\n        X = np.zeros((tmax,self.dim))\n        X[tmax-1] = X0\n        for t in range(tmax-2,-1,-1):\n            sol = sp.optimize.root(self.sim_model_back_help,\n                                   X[t+1],\n                                   args=(X[t+1]),method='hybr')\n            X[t] = sol.x\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parents_from_boolRule(self,rule):\n        rule_pa = rule.replace('(','').replace(')','').replace('or','').replace('and','').replace('not','')\n        rule_pa = rule_pa.split()\n        # if there are no parents, continue\n        if not rule_pa:\n            return []\n        # check whether these are meaningful parents\n        pa_old = []\n        pa_delete = []\n        for pa in rule_pa:\n            if pa not in self.varNames.keys():\n                settings.m(0,'list of available variables:')\n                settings.m(0,list(self.varNames.keys()))\n                message = ('processing of rule \"' + rule\n                             + ' yields an invalid parent: ' + pa\n                             + ' | check whether the syntax is correct: \\n'\n                             + 'only python expressions \"(\",\")\",\"or\",\"and\",\"not\" '\n                             + 'are allowed, variable names and expressions have to be separated '\n                             + 'by white spaces')\n                raise ValueError(message)\n            if pa in pa_old:\n                pa_delete.append(pa)\n        for pa in pa_delete:\n            rule_pa.remove(pa)\n        return rule_pa", "response": "Determine parents based on boolean updaterule."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute coefficients for tuple space from boolean update rules.", "response": "def build_boolCoeff(self):\n        ''' Compute coefficients for tuple space.\n        '''\n        # coefficients for hill functions from boolean update rules\n        self.boolCoeff = collections.OrderedDict([(s,[]) for s in self.varNames.keys()])\n        # parents\n        self.pas = collections.OrderedDict([(s,[]) for s in self.varNames.keys()])\n        #\n        for key in self.boolRules.keys():\n            rule = self.boolRules[key]\n            self.pas[key] = self.parents_from_boolRule(rule)\n            pasIndices = [self.varNames[pa] for pa in self.pas[key]]\n            # check whether there are coupling matrix entries for each parent\n            for g in range(self.dim):\n                if g in pasIndices:\n                    if np.abs(self.Coupl[self.varNames[key],g]) < 1e-10:\n                        raise ValueError('specify coupling value for '+str(key)+' <- '+str(g))\n                else:\n                    if np.abs(self.Coupl[self.varNames[key],g]) > 1e-10:\n                        raise ValueError('there should be no coupling value for '+str(key)+' <- '+str(g))\n            if self.verbosity > 1:\n                settings.m(0,'...'+key)\n                settings.m(0,rule)\n                settings.m(0,rule_pa)\n            # now evaluate coefficients\n            for tuple in list(itertools.product([False,True],repeat=len(self.pas[key]))):\n                if self.process_rule(rule,self.pas[key],tuple):\n                    self.boolCoeff[key].append(tuple)\n            #\n            if self.verbosity > 1:\n                settings.m(0,self.boolCoeff[key])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess a string that denotes a boolean rule.", "response": "def process_rule(self,rule,pa,tuple):\n        ''' Process a string that denotes a boolean rule.\n        '''\n        for i,v in enumerate(tuple):\n            rule = rule.replace(pa[i],str(v))\n        return eval(rule)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsimulate data given an adjacency matrix and a model.", "response": "def sim_givenAdj(self, Adj: np.array, model='line'):\n        \"\"\"\\\n        Simulate data given only an adjacancy matrix and a model.\n\n        The model is a bivariate funtional dependence. The adjacancy matrix\n        needs to be acyclic.\n\n        Parameters\n        ----------\n        Adj\n            adjacancy matrix of shape (dim,dim).\n\n        Returns\n        -------\n        Data array of shape (n_samples,dim).\n        \"\"\"\n        # nice examples\n        examples = [{'func' : 'sawtooth', 'gdist' : 'uniform',\n                     'sigma_glob' : 1.8, 'sigma_noise' : 0.1}]\n\n        # nr of samples\n        n_samples = 100\n\n        # noise\n        sigma_glob = 1.8\n        sigma_noise = 0.4\n\n        # coupling function / model\n        func = self.funcs[model]\n\n        # glob distribution\n        sourcedist = 'uniform'\n\n        # loop over source nodes\n        dim = Adj.shape[0]\n        X = np.zeros((n_samples,dim))\n        # source nodes have no parents themselves\n        nrpar = 0\n        children = list(range(dim))\n        parents = []\n        for gp in range(dim):\n            if Adj[gp,:].sum() == nrpar:\n                if sourcedist == 'gaussian':\n                    X[:,gp] = np.random.normal(0,sigma_glob,n_samples)\n                if sourcedist == 'uniform':\n                    X[:,gp] = np.random.uniform(-sigma_glob,sigma_glob,n_samples)\n                parents.append(gp)\n                children.remove(gp)\n\n        # all of the following guarantees for 3 dim, that we generate the data\n        # in the correct sequence\n        # then compute all nodes that have 1 parent, then those with 2 parents\n        children_sorted = []\n        nrchildren_par = np.zeros(dim)\n        nrchildren_par[0] = len(parents)\n        for nrpar in range(1,dim):\n            # loop over child nodes\n            for gp in children:\n                if Adj[gp,:].sum() == nrpar:\n                    children_sorted.append(gp)\n                    nrchildren_par[nrpar] += 1\n        # if there is more than a child with a single parent\n        # order these children (there are two in three dim)\n        # by distance to the source/parent\n        if nrchildren_par[1] > 1:\n            if Adj[children_sorted[0],parents[0]] == 0:\n                help = children_sorted[0]\n                children_sorted[0] = children_sorted[1]\n                children_sorted[1] = help\n\n        for gp in children_sorted:\n            for g in range(dim):\n                if Adj[gp,g] > 0:\n                    X[:,gp] += 1./Adj[gp,:].sum()*func(X[:,g])\n            X[:,gp] += np.random.normal(0,sigma_noise,n_samples)\n\n#         fig = pl.figure()\n#         fig.add_subplot(311)\n#         pl.plot(X[:,0],X[:,1],'.',mec='white')\n#         fig.add_subplot(312)\n#         pl.plot(X[:,1],X[:,2],'.',mec='white')\n#         fig.add_subplot(313)\n#         pl.plot(X[:,2],X[:,0],'.',mec='white')\n#         pl.show()\n\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsimulating data to model combi regulation.", "response": "def sim_combi(self):\n        \"\"\" Simulate data to model combi regulation.\n        \"\"\"\n        n_samples = 500\n        sigma_glob = 1.8\n\n        X = np.zeros((n_samples,3))\n\n        X[:,0] = np.random.uniform(-sigma_glob,sigma_glob,n_samples)\n        X[:,1] = np.random.uniform(-sigma_glob,sigma_glob,n_samples)\n\n        func = self.funcs['tanh']\n\n        # XOR type\n#         X[:,2] = (func(X[:,0])*sp.stats.norm.pdf(X[:,1],0,0.2)\n#                   + func(X[:,1])*sp.stats.norm.pdf(X[:,0],0,0.2))\n        # AND type / diagonal\n#         X[:,2] = (func(X[:,0]+X[:,1])*sp.stats.norm.pdf(X[:,1]-X[:,0],0,0.2))\n        # AND type / horizontal\n        X[:,2] = (func(X[:,0])*sp.stats.norm.cdf(X[:,1],1,0.2))\n\n        pl.scatter(X[:,0],X[:,1],c=X[:,2],edgecolor='face')\n        pl.show()\n\n        pl.plot(X[:,1],X[:,2],'.')\n        pl.show()\n\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef highly_variable_genes(adata_or_result, log=False, show=None, save=None, highly_variable_genes=True):\n    if isinstance(adata_or_result, AnnData):\n        result = adata_or_result.var\n    else:\n        result = adata_or_result\n    if highly_variable_genes:\n        gene_subset = result.highly_variable\n    else:\n        gene_subset = result.gene_subset        \n    means = result.means\n    dispersions = result.dispersions\n    dispersions_norm = result.dispersions_norm\n    size = rcParams['figure.figsize']\n    pl.figure(figsize=(2*size[0], size[1]))\n    pl.subplots_adjust(wspace=0.3)\n    for idx, d in enumerate([dispersions_norm, dispersions]):\n        pl.subplot(1, 2, idx + 1)\n        for label, color, mask in zip(['highly variable genes', 'other genes'],\n                                      ['black', 'grey'],\n                                      [gene_subset, ~gene_subset]):\n            if False: means_, disps_ = np.log10(means[mask]), np.log10(d[mask])\n            else: means_, disps_ = means[mask], d[mask]\n            pl.scatter(means_, disps_, label=label, c=color, s=1)\n        if log:  # there's a bug in autoscale\n            pl.xscale('log')\n            pl.yscale('log')\n            min_dispersion = np.min(dispersions)\n            y_min = 0.95*min_dispersion if min_dispersion > 0 else 1e-1\n            pl.xlim(0.95*np.min(means), 1.05*np.max(means))\n            pl.ylim(y_min, 1.05*np.max(dispersions))\n        if idx == 0: pl.legend()\n        pl.xlabel(('$log_{10}$ ' if False else '') + 'mean expressions of genes')\n        pl.ylabel(('$log_{10}$ ' if False else '') + 'dispersions of genes'\n                  + (' (normalized)' if idx == 0 else ' (not normalized)'))\n    utils.savefig_or_show('filter_genes_dispersion', show=show, save=save)", "response": "Highly variable genes plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filter_genes_dispersion(result, log=False, show=None, save=None):\n    highly_variable_genes(result, log=False, show=None, save=None, highly_variable_genes=False)", "response": "Filter out genes dispersions versus means for genes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _calc_overlap_count(\n    markers1: dict,\n    markers2: dict,\n):\n    \"\"\"Calculate overlap count between the values of two dictionaries\n\n    Note: dict values must be sets\n    \"\"\"\n    overlaps=np.zeros((len(markers1), len(markers2)))\n\n    j=0\n    for marker_group in markers1:\n        tmp = [len(markers2[i].intersection(markers1[marker_group])) for i in markers2.keys()]\n        overlaps[j,:] = tmp\n        j += 1\n\n    return overlaps", "response": "Calculate the overlap count between the values of two dictionaries\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the overlap coefficient between the values of two dictionaries", "response": "def _calc_overlap_coef(\n    markers1: dict,\n    markers2: dict,\n):\n    \"\"\"Calculate overlap coefficient between the values of two dictionaries\n\n    Note: dict values must be sets\n    \"\"\"\n    overlap_coef=np.zeros((len(markers1), len(markers2)))\n\n    j=0\n    for marker_group in markers1:\n        tmp = [len(markers2[i].intersection(markers1[marker_group]))/\n               max(min(len(markers2[i]), len(markers1[marker_group])),1) for i in markers2.keys()]\n        overlap_coef[j,:] = tmp\n        j += 1\n\n    return overlap_coef"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the jaccard index between the values of two dictionaries", "response": "def _calc_jaccard(\n    markers1: dict,\n    markers2: dict,\n):\n    \"\"\"Calculate jaccard index between the values of two dictionaries\n\n    Note: dict values must be sets\n    \"\"\"\n    jacc_results=np.zeros((len(markers1), len(markers2)))\n\n    j=0\n    for marker_group in markers1:\n        tmp = [len(markers2[i].intersection(markers1[marker_group]))/\n               len(markers2[i].union(markers1[marker_group])) for i in markers2.keys()]\n        jacc_results[j,:] = tmp\n        j += 1\n\n    return jacc_results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating an overlap score between data - deriven marker genes and a single marker gene set.", "response": "def marker_gene_overlap(\n    adata: AnnData,\n    reference_markers: Union[Dict[str, set], Dict[str,list]],\n    *,\n    key: str = 'rank_genes_groups',\n    method: Optional[str] = 'overlap_count',\n    normalize: Union[str, None] = None,\n    top_n_markers: Optional[int] = None,\n    adj_pval_threshold: Optional[float] = None,\n    key_added: Optional[str] = 'marker_gene_overlap',\n    inplace: Optional[bool] = False\n):\n    \"\"\"Calculate an overlap score between data-deriven marker genes and \n    provided markers\n\n    Marker gene overlap scores can be quoted as overlap counts, overlap \n    coefficients, or jaccard indices. The method returns a pandas dataframe\n    which can be used to annotate clusters based on marker gene overlaps.\n\n    This function was written by Malte Luecken.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    reference_markers\n        A marker gene dictionary object. Keys should be strings with the \n        cell identity name and values are sets or lists of strings which match \n        format of `adata.var_name`.\n    key\n        The key in `adata.uns` where the rank_genes_groups output is stored.\n        By default this is `'rank_genes_groups'`.\n    method : `{'overlap_count', 'overlap_coef', 'jaccard'}`, optional \n        (default: `overlap_count`)\n        Method to calculate marker gene overlap. `'overlap_count'` uses the\n        intersection of the gene set, `'overlap_coef'` uses the overlap \n        coefficient, and `'jaccard'` uses the Jaccard index.\n    normalize : `{'reference', 'data', 'None'}`, optional (default: `None`)\n        Normalization option for the marker gene overlap output. This parameter\n        can only be set when `method` is set to `'overlap_count'`. `'reference'`\n        normalizes the data by the total number of marker genes given in the \n        reference annotation per group. `'data'` normalizes the data by the\n        total number of marker genes used for each cluster.\n    top_n_markers\n        The number of top data-derived marker genes to use. By default all \n        calculated marker genes are used. If `adj_pval_threshold` is set along\n        with `top_n_markers`, then `adj_pval_threshold` is ignored.\n    adj_pval_threshold\n        A significance threshold on the adjusted p-values to select marker \n        genes. This can only be used when adjusted p-values are calculated by \n        `sc.tl.rank_genes_groups()`. If `adj_pval_threshold` is set along with \n        `top_n_markers`, then `adj_pval_threshold` is ignored.\n    key_added\n        Name of the `.uns` field that will contain the marker overlap scores.\n    inplace\n        Return a marker gene dataframe or store it inplace in `adata.uns`.\n\n\n    Returns\n    -------\n    A pandas dataframe with the marker gene overlap scores if `inplace=False`.\n    For `inplace=True` `adata.uns` is updated with an additional field \n    specified by the `key_added` parameter (default = 'marker_gene_overlap'). \n\n\n    Examples\n    --------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.pp.pca(adata, svd_solver='arpack')\n    >>> sc.pp.neighbors(adata)\n    >>> sc.tl.louvain(adata)\n    >>> sc.tl.rank_genes_groups(adata, groupby='louvain')\n    >>> marker_genes = {'CD4 T cells':{'IL7R'},'CD14+ Monocytes':{'CD14', \n    ...                 'LYZ'}, 'B cells':{'MS4A1'}, 'CD8 T cells':{'CD8A'}, \n    ...                 'NK cells':{'GNLY', 'NKG7'}, 'FCGR3A+ Monocytes':\n    ...                 {'FCGR3A', 'MS4A7'}, 'Dendritic Cells':{'FCER1A', \n    ...                 'CST3'}, 'Megakaryocytes':{'PPBP'}}\n    >>> marker_matches = sc.tl.marker_gene_overlap(adata, marker_genes)\n    \"\"\"\n    # Test user inputs\n    if inplace:\n        raise NotImplementedError('Writing Pandas dataframes to h5ad is '\n                                  'currently under development.\\n'\n                                  'Please use `inplace=False`.')\n\n    if key not in adata.uns:\n        raise ValueError('Could not find marker gene data. '\n                         'Please run `sc.tl.rank_genes_groups()` first.')\n\n    avail_methods = {'overlap_count', 'overlap_coef', 'jaccard', 'enrich'}\n    if method not in avail_methods:\n        raise ValueError('Method must be one of {}.'.format(avail_methods))\n    \n    if normalize == 'None':\n        normalize = None\n\n    avail_norm = {'reference', 'data', None}\n    if normalize not in avail_norm:\n        raise ValueError('Normalize must be one of {}.'.format(avail_norm))\n    \n    if normalize is not None and method != 'overlap_count':\n        raise ValueError('Can only normalize with method=`overlap_count`.')\n\n    if not np.all([isinstance(val, set) for val in reference_markers.values()]):\n        try:\n            reference_markers = {key:set(val) for key,val\n                                 in reference_markers.items()}\n        except:\n            raise ValueError('Please ensure that `reference_markers` contains '\n                             'sets or lists of markers as values.')\n\n    if adj_pval_threshold is not None:\n        if 'pvals_adj' not in adata.uns[key]:\n            raise ValueError('Could not find adjusted p-value data. '\n                             'Please run `sc.tl.rank_genes_groups()` with a '\n                             'method that outputs adjusted p-values.')\n\n        if adj_pval_threshold < 0:\n            logg.warn('`adj_pval_threshold` was set below 0. '\n                      'Threshold will be set to 0.')\n            adj_pval_threshold = 0\n\n        if adj_pval_threshold > 1:\n            logg.warn('`adj_pval_threshold` was set above 1. '\n                      'Threshold will be set to 1.')\n            adj_pval_threshold = 1\n\n        if top_n_markers is not None:\n            logg.warn('Both `adj_pval_threshold` and `top_n_markers` is set. '\n                      '`adj_pval_threshold` will be ignored.')\n            \n    if top_n_markers is not None:\n        if top_n_markers < 1:\n            logg.warn('`top_n_markers` was set below 1. '\n                      '`top_n_markers` will be set to 1.')\n            top_n_markers = 1\n            \n    # Get data-derived marker genes in a dictionary of sets\n    data_markers = dict()\n    cluster_ids = adata.uns[key]['names'].dtype.names\n\n    for group in cluster_ids:\n\n        if top_n_markers is not None:\n            n_genes = min(top_n_markers, adata.uns[key]['names'].shape[0])\n            data_markers[group] = set(adata.uns[key]['names'][group][:n_genes])\n\n        elif adj_pval_threshold is not None:\n            n_genes = (adata.uns[key]['pvals_adj'][group] < adj_pval_threshold).sum()\n            data_markers[group] = set(adata.uns[key]['names'][group][:n_genes])\n\n            if n_genes == 0:\n                logg.warn('No marker genes passed the significance threshold of'\n                          ' {} for cluster {!r}.'.format(adj_pval_threshold, \n                                                        group))\n        else:\n            data_markers[group] = set(adata.uns[key]['names'][group])\n\n    # Find overlaps\n    if method == 'overlap_count':\n        marker_match = _calc_overlap_count(reference_markers, data_markers)\n\n        if normalize == 'reference':\n            # Ensure rows sum to 1\n            ref_lengths = np.array([len(reference_markers[m_group]) \n                                    for m_group in reference_markers])\n            marker_match = marker_match/ref_lengths[:,np.newaxis]\n            marker_match = np.nan_to_num(marker_match)\n\n        elif normalize == 'data':\n            #Ensure columns sum to 1\n            data_lengths = np.array([len(data_markers[dat_group]) \n                                     for dat_group in data_markers])\n            marker_match = marker_match/data_lengths\n            marker_match = np.nan_to_num(marker_match)\n            \n    elif method == 'overlap_coef':\n        marker_match = _calc_overlap_coef(reference_markers, data_markers)\n\n    elif method == 'jaccard':\n        marker_match = _calc_jaccard(reference_markers, data_markers)\n        \n    #Note:\n    # Could add an 'enrich' option here (fisher's exact test or hypergeometric \n    # test), but that would require knowledge of the size of the space from which\n    # the reference marker gene set was taken. This is at best approximately \n    # known. \n    \n    # Create a pandas dataframe with the results\n    marker_groups = list(reference_markers.keys())\n    clusters = list(cluster_ids)\n    marker_matching_df = pd.DataFrame(marker_match, index=marker_groups, \n                                      columns=clusters)\n\n    # Store the results\n    if inplace: \n        adata.uns[key_added] = marker_matching_df\n\n        logg.hint('added\\n'\n                  '    \\'{}\\', marker overlap scores (adata.uns)'\n                  .format(key_added))\n\n    else:\n        return marker_matching_df"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nranks genes according to contributions to PCs.", "response": "def pca_loadings(adata, components=None, show=None, save=None):\n    \"\"\"Rank genes according to contributions to PCs.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    components : str or list of integers, optional\n        For example, ``'1,2,3'`` means ``[1, 2, 3]``, first, second, third\n        principal component.\n    show : bool, optional (default: `None`)\n        Show the plot, do not return axis.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on {'.pdf', '.png', '.svg'}.\n    \"\"\"\n    if components is None: components = [1, 2, 3]\n    elif isinstance(components, str): components = components.split(',')\n    components = np.array(components) - 1\n    ranking(adata, 'varm', 'PCs', indices=components)\n    utils.savefig_or_show('pca_loadings', show=show, save=save)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pca_variance_ratio(adata, n_pcs=30, log=False, show=None, save=None):\n    ranking(adata, 'uns', 'variance_ratio', n_points=n_pcs, dictionary='pca', labels='PC', log=log)\n    utils.savefig_or_show('pca_variance_ratio', show=show, save=save)", "response": "Plot the variance ratio of the PCs in the n_pcs nearest points to the PCs in the data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dpt_timeseries(adata, color_map=None, show=None, save=None, as_heatmap=True):\n    if adata.n_vars > 100:\n        logg.warn('Plotting more than 100 genes might take some while,'\n                  'consider selecting only highly variable genes, for example.')\n    # only if number of genes is not too high\n    if as_heatmap:\n        # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d\n        timeseries_as_heatmap(adata.X[adata.obs['dpt_order_indices'].values],\n                              var_names=adata.var_names,\n                              highlightsX=adata.uns['dpt_changepoints'],\n                              color_map=color_map)\n    else:\n        # plot time series as gene expression vs time\n        timeseries(adata.X[adata.obs['dpt_order_indices'].values],\n                   var_names=adata.var_names,\n                   highlightsX=adata.uns['dpt_changepoints'],\n                   xlim=[0, 1.3*adata.X.shape[0]])\n    pl.xlabel('dpt order')\n    utils.savefig_or_show('dpt_timeseries', save=save, show=show)", "response": "Plot the time series of the pseudotime series."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot groups and pseudotime.", "response": "def dpt_groups_pseudotime(adata, color_map=None, palette=None, show=None, save=None):\n    \"\"\"Plot groups and pseudotime.\"\"\"\n    pl.figure()\n    pl.subplot(211)\n    timeseries_subplot(adata.obs['dpt_groups'].cat.codes,\n                       time=adata.obs['dpt_order'].values,\n                       color=np.asarray(adata.obs['dpt_groups']),\n                       highlightsX=adata.uns['dpt_changepoints'],\n                       ylabel='dpt groups',\n                       yticks=(np.arange(len(adata.obs['dpt_groups'].cat.categories), dtype=int)\n                                     if len(adata.obs['dpt_groups'].cat.categories) < 5 else None),\n                       palette=palette)\n    pl.subplot(212)\n    timeseries_subplot(adata.obs['dpt_pseudotime'].values,\n                       time=adata.obs['dpt_order'].values,\n                       color=adata.obs['dpt_pseudotime'].values,\n                       xlabel='dpt order',\n                       highlightsX=adata.uns['dpt_changepoints'],\n                       ylabel='pseudotime',\n                       yticks=[0, 1],\n                       color_map=color_map)\n    utils.savefig_or_show('dpt_groups_pseudotime', save=save, show=show)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _rank_genes_groups_plot(adata, plot_type='heatmap', groups=None,\n                            n_genes=10, groupby=None, key=None,\n                            show=None, save=None, **kwds):\n    \"\"\"\\\n    Plot ranking of genes using the specified plot type\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    groups : `str` or `list` of `str`\n        The groups for which to show the gene ranking.\n    n_genes : `int`, optional (default: 10)\n        Number of genes to show.\n    groupby : `str` or `None`, optional (default: `None`)\n        The key of the observation grouping to consider. By default,\n        the groupby is chosen from the rank genes groups parameter but\n        other groupby options can be used.\n    {show_save_ax}\n    \"\"\"\n    if key is None:\n        key = 'rank_genes_groups'\n\n    if 'dendrogram' not in kwds:\n        kwds['dendrogram'] = True\n    if groupby is None:\n        groupby = str(adata.uns[key]['params']['groupby'])\n    group_names = (adata.uns[key]['names'].dtype.names\n                   if groups is None else groups)\n\n    gene_names = []\n    start = 0\n    group_positions = []\n    group_names_valid = []\n    for group in group_names:\n        # get all genes that are 'not-nan'\n        genes_list = [gene for gene in adata.uns[key]['names'][group] if not pd.isnull(gene)][:n_genes]\n        if len(genes_list) == 0:\n            logg.warn(\"No genes found for group {}\".format(group))\n            continue\n        gene_names.extend(genes_list)\n        end = start + len(genes_list)\n        group_positions.append((start, end -1))\n        group_names_valid.append(group)\n        start = end\n\n    group_names = group_names_valid\n    if plot_type == 'dotplot':\n        from .._anndata import dotplot\n        dotplot(adata, gene_names, groupby, var_group_labels=group_names,\n                var_group_positions=group_positions, show=show, save=save, **kwds)\n\n    elif plot_type == 'heatmap':\n        from .._anndata import heatmap\n        heatmap(adata, gene_names, groupby, var_group_labels=group_names,\n                var_group_positions=group_positions, show=show, save=save, **kwds)\n\n    elif plot_type == 'stacked_violin':\n        from .._anndata import stacked_violin\n        return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,\n                       var_group_positions=group_positions, show=show, save=save, **kwds)\n\n    elif plot_type == 'tracksplot':\n        from .._anndata import tracksplot\n        return tracksplot(adata, gene_names, groupby, var_group_labels=group_names,\n                       var_group_positions=group_positions, show=show, save=save, **kwds)\n\n    elif plot_type == 'matrixplot':\n        from .._anndata import matrixplot\n        matrixplot(adata, gene_names, groupby, var_group_labels=group_names,\n                   var_group_positions=group_positions, show=show, save=save, **kwds)", "response": "Plot ranking of genes using the specified plot type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrank genes using heatmap plot.", "response": "def rank_genes_groups_heatmap(adata, groups=None, n_genes=10, groupby=None, key=None,\n                              show=None, save=None, **kwds):\n    \"\"\"\\\n    Plot ranking of genes using heatmap plot (see `scanpy.api.pl.heatmap`)\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    groups : `str` or `list` of `str`\n        The groups for which to show the gene ranking.\n    n_genes : `int`, optional (default: 10)\n        Number of genes to show.\n    groupby : `str` or `None`, optional (default: `None`)\n        The key of the observation grouping to consider. By default,\n        the groupby is chosen from the rank genes groups parameter but\n        other groupby options can be used.  It is expected that\n        groupby is a categorical. If groupby is not a categorical observation,\n        it would be subdivided into `num_categories` (see `scanpy.api.pl.heatmap`).\n    key : `str`\n        Key used to store the ranking results in `adata.uns`.\n    **kwds : keyword arguments\n        Are passed to `scanpy.api.pl.heatmap`.\n    {show_save_ax}\n    \"\"\"\n\n    _rank_genes_groups_plot(adata, plot_type='heatmap', groups=groups, n_genes=n_genes,\n                            groupby=groupby, key=key, show=show, save=save, **kwds)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots the results of simulation.", "response": "def sim(adata, tmax_realization=None, as_heatmap=False, shuffle=False,\n        show=None, save=None):\n    \"\"\"Plot results of simulation.\n\n    Parameters\n    ----------\n    as_heatmap : bool (default: False)\n        Plot the timeseries as heatmap.\n    tmax_realization : int or None (default: False)\n        Number of observations in one realization of the time series. The data matrix\n        adata.X consists in concatenated realizations.\n    shuffle : bool, optional (default: False)\n        Shuffle the data.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.\n    show : bool, optional (default: `None`)\n        Show the plot, do not return axis.\n    \"\"\"\n    from ... import utils as sc_utils\n    if tmax_realization is not None: tmax = tmax_realization\n    elif 'tmax_write' in adata.uns: tmax = adata.uns['tmax_write']\n    else: tmax = adata.n_obs\n    n_realizations = adata.n_obs/tmax\n    if not shuffle:\n        if not as_heatmap:\n            timeseries(adata.X,\n                       var_names=adata.var_names,\n                       xlim=[0, 1.25*adata.n_obs],\n                       highlightsX=np.arange(tmax, n_realizations*tmax, tmax),\n                       xlabel='realizations')\n        else:\n            # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d\n            timeseries_as_heatmap(adata.X,\n                                  var_names=adata.var_names,\n                                  highlightsX=np.arange(tmax, n_realizations*tmax, tmax))\n        pl.xticks(np.arange(0, n_realizations*tmax, tmax),\n                  np.arange(n_realizations).astype(int) + 1)\n        utils.savefig_or_show('sim', save=save, show=show)\n    else:\n        # shuffled data\n        X = adata.X\n        X, rows = sc_utils.subsample(X, seed=1)\n        timeseries(X,\n                   var_names=adata.var_names,\n                   xlim=[0, 1.25*adata.n_obs],\n                   highlightsX=np.arange(tmax, n_realizations*tmax, tmax),\n                   xlabel='index (arbitrary order)')\n        utils.savefig_or_show('sim_shuffled', save=save, show=show)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting the density of cells in an embedding.", "response": "def embedding_density(\n    adata: AnnData,\n    basis: str,\n    key: str,\n    *,\n    group: Optional[str] = None,\n    color_map: Union[Colormap, str] = 'YlOrRd',\n    bg_dotsize: Optional[int] = 80,\n    fg_dotsize:  Optional[int] = 180,\n    vmax:  Optional[int] = 1,\n    vmin:  Optional[int] = 0,\n    save: Union[bool, str, None] = None,\n    **kwargs\n):\n    \"\"\"Plot the density of cells in an embedding (per condition)\n\n    Plots the gaussian kernel density estimates (over condition) from the\n    `sc.tl.embedding_density()` output.\n\n    This function was written by Sophie Tritschler and implemented into\n    Scanpy by Malte Luecken.\n    \n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    basis\n        The embedding over which the density was calculated. This embedded\n        representation should be found in `adata.obsm['X_[basis]']``.\n    key\n        Name of the `.obs` covariate that contains the density estimates\n    group\n        The category in the categorical observation annotation to be plotted.\n        For example, 'G1' in the cell cycle 'phase' covariate.\n    color_map\n        Matplolib color map to use for density plotting.\n    bg_dotsize\n        Dot size for background data points not in the `group`.\n    fg_dotsize\n        Dot size for foreground data points in the `group`.\n    vmax\n        Density that corresponds to color bar maximum.\n    vmin\n        Density that corresponds to color bar minimum.\n    {show_save_ax}\n\n    Examples\n    --------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.umap(adata)\n    >>> sc.tl.embedding_density(adata, basis='umap', groupby='phase')\n    >>> sc.pl.embedding_density(adata, basis='umap', key='umap_density_phase', \n    ...                         group='G1')\n    >>> sc.pl.embedding_density(adata, basis='umap', key='umap_density_phase', \n    ...                         group='S')\n    \"\"\"\n    sanitize_anndata(adata)\n    \n    # Test user inputs\n    basis = basis.lower()\n\n    if basis == 'fa':\n        basis = 'draw_graph_fa'\n\n    if 'X_'+basis not in adata.obsm_keys():\n        raise ValueError('Cannot find the embedded representation `adata.obsm[X_{!r}]`. '\n                         'Compute the embedding first.'.format(basis))\n\n    if key not in adata.obs:\n        raise ValueError('Please run `sc.tl.embedding_density()` first and specify the correct key.')\n\n    if key+'_params' not in adata.uns:\n        raise ValueError('Please run `sc.tl.embedding_density()` first and specify the correct key.')\n\n    if 'components' in kwargs:\n        logg.warn('Components were specified, but will be ignored. Only the '\n                  'components used to calculate the density can be plotted.')\n        del kwargs['components']\n\n    components = adata.uns[key+'_params']['components']\n    groupby = adata.uns[key+'_params']['covariate']\n\n    if (group is None) and (groupby is not None):\n        raise ValueError('Densities were calculated over an `.obs` covariate. '\n                         'Please specify a group from this covariate to plot.')\n\n    if (group is not None) and (group not in adata.obs[groupby].cat.categories):\n        raise ValueError('Please specify a group from the `.obs` category over which the density '\n                         'was calculated.')\n\n    if (np.min(adata.obs[key]) < 0) or (np.max(adata.obs[key]) > 1):\n        raise ValueError('Densities should be scaled between 0 and 1.')\n    \n    # Define plotting data\n    dens_values = -np.ones(adata.n_obs)\n    dot_sizes = np.ones(adata.n_obs)*bg_dotsize\n\n    if group is not None:\n        group_mask = (adata.obs[groupby] == group)\n        dens_values[group_mask] = adata.obs[key][group_mask]\n        dot_sizes[group_mask] = np.ones(sum(group_mask))*fg_dotsize\n\n    else:\n        dens_values = adata.obs[key]\n        dot_sizes = np.ones(adata.n_obs)*fg_dotsize\n\n    # Make the color map\n    if isinstance(color_map, str):\n        cmap = cm.get_cmap(color_map)\n    else:\n        cmap = color_map\n\n    #norm = colors.Normalize(vmin=-1, vmax=1)\n    adata_vis = adata.copy()\n    adata_vis.obs['Density'] = dens_values\n\n    norm = colors.Normalize(vmin=vmin, vmax=vmax)\n    cmap.set_over('black')\n    cmap.set_under('lightgray')\n    \n    # Ensure title is blank as default\n    if 'title' not in kwargs:\n        title=\"\"\n    else:\n        title = kwargs.pop('title')\n\n    # Plot the graph\n    return plot_scatter(adata_vis, basis, components=components, color='Density',\n                        color_map=cmap, norm=norm, size=dot_sizes, vmax=vmax,\n                        vmin=vmin, save=save, title=title, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef palantir(adata: AnnData):\n\n    logg.info('Palantir diffusion maps', r=True)\n\n    class _wrapper_cls(object):\n        \"\"\"\n        A wrapper class to instantiate a new object that wraps `palantir` as an\n        attribute reference attached to the class, together with other attribute\n        references. The class uses instance variables, to preprocess and generate\n        data using the embedded palantir package.\n        Pre-processing of data is important step before start using the palantir\n        methods.\n        palantir accepts as input a Counts matrix: Cells x Genes.\n\n        Methods used are:\n            - instantiation initiation\n            - instance function to embed palantir\n            - pre-processing of input data\n        \"\"\"\n\n        def __init__(self ,\n                     adata,\n                     func=None ,\n                     normalize = False,\n                     log_transform = False,\n                     filter_low = False\n                    ):\n            \"\"\"\n            Parameters\n            ----------\n            adata : AnnData, or Dataframe of cells X genes\n            func : function wrapper to import palantir (not to be used)\n            normalize : `bool` (default: `False`)\n                property setter passed to palantir to normalize using palantir method\n                `palantir.preprocess.normalize_counts`.\n            log_transform : `bool` (default: `False`)\n                property setter passed to palantir. Some datasets show better signal in the log\n                scale. Applied using `palantir.preprocess.log_transform`\n            filter_low : `bool` (default: `False`)\n                property setter passed to palantir to remove low molecule count cells and low detection genes\n            \"\"\"\n\n            # instantiate variables\n            self.func = func\n            self.adata = adata\n            self._normalize = normalize\n            self._log_transform = log_transform\n            self._filter_low = filter_low\n\n            try:\n                # for AnnData\n                self.data_df = self.adata.to_df()\n            except AttributeError:\n                # assume the data is a cell X genes Dataframe\n                logg.info('Assuming the data is a cell X genes Dataframe',\n                \t      r=True)\n\n            # load palantir\n            self.__call__()\n            logg.info('palantir loaded ...', r=True)\n\n        def __call__(self):\n            \"\"\"\n            Call for function to import palantir and instantiate it as a class\n            attribute\n            \"\"\"\n            self.palantir = self.func()\n\n        def process(self):\n\n            \"\"\"\n            A method to run `palantir` on input Data Frame\n            \"\"\"\n\n            # Principal component analysis\n            logg.info('PCA in progress ...', r=True)\n\n            self.pca_projections, self.var_r = self.palantir.utils.run_pca(self.data_df)\n\n            adata.uns['palantir_pca_results'] = {}\n            adata.uns['palantir_pca_results']['pca_projections'] = self.pca_projections\n            adata.uns['palantir_pca_results']['variance_ratio'] = self.var_r\n\n            # Diffusion maps\n            logg.info('Diffusion maps in progress ...', r=True)\n\n            self.dm_res = self.palantir.utils.run_diffusion_maps(self.pca_projections)\n            self.ms_data = self.palantir.utils.determine_multiscale_space(self.dm_res)\n\n            adata.uns['palantir_diff_maps'] = self.dm_res\n            adata.uns['palantir_ms_data'] = self.ms_data\n\n            # tSNE visualization\n            logg.info('tSNE in progress ...', r=True)\n\n            self.tsne = self.palantir.utils.run_tsne(self.ms_data)\n\n            adata.uns['palantir_tsne'] = self.tsne\n\n            # MAGIC imputation\n            logg.info('imputation in progress ...', r=True)\n\n            self.imp_df = self.palantir.utils.run_magic_imputation(self.data_df, self.dm_res)\n\n            adata.uns['palantir_imp_df'] = self.imp_df\n\n            logg.info('End of processing, start plotting.', r=True)\n\n        @property\n        def normalize(self):\n            return self._normalize\n        @normalize.setter\n        def normalize(self , value):\n            if value is True:\n                self.data_df = self.palantir.preprocess.normalize_counts(self.data_df)\n                adata.uns['palantir_norm_data'] = self.data_df\n                logg.info('data normalized ...', r=True)\n\n        @property\n        def log_transform(self):\n            return self._log_transform\n        @log_transform.setter\n        def log_transform(self , value):\n            if value is True:\n                self.data_df = self.palantir.preprocess.log_transform(self.data_df)\n                adata.uns['palantir_norm_data'] = self.data_df\n                logg.info('data log transformed ...', r=True)\n\n        @property\n        def filter_low(self):\n            return self._filter_low\n        @filter_low.setter\n        def filter_low(self , value):\n            if value is True:\n                self.data_df = self.palantir.preprocess.filter_counts_data(self.data_df)\n                adata.uns['palantir_norm_data'] = self.data_df\n                logg.info('data filtered for low counts:\\n\\t' +\\\n                          'cell_min_molecules=1000\\n\\tgenes_min_cells=10',\n                          r=True)\n\n\n    def wrapper_cls(adata, func=None):\n        \"\"\"\n        Class wrapper to pass a function to the class alongside positional argument\n        \"\"\"\n        if func:\n            return _wrapper_cls(func)\n        else:\n            def wrapper(func):\n                return _wrapper_cls(adata, func)\n            return wrapper\n\n    # import palantir and wrap it in a function passed to the wrapper class\n    # this method allows passing positional argument of adata to `_wrapper_cls`\n    @wrapper_cls(adata)\n    def _run():\n        import importlib\n        try:\n            palantir = importlib.import_module('palantir')\n        except ImportError:\n            raise ImportError(\n                '\\nplease install palantir: \\n\\n\\t'\n                'git clone git://github.com/dpeerlab/Palantir.git\\n\\t'\n                'cd Palantir\\n\\t'\n                'sudo -H pip3 install .\\n')\n        return palantir\n    return _run", "response": "Run Diffusion maps using the adaptive anisotropic kernel."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexporting to a SPRING project.", "response": "def spring_project(\n        adata, project_dir, embedding_method,\n        subplot_name = None,\n        cell_groupings=None, custom_color_tracks=None,\n        total_counts_key = 'n_counts', overwrite = False\n        ):\n    \"\"\"Exports to a SPRING project directory [Weinreb17]_.\n\n    Visualize annotation present in `adata`. By default, export all gene expression data\n    from `adata.raw` and categorical and continuous annotations present in `adata.obs`.\n\n    See `SPRING <https://github.com/AllonKleinLab/SPRING>`__ or [Weinreb17]_ for details.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix: `adata.uns['neighbors']` needs to\n        be present.\n    project_dir : `str`\n        Path to directory for exported SPRING files.\n    embedding_method: `str`\n        Name of a 2-D embedding in `adata.obsm`\n    subplot_name: `str`, optional (default: `None`)\n        Name of subplot folder to be created at `project_dir+\"/\"+subplot_name`\n    cell_groupings : `str`, `list` of `str`, optional (default: `None`)\n        Instead of importing all categorical annotations when `None`,\n        pass a list of keys for `adata.obs`.\n    custom_color_tracks : `str`, `list` of `str`, optional (default: `None`)\n        Specify specific `adata.obs` keys for continuous coloring.\n    total_counts_key: `str`, optional (default: \"n_counts\")\n        Name of key for total transcript counts in `adata.obs`.\n    overwrite: `boolean`, optional (default: `False`)\n        When `True`, existing counts matrices in `project_dir` are overwritten.\n\n    Examples\n    --------\n    See this `tutorial <https://github.com/theislab/scanpy_usage/tree/master/171111_SPRING_export>`__.\n    \"\"\"\n\n    # need to get nearest neighbors first\n    if 'neighbors' not in adata.uns:\n        raise ValueError('Run `sc.pp.neighbors` first.')\n\n    # check that requested 2-D embedding has been generated\n    if embedding_method not in adata.obsm_keys():\n        if 'X_' + embedding_method in adata.obsm_keys():\n            embedding_method = 'X_' + embedding_method\n        else:\n            if embedding_method in adata.uns:\n                embedding_method = 'X_' + embedding_method + '_' + adata.uns[embedding_method]['params']['layout']\n            else:\n                raise ValueError('Run the specified embedding method `%s` first.' %embedding_method)\n\n    coords = adata.obsm[embedding_method]\n\n    # Make project directory and subplot directory (subplot has same name as project)\n    # For now, the subplot is just all cells in adata\n    project_dir = project_dir.rstrip('/') + '/'\n    if subplot_name is None:\n        subplot_dir = project_dir + project_dir.split('/')[-2] + '/'\n    else:\n        subplot_dir = project_dir + subplot_name + '/'\n    if not os.path.exists(subplot_dir):\n        os.makedirs(subplot_dir)\n    print(\"Writing subplot to %s\" %subplot_dir)\n\n    # Write counts matrices as hdf5 files and npz if they do not already exist\n    # or if user requires overwrite.\n    # To do: check if Alex's h5sparse format will allow fast loading from just\n    # one file.\n    write_counts_matrices = True\n    base_dir_filelist = ['counts_norm_sparse_genes.hdf5', 'counts_norm_sparse_cells.hdf5',\n                         'counts_norm.npz', 'total_counts.txt', 'genes.txt']\n    if all([os.path.isfile(project_dir + f) for f in base_dir_filelist]):\n        if not overwrite:\n            logg.warning('%s is an existing SPRING folder. A new subplot will be created, but '\n                         'you must set `overwrite=True` to overwrite counts matrices.' %project_dir)\n            write_counts_matrices = False\n        else:\n            logg.warning('Overwriting the files in %s.' %project_dir)\n\n    # Ideally, all genes will be written from adata.raw\n    if adata.raw is not None:\n        E = adata.raw.X.tocsc()\n        gene_list = list(adata.raw.var_names)\n    else:\n        E = adata.X.tocsc()\n        gene_list = list(adata.var_names)\n\n    # Keep track of total counts per cell if present\n    if total_counts_key in adata.obs:\n        total_counts = np.array(adata.obs[total_counts_key])\n    else:\n        total_counts = E.sum(1).A1\n\n    # Write the counts matrices to project directory\n    if write_counts_matrices:\n        write_hdf5_genes(E, gene_list, project_dir + 'counts_norm_sparse_genes.hdf5')\n        write_hdf5_cells(E, project_dir + 'counts_norm_sparse_cells.hdf5')\n        write_sparse_npz(E, project_dir + 'counts_norm.npz')\n        with open(project_dir + 'genes.txt', 'w') as o:\n            for g in gene_list:\n                o.write(g + '\\n')\n        np.savetxt(project_dir + 'total_counts.txt', total_counts)\n\n    # Get categorical and continuous metadata\n    categorical_extras = {}\n    continuous_extras = {}\n    if cell_groupings is None:\n        for obs_name in adata.obs:\n            if is_categorical(adata.obs[obs_name]):\n                categorical_extras[obs_name] = [str(x) for x in adata.obs[obs_name]]\n    else:\n        if isinstance(cell_groupings, str):\n            cell_groupings = [cell_groupings]\n        for obs_name in cell_groupings:\n            if obs_name not in adata.obs:\n                logg.warning('Cell grouping \"%s\" is not in adata.obs' %obs_name)\n            elif is_categorical(adata.obs[obs_name]):\n                categorical_extras[obs_name] = [str(x) for x in adata.obs[obs_name]]\n            else:\n                logg.warning('Cell grouping \"%s\" is not a categorical variable' %obs_name)\n    if custom_color_tracks is None:\n        for obs_name in adata.obs:\n            if not is_categorical(adata.obs[obs_name]):\n                continuous_extras[obs_name] = np.array(adata.obs[obs_name])\n    else:\n        if isinstance(custom_color_tracks, str):\n            custom_color_tracks = [custom_color_tracks]\n        for obs_name in custom_color_tracks:\n            if obs_name not in adata.obs:\n                logg.warning('Custom color track \"%s\" is not in adata.obs' %obs_name)\n            elif not is_categorical(adata.obs[obs_name]):\n                continuous_extras[obs_name] = np.array(adata.obs[obs_name])\n            else:\n                logg.warning('Custom color track \"%s\" is not a continuous variable' %obs_name)\n\n\n    # Write continuous colors\n    continuous_extras['Uniform'] = np.zeros(E.shape[0])\n    write_color_tracks(continuous_extras, subplot_dir+'color_data_gene_sets.csv')\n\n    # Create and write a dictionary of color profiles to be used by the visualizer\n    color_stats = {}\n    color_stats = get_color_stats_genes(color_stats, E, gene_list)\n    color_stats = get_color_stats_custom(color_stats, continuous_extras)\n    write_color_stats(subplot_dir + 'color_stats.json', color_stats)\n\n    # Write categorical data\n    categorical_coloring_data = {}\n    categorical_coloring_data = build_categ_colors(categorical_coloring_data, categorical_extras)\n    write_cell_groupings(subplot_dir+'categorical_coloring_data.json', categorical_coloring_data)\n\n    # Write graph in two formats for backwards compatibility\n    edges = get_edges(adata)\n    write_graph(subplot_dir + 'graph_data.json', E.shape[0], edges)\n    write_edges(subplot_dir + 'edges.csv', edges)\n\n\n    # Write cell filter; for now, subplots must be generated from within SPRING,\n    # so cell filter includes all cells.\n    np.savetxt(subplot_dir + 'cell_filter.txt', np.arange(E.shape[0]), fmt='%i')\n    np.save(subplot_dir + 'cell_filter.npy', np.arange(E.shape[0]))\n\n    # Write 2-D coordinates, after adjusting to roughly match SPRING's default d3js force layout parameters\n    coords = coords - coords.min(0)[None,:]\n    coords = coords * (np.array([1000, 1000]) / coords.ptp(0))[None,:] + np.array([200,-200])[None,:]\n    np.savetxt(subplot_dir + 'coordinates.txt',\n               np.hstack((np.arange(E.shape[0])[:,None], coords)),\n               fmt='%i,%.6f,%.6f')\n\n    # Write some useful intermediates, if they exist\n    if 'X_pca' in adata.obsm_keys():\n        np.savez_compressed(subplot_dir + 'intermediates.npz',\n                            Epca=adata.obsm['X_pca'],\n                            total_counts = total_counts)\n\n    # Write PAGA data, if present\n    if 'paga' in adata.uns:\n        clusts = np.array(adata.obs[adata.uns['paga']['groups']].cat.codes)\n        uniq_clusts = adata.obs[adata.uns['paga']['groups']].cat.categories\n        paga_coords = [coords[clusts==i,:].mean(0) for i in range(len(uniq_clusts))]\n        export_PAGA_to_SPRING(adata, paga_coords, subplot_dir + 'PAGA_data.json')\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting a list of genes to an HDF5 file.", "response": "def write_hdf5_genes(E, gene_list, filename):\n    '''SPRING standard: filename = main_spring_dir + \"counts_norm_sparse_genes.hdf5\"'''\n\n    E = E.tocsc()\n\n    hf = h5py.File(filename, 'w')\n    counts_group = hf.create_group('counts')\n    cix_group = hf.create_group('cell_ix')\n\n    hf.attrs['ncells'] = E.shape[0]\n    hf.attrs['ngenes'] = E.shape[1]\n\n    for iG, g in enumerate(gene_list):\n        counts = E[:,iG].A.squeeze()\n        cell_ix = np.nonzero(counts)[0]\n        counts = counts[cell_ix]\n        counts_group.create_dataset(g, data = counts)\n        cix_group.create_dataset(g, data = cell_ix)\n\n    hf.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites the sparse cells to an HDF5 file.", "response": "def write_hdf5_cells(E, filename):\n    '''SPRING standard: filename = main_spring_dir + \"counts_norm_sparse_cells.hdf5\" '''\n\n    E = E.tocsr()\n\n    hf = h5py.File(filename, 'w')\n    counts_group = hf.create_group('counts')\n    gix_group = hf.create_group('gene_ix')\n\n    hf.attrs['ncells'] = E.shape[0]\n    hf.attrs['ngenes'] = E.shape[1]\n\n    for iC in range(E.shape[0]):\n        counts = E[iC,:].A.squeeze()\n        gene_ix = np.nonzero(counts)[0]\n        counts = counts[gene_ix]\n        counts_group.create_dataset(str(iC), data = counts)\n        gix_group.create_dataset(str(iC), data = gene_ix)\n\n    hf.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite a sparse NPY file.", "response": "def write_sparse_npz(E, filename, compressed = False):\n    ''' SPRING standard: filename = main_spring_dir + \"/counts_norm.npz\"'''\n    E = E.tocsc()\n    scipy.sparse.save_npz(filename, E, compressed = compressed)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexports the given adata to a UCSC Cell Browser project directory.", "response": "def cellbrowser(\n    adata, data_dir, data_name,\n    embedding_keys = None,\n    annot_keys = [\"louvain\", \"percent_mito\", \"n_genes\", \"n_counts\"],\n    cluster_field = \"louvain\",\n    nb_marker = 50,\n    skip_matrix = False,\n    html_dir = None,\n    port = None,\n    do_debug = False\n):\n    \"\"\"\n    Export adata to a UCSC Cell Browser project directory. If `html_dir` is\n    set, subsequently build the html files from the project directory into\n    `html_dir`.  If `port` is set, start an HTTP server in the background and\n    serve `html_dir` on `port`.\n\n    By default, export all gene expression data from `adata.raw`, the\n    annotations `louvain`, `percent_mito`, `n_genes` and `n_counts` and the top\n    `nb_marker` cluster markers. All existing files in data_dir are\n    overwritten, except cellbrowser.conf.\n\n    See `UCSC Cellbrowser <https://github.com/maximilianh/cellBrowser>`__ for\n    details.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix\n    data_dir : `str`\n        Path to directory for exported Cell Browser files.\n        Usually these are the files `exprMatrix.tsv.gz`, `meta.tsv`,\n        coordinate files like `tsne.coords.tsv`,\n        and cluster marker gene lists like `markers.tsv`.\n        A file `cellbrowser.conf` is also created with pointers to these files.\n        As a result, each adata object should have its own project_dir.\n    data_name : `str`\n        Name of dataset in Cell Browser, a string without special characters.\n        This is written to `data_dir`/cellbrowser.conf.\n        Ideally this is a short unique name for the dataset,\n        like \"pbmc3k\" or \"tabulamuris\".\n    embedding_keys: `list` of `str` or `dict` of `key (str)`->`display label (str)`\n        2-D embeddings in `adata.obsm` to export.\n        The prefix \"`X_`\" or \"`X_draw_graph_`\" is not necessary.\n        Coordinates missing from `adata` are skipped.\n        By default, these keys are tried: [\"tsne\", \"umap\", \"pagaFa\", \"pagaFr\",\n        \"pagaUmap\", \"phate\", \"fa\", \"fr\", \"kk\", \"drl\", \"rt\"].\n        For these, default display labels are automatically used.\n        For other values, you can specify a dictionary instead of a list,\n        the values of the dictionary are then the display labels for the\n        coordinates, e.g. `{'tsne' : \"t-SNE by Scanpy\"}`\n    annot_keys: `list` of `str` or `dict` of `key (str)`->`display label (str)`\n        Annotations in `adata.obsm` to export.\n        Can be a dictionary with key -> display label.\n    skip_matrix: `boolean`\n        Do not export the matrix.\n        If you had previously exported this adata into the same `data_dir`,\n        then there is no need to export the whole matrix again.\n        This option will make the export a lot faster,\n        e.g. when only coordinates or meta data were changed.\n    html_dir: `str`\n        If this variable is set, the export will build html\n        files from `data_dir` to `html_dir`, creating html/js/json files.\n        Usually there is one global html output directory for all datasets.\n        Often, `html_dir` is located under a webserver's (like Apache)\n        htdocs directory or is copied to one.\n        A directory `html_dir`/`project_name` will be created and\n        an index.html will be created under `html_dir` for all subdirectories.\n        Existing files will be overwritten.\n        If do not to use html_dir,\n        you can use the command line tool `cbBuild` to build the html directory.\n    port: `int`\n        If this variable and `html_dir` are set,\n        Python's built-in web server will be spawned as a daemon in the\n        background and serve the files under `html_dir`.\n        To kill the process, call `cellbrowser.cellbrowser.stop()`.\n    do_debug: `boolean`\n        Activate debugging output\n\n    Examples\n    --------\n    See this\n    `tutorial <https://github.com/theislab/scanpy_usage/tree/master/181126_Cellbrowser_exports>`__.\n    \"\"\"\n\n    try:\n        import cellbrowser.cellbrowser as cb\n    except ImportError:\n        print(\"The package cellbrowser is not installed. Install with 'pip \"\n                \"install cellbrowser' and retry.\")\n\n    cb.setDebug(do_debug)\n\n    cb.scanpyToCellbrowser(adata, data_dir, data_name,\n            coordFields=embedding_keys,\n            metaFields=annot_keys,\n            clusterField=cluster_field,\n            nb_marker=nb_marker,\n            skipMatrix=skip_matrix,\n            doDebug = None\n    )\n\n    if html_dir:\n        cb.build(data_dir, html_dir, doDebug=None)\n    if port:\n        cb.serve(html_dir, port)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef draw_graph(\n        adata,\n        layout='fa',\n        init_pos=None,\n        root=None,\n        random_state=0,\n        n_jobs=None,\n        adjacency=None,\n        key_added_ext=None,\n        copy=False,\n        **kwds):\n    \"\"\"Force-directed graph drawing [Islam11]_ [Jacomy14]_ [Chippada18]_.\n\n    An alternative to tSNE that often preserves the topology of the data\n    better. This requires to run :func:`~scanpy.api.pp.neighbors`, first.\n\n    The default layout ('fa', `ForceAtlas2`) [Jacomy14]_ uses the package `fa2\n    <https://github.com/bhargavchippada/forceatlas2>`__ [Chippada18]_, which can\n    be installed via `pip install fa2`.\n\n    `Force-directed graph drawing\n    <https://en.wikipedia.org/wiki/Force-directed_graph_drawing>`__ describes a\n    class of long-established algorithms for visualizing graphs. It has been\n    suggested for visualizing single-cell data by [Islam11]_. Many other layouts\n    as implemented in igraph [Csardi06]_ are available. Similar approaches have\n    been used by [Zunder15]_ or [Weinreb17]_.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    layout : `str`, optional (default: 'fa')\n        'fa' (`ForceAtlas2`) or any valid `igraph layout\n        <http://igraph.org/c/doc/igraph-Layout.html>`__. Of particular interest\n        are 'fr' (Fruchterman Reingold), 'grid_fr' (Grid Fruchterman Reingold,\n        faster than 'fr'), 'kk' (Kamadi Kawai', slower than 'fr'), 'lgl' (Large\n        Graph, very fast), 'drl' (Distributed Recursive Layout, pretty fast) and\n        'rt' (Reingold Tilford tree layout).\n    root : `int` or `None`, optional (default: `None`)\n        Root for tree layouts.\n    random_state : `int` or `None`, optional (default: 0)\n        For layouts with random initialization like 'fr', change this to use\n        different intial states for the optimization. If `None`, no seed is set.\n    adjacency : sparse matrix or `None`, optional (default: `None`)\n        Sparse adjacency matrix of the graph, defaults to\n        `adata.uns['neighbors']['connectivities']`.\n    key_ext : `str`, optional (default: `None`)\n        By default, append `layout`.\n    proceed : `bool`, optional (default: `None`)\n        Continue computation, starting off with 'X_draw_graph_`layout`'.\n    init_pos : {'paga', any valid 2d-`.obsm` key, `False`}, optional (default: `False`)\n        Use precomputed coordinates for initialization.\n    copy : `bool` (default: `False`)\n        Return a copy instead of writing to adata.\n    **kwds : further parameters\n        Parameters of chosen igraph layout. See, e.g.,\n        `fruchterman_reingold <http://igraph.org/python/doc/igraph.Graph-class.html#layout_fruchterman_reingold>`__. One of the most important ones is `maxiter`.\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following field.\n\n    **X_draw_graph_layout** : `adata.obsm`\n        Coordinates of graph layout. E.g. for layout='fa' (the default), the field is called 'X_draw_graph_fa'\n    \"\"\"\n    logg.info('drawing single-cell graph using layout \"{}\"'.format(layout),\n              r=True)\n    avail_layouts = {'fr', 'drl', 'kk', 'grid_fr', 'lgl', 'rt', 'rt_circular', 'fa'}\n    if layout not in avail_layouts:\n        raise ValueError('Provide a valid layout, one of {}.'.format(avail_layouts))\n    adata = adata.copy() if copy else adata\n    if adjacency is None and 'neighbors' not in adata.uns:\n        raise ValueError(\n            'You need to run `pp.neighbors` first to compute a neighborhood graph.')\n    if adjacency is None:\n        adjacency = adata.uns['neighbors']['connectivities']\n    key_added = 'X_draw_graph_' + (layout if key_added_ext is None else key_added_ext)\n    # init coordinates\n    if init_pos in adata.obsm.keys():\n        init_coords = adata.obsm[init_pos]\n    elif init_pos == 'paga':\n        init_coords = get_init_pos_from_paga(adata, adjacency, random_state=random_state)\n    else:\n        np.random.seed(random_state)\n        init_coords = np.random.random((adjacency.shape[0], 2))\n    # see whether fa2 is installed\n    if layout == 'fa':\n        try:\n            from fa2 import ForceAtlas2\n        except:\n            logg.warn('Package \\'fa2\\' is not installed, falling back to layout \\'fr\\'.'\n                      'To use the faster and better ForceAtlas2 layout, '\n                      'install package \\'fa2\\' (`pip install fa2`).')\n            layout = 'fr'\n    # actual drawing\n    if layout == 'fa':\n        forceatlas2 = ForceAtlas2(\n            # Behavior alternatives\n            outboundAttractionDistribution=False,  # Dissuade hubs\n            linLogMode=False,  # NOT IMPLEMENTED\n            adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n            edgeWeightInfluence=1.0,\n            # Performance\n            jitterTolerance=1.0,  # Tolerance\n            barnesHutOptimize=True,\n            barnesHutTheta=1.2,\n            multiThreaded=False,  # NOT IMPLEMENTED\n            # Tuning\n            scalingRatio=2.0,\n            strongGravityMode=False,\n            gravity=1.0,\n            # Log\n            verbose=False)\n        if 'maxiter' in kwds:\n            iterations = kwds['maxiter']\n        elif 'iterations' in kwds:\n            iterations = kwds['iterations']\n        else:\n            iterations = 500\n        positions = forceatlas2.forceatlas2(\n            adjacency, pos=init_coords, iterations=iterations)\n        positions = np.array(positions)\n    else:\n        g = utils.get_igraph_from_adjacency(adjacency)\n        if layout in {'fr', 'drl', 'kk', 'grid_fr'}:\n            ig_layout = g.layout(layout, seed=init_coords.tolist(), **kwds)\n        elif 'rt' in layout:\n            if root is not None: root = [root]\n            ig_layout = g.layout(layout, root=root, **kwds)\n        else:\n            ig_layout = g.layout(layout, **kwds)\n        positions = np.array(ig_layout.coords)\n    adata.uns['draw_graph'] = {}\n    adata.uns['draw_graph']['params'] = {'layout': layout, 'random_state': random_state}\n    key_added = 'X_draw_graph_' + (layout if key_added_ext is None else key_added_ext)\n    adata.obsm[key_added] = positions\n    logg.info('    finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\\n')\n    logg.hint('added\\n'\n              '    \\'{}\\', graph_drawing coordinates (adata.obsm)'\n              .format(key_added))\n    return adata if copy else None", "response": "Draw a graph from the given data matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the figure parameters for a specific entry in the current object.", "response": "def set_figure_params(\n        self,\n        scanpy=True,\n        dpi=80,\n        dpi_save=150,\n        frameon=True,\n        vector_friendly=True,\n        fontsize=14,\n        color_map=None,\n        format=\"pdf\",\n        transparent=False,\n        ipython_format=\"png2x\",\n    ):\n        \"\"\"Set resolution/size, styling and format of figures.\n\n        Parameters\n        ----------\n        scanpy : `bool`, optional (default: `True`)\n            Init default values for ``matplotlib.rcParams`` suited for Scanpy.\n        dpi : `int`, optional (default: `80`)\n            Resolution of rendered figures - this influences the size of figures in notebooks.\n        dpi_save : `int`, optional (default: `150`)\n            Resolution of saved figures. This should typically be higher to achieve\n            publication quality.\n        frameon : `bool`, optional (default: `True`)\n            Add frames and axes labels to scatter plots.\n        vector_friendly : `bool`, optional (default: `True`)\n            Plot scatter plots using `png` backend even when exporting as `pdf` or `svg`.\n        fontsize : `int`, optional (default: 14)\n            Set the fontsize for several `rcParams` entries. Ignored if `scanpy=False`.\n        color_map : `str`, optional (default: `None`)\n            Convenience method for setting the default color map. Ignored if `scanpy=False`.\n        format : {'png', 'pdf', 'svg', etc.}, optional (default: 'pdf')\n            This sets the default format for saving figures: `file_format_figs`.\n        transparent : `bool`, optional (default: `True`)\n            Save figures with transparent back ground. Sets\n            `rcParams['savefig.transparent']`.\n        ipython_format : list of `str`, optional (default: 'png2x')\n            Only concerns the notebook/IPython environment; see\n            `IPython.core.display.set_matplotlib_formats` for more details.\n        \"\"\"\n        try:\n            import IPython\n            IPython.core.display.set_matplotlib_formats(ipython_format)\n        except:\n            pass\n        from matplotlib import rcParams\n        self._vector_friendly = vector_friendly\n        self.file_format_figs = format\n        if dpi is not None:\n            rcParams[\"figure.dpi\"] = dpi\n        if dpi_save is not None:\n            rcParams[\"savefig.dpi\"] = dpi_save\n        if transparent is not None:\n            rcParams[\"savefig.transparent\"] = transparent\n        if scanpy:\n            from .plotting._rcmod import set_rcParams_scanpy\n            set_rcParams_scanpy(fontsize=fontsize, color_map=color_map)\n        self._frameon = frameon"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef umap(adata, **kwargs) -> Union[Axes, List[Axes], None]:\n    return plot_scatter(adata, 'umap', **kwargs)", "response": "Plots a scatter plot in UMAP basis."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot a scatter plot in tSNE basis.", "response": "def tsne(adata, **kwargs) -> Union[Axes, List[Axes], None]:\n    \"\"\"\\\n    Scatter plot in tSNE basis.\n\n    Parameters\n    ----------\n    {adata_color_etc}\n    {edges_arrows}\n    {scatter_bulk}\n    {show_save_ax}\n\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n    \"\"\"\n    return plot_scatter(adata, 'tsne', **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef diffmap(adata, **kwargs) -> Union[Axes, List[Axes], None]:\n    return plot_scatter(adata, 'diffmap', **kwargs)", "response": "Plots a diffusion map plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndraws a graph in the graph - drawing basis.", "response": "def draw_graph(adata, layout=None, **kwargs) -> Union[Axes, List[Axes], None]:\n    \"\"\"\\\n    Scatter plot in graph-drawing basis.\n\n    Parameters\n    ----------\n    {adata_color_etc}\n    layout : {{'fa', 'fr', 'drl', ...}}, optional (default: last computed)\n        One of the `draw_graph` layouts, see\n        :func:`~scanpy.api.tl.draw_graph`. By default, the last computed layout\n        is used.\n    {edges_arrows}\n    {scatter_bulk}\n    {show_save_ax}\n\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n    \"\"\"\n    if layout is None:\n        layout = str(adata.uns['draw_graph']['params']['layout'])\n    basis = 'draw_graph_' + layout\n    if 'X_' + basis not in adata.obsm_keys():\n        raise ValueError('Did not find {} in adata.obs. Did you compute layout {}?'\n                         .format('draw_graph_' + layout, layout))\n\n    return plot_scatter(adata, basis, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pca(adata, **kwargs) -> Union[Axes, List[Axes], None]:\n    return plot_scatter(adata, 'pca', **kwargs)", "response": "Plots a scatter plot in PCA coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the data points corresponding to the selected basis and components.", "response": "def _get_data_points(adata, basis, projection, components) -> Tuple[List[np.ndarray], List[Tuple[int, int]]]:\n    \"\"\"\n    Returns the data points corresponding to the selected basis, projection and/or components.\n\n    Because multiple components are given (eg components=['1,2', '2,3'] the\n    returned data are lists, containing each of the components. When only one component is plotted\n    the list length is 1.\n\n    Returns\n    -------\n    data_points : list\n        Each entry is a numpy array containing the data points\n    components : list\n        The cleaned list of components. Eg. [(0,1)] or [(0,1), (1,2)]\n        for components = [1,2] and components=['1,2', '2,3'] respectively\n    \"\"\"\n    n_dims = 2\n    if projection == '3d':\n        # check if the data has a third dimension\n        if adata.obsm['X_' + basis].shape[1] == 2:\n            if settings._low_resolution_warning:\n                logg.warn('Selected projections is \"3d\" but only two dimensions '\n                          'are available. Only these two dimensions will be plotted')\n        else:\n            n_dims = 3\n\n    if components == 'all':\n        components = ['{},{}'.format(*((i, i+1) if i % 2 == 1 else (i+1, i)))\n            for i in range(1, adata.obsm['X_{}'.format(basis)].shape[1])]\n\n    components_list = []\n    offset = 0\n    if basis == 'diffmap': offset = 1\n    if components is not None:\n        # components have different formats, either a list with integers, a string\n        # or a list of strings.\n\n        if isinstance(components, str):\n            # eg: components='1,2'\n            components_list.append(tuple(int(x.strip()) - 1 + offset for x in components.split(',')))\n\n        elif isinstance(components, abc.Sequence):\n            if isinstance(components[0], int):\n                # components=[1,2]\n                components_list.append(tuple(int(x) - 1 + offset for x in components))\n            else:\n                # in this case, the components are str\n                # eg: components=['1,2'] or components=['1,2', '2,3]\n                # More than one component can be given and is stored\n                # as a new item of components_list\n                for comp in components:\n                    components_list.append(tuple(int(x.strip()) - 1 + offset for x in comp.split(',')))\n\n        else:\n            raise ValueError(\"Given components: '{}' are not valid. Please check. \"\n                             \"A valid example is `components='2,3'`\")\n        # check if the components are present in the data\n        try:\n            data_points = []\n            for comp in components_list:\n                data_points.append(adata.obsm['X_' + basis][:, comp])\n        except:\n            raise ValueError(\"Given components: '{}' are not valid. Please check. \"\n                             \"A valid example is `components='2,3'`\")\n\n        if basis == 'diffmap':\n            # remove the offset added in the case of diffmap, such that\n            # plot_scatter can print the labels correctly.\n            components_list = [tuple(number-1 for number in comp) for comp in components_list]\n    else:\n        data_points = [adata.obsm['X_' + basis][:, offset:offset+n_dims]]\n        components_list = []\n    return data_points, components_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_legend_or_colorbar(adata, ax, cax, categorical, value_to_plot, legend_loc,\n                            scatter_array, legend_fontweight, legend_fontsize,\n                            groups, multi_panel):\n    \"\"\"\n    Adds a color bar or a legend to the given ax. A legend is added when the\n    data is categorical and a color bar is added when a continuous value was used.\n\n    \"\"\"\n    # add legends or colorbars\n    if categorical is True:\n        # add legend to figure\n        categories = list(adata.obs[value_to_plot].cat.categories)\n        colors = adata.uns[value_to_plot + '_colors']\n\n        if multi_panel is True:\n            # Shrink current axis by 10% to fit legend and match\n            # size of plots that are not categorical\n            box = ax.get_position()\n            ax.set_position([box.x0, box.y0, box.width * 0.91, box.height])\n\n        if groups is not None:\n            # only label groups with the respective color\n            colors = [colors[categories.index(x)] for x in groups]\n            categories = groups\n\n        if legend_loc == 'right margin':\n            for idx, label in enumerate(categories):\n                color = colors[idx]\n                # use empty scatter to set labels\n                ax.scatter([], [], c=color, label=label)\n            ax.legend(\n                frameon=False, loc='center left',\n                bbox_to_anchor=(1, 0.5),\n                ncol=(1 if len(categories) <= 14\n                      else 2 if len(categories) <= 30 else 3),\n                fontsize=legend_fontsize)\n\n        if legend_loc == 'on data':\n            # identify centroids to put labels\n            all_pos = np.zeros((len(categories), 2))\n            for ilabel, label in enumerate(categories):\n                _scatter = scatter_array[adata.obs[value_to_plot] == label, :]\n                x_pos, y_pos = np.median(_scatter, axis=0)\n\n                ax.text(x_pos, y_pos, label,\n                        weight=legend_fontweight,\n                        verticalalignment='center',\n                        horizontalalignment='center',\n                        fontsize=legend_fontsize)\n\n                all_pos[ilabel] = [x_pos, y_pos]\n            # this is temporary storage for access by other tools\n            utils._tmp_cluster_pos = all_pos\n    else:\n        # add colorbar to figure\n        pl.colorbar(cax, ax=ax, pad=0.01, fraction=0.08, aspect=30)", "response": "Add a legend or color bar to the given ax."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the colors for a categorical observation.", "response": "def _set_colors_for_categorical_obs(adata, value_to_plot, palette):\n    \"\"\"\n    Sets the adata.uns[value_to_plot + '_colors'] according to the given palette\n\n    Parameters\n    ----------\n    adata\n        annData object\n    value_to_plot\n        name of a valid categorical observation\n    palette\n        Palette should be either a valid :func:`~matplotlib.pyplot.colormaps` string,\n        a list of colors (in a format that can be understood by matplotlib,\n        eg. RGB, RGBS, hex, or a cycler object with key='color'\n\n    Returns\n    -------\n    None\n    \"\"\"\n    from matplotlib.colors import to_hex\n    from cycler import Cycler, cycler\n\n    categories = adata.obs[value_to_plot].cat.categories\n    # check is palette is a valid matplotlib colormap\n    if isinstance(palette, str) and palette in pl.colormaps():\n        # this creates a palette from a colormap. E.g. 'Accent, Dark2, tab20'\n        cmap = pl.get_cmap(palette)\n        colors_list = [to_hex(x) for x in cmap(np.linspace(0, 1, len(categories)))]\n\n    else:\n        # check if palette is a list and convert it to a cycler, thus\n        # it doesnt matter if the list is shorter than the categories length:\n        if isinstance(palette, abc.Sequence):\n            if len(palette) < len(categories):\n                logg.warn(\"Length of palette colors is smaller than the number of \"\n                          \"categories (palette length: {}, categories length: {}. \"\n                          \"Some categories will have the same color.\"\n                          .format(len(palette), len(categories)))\n            # check that colors are valid\n            _color_list = []\n            for color in palette:\n                if not is_color_like(color):\n                    # check if the color is a valid R color and translate it\n                    # to a valid hex color value\n                    if color in utils.additional_colors:\n                        color = utils.additional_colors[color]\n                    else:\n                        raise ValueError(\"The following color value of the given palette is not valid: {}\".format(color))\n                _color_list.append(color)\n\n            palette = cycler(color=_color_list)\n        if not isinstance(palette, Cycler):\n            raise ValueError(\"Please check that the value of 'palette' is a \"\n                             \"valid matplotlib colormap string (eg. Set2), a \"\n                             \"list of color names or a cycler with a 'color' key.\")\n        if 'color' not in palette.keys:\n            raise ValueError(\"Please set the palette key 'color'.\")\n\n        cc = palette()\n        colors_list = [to_hex(next(cc)['color']) for x in range(len(categories))]\n\n    adata.uns[value_to_plot + '_colors'] = colors_list"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the adata. uns [ value_to_plot + _colors ] using default color palettes.", "response": "def _set_default_colors_for_categorical_obs(adata, value_to_plot):\n    \"\"\"\n    Sets the adata.uns[value_to_plot + '_colors'] using default color palettes\n\n    Parameters\n    ----------\n    adata : annData object\n    value_to_plot : name of a valid categorical observation\n\n    Returns\n    -------\n    None\n    \"\"\"\n    from .. import palettes\n\n    categories = adata.obs[value_to_plot].cat.categories\n    length = len(categories)\n\n    # check if default matplotlib palette has enough colors\n    if len(rcParams['axes.prop_cycle'].by_key()['color']) >= length:\n        cc = rcParams['axes.prop_cycle']()\n        palette = [next(cc)['color'] for _ in range(length)]\n\n    else:\n        if length <= 28:\n            palette = palettes.default_26\n        elif length <= len(palettes.default_64):  # 103 colors\n            palette = palettes.default_64\n        else:\n            palette = ['grey' for i in range(length)]\n            logg.info('the obs value: \"{}\" has more than 103 categories. Uniform '\n                      '\\'grey\\' color will be used for all categories.')\n\n    adata.uns[value_to_plot + '_colors'] = palette[:length]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the color values for each data point.", "response": "def _get_color_values(adata, value_to_plot, groups=None, palette=None, use_raw=False,\n                      gene_symbols=None, layer=None):\n    \"\"\"\n    Returns the value or color associated to each data point.\n    For categorical data, the return value is list of colors taken\n    from the category palette or from the given `palette` value.\n\n    For non-categorical data, the values are returned\n    \"\"\"\n\n    ###\n    # when plotting, the color of the dots is determined for each plot\n    # the data is either categorical or continuous and the data could be in\n    # 'obs' or in 'var'\n    categorical = False\n    if value_to_plot is None:\n        color_vector = 'lightgray'\n    # check if value to plot is in obs\n    elif value_to_plot in adata.obs.columns:\n        if is_categorical_dtype(adata.obs[value_to_plot]):\n            categorical = True\n\n            if palette:\n                # use category colors base on given palette\n                _set_colors_for_categorical_obs(adata, value_to_plot, palette)\n            else:\n                if value_to_plot + '_colors' not in adata.uns or \\\n                    len(adata.uns[value_to_plot + '_colors']) < len(adata.obs[value_to_plot].cat.categories):\n                    #  set a default palette in case that no colors or few colors are found\n                    _set_default_colors_for_categorical_obs(adata, value_to_plot)\n                else:\n                    # check that the colors in 'uns' are valid\n                    _palette = []\n                    for color in adata.uns[value_to_plot + '_colors']:\n                        if not is_color_like(color):\n                            # check if the color is a valid R color and translate it\n                            # to a valid hex color value\n                            if color in utils.additional_colors:\n                                color = utils.additional_colors[color]\n                            else:\n                                logg.warn(\"The following color value found in adata.uns['{}'] \"\n                                          \" is not valid: '{}'. Default colors are used.\".format(value_to_plot + '_colors', color))\n                                _set_default_colors_for_categorical_obs(adata, value_to_plot)\n                                _palette = None\n                                break\n                        _palette.append(color)\n                    if _palette is not None:\n                        adata.uns[value_to_plot + '_colors'] = _palette\n            # for categorical data, colors should be\n            # stored in adata.uns[value_to_plot + '_colors']\n            # Obtain color vector by converting every category\n            # into its respective color\n\n            color_vector = [adata.uns[value_to_plot + '_colors'][x] for x in adata.obs[value_to_plot].cat.codes]\n            if groups is not None:\n                if isinstance(groups, str):\n                    groups = [groups]\n                color_vector = np.array(color_vector, dtype='<U15')\n                # set color to 'light gray' for all values\n                # that are not in the groups\n                color_vector[~adata.obs[value_to_plot].isin(groups)] = \"lightgray\"\n        else:\n            color_vector = adata.obs[value_to_plot].values\n    # when value_to_plot is not in adata.obs\n    else:\n        if gene_symbols is not None and gene_symbols in adata.var.columns:\n            if value_to_plot not in adata.var[gene_symbols].values:\n                logg.error(\"Gene symbol {!r} not found in given gene_symbols \"\n                           \"column: {!r}\".format(value_to_plot, gene_symbols))\n                return\n            value_to_plot = adata.var[adata.var[gene_symbols] == value_to_plot].index[0]\n        if layer is not None and value_to_plot in adata.var_names:\n            if layer not in adata.layers.keys():\n                raise KeyError('Selected layer: {} is not in the layers list. The list of '\n                               'valid layers is: {}'.format(layer, adata.layers.keys()))\n            color_vector = adata[:, value_to_plot].layers[layer]\n        elif use_raw and value_to_plot in adata.raw.var_names:\n            color_vector = adata.raw[:, value_to_plot].X\n        elif value_to_plot in adata.var_names:\n            color_vector = adata[:, value_to_plot].X\n        else:\n            raise ValueError(\"The passed `color` {} is not a valid observation annotation \"\n                             \"or variable name. Valid observation annotation keys are: {}\"\n                             .format(value_to_plot, adata.obs.columns))\n\n    return color_vector, categorical"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert the basis string into the proper name.", "response": "def _basis2name(basis):\n    \"\"\"\n    converts the 'basis' into the proper name.\n    \"\"\"\n\n    component_name = (\n        'DC' if basis == 'diffmap'\n        else 'tSNE' if basis == 'tsne'\n        else 'UMAP' if basis == 'umap'\n        else 'PC' if basis == 'pca'\n        else basis.replace('draw_graph_', '').upper() if 'draw_graph' in basis\n        else basis)\n    return component_name"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the correlation matrix for the specified genes.", "response": "def correlation_matrix(adata, name_list=None, groupby=None, group=None, n_genes=20, data='Complete', method='pearson', annotation_key=None):\n    \"\"\"Calculate correlation matrix.\n\n        Calculate a correlation matrix for genes strored in sample annotation using :func:`~scanpy.api.tl.rank_genes_groups`.\n\n        Parameters\n        ----------\n        adata : :class:`~anndata.AnnData`\n            Annotated data matrix.\n        name_list : list, optional (default: None)\n            Takes a list of genes for which to calculate the correlation matrix\n        groupby : `str`, optional (default: None)\n            If no name list is passed, genes are selected from the\n            results of rank_gene_groups. Then this is the key of the sample grouping to consider.\n            Note that in this case also a group index has to be specified.\n        group : `int`, optional (default: None)\n            Group index for which the correlation matrix for top_ranked genes should be calculated.\n            Currently only int is supported, will change very soon\n        n_genes : `int`, optional (default: 20)\n            For how many genes to calculate correlation matrix? If specified, cuts the name list\n            (in whatever order it is passed).\n        data : {'Complete', 'Group', 'Rest'}, optional (default: 'Complete')\n            At the moment, this is only relevant for the case that name_list is drawn from rank_gene_groups results.\n            If specified, collects mask for the called group and then takes only those cells specified.\n            If 'Complete', calculate correlation using full data\n            If 'Group', calculate correlation within the selected group.\n            If 'Rest', calculate corrlation for everything except the group\n        method : {\u2018pearson\u2019, \u2018kendall\u2019, \u2018spearman\u2019} optional (default: 'pearson')\n            Which kind of correlation coefficient to use\n            pearson : standard correlation coefficient\n            kendall : Kendall Tau correlation coefficient\n            spearman : Spearman rank correlation\n        annotation_key: String, optional (default: None)\n            Allows to define the name of the anndata entry where results are stored.\n\n\n\n    \"\"\"\n\n    # TODO: At the moment, only works for int identifiers\n\n    ### If no genes are passed, selects ranked genes from sample annotation.\n    ### At the moment, only calculate one table (Think about what comes next)\n    if name_list is None:\n        name_list = list()\n        for j, k in enumerate(adata.uns['rank_genes_groups_gene_names']):\n            if j >=n_genes:\n                break\n            name_list.append(adata.uns['rank_genes_groups_gene_names'][j][group])\n    else:\n        if len(name_list)>n_genes:\n            name_list=name_list[0:n_genes]\n\n\n    # If special method (later) , truncate\n    adata_relevant = adata[:, name_list]\n    # This line just makes group_mask access easier. Nothing else but 'all' will stand here.\n    groups = 'all'\n    if data == 'Complete' or groupby is None:\n        if issparse(adata_relevant.X):\n            Data_array = adata_relevant.X.todense()\n        else:\n            Data_array = adata_relevant.X\n    else:\n        # get group_mask\n        groups_order, groups_masks = utils.select_groups(\n            adata, groups, groupby)\n        if data == 'Group':\n            if issparse(adata_relevant.X):\n                Data_array = adata_relevant.X[groups_masks[group], :].todense()\n            else:\n                Data_array = adata_relevant.X[groups_masks[group], :]\n        elif data == 'Rest':\n            if issparse(adata_relevant.X):\n                Data_array = adata_relevant.X[~groups_masks[group], :].todense()\n            else:\n                Data_array = adata_relevant.X[~groups_masks[group], :]\n        else:\n            logg.error('data argument should be either <Complete> or <Group> or <Rest>' )\n\n    # Distinguish between sparse and non-sparse data\n\n\n    DF_array = pd.DataFrame(Data_array, columns=name_list)\n    cor_table = DF_array.corr(method=method)\n    if annotation_key is None:\n        if groupby is None:\n            adata.uns['Correlation_matrix'] = cor_table\n        else:\n            adata.uns['Correlation_matrix'+groupby+str(group)]=cor_table\n    else:\n        adata.uns[annotation_key] = cor_table"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates ROC and AUC for genes in the sample annotation using rank_genes_groups. py.", "response": "def ROC_AUC_analysis(adata,groupby,group=None, n_genes=100):\n    \"\"\"Calculate correlation matrix.\n\n            Calculate a correlation matrix for genes strored in sample annotation using rank_genes_groups.py\n\n            Parameters\n            ----------\n            adata : :class:`~anndata.AnnData`\n                Annotated data matrix.\n            groupby : `str`\n                The key of the sample grouping to consider.\n            group : `str`, int, optional (default: None)\n                Group name or index for which the correlation matrix for top_ranked genes should be calculated.\n                If no parameter is passed, ROC/AUC is calculated for all groups\n            n_genes : `int`, optional (default: 100)\n                For how many genes to calculate ROC and AUC. If no parameter is passed, calculation is done for\n                all stored top ranked genes.\n\n        \"\"\"\n    if group is None:\n        pass\n        # TODO: Loop over all groups instead of just taking one.\n\n    # Assume group takes an int value for one group for the moment.\n    name_list = list()\n    for j, k in enumerate(adata.uns['rank_genes_groups_gene_names']):\n        if j >= n_genes:\n            break\n        name_list.append(adata.uns['rank_genes_groups_gene_names'][j][group])\n\n\n    # TODO: For the moment, see that everything works for comparison against the rest. Resolve issues later.\n    groups = 'all'\n    groups_order, groups_masks = utils.select_groups(\n        adata, groups, groupby)\n\n    # Use usual convention, better for looping later.\n    imask = group\n    mask = groups_masks[group]\n\n    # TODO: Allow for sample weighting requires better mask access... later\n\n    # We store calculated data in dict, access it via dict to dict. Check if this is the best way.\n    fpr={}\n    tpr={}\n    thresholds={}\n    roc_auc={}\n    y_true=mask\n    for i, j in enumerate(name_list):\n        vec=adata[:,[j]].X\n        if issparse(vec):\n            y_score = vec.todense()\n        else:\n            y_score = vec\n\n        fpr[name_list[i]], tpr[name_list[i]], thresholds[name_list[i]] = metrics.roc_curve(y_true, y_score, pos_label=None, sample_weight=None, drop_intermediate=False)\n        roc_auc[name_list[i]]=metrics.auc(fpr[name_list[i]],tpr[name_list[i]])\n    adata.uns['ROCfpr' +groupby+ str(group)] = fpr\n    adata.uns['ROCtpr' +groupby+ str(group)] = tpr\n    adata.uns['ROCthresholds' +groupby+ str(group)] = thresholds\n    adata.uns['ROC_AUC' + groupby + str(group)] = roc_auc"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef recipe_seurat(adata, log=True, plot=False, copy=False):\n    if copy: adata = adata.copy()\n    pp.filter_cells(adata, min_genes=200)\n    pp.filter_genes(adata, min_cells=3)\n    pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n    filter_result = filter_genes_dispersion(\n        adata.X, min_mean=0.0125, max_mean=3, min_disp=0.5, log=not log)\n    if plot:\n        from ..plotting import _preprocessing as ppp  # should not import at the top of the file\n        ppp.filter_genes_dispersion(filter_result, log=not log)\n    adata._inplace_subset_var(filter_result.gene_subset)  # filter genes\n    if log: pp.log1p(adata)\n    pp.scale(adata, max_value=10)\n    return adata if copy else None", "response": "This is a recipe for Seurat."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dendrogram(adata: AnnData, groupby: str,\n               n_pcs: Optional[int]=None,\n               use_rep: Optional[str]=None,\n               var_names: Optional[List[str]]=None,\n               use_raw: Optional[bool]=None,\n               cor_method: Optional[str]='pearson',\n               linkage_method: Optional[str]='complete',\n               key_added: Optional[str]=None) -> None:\n\n    \"\"\"\\\n    Computes a hierarchical clustering for the given `groupby` categories.\n\n    By default, the PCA representation is used unless `.X` has less than 50 variables.\n\n    Alternatively, a list of `var_names` (e.g. genes) can be given.\n\n    Average values of either `var_names` or components are used to compute a correlation matrix.\n\n    The hierarchical clustering can be visualized using `sc.pl.dendrogram` or multiple other\n    visualizations that can include a dendrogram: `matrixplot`, `heatmap`, `dotplot` and `stacked_violin`\n\n    .. note::\n        The computation of the hierarchical clustering is based on predefined groups and not\n        per cell. The correlation matrix is computed using by default pearson but other methods\n        are available.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix\n    {n_pcs}\n    {use_rep}\n    var_names : `list of str` (default: None)\n        List of var_names to use for computing the hierarchical clustering. If `var_names` is given,\n        then `use_rep` and `n_pcs` is ignored.\n    use_raw : `bool`, optional (default: None)\n        Only when `var_names` is not None. Use `raw` attribute of `adata` if present.\n    cor_method : `str`, optional (default: `\"pearson\"`)\n        correlation method to use. Options are 'pearson', 'kendall', and 'spearman'\n    linkage_method : `str`, optional (default: `\"complete\"`)\n        linkage method to use. See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n        for more information.\n    key_added : : `str`, optional (default: `None`)\n        By default, the dendrogram information is added to `.uns['dendrogram_' + groupby]`. Notice\n        that the `groupby` information is added to the dendrogram.\n\n    Returns\n    -------\n    adata.uns['dendrogram'] (or instead of 'dendrogram' the value selected for `key_added`) is updated\n    with the dendrogram information\n\n    Examples\n    --------\n\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.dendrogram(adata, groupby='bulk_labels')\n    >>> sc.pl.dendrogram(adata)\n    >>> sc.pl.dotplot(adata, ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ'],\n    ...               groupby='bulk_labels', dendrogram=True)\n    \"\"\"\n    if groupby not in adata.obs_keys():\n        raise ValueError('groupby has to be a valid observation. Given value: {}, '\n                         'valid observations: {}'.format(groupby, adata.obs_keys()))\n    if not is_categorical_dtype(adata.obs[groupby]):\n        # if the groupby column is not categorical, turn it into one\n        # by subdividing into  `num_categories` categories\n        raise ValueError('groupby has to be a categorical observation. Given value: {}, '\n                         'Column type: {}'.format(groupby, adata.obs[groupby].dtype))\n\n    if var_names is None:\n        rep_df = pd.DataFrame(choose_representation(adata, use_rep=use_rep, n_pcs=n_pcs))\n        rep_df.set_index(adata.obs[groupby], inplace=True)\n        categories = rep_df.index.categories\n    else:\n        if use_raw is None and adata.raw is not None: use_raw = True\n        gene_names = adata.raw.var_names if use_raw else adata.var_names\n        from ..plotting._anndata import _prepare_dataframe\n        categories, rep_df = _prepare_dataframe(adata, gene_names, groupby, use_raw)\n\n    if key_added is None:\n        key_added = 'dendrogram_' + groupby\n\n    logg.info('Storing dendrogram info using `.uns[{!r}]`'.format(key_added))\n    # aggregate values within categories using 'mean'\n    mean_df = rep_df.groupby(level=0).mean()\n\n    import scipy.cluster.hierarchy as sch\n\n    corr_matrix = mean_df.T.corr(method=cor_method)\n    z_var = sch.linkage(corr_matrix, method=linkage_method)\n    dendro_info = sch.dendrogram(z_var, labels=categories, no_plot=True)\n\n    # order of groupby categories\n    categories_idx_ordered = dendro_info['leaves']\n\n    adata.uns[key_added] = {'linkage': z_var,\n                            'groupby': groupby,\n                            'use_rep': use_rep,\n                            'cor_method': cor_method,\n                            'linkage_method': linkage_method,\n                            'categories_idx_ordered': categories_idx_ordered,\n                            'dendrogram_info': dendro_info,\n                            'correlation_matrix': corr_matrix.values}", "response": "Computes a hierarchical clustering for the given groupby categories."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a single - cell phonetic trajectory embedding of the given data matrix.", "response": "def phate(\n        adata,\n        n_components=2,\n        k=5,\n        a=15,\n        n_landmark=2000,\n        t='auto',\n        gamma=1,\n        n_pca=100,\n        knn_dist='euclidean',\n        mds_dist='euclidean',\n        mds='metric',\n        n_jobs=None,\n        random_state=None,\n        verbose=None,\n        copy=False,\n        **kwargs):\n    \"\"\"PHATE [Moon17]_.\n\n    Potential of Heat-diffusion for Affinity-based Trajectory Embedding (PHATE)\n    embeds high dimensional single-cell data into two or three dimensions for\n    visualization of biological progressions.\n\n    For more information and access to the object-oriented interface, read the\n    `PHATE documentation <https://phate.readthedocs.io/>`__.  For\n    tutorials, bug reports, and R/MATLAB implementations, visit the `PHATE\n    GitHub page <https://github.com/KrishnaswamyLab/PHATE/>`__. For help\n    using PHATE, go `here <https://krishnaswamylab.org/get-help>`__.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    n_components : `int`, optional (default: 2)\n        number of dimensions in which the data will be embedded\n    k : `int`, optional (default: 5)\n        number of nearest neighbors on which to build kernel\n    a : `int`, optional (default: 15)\n        sets decay rate of kernel tails.\n        If None, alpha decaying kernel is not used\n    n_landmark : `int`, optional (default: 2000)\n        number of landmarks to use in fast PHATE\n    t : `int` or 'auto', optional (default: 'auto')\n        power to which the diffusion operator is powered\n        sets the level of diffusion. If 'auto', t is selected\n        according to the knee point in the Von Neumann Entropy of\n        the diffusion operator\n    gamma : float, optional, default: 1\n        Informational distance constant between -1 and 1.\n        `gamma=1` gives the PHATE log potential, `gamma=0` gives\n        a square root potential.\n    n_pca : `int`, optional (default: 100)\n        Number of principal components to use for calculating\n        neighborhoods. For extremely large datasets, using\n        n_pca < 20 allows neighborhoods to be calculated in\n        log(n_samples) time.\n    knn_dist : string, optional (default: 'euclidean')\n        recommended values: 'euclidean' and 'cosine'\n        Any metric from `scipy.spatial.distance` can be used\n        distance metric for building kNN graph\n    mds_dist : string, optional (default: 'euclidean')\n        recommended values: 'euclidean' and 'cosine'\n        Any metric from `scipy.spatial.distance` can be used\n        distance metric for MDS\n    mds : {'classic', 'metric', 'nonmetric'}, optional (default: 'metric')\n        Selects which MDS algorithm is used for dimensionality reduction\n    n_jobs : `int` or `None`, optional (default: `sc.settings.n_jobs`)\n        The number of jobs to use for the computation.\n        If `None`, `sc.settings.n_jobs` is used.\n        If -1 all CPUs are used. If 1 is given, no parallel computing code is\n        used at all, which is useful for debugging.\n        For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for\n        n_jobs = -2, all CPUs but one are used\n    random_state : `int`, `numpy.RandomState` or `None`, optional (default: `None`)\n        Random seed. Defaults to the global `numpy` random number generator\n    verbose : `bool`, `int` or `None`, optional (default: `sc.settings.verbosity`)\n        If `True` or an integer `>= 2`, print status messages.\n        If `None`, `sc.settings.verbosity` is used.\n    copy : `bool` (default: `False`)\n        Return a copy instead of writing to `adata`.\n    kwargs : additional arguments to `phate.PHATE`\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n\n    **X_phate** : `np.ndarray`, (`adata.obs`, shape=[n_samples, n_components], dtype `float`)\n        PHATE coordinates of data.\n\n    Examples\n    --------\n    >>> import scanpy.api as sc\n    >>> import phate\n    >>> tree_data, tree_clusters = phate.tree.gen_dla(n_dim=100,\n                                                      n_branch=20,\n                                                      branch_length=100)\n    >>> tree_data.shape\n    (2000, 100)\n    >>> adata = sc.AnnData(tree_data)\n    >>> sc.tl.phate(adata, k=5, a=20, t=150)\n    >>> adata.obsm['X_phate'].shape\n    (2000, 2)\n    >>> sc.pl.phate(adata)\n    \"\"\"\n    logg.info('computing PHATE', r=True)\n    adata = adata.copy() if copy else adata\n    verbose = settings.verbosity if verbose is None else verbose\n    if isinstance(settings.verbosity, (str, int)):\n        verbose = _settings_verbosity_greater_or_equal_than(2)\n    n_jobs = settings.n_jobs if n_jobs is None else n_jobs\n    try:\n        import phate\n    except ImportError:\n        raise ImportError(\n            'You need to install the package `phate`: please run `pip install '\n            '--user phate` in a terminal.')\n    X_phate = phate.PHATE(\n        n_components=n_components,\n        k=k,\n        a=a,\n        n_landmark=n_landmark,\n        t=t,\n        gamma=gamma,\n        n_pca=n_pca,\n        knn_dist=knn_dist,\n        mds_dist=mds_dist,\n        mds=mds,\n        n_jobs=n_jobs,\n        random_state=random_state,\n        verbose=verbose,\n        **kwargs\n    ).fit_transform(adata)\n    logg.info('    finished', time=True,\n              end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\\n')\n    # update AnnData instance\n    adata.obsm['X_phate'] = X_phate  # annotate samples with PHATE coordinates\n    logg.hint('added\\n'\n              '    \\'X_phate\\', PHATE coordinates (adata.obsm)')\n    return adata if copy else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncompares the abstracted graph with the PAGA graph.", "response": "def paga_compare(\n        adata,\n        basis=None,\n        edges=False,\n        color=None,\n        alpha=None,\n        groups=None,\n        components=None,\n        projection='2d',\n        legend_loc='on data',\n        legend_fontsize=None,\n        legend_fontweight='bold',\n        color_map=None,\n        palette=None,\n        frameon=False,\n        size=None,\n        title=None,\n        right_margin=None,\n        left_margin=0.05,\n        show=None,\n        save=None,\n        title_graph=None,\n        groups_graph=None,\n        **paga_graph_params):\n    \"\"\"Scatter and PAGA graph side-by-side.\n\n    Consists in a scatter plot and the abstracted graph. See\n    :func:`~scanpy.api.pl.paga` for all related parameters.\n\n    See :func:`~scanpy.api.pl.paga_path` for visualizing gene changes along paths\n    through the abstracted graph.\n\n    Additional parameters are as follows.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    kwds_scatter : `dict`\n        Keywords for :func:`~scanpy.api.pl.scatter`.\n    kwds_paga : `dict`\n        Keywords for :func:`~scanpy.api.pl.paga`.\n\n    Returns\n    -------\n    A list of `matplotlib.axes.Axes` if `show` is `False`.\n    \"\"\"\n    axs, _, _, _ = utils.setup_axes(panels=[0, 1],\n                                    right_margin=right_margin)\n    if color is None:\n        color = adata.uns['paga']['groups']\n    suptitle = None  # common title for entire figure\n    if title_graph is None:\n        suptitle = color if title is None else title\n        title, title_graph = '', ''\n    if basis is None:\n        if 'X_draw_graph_fa' in adata.obsm.keys():\n            basis = 'draw_graph_fa'\n        elif 'X_umap' in adata.obsm.keys():\n            basis = 'umap'\n        elif 'X_tsne' in adata.obsm.keys():\n            basis = 'tsne'\n        elif 'X_draw_graph_fr' in adata.obsm.keys():\n            basis = 'draw_graph_fr'\n        else:\n            basis = 'umap'\n    from .scatterplots import plot_scatter\n    plot_scatter(\n        adata,\n        ax=axs[0],\n        basis=basis,\n        color=color,\n        edges=edges,\n        alpha=alpha,\n        groups=groups,\n        components=components,\n        legend_loc=legend_loc,\n        legend_fontsize=legend_fontsize,\n        legend_fontweight=legend_fontweight,\n        color_map=color_map,\n        palette=palette,\n        frameon=frameon,\n        size=size,\n        title=title,\n        show=False,\n        save=False)\n    if 'pos' not in paga_graph_params:\n        if color == adata.uns['paga']['groups']:\n            paga_graph_params['pos'] = utils._tmp_cluster_pos\n        else:\n            paga_graph_params['pos'] = adata.uns['paga']['pos']\n    xlim, ylim = axs[0].get_xlim(), axs[0].get_ylim()\n    axs[1].set_xlim(xlim)\n    axs[1].set_ylim(ylim)\n    if 'labels' in paga_graph_params:\n        labels = paga_graph_params.pop('labels')\n    else:\n        labels = groups_graph\n    paga(\n        adata,\n        ax=axs[1],\n        show=False,\n        save=False,\n        title=title_graph,\n        labels=labels,\n        colors=color,\n        frameon=frameon,\n        **paga_graph_params)\n    if suptitle is not None: pl.suptitle(suptitle)\n    utils.savefig_or_show('paga_compare', show=show, save=save)\n    if show == False: return axs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef paga(\n    adata,\n    threshold=None,\n    color=None,\n    layout=None,\n    layout_kwds={},\n    init_pos=None,\n    root=0,\n    labels=None,\n    single_component=False,\n    solid_edges='connectivities',\n    dashed_edges=None,\n    transitions=None,\n    fontsize=None,\n    fontweight='bold',\n    text_kwds={},\n    node_size_scale=1,\n    node_size_power=0.5,\n    edge_width_scale=1,\n    min_edge_width=None,\n    max_edge_width=None,\n    arrowsize=30,\n    title=None,\n    left_margin=0.01,\n    random_state=0,\n    pos=None,\n    normalize_to_color=False,\n    cmap=None,\n    cax=None,\n    colorbar=None,\n    cb_kwds={},\n    frameon=None,\n    add_pos=True,\n    export_to_gexf=False,\n    use_raw=True,\n    colors=None,   # backwards compat\n    groups=None,  # backwards compat\n    plot=True,\n    show=None,\n    save=None,\n    ax=None,\n) -> Union[Axes, List[Axes], None]:\n    \"\"\"Plot the PAGA graph through thresholding low-connectivity edges.\n\n    Compute a coarse-grained layout of the data. Reuse this by passing\n    `init_pos='paga'` to :func:`~scanpy.api.tl.umap` or\n    :func:`~scanpy.api.tl.draw_graph` and obtain embeddings with more meaningful\n    global topology [Wolf19]_.\n\n    This uses ForceAtlas2 or igraph's layout algorithms for most layouts [Csardi06]_.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    threshold : `float` or `None`, optional (default: 0.01)\n        Do not draw edges for weights below this threshold. Set to 0 if you want\n        all edges. Discarding low-connectivity edges helps in getting a much\n        clearer picture of the graph.\n    color : gene name or obs. annotation, optional (default: `None`)\n        The node colors. Also plots the degree of the abstracted graph when\n        passing {'degree_dashed', 'degree_solid'}.\n    labels : `None`, `str`, `list`, `dict`, optional (default: `None`)\n        The node labels. If `None`, this defaults to the group labels stored in\n        the categorical for which :func:`~scanpy.api.tl.paga` has been computed.\n    pos : `np.ndarray`, filename of `.gdf` file,  optional (default: `None`)\n        Two-column array-like storing the x and y coordinates for drawing.\n        Otherwise, path to a `.gdf` file that has been exported from Gephi or\n        a similar graph visualization software.\n    layout : {'fa', 'fr', 'rt', 'rt_circular', 'eq_tree', ...}, optional (default: 'fr')\n        Plotting layout that computes positions. 'fa' stands for ForceAtlas2, 'fr' stands for\n        Fruchterman-Reingold, 'rt' stands for Reingold Tilford. 'eq_tree' stands\n        for 'eqally spaced tree'. All but 'fa' and 'eq_tree' are igraph\n        layouts. All other igraph layouts are also permitted. See also parameter\n        `pos` and :func:`~scanpy.api.tl.draw_graph`.\n    init_pos : `np.ndarray`, optional (default: `None`)\n        Two-column array storing the x and y coordinates for initializing the\n        layout.\n    random_state : `int` or `None`, optional (default: 0)\n        For layouts with random initialization like 'fr', change this to use\n        different intial states for the optimization. If `None`, the initial\n        state is not reproducible.\n    root : `int`, `str` or list of `int`, optional (default: 0)\n        If choosing a tree layout, this is the index of the root node or a list\n        of root node indices. If this is a non-empty vector then the supplied\n        node IDs are used as the roots of the trees (or a single tree if the\n        graph is connected). If this is `None` or an empty list, the root\n        vertices are automatically calculated based on topological sorting.\n    transitions : `str` or `None`, optional (default: `None`)\n        Key for `.uns['paga']` that specifies the matrix that - for instance\n        `'transistions_confidence'` - that specifies the matrix that stores the\n        arrows.\n    solid_edges : `str`, optional (default: 'paga_connectivities')\n        Key for `.uns['paga']` that specifies the matrix that stores the edges\n        to be drawn solid black.\n    dashed_edges : `str` or `None`, optional (default: `None`)\n        Key for `.uns['paga']` that specifies the matrix that stores the edges\n        to be drawn dashed grey. If `None`, no dashed edges are drawn.\n    single_component : `bool`, optional (default: `False`)\n        Restrict to largest connected component.\n    fontsize : `int` (default: `None`)\n        Font size for node labels.\n    text_kwds : keywords for `matplotlib.text`\n        See `here\n        <https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.text.html#matplotlib.axes.Axes.text>`_.\n    node_size_scale : `float` (default: 1.0)\n        Increase or decrease the size of the nodes.\n    node_size_power : `float` (default: 0.5)\n        The power with which groups sizes influence the radius of the nodes.\n    edge_width_scale : `float`, optional (default: 5)\n        Edge with scale in units of `rcParams['lines.linewidth']`.\n    min_edge_width : `float`, optional (default: `None`)\n        Min width of solid edges.\n    max_edge_width : `float`, optional (default: `None`)\n        Max width of solid and dashed edges.\n    arrowsize : `int`, optional (default: 30)\n       For directed graphs, choose the size of the arrow head head's length and\n       width. See :py:class: `matplotlib.patches.FancyArrowPatch` for attribute\n       `mutation_scale` for more info.\n    export_to_gexf : `bool`, optional (default: `None`)\n        Export to gexf format to be read by graph visualization programs such as\n        Gephi.\n    normalize_to_color : `bool`, optional (default: `False`)\n        Whether to normalize categorical plots to `color` or the underlying\n        grouping.\n    cmap : color map\n        The color map.\n    cax : :class:`~matplotlib.axes.Axes`\n        A matplotlib axes object for a potential colorbar.\n    cb_kwds : colorbar keywords\n        See `here\n        <https://matplotlib.org/api/colorbar_api.html#matplotlib.colorbar.ColorbarBase>`__,\n        for instance, `ticks`.\n    add_pos : `bool`, optional (default: `True`)\n        Add the positions to `adata.uns['paga']`.\n    title : `str`, optional (default: `None`)\n        Provide a title.\n    frameon : `bool`, optional (default: `None`)\n        Draw a frame around the PAGA graph.\n    hide : `bool`, optional (default: `False`)\n        Do not create a plot.\n    plot : `bool`, optional (default: `True`)\n        If `False`, do not create the figure, simply compute the layout.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on \\\\{'.pdf', '.png', '.svg'\\\\}.\n    ax : :class:`~matplotlib.axes.Axes`\n        A matplotlib axes object.\n\n    Returns\n    -------\n    If `show==False`, one or more :class:`~matplotlib.axes.Axes` objects.\n    Adds `'pos'` to `adata.uns['paga']` if `add_pos` is `True`.\n\n    Notes\n    -----\n    When initializing the positions, note that - for some reason - igraph\n    mirrors coordinates along the x axis... that is, you should increase the\n    `maxiter` parameter by 1 if the layout is flipped.\n\n    See also\n    --------\n    tl.paga\n    pl.paga_compare\n    pl.paga_path\n    \"\"\"\n    if groups is not None:  # backwards compat\n        labels = groups\n        logg.warn('`groups` is deprecated in `pl.paga`: use `labels` instead')\n    if colors is None:\n        colors = color\n    # colors is a list that contains no lists\n    groups_key = adata.uns['paga']['groups']\n    if ((isinstance(colors, Iterable) and len(colors) == len(adata.obs[groups_key].cat.categories))\n        or colors is None or isinstance(colors, str)):\n        colors = [colors]\n\n    if frameon is None:\n        frameon = settings._frameon\n\n    # labels is a list that contains no lists\n    if ((isinstance(labels, Iterable) and len(labels) == len(adata.obs[groups_key].cat.categories))\n        or labels is None or isinstance(labels, (str, dict))):\n        labels = [labels for i in range(len(colors))]\n\n    if title is None and len(colors) > 1:\n        title = [c for c in colors]\n    elif isinstance(title, str):\n        title = [title for c in colors]\n    elif title is None:\n        title = [None for c in colors]\n\n    if colorbar is None:\n        var_names = adata.var_names if adata.raw is None else adata.raw.var_names\n        colorbars = [True if c in var_names else False for c in colors]\n    else:\n        colorbars = [False for c in colors]\n\n    if isinstance(root, str):\n        if root in labels:\n            root = list(labels).index(root)\n        else:\n            raise ValueError(\n                'If `root` is a string, it needs to be one of {} not \\'{}\\'.'\n                .format(labels, root))\n    if isinstance(root, list) and root[0] in labels:\n        root = [list(labels).index(r) for r in root]\n\n    # define the adjacency matrices\n    adjacency_solid = adata.uns['paga'][solid_edges].copy()\n    adjacency_dashed = None\n    if threshold is None:\n        threshold = 0.01  # default threshold\n    if threshold > 0:\n        adjacency_solid.data[adjacency_solid.data < threshold] = 0\n        adjacency_solid.eliminate_zeros()\n    if dashed_edges is not None:\n        adjacency_dashed = adata.uns['paga'][dashed_edges].copy()\n        if threshold > 0:\n            adjacency_dashed.data[adjacency_dashed.data < threshold] = 0\n            adjacency_dashed.eliminate_zeros()\n\n    # compute positions\n    if pos is None:\n        adj_tree = None\n        if layout in {'rt', 'rt_circular', 'eq_tree'}:\n            adj_tree = adata.uns['paga']['connectivities_tree']\n        pos = _compute_pos(\n            adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root)\n\n    if plot:\n        if ax is None:\n            axs, panel_pos, draw_region_width, figure_width = utils.setup_axes(\n                panels=colors, colorbars=colorbars)\n        else:\n            axs = ax\n\n        if len(colors) == 1 and not isinstance(axs, list):\n            axs = [axs]\n\n        for icolor, c in enumerate(colors):\n            if title[icolor] is not None:\n                axs[icolor].set_title(title[icolor])\n            sct = _paga_graph(\n                adata,\n                axs[icolor],\n                colors=c,\n                solid_edges=solid_edges,\n                dashed_edges=dashed_edges,\n                transitions=transitions,\n                threshold=threshold,\n                adjacency_solid=adjacency_solid,\n                adjacency_dashed=adjacency_dashed,\n                root=root,\n                labels=labels[icolor],\n                fontsize=fontsize,\n                fontweight=fontweight,\n                text_kwds=text_kwds,\n                node_size_scale=node_size_scale,\n                node_size_power=node_size_power,\n                edge_width_scale=edge_width_scale,\n                min_edge_width=min_edge_width,\n                max_edge_width=max_edge_width,\n                normalize_to_color=normalize_to_color,\n                frameon=frameon,\n                cmap=cmap,\n                cax=cax,\n                colorbar=colorbars[icolor],\n                cb_kwds=cb_kwds,\n                use_raw=use_raw,\n                title=title[icolor],\n                export_to_gexf=export_to_gexf,\n                single_component=single_component,\n                arrowsize=arrowsize,\n                pos=pos)\n            if colorbars[icolor]:\n                bottom = panel_pos[0][0]\n                height = panel_pos[1][0] - bottom\n                width = 0.006 * draw_region_width / len(colors)\n                left = panel_pos[2][2*icolor+1] + 0.2 * width\n                rectangle = [left, bottom, width, height]\n                fig = pl.gcf()\n                ax_cb = fig.add_axes(rectangle)\n                cb = pl.colorbar(sct, format=ticker.FuncFormatter(utils.ticks_formatter),\n                                 cax=ax_cb)\n    if add_pos:\n        adata.uns['paga']['pos'] = pos\n        logg.hint('added \\'pos\\', the PAGA positions (adata.uns[\\'paga\\'])')\n    if plot:\n        utils.savefig_or_show('paga', show=show, save=save)\n        if len(colors) == 1 and isinstance(axs, list): axs = axs[0]\n        return axs if show == False else None", "response": "Plot the PAGA graph through thresholding low - connectivity edges."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef paga_path(\n    adata,\n    nodes,\n    keys,\n    use_raw=True,\n    annotations=['dpt_pseudotime'],\n    color_map=None,\n    color_maps_annotations={'dpt_pseudotime': 'Greys'},\n    palette_groups=None,\n    n_avg=1,\n    groups_key=None,\n    xlim=[None, None],\n    title=None,\n    left_margin=None,\n    ytick_fontsize=None,\n    title_fontsize=None,\n    show_node_names=True,\n    show_yticks=True,\n    show_colorbar=True,\n    legend_fontsize=None,\n    legend_fontweight=None,\n    normalize_to_zero_one=False,\n    as_heatmap=True,\n    return_data=False,\n    show=None,\n    save=None,\n    ax=None,\n) -> Optional[Axes]:\n    \"\"\"Gene expression and annotation changes along paths in the abstracted graph.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        An annotated data matrix.\n    nodes : list of group names or their category indices\n        A path through nodes of the abstracted graph, that is, names or indices\n        (within `.categories`) of groups that have been used to run PAGA.\n    keys : list of str\n        Either variables in `adata.var_names` or annotations in\n        `adata.obs`. They are plotted using `color_map`.\n    use_raw : `bool`, optional (default: `True`)\n        Use `adata.raw` for retrieving gene expressions if it has been set.\n    annotations : list of annotations, optional (default: ['dpt_pseudotime'])\n        Plot these keys with `color_maps_annotations`. Need to be keys for\n        `adata.obs`.\n    color_map : color map for plotting keys or `None`, optional (default: `None`)\n        Matplotlib colormap.\n    color_maps_annotations : dict storing color maps or `None`, optional (default: {'dpt_pseudotime': 'Greys'})\n        Color maps for plotting the annotations. Keys of the dictionary must\n        appear in `annotations`.\n    palette_groups : list of colors or `None`, optional (default: `None`)\n        Ususally, use the same `sc.pl.palettes...` as used for coloring the\n        abstracted graph.\n    n_avg : `int`, optional (default: 1)\n        Number of data points to include in computation of running average.\n    groups_key : `str`, optional (default: `None`)\n        Key of the grouping used to run PAGA. If `None`, defaults to\n        `adata.uns['paga']['groups']`.\n    as_heatmap : `bool`, optional (default: `True`)\n        Plot the timeseries as heatmap. If not plotting as heatmap,\n        `annotations` have no effect.\n    show_node_names : `bool`, optional (default: `True`)\n        Plot the node names on the nodes bar.\n    show_colorbar : `bool`, optional (default: `True`)\n        Show the colorbar.\n    show_yticks : `bool`, optional (default: `True`)\n        Show the y ticks.\n    normalize_to_zero_one : `bool`, optional (default: `True`)\n        Shift and scale the running average to [0, 1] per gene.\n    return_data : `bool`, optional (default: `False`)\n        Return the timeseries data in addition to the axes if `True`.\n    show : `bool`, optional (default: `None`)\n         Show the plot, do not return axis.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on \\\\{'.pdf', '.png', '.svg'\\\\}.\n    ax : :class:`~matplotlib.axes.Axes`\n         A matplotlib axes object.\n\n    Returns\n    -------\n    A :class:`~matplotlib.axes.Axes` object, if `ax` is `None`, else `None`.\n    If `return_data`, return the timeseries data in addition to an axes.\n    \"\"\"\n    ax_was_none = ax is None\n\n    if groups_key is None:\n        if 'groups' not in adata.uns['paga']:\n            raise KeyError(\n                'Pass the key of the grouping with which you ran PAGA, '\n                'using the parameter `groups_key`.')\n        groups_key = adata.uns['paga']['groups']\n    groups_names = adata.obs[groups_key].cat.categories\n\n    if 'dpt_pseudotime' not in adata.obs.keys():\n        raise ValueError(\n            '`pl.paga_path` requires computation of a pseudotime `tl.dpt` '\n            'for ordering at single-cell resolution')\n\n    if palette_groups is None:\n        utils.add_colors_for_categorical_sample_annotation(adata, groups_key)\n        palette_groups = adata.uns[groups_key + '_colors']\n\n    def moving_average(a):\n        return sc_utils.moving_average(a, n_avg)\n\n    ax = pl.gca() if ax is None else ax\n    from matplotlib import transforms\n    trans = transforms.blended_transform_factory(\n        ax.transData, ax.transAxes)\n    X = []\n    x_tick_locs = [0]\n    x_tick_labels = []\n    groups = []\n    anno_dict = {anno: [] for anno in annotations}\n    if isinstance(nodes[0], str):\n        nodes_ints = []\n        groups_names_set = set(groups_names)\n        for node in nodes:\n            if node not in groups_names_set:\n                raise ValueError(\n                    'Each node/group needs to be one of {} (`groups_key`=\\'{}\\') not \\'{}\\'.'\n                    .format(groups_names.tolist(), groups_key, node))\n            nodes_ints.append(groups_names.get_loc(node))\n        nodes_strs = nodes\n    else:\n        nodes_ints = nodes\n        nodes_strs = [groups_names[node] for node in nodes]\n\n    adata_X = adata\n    if use_raw and adata.raw is not None:\n        adata_X = adata.raw\n\n    for ikey, key in enumerate(keys):\n        x = []\n        for igroup, group in enumerate(nodes_ints):\n            idcs = np.arange(adata.n_obs)[\n                adata.obs[groups_key].values == nodes_strs[igroup]]\n            if len(idcs) == 0:\n                raise ValueError(\n                    'Did not find data points that match '\n                    '`adata.obs[{}].values == str({})`.'\n                    'Check whether adata.obs[{}] actually contains what you expect.'\n                    .format(groups_key, group, groups_key))\n            idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[\n                adata.obs[groups_key].values == nodes_strs[igroup]])\n            idcs = idcs[idcs_group]\n            if key in adata.obs_keys(): x += list(adata.obs[key].values[idcs])\n            else: x += list(adata_X[:, key].X[idcs])\n            if ikey == 0:\n                groups += [group for i in range(len(idcs))]\n                x_tick_locs.append(len(x))\n                for anno in annotations:\n                    series = adata.obs[anno]\n                    if is_categorical_dtype(series): series = series.cat.codes\n                    anno_dict[anno] += list(series.values[idcs])\n        if n_avg > 1:\n            old_len_x = len(x)\n            x = moving_average(x)\n            if ikey == 0:\n                for key in annotations:\n                    if not isinstance(anno_dict[key][0], str):\n                        anno_dict[key] = moving_average(anno_dict[key])\n        if normalize_to_zero_one:\n            x -= np.min(x)\n            x /= np.max(x)\n        X.append(x)\n        if not as_heatmap:\n            ax.plot(x[xlim[0]:xlim[1]], label=key)\n        if ikey == 0:\n            for igroup, group in enumerate(nodes):\n                if len(groups_names) > 0 and group not in groups_names:\n                    label = groups_names[group]\n                else:\n                    label = group\n                x_tick_labels.append(label)\n    X = np.array(X)\n    if as_heatmap:\n        img = ax.imshow(X, aspect='auto', interpolation='nearest',\n                        cmap=color_map)\n        if show_yticks:\n            ax.set_yticks(range(len(X)))\n            ax.set_yticklabels(keys, fontsize=ytick_fontsize)\n        else:\n            ax.set_yticks([])\n        ax.set_frame_on(False)\n        ax.set_xticks([])\n        ax.tick_params(axis='both', which='both', length=0)\n        ax.grid(False)\n        if show_colorbar:\n            pl.colorbar(img, ax=ax)\n        left_margin = 0.2 if left_margin is None else left_margin\n        pl.subplots_adjust(left=left_margin)\n    else:\n        left_margin = 0.4 if left_margin is None else left_margin\n        if len(keys) > 1:\n            pl.legend(frameon=False, loc='center left',\n                      bbox_to_anchor=(-left_margin, 0.5),\n                      fontsize=legend_fontsize)\n    xlabel = groups_key\n    if not as_heatmap:\n        ax.set_xlabel(xlabel)\n        pl.yticks([])\n        if len(keys) == 1: pl.ylabel(keys[0] + ' (a.u.)')\n    else:\n        import matplotlib.colors\n        # groups bar\n        ax_bounds = ax.get_position().bounds\n        groups_axis = pl.axes([ax_bounds[0],\n                               ax_bounds[1] - ax_bounds[3] / len(keys),\n                               ax_bounds[2],\n                               ax_bounds[3] / len(keys)])\n        groups = np.array(groups)[None, :]\n        groups_axis.imshow(groups, aspect='auto',\n                           interpolation=\"nearest\",\n                           cmap=matplotlib.colors.ListedColormap(\n                               # the following line doesn't work because of normalization\n                               # adata.uns['paga_groups_colors'])\n                               palette_groups[np.min(groups).astype(int):],\n                               N=int(np.max(groups)+1-np.min(groups))))\n        if show_yticks:\n            groups_axis.set_yticklabels(['', xlabel, ''], fontsize=ytick_fontsize)\n        else:\n            groups_axis.set_yticks([])\n        groups_axis.set_frame_on(False)\n        if show_node_names:\n            ypos = (groups_axis.get_ylim()[1] + groups_axis.get_ylim()[0])/2\n            x_tick_locs = sc_utils.moving_average(x_tick_locs, n=2)\n            for ilabel, label in enumerate(x_tick_labels):\n                groups_axis.text(x_tick_locs[ilabel], ypos, x_tick_labels[ilabel],\n                                 fontdict={'horizontalalignment': 'center',\n                                           'verticalalignment': 'center'})\n        groups_axis.set_xticks([])\n        groups_axis.grid(False)\n        groups_axis.tick_params(axis='both', which='both', length=0)\n        # further annotations\n        y_shift = ax_bounds[3] / len(keys)\n        for ianno, anno in enumerate(annotations):\n            if ianno > 0: y_shift = ax_bounds[3] / len(keys) / 2\n            anno_axis = pl.axes([ax_bounds[0],\n                                 ax_bounds[1] - (ianno+2) * y_shift,\n                                 ax_bounds[2],\n                                 y_shift])\n            arr = np.array(anno_dict[anno])[None, :]\n            if anno not in color_maps_annotations:\n                color_map_anno = ('Vega10' if is_categorical_dtype(adata.obs[anno])\n                                  else 'Greys')\n            else:\n                color_map_anno = color_maps_annotations[anno]\n            img = anno_axis.imshow(arr, aspect='auto',\n                                   interpolation='nearest',\n                                   cmap=color_map_anno)\n            if show_yticks:\n                anno_axis.set_yticklabels(['', anno, ''],\n                                          fontsize=ytick_fontsize)\n                anno_axis.tick_params(axis='both', which='both', length=0)\n            else:\n                anno_axis.set_yticks([])\n            anno_axis.set_frame_on(False)\n            anno_axis.set_xticks([])\n            anno_axis.grid(False)\n    if title is not None: ax.set_title(title, fontsize=title_fontsize)\n    if show is None and not ax_was_none: show = False\n    else: show = settings.autoshow if show is None else show\n    utils.savefig_or_show('paga_path', show=show, save=save)\n    if return_data:\n        df = pd.DataFrame(data=X.T, columns=keys)\n        df['groups'] = moving_average(groups)  # groups is without moving average, yet\n        if 'dpt_pseudotime' in anno_dict:\n            df['distance'] = anno_dict['dpt_pseudotime'].T\n        return ax, df if ax_was_none and show == False else df\n    else:\n        return ax if ax_was_none and show == False else None", "response": "Plot a path through nodes of the abstracted graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef paga_adjacency(\n        adata,\n        adjacency='connectivities',\n        adjacency_tree='connectivities_tree',\n        as_heatmap=True,\n        color_map=None,\n        show=None,\n        save=None):\n    \"\"\"Connectivity of paga groups.\n    \"\"\"\n    connectivity = adata.uns[adjacency].toarray()\n    connectivity_select = adata.uns[adjacency_tree]\n    if as_heatmap:\n        matrix(connectivity, color_map=color_map, show=False)\n        for i in range(connectivity_select.shape[0]):\n            neighbors = connectivity_select[i].nonzero()[1]\n            pl.scatter([i for j in neighbors], neighbors, color='black', s=1)\n    # as a stripplot\n    else:\n        pl.figure()\n        for i, cs in enumerate(connectivity):\n            x = [i for j, d in enumerate(cs) if i != j]\n            y = [c for j, c in enumerate(cs) if i != j]\n            pl.scatter(x, y, color='gray', s=1)\n            neighbors = connectivity_select[i].nonzero()[1]\n            pl.scatter([i for j in neighbors],\n                       cs[neighbors], color='black', s=1)\n    utils.savefig_or_show('paga_connectivity', show=show, save=save)", "response": "Calculates the paga connectivity of the paga groups."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nscattering plot along observations or variables axes.", "response": "def scatter(\n        adata,\n        x=None,\n        y=None,\n        color=None,\n        use_raw=None,\n        layers='X',\n        sort_order=True,\n        alpha=None,\n        basis=None,\n        groups=None,\n        components=None,\n        projection='2d',\n        legend_loc='right margin',\n        legend_fontsize=None,\n        legend_fontweight=None,\n        color_map=None,\n        palette=None,\n        frameon=None,\n        right_margin=None,\n        left_margin=None,\n        size=None,\n        title=None,\n        show=None,\n        save=None,\n        ax=None):\n    \"\"\"\\\n    Scatter plot along observations or variables axes.\n\n    Color the plot using annotations of observations (`.obs`), variables\n    (`.var`) or expression of genes (`.var_names`).\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    x : `str` or `None`\n        x coordinate.\n    y : `str` or `None`\n        y coordinate.\n    color : string or list of strings, optional (default: `None`)\n        Keys for annotations of observations/cells or variables/genes, e.g.,\n        `'ann1'` or `['ann1', 'ann2']`.\n    use_raw : `bool`, optional (default: `None`)\n        Use `raw` attribute of `adata` if present.\n    layers : `str` or tuple of strings, optional (default: `X`)\n        Use the `layers` attribute of `adata` if present: specify the layer for\n        `x`, `y` and `color`. If `layers` is a string, then it is expanded to\n        `(layers, layers, layers)`.\n    basis : {{'pca', 'tsne', 'umap', 'diffmap', 'draw_graph_fr', etc.}}\n        String that denotes a plotting tool that computed coordinates.\n    {scatter_temp}\n    {show_save_ax}\n\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n    \"\"\"\n    if basis is not None:\n        axs = _scatter_obs(\n            adata=adata,\n            x=x,\n            y=y,\n            color=color,\n            use_raw=use_raw,\n            layers=layers,\n            sort_order=sort_order,\n            alpha=alpha,\n            basis=basis,\n            groups=groups,\n            components=components,\n            projection=projection,\n            legend_loc=legend_loc,\n            legend_fontsize=legend_fontsize,\n            legend_fontweight=legend_fontweight,\n            color_map=color_map,\n            palette=palette,\n            frameon=frameon,\n            right_margin=right_margin,\n            left_margin=left_margin,\n            size=size,\n            title=title,\n            show=show,\n            save=save,\n            ax=ax)\n    elif x is not None and y is not None:\n        if ((x in adata.obs.keys() or x in adata.var.index)\n            and (y in adata.obs.keys() or y in adata.var.index)\n            and (color is None or color in adata.obs.keys() or color in adata.var.index)):\n            axs = _scatter_obs(\n                adata=adata,\n                x=x,\n                y=y,\n                color=color,\n                use_raw=use_raw,\n                layers=layers,\n                sort_order=sort_order,\n                alpha=alpha,\n                basis=basis,\n                groups=groups,\n                components=components,\n                projection=projection,\n                legend_loc=legend_loc,\n                legend_fontsize=legend_fontsize,\n                legend_fontweight=legend_fontweight,\n                color_map=color_map,\n                palette=palette,\n                frameon=frameon,\n                right_margin=right_margin,\n                left_margin=left_margin,\n                size=size,\n                title=title,\n                show=show,\n                save=save,\n                ax=ax)\n        elif ((x in adata.var.keys() or x in adata.obs.index)\n                and (y in adata.var.keys() or y in adata.obs.index)\n                and (color is None or color in adata.var.keys() or color in adata.obs.index)):\n            axs = _scatter_var(\n                adata=adata,\n                x=x,\n                y=y,\n                color=color,\n                use_raw=use_raw,\n                layers=layers,\n                sort_order=sort_order,\n                alpha=alpha,\n                basis=basis,\n                groups=groups,\n                components=components,\n                projection=projection,\n                legend_loc=legend_loc,\n                legend_fontsize=legend_fontsize,\n                legend_fontweight=legend_fontweight,\n                color_map=color_map,\n                palette=palette,\n                frameon=frameon,\n                right_margin=right_margin,\n                left_margin=left_margin,\n                size=size,\n                title=title,\n                show=show,\n                save=save,\n                ax=ax)\n        else:\n            raise ValueError(\n                '`x`, `y`, and potential `color` inputs must all come from either `.obs` or `.var`')\n    else:\n        raise ValueError('Either provide a `basis` or `x` and `y`.')\n    return axs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nscattering the log - array into a single object.", "response": "def _scatter_obs(\n        adata,\n        x=None,\n        y=None,\n        color=None,\n        use_raw=None,\n        layers='X',\n        sort_order=True,\n        alpha=None,\n        basis=None,\n        groups=None,\n        components=None,\n        projection='2d',\n        legend_loc='right margin',\n        legend_fontsize=None,\n        legend_fontweight=None,\n        color_map=None,\n        palette=None,\n        frameon=None,\n        right_margin=None,\n        left_margin=None,\n        size=None,\n        title=None,\n        show=None,\n        save=None,\n        ax=None):\n    \"\"\"See docstring of scatter.\"\"\"\n    sanitize_anndata(adata)\n    from scipy.sparse import issparse\n    if use_raw is None and adata.raw is not None: use_raw = True\n\n    # process layers\n    if layers is None:\n        layers = 'X'\n    if isinstance(layers, str) and (layers == 'X' or layers in adata.layers.keys()):\n        layers = (layers, layers, layers)\n    elif isinstance(layers, (tuple, list)) and len(layers) == 3:\n        for layer in layers:\n            if layer not in adata.layers.keys() and layer != 'X':\n                raise ValueError(\n                    '`layers` should have elements that are either \\'X\\' or in adata.layers.keys().')\n    else:\n        raise ValueError('`layers` should be a string or a list/tuple of length 3.')\n    if use_raw and (layers != ('X', 'X', 'X') or layers != ['X', 'X', 'X']):\n        ValueError('`use_raw` must be `False` if layers other than \\'X\\' are used.')\n\n    if legend_loc not in VALID_LEGENDLOCS:\n        raise ValueError(\n            'Invalid `legend_loc`, need to be one of: {}.'.format(VALID_LEGENDLOCS))\n    if components is None: components = '1,2' if '2d' in projection else '1,2,3'\n    if isinstance(components, str): components = components.split(',')\n    components = np.array(components).astype(int) - 1\n    keys = ['grey'] if color is None else [color] if isinstance(color, str) else color\n    if title is not None and isinstance(title, str):\n        title = [title]\n    highlights = adata.uns['highlights'] if 'highlights' in adata.uns else []\n    if basis is not None:\n        try:\n            # ignore the '0th' diffusion component\n            if basis == 'diffmap': components += 1\n            Y = adata.obsm['X_' + basis][:, components]\n            # correct the component vector for use in labeling etc.\n            if basis == 'diffmap': components -= 1\n        except KeyError:\n            raise KeyError('compute coordinates using visualization tool {} first'\n                           .format(basis))\n    elif x is not None and y is not None:\n        x_arr = adata._get_obs_array(x, use_raw=use_raw, layer=layers[0])\n        y_arr = adata._get_obs_array(y, use_raw=use_raw, layer=layers[1])\n\n        x_arr = x_arr.toarray().flatten() if issparse(x_arr) else x_arr\n        y_arr = y_arr.toarray().flatten() if issparse(y_arr) else y_arr\n\n        Y = np.c_[x_arr[:, None], y_arr[:, None]]\n    else:\n        raise ValueError('Either provide a `basis` or `x` and `y`.')\n\n    if size is None:\n        n = Y.shape[0]\n        size = 120000 / n\n\n    if legend_loc.startswith('on data') and legend_fontsize is None:\n        legend_fontsize = rcParams['legend.fontsize']\n    elif legend_fontsize is None:\n        legend_fontsize = rcParams['legend.fontsize']\n\n    palette_was_none = False\n    if palette is None: palette_was_none = True\n    if isinstance(palette, list):\n        if not is_color_like(palette[0]):\n            palettes = palette\n        else:\n            palettes = [palette]\n    else:\n        palettes = [palette for i in range(len(keys))]\n    for i, palette in enumerate(palettes):\n        palettes[i] = utils.default_palette(palette)\n\n    if basis is not None:\n        component_name = (\n            'DC' if basis == 'diffmap'\n            else 'tSNE' if basis == 'tsne'\n            else 'UMAP' if basis == 'umap'\n            else 'PC' if basis == 'pca'\n            else basis.replace('draw_graph_', '').upper() if 'draw_graph' in basis\n            else basis)\n    else:\n        component_name = None\n    axis_labels = (x, y) if component_name is None else None\n    show_ticks = True if component_name is None else False\n\n    # generate the colors\n    color_ids = []\n    categoricals = []\n    colorbars = []\n    for ikey, key in enumerate(keys):\n        c = 'white'\n        categorical = False  # by default, assume continuous or flat color\n        colorbar = None\n        # test whether we have categorial or continuous annotation\n        if key in adata.obs_keys():\n            if is_categorical_dtype(adata.obs[key]):\n                categorical = True\n            else:\n                c = adata.obs[key]\n        # coloring according to gene expression\n        elif (use_raw\n              and adata.raw is not None\n              and key in adata.raw.var_names):\n            c = adata.raw[:, key].X\n        elif key in adata.var_names:\n            c = adata[:, key].X if layers[2] == 'X' else adata[:, key].layers[layers[2]]\n            c = c.toarray().flatten() if issparse(c) else c\n        elif is_color_like(key):  # a flat color\n            c = key\n            colorbar = False\n        else:\n            raise ValueError(\n                'key \\'{}\\' is invalid! pass valid observation annotation, '\n                'one of {} or a gene name {}'\n                .format(key, adata.obs_keys(), adata.var_names))\n        if colorbar is None:\n            colorbar = not categorical\n        colorbars.append(colorbar)\n        if categorical: categoricals.append(ikey)\n        color_ids.append(c)\n\n    if right_margin is None and len(categoricals) > 0:\n        if legend_loc == 'right margin': right_margin = 0.5\n    if title is None and keys[0] is not None:\n        title = [key.replace('_', ' ') if not is_color_like(key) else '' for key in keys]\n\n    axs = scatter_base(Y,\n                       title=title,\n                       alpha=alpha,\n                       component_name=component_name,\n                       axis_labels=axis_labels,\n                       component_indexnames=components + 1,\n                       projection=projection,\n                       colors=color_ids,\n                       highlights=highlights,\n                       colorbars=colorbars,\n                       right_margin=right_margin,\n                       left_margin=left_margin,\n                       sizes=[size for c in keys],\n                       color_map=color_map,\n                       show_ticks=show_ticks,\n                       ax=ax)\n\n    def add_centroid(centroids, name, Y, mask):\n        Y_mask = Y[mask]\n        if Y_mask.shape[0] == 0: return\n        median = np.median(Y_mask, axis=0)\n        i = np.argmin(np.sum(np.abs(Y_mask - median), axis=1))\n        centroids[name] = Y_mask[i]\n\n    # loop over all categorical annotation and plot it\n    for i, ikey in enumerate(categoricals):\n        palette = palettes[i]\n        key = keys[ikey]\n        utils.add_colors_for_categorical_sample_annotation(\n            adata, key, palette, force_update_colors=not palette_was_none)\n        # actually plot the groups\n        mask_remaining = np.ones(Y.shape[0], dtype=bool)\n        centroids = {}\n        if groups is None:\n            for iname, name in enumerate(adata.obs[key].cat.categories):\n                if name not in settings.categories_to_ignore:\n                    mask = scatter_group(axs[ikey], key, iname,\n                                         adata, Y, projection, size=size, alpha=alpha)\n                    mask_remaining[mask] = False\n                    if legend_loc.startswith('on data'): add_centroid(centroids, name, Y, mask)\n        else:\n            groups = [groups] if isinstance(groups, str) else groups\n            for name in groups:\n                if name not in set(adata.obs[key].cat.categories):\n                    raise ValueError('\"' + name + '\" is invalid!'\n                                     + ' specify valid name, one of '\n                                     + str(adata.obs[key].cat.categories))\n                else:\n                    iname = np.flatnonzero(adata.obs[key].cat.categories.values == name)[0]\n                    mask = scatter_group(axs[ikey], key, iname,\n                                         adata, Y, projection, size=size, alpha=alpha)\n                    if legend_loc.startswith('on data'): add_centroid(centroids, name, Y, mask)\n                    mask_remaining[mask] = False\n        if mask_remaining.sum() > 0:\n            data = [Y[mask_remaining, 0], Y[mask_remaining, 1]]\n            if projection == '3d': data.append(Y[mask_remaining, 2])\n            axs[ikey].scatter(*data, marker='.', c='lightgrey', s=size,\n                                    edgecolors='none', zorder=-1)\n        legend = None\n        if legend_loc.startswith('on data'):\n            if legend_fontweight is None:\n                legend_fontweight = 'bold'\n            for name, pos in centroids.items():\n                axs[ikey].text(pos[0], pos[1], name,\n                               weight=legend_fontweight,\n                               verticalalignment='center',\n                               horizontalalignment='center',\n                               fontsize=legend_fontsize)\n            all_pos = np.zeros((len(adata.obs[key].cat.categories), 2))\n            for iname, name in enumerate(adata.obs[key].cat.categories):\n                if name in centroids:\n                    all_pos[iname] = centroids[name]\n                else:\n                    all_pos[iname] = [np.nan, np.nan]\n            utils._tmp_cluster_pos = all_pos\n            if legend_loc == 'on data export':\n                filename = settings.writedir + 'pos.csv'\n                logg.msg('exporting label positions to {}'.format(filename), v=1)\n                if settings.writedir != '' and not os.path.exists(settings.writedir):\n                    os.makedirs(settings.writedir)\n                np.savetxt(filename, all_pos, delimiter=',')\n        elif legend_loc == 'right margin':\n            legend = axs[ikey].legend(\n                frameon=False, loc='center left',\n                bbox_to_anchor=(1, 0.5),\n                ncol=(1 if len(adata.obs[key].cat.categories) <= 14\n                      else 2 if len(adata.obs[key].cat.categories) <= 30 else 3),\n                fontsize=legend_fontsize)\n        elif legend_loc != 'none':\n            legend = axs[ikey].legend(\n                frameon=False, loc=legend_loc, fontsize=legend_fontsize)\n        if legend is not None:\n            for handle in legend.legendHandles: handle.set_sizes([300.0])\n\n    # draw a frame around the scatter\n    frameon = settings._frameon if frameon is None else frameon\n    if not frameon and x is None and y is None:\n        for ax in axs:\n            ax.set_xlabel('')\n            ax.set_ylabel('')\n            ax.set_frame_on(False)\n\n    utils.savefig_or_show('scatter' if basis is None else basis, show=show, save=save)\n    if show == False: return axs if len(keys) > 1 else axs[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot the ranking of a set of items in a single object.", "response": "def ranking(adata, attr, keys, dictionary=None, indices=None,\n            labels=None, color='black', n_points=30,\n            log=False, show=None):\n    \"\"\"Plot rankings.\n\n    See, for example, how this is used in pl.pca_ranking.\n\n    Parameters\n    ----------\n    adata : AnnData\n        The data.\n    attr : {'var', 'obs', 'uns', 'varm', 'obsm'}\n        The attribute of AnnData that contains the score.\n    keys : str or list of str\n        The scores to look up an array from the attribute of adata.\n\n    Returns\n    -------\n    Returns matplotlib gridspec with access to the axes.\n    \"\"\"\n    if isinstance(keys, str) and indices is not None:\n        scores = getattr(adata, attr)[keys][:, indices]\n        keys = ['{}{}'.format(keys[:-1], i+1) for i in indices]\n    else:\n        if dictionary is None:\n            scores = getattr(adata, attr)[keys]\n        else:\n            scores = getattr(adata, attr)[dictionary][keys]\n    n_panels = len(keys) if isinstance(keys, list) else 1\n    if n_panels == 1: scores, keys = scores[:, None], [keys]\n    if log: scores = np.log(scores)\n    if labels is None:\n        labels = adata.var_names if attr in {'var', 'varm'} else np.arange(scores.shape[0]).astype(str)\n    if isinstance(labels, str):\n        labels = [labels + str(i+1) for i in range(scores.shape[0])]\n    if n_panels <= 5: n_rows, n_cols = 1, n_panels\n    else: n_rows, n_cols = 2, int(n_panels/2 + 0.5)\n    fig = pl.figure(figsize=(n_cols * rcParams['figure.figsize'][0],\n                             n_rows * rcParams['figure.figsize'][1]))\n    left, bottom = 0.2/n_cols, 0.13/n_rows\n    gs = gridspec.GridSpec(nrows=n_rows, ncols=n_cols, wspace=0.2,\n                           left=left, bottom=bottom,\n                           right=1-(n_cols-1)*left-0.01/n_cols,\n                           top=1-(n_rows-1)*bottom-0.1/n_rows)\n    for iscore, score in enumerate(scores.T):\n        pl.subplot(gs[iscore])\n        indices = np.argsort(score)[::-1][:n_points+1]\n        for ig, g in enumerate(indices):\n            pl.text(ig, score[g], labels[g], color=color,\n                    rotation='vertical', verticalalignment='bottom',\n                    horizontalalignment='center', fontsize=8)\n        pl.title(keys[iscore].replace('_', ' '))\n        if n_panels <= 5 or iscore > n_cols: pl.xlabel('ranking')\n        pl.xlim(-0.9, ig + 0.9)\n        score_min, score_max = np.min(score[indices]), np.max(score[indices])\n        pl.ylim((0.95 if score_min > 0 else 1.05) * score_min,\n                (1.05 if score_max > 0 else 0.95) * score_max)\n    if show == False: return gs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef violin(adata, keys, groupby=None, log=False, use_raw=None, stripplot=True, jitter=True,\n           size=1, scale='width', order=None, multi_panel=None, show=None,\n           xlabel='', rotation=None, save=None, ax=None, **kwds):\n    \"\"\"\\\n    Violin plot.\n\n    Wraps `seaborn.violinplot` for :class:`~anndata.AnnData`.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    keys : `str` or list of `str`\n        Keys for accessing variables of `.var_names` or fields of `.obs`.\n    groupby : `str` or `None`, optional (default: `None`)\n        The key of the observation grouping to consider.\n    log : `bool`, optional (default: `False`)\n        Plot on logarithmic axis.\n    use_raw : `bool`, optional (default: `None`)\n        Use `raw` attribute of `adata` if present.\n    multi_panel : `bool`, optional (default: `False`)\n        Display keys in multiple panels also when `groupby is not None`.\n    stripplot : `bool` optional (default: `True`)\n        Add a stripplot on top of the violin plot.\n        See `seaborn.stripplot`.\n    jitter : `float` or `bool`, optional (default: `True`)\n        Add jitter to the stripplot (only when stripplot is True)\n        See `seaborn.stripplot`.\n    size : int, optional (default: 1)\n        Size of the jitter points.\n    order : list of str, optional (default: `True`)\n        Order in which to show the categories.\n    scale : {{'area', 'count', 'width'}}, optional (default: 'width')\n        The method used to scale the width of each violin. If 'area', each\n        violin will have the same area. If 'count', the width of the violins\n        will be scaled by the number of observations in that bin. If 'width',\n        each violin will have the same width.\n    xlabel : `str`, optional (default: `''`)\n        Label of the x axis. Defaults to `groupby` if `rotation` is `None`,\n        otherwise, no label is shown.\n    rotation : `float`, optional (default: `None`)\n        Rotation of xtick labels.\n    {show_save_ax}\n    **kwds : keyword arguments\n        Are passed to `seaborn.violinplot`.\n\n    Returns\n    -------\n    A :class:`~matplotlib.axes.Axes` object if `ax` is `None` else `None`.\n    \"\"\"\n    sanitize_anndata(adata)\n    if use_raw is None and adata.raw is not None: use_raw = True\n    if isinstance(keys, str): keys = [keys]\n    obs_keys = False\n    for key in keys:\n        if key in adata.obs_keys(): obs_keys = True\n        if obs_keys and key not in set(adata.obs_keys()):\n            raise ValueError(\n                'Either use observation keys or variable names, but do not mix. '\n                'Did not find {} in adata.obs_keys().'.format(key))\n    if obs_keys:\n        obs_df = adata.obs\n    else:\n        if groupby is None: obs_df = pd.DataFrame()\n        else: obs_df = pd.DataFrame(adata.obs[groupby])\n        for key in keys:\n            if adata.raw is not None and use_raw:\n                X_col = adata.raw[:, key].X\n            else:\n                X_col = adata[:, key].X\n            obs_df[key] = X_col\n    if groupby is None:\n        obs_tidy = pd.melt(obs_df, value_vars=keys)\n        x = 'variable'\n        ys = ['value']\n    else:\n        obs_tidy = obs_df\n        x = groupby\n        ys = keys\n    if multi_panel:\n        if groupby is None and len(ys) == 1:\n            # This is a quick and dirty way for adapting scales across several\n            # keys if groupby is None.\n            y = ys[0]\n            g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False)\n            # don't really know why this gives a warning without passing `order`\n            g = g.map(sns.violinplot, y, inner=None, orient='vertical',\n                      scale=scale, order=keys, **kwds)\n            if stripplot:\n                g = g.map(sns.stripplot, y, orient='vertical', jitter=jitter, size=size, order=keys,\n                          color='black')\n            if log:\n                g.set(yscale='log')\n            g.set_titles(col_template='{col_name}').set_xlabels('')\n            if rotation is not None:\n                for ax in g.axes[0]:\n                    ax.tick_params(labelrotation=rotation)\n\n    else:\n        if ax is None:\n            axs, _, _, _ = setup_axes(\n                ax=ax, panels=['x'] if groupby is None else keys, show_ticks=True, right_margin=0.3)\n        else:\n            axs = [ax]\n        for ax, y in zip(axs, ys):\n            ax = sns.violinplot(x, y=y, data=obs_tidy, inner=None, order=order,\n                                orient='vertical', scale=scale, ax=ax, **kwds)\n            if stripplot:\n                ax = sns.stripplot(x, y=y, data=obs_tidy, order=order,\n                                   jitter=jitter, color='black', size=size, ax=ax)\n            if xlabel == '' and groupby is not None and rotation is None:\n                xlabel = groupby.replace('_', ' ')\n            ax.set_xlabel(xlabel)\n            if log:\n                ax.set_yscale('log')\n            if rotation is not None:\n                ax.tick_params(labelrotation=rotation)\n    utils.savefig_or_show('violin', show=show, save=save)\n    if show is False:\n        if multi_panel:\n            return g\n        elif len(axs) == 1:\n            return axs[0]\n        else:\n            return axs", "response": "\\ Seaborn Violin plot."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stacked_violin(adata, var_names, groupby=None, log=False, use_raw=None, num_categories=7,\n                   figsize=None,  dendrogram=False, gene_symbols=None,\n                   var_group_positions=None, var_group_labels=None, standard_scale=None,\n                   var_group_rotation=None, layer=None, stripplot=False, jitter=False, size=1,\n                   scale='width', order=None, swap_axes=False, show=None, save=None,\n                   row_palette='muted', **kwds):\n    \"\"\"\\\n    Stacked violin plots.\n\n    Makes a compact image composed of individual violin plots (from `seaborn.violinplot`)\n    stacked on top of each other. Useful to visualize gene expression per cluster.\n\n    Wraps `seaborn.violinplot` for :class:`~anndata.AnnData`.\n\n    Parameters\n    ----------\n    {common_plot_args}\n    stripplot : `bool` optional (default: `True`)\n        Add a stripplot on top of the violin plot.\n        See `seaborn.stripplot`.\n    jitter : `float` or `bool`, optional (default: `True`)\n        Add jitter to the stripplot (only when stripplot is True)\n        See `seaborn.stripplot`.\n    size : int, optional (default: 1)\n        Size of the jitter points.\n    order : list of str, optional (default: `True`)\n        Order in which to show the categories.\n    scale : {{'area', 'count', 'width'}}, optional (default: 'width')\n        The method used to scale the width of each violin. If 'area', each\n        violin will have the same area. If 'count', the width of the violins\n        will be scaled by the number of observations in that bin. If 'width',\n        each violin will have the same width.\n    row_palette: `str` (default: `muted`)\n        The row palette determines the colors to use in each of the stacked violin plots. The value\n        should be a valid seaborn palette name or a valic matplotlib colormap\n        (see https://seaborn.pydata.org/generated/seaborn.color_palette.html). Alternatively,\n        a single color name or hex value can be passed. E.g. 'red' or '#cc33ff'\n    standard_scale : {{'var', 'obs'}}, optional (default: None)\n        Whether or not to standardize that dimension between 0 and 1, meaning for each variable or observation,\n        subtract the minimum and divide each by its maximum.\n    swap_axes: `bool`, optional (default: `False`)\n         By default, the x axis contains `var_names` (e.g. genes) and the y axis the `groupby` categories.\n         By setting `swap_axes` then x are the `groupby` categories and y the `var_names`. When swapping\n         axes var_group_positions are no longer used\n    {show_save_ax}\n    **kwds : keyword arguments\n        Are passed to `seaborn.violinplot`.\n\n    Returns\n    -------\n    List of :class:`~matplotlib.axes.Axes`\n\n    Examples\n    -------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.pl.stacked_violin(adata, ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ'],\n    ...                      groupby='bulk_labels', dendrogram=True)\n\n    \"\"\"\n    if use_raw is None and adata.raw is not None: use_raw = True\n    has_var_groups = True if var_group_positions is not None and len(var_group_positions) > 0 else False\n    if isinstance(var_names, str):\n        var_names = [var_names]\n    categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories,\n                                              gene_symbols=gene_symbols, layer=layer)\n\n    if standard_scale == 'obs':\n        obs_tidy = obs_tidy.sub(obs_tidy.min(1), axis=0)\n        obs_tidy = obs_tidy.div(obs_tidy.max(1), axis=0).fillna(0)\n    elif standard_scale == 'var':\n        obs_tidy -= obs_tidy.min(0)\n        obs_tidy = (obs_tidy / obs_tidy.max(0)).fillna(0)\n    elif standard_scale is None:\n        pass\n    else:\n        logg.warn('Unknown type for standard_scale, ignored')\n\n\n    if 'color' in kwds:\n        row_palette = kwds['color']\n        # remove color from kwds in case is set to avoid an error caused by\n        # double parameters\n        del (kwds['color'])\n    if 'linewidth' not in kwds:\n        # for the tiny violin plots used, is best\n        # to use a thin lindwidth.\n        kwds['linewidth'] = 0.5\n    if groupby is None or len(categories) <= 1:\n        # dendrogram can only be computed  between groupby categories\n        dendrogram = False\n\n    if dendrogram:\n        dendro_data = _reorder_categories_after_dendrogram(adata, groupby, dendrogram,\n                                                           var_names=var_names,\n                                                           var_group_labels=var_group_labels,\n                                                           var_group_positions=var_group_positions)\n\n        var_group_labels = dendro_data['var_group_labels']\n        var_group_positions = dendro_data['var_group_positions']\n\n        # reorder obs_tidy\n        if dendro_data['var_names_idx_ordered'] is not None:\n            obs_tidy = obs_tidy.iloc[:, dendro_data['var_names_idx_ordered']]\n            var_names = [var_names[x] for x in dendro_data['var_names_idx_ordered']]\n\n        obs_tidy.index = obs_tidy.index.reorder_categories(\n            [categories[x] for x in dendro_data['categories_idx_ordered']], ordered=True)\n        categories = [categories[x] for x in dendro_data['categories_idx_ordered']]\n\n    global count\n    count = 0\n\n    def rename_cols_to_int(value):\n        global count\n        count += 1\n        return count\n\n    # All columns should have a unique name, otherwise the\n    # pd.melt object that is passed to seaborn will merge non-unique columns.\n    # Here, I simply rename the columns using a count from 1..n using the\n    # mapping function `rename_cols_to_int` to solve the problem.\n    obs_tidy.rename(rename_cols_to_int, axis='columns', inplace=True)\n\n    if not swap_axes:\n        # plot image in which x = var_names and y = groupby categories\n\n        dendro_width = 1.4 if dendrogram else 0\n\n        if figsize is None:\n            height = len(categories) * 0.2 + 3\n            width = len(var_names) * 0.2 + 1 + dendro_width\n        else:\n            width, height = figsize\n\n        num_rows = len(categories)\n        height_ratios = None\n        if has_var_groups:\n            # add some space in case 'brackets' want to be plotted on top of the image\n            num_rows += 2  # +2 to add the row for the brackets and a spacer\n            height_ratios = [0.2, 0.05] + [float(height) / len(categories)] * len(categories)\n            categories = [None, None] + list(categories)\n        fig = pl.figure(figsize=(width, height))\n\n        # define a layout of nrows = len(categories) rows x 2 columns\n        # each row is one violin plot. Second column is reserved for dendrogram (if any)\n        # if var_group_positions is defined, a new row is added\n        axs = gridspec.GridSpec(nrows=num_rows, ncols=2, height_ratios=height_ratios,\n                                width_ratios=[width, dendro_width], wspace=0.08)\n        axs_list = []\n        if dendrogram:\n            first_plot_idx = 1 if has_var_groups else 0\n            dendro_ax = fig.add_subplot(axs[first_plot_idx:, 1])\n            _plot_dendrogram(dendro_ax, adata, groupby, dendrogram_key=dendrogram)\n            axs_list.append(dendro_ax)\n        ax0 = None\n        if is_color_like(row_palette):\n            row_colors = [row_palette] * len(categories)\n        else:\n            row_colors = sns.color_palette(row_palette, n_colors=len(categories))\n        for idx in range(num_rows)[::-1]:  # iterate in reverse to start on the bottom plot\n                                           # this facilitates adding the brackets plot (if\n                                           # needed) by sharing the x axis with a previous\n                                           # violing plot.\n            category = categories[idx]\n            if has_var_groups and idx <= 1:\n                # if var_group_positions is given, axs[0] and axs[1] are the location for the\n                # brackets and a spacer (axs[1])\n                if idx == 0:\n                    brackets_ax = fig.add_subplot(axs[0], sharex=ax0)\n                    _plot_gene_groups_brackets(brackets_ax, group_positions=var_group_positions,\n                                               group_labels=var_group_labels,\n                                               rotation=var_group_rotation)\n\n                continue\n\n            df = pd.melt(obs_tidy[obs_tidy.index == category], value_vars=obs_tidy.columns)\n            if ax0 is None:\n                ax = fig.add_subplot(axs[idx, 0])\n                ax0 = ax\n\n            else:\n                ax = fig.add_subplot(axs[idx, 0])\n\n            axs_list.append(ax)\n\n            ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,\n                                orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds)\n\n            if stripplot:\n                ax = sns.stripplot('variable', y='value', data=df, order=order,\n                                   jitter=jitter, color='black', size=size, ax=ax)\n\n            # remove the grids because in such a compact plot are unnecessary\n            ax.grid(False)\n\n            ax.tick_params(axis='y', left=False, right=True, labelright=True,\n                           labelleft=False, labelsize='x-small', length=1, pad=1)\n\n            ax.set_ylabel(category, rotation=0, fontsize='small', labelpad=8, ha='right', va='center')\n            ax.set_xlabel('')\n            if log:\n                ax.set_yscale('log')\n\n            if idx < num_rows - 1:\n                # remove the xticks labels except for the last processed plot (first from bottom-up).\n                # Because the plots share the x axis it is redundant and less compact to plot the\n                # axis ticks and labels for each plot\n                ax.set_xticklabels([])\n                ax.tick_params(axis='x', bottom=False, top=False, labeltop=False, labelbottom=False)\n            else:\n                ax.set_xticklabels(var_names)\n\n        ax0.tick_params(axis='x', labelrotation=90, labelsize='small')\n\n    else:\n        # plot image in which x = group by and y = var_names\n        dendro_height = 3 if dendrogram else 0\n        vargroups_width = 0.45 if has_var_groups else 0\n        if figsize is None:\n            height = len(var_names) * 0.3 + dendro_height\n            width = len(categories) * 0.4 + vargroups_width\n        else:\n            width, height = figsize\n\n        fig = pl.figure(figsize=(width, height))\n\n        # define a layout of nrows = var_names x 1 columns\n        # if plot dendrogram a row is added\n        # each row is one violin plot.\n        num_rows = len(var_names) + 1 # +1 to account for dendrogram\n        height_ratios = [dendro_height] + ([1] * len(var_names))\n\n        axs = gridspec.GridSpec(nrows=num_rows, ncols=2,\n                                height_ratios=height_ratios, wspace=0.2,\n                                width_ratios=[width - vargroups_width, vargroups_width])\n\n        axs_list = []\n        if dendrogram:\n            dendro_ax = fig.add_subplot(axs[0])\n            _plot_dendrogram(dendro_ax, adata, groupby, orientation='top', dendrogram_key=dendrogram)\n            axs_list.append(dendro_ax)\n        first_ax = None\n        if is_color_like(row_palette):\n            row_colors = [row_palette] * len(var_names)\n        else:\n            row_colors = sns.color_palette(row_palette, n_colors=len(var_names))\n        for idx, y in enumerate(obs_tidy.columns):\n            ax_idx = idx + 1  # +1 to account that idx 0 is the dendrogram\n            if first_ax is None:\n                ax = fig.add_subplot(axs[ax_idx, 0])\n                first_ax = ax\n            else:\n                ax = fig.add_subplot(axs[ax_idx, 0])\n            axs_list.append(ax)\n            ax = sns.violinplot(x=obs_tidy.index, y=y, data=obs_tidy, inner=None, order=order,\n                                orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds)\n            if stripplot:\n                ax = sns.stripplot(x=obs_tidy.index, y=y, data=obs_tidy, order=order,\n                                   jitter=jitter, color='black', size=size, ax=ax)\n\n            ax.set_ylabel(var_names[idx], rotation=0, fontsize='small', labelpad=8, ha='right', va='center')\n            # remove the grids because in such a compact plot are unnecessary\n            ax.grid(False)\n            ax.tick_params(axis='y', right=True, labelright=True, left=False,\n                           labelleft=False, labelrotation=0, labelsize='x-small')\n            ax.tick_params(axis='x', labelsize='small')\n\n            # remove the xticks labels except for the last processed plot (first from bottom-up).\n            # Because the plots share the x axis it is redundant and less compact to plot the\n            # axis for each plot\n            if idx < len(var_names) - 1:\n                ax.tick_params(labelbottom=False, labeltop=False, bottom=False, top=False)\n                ax.set_xlabel('')\n            if log:\n                ax.set_yscale('log')\n\n            if max([len(x) for x in categories]) > 1:\n                ax.tick_params(axis='x', labelrotation=90)\n\n        if has_var_groups:\n            start = 1 if dendrogram else 0\n            gene_groups_ax = fig.add_subplot(axs[start:, 1])\n            arr = []\n            for idx, pos in enumerate(var_group_positions):\n                arr += [idx] * (pos[1]+1 - pos[0])\n            _plot_gene_groups_brackets(gene_groups_ax, var_group_positions, var_group_labels,\n                                       left_adjustment=0.3, right_adjustment=0.7, orientation='right')\n            gene_groups_ax.set_ylim(len(var_names), 0)\n            axs_list.append(gene_groups_ax)\n\n    # remove the spacing between subplots\n    pl.subplots_adjust(wspace=0, hspace=0)\n\n    utils.savefig_or_show('stacked_violin', show=show, save=save)\n\n    return axs_list", "response": "Generates a compact image composed of individual violin plots."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef heatmap(adata, var_names, groupby=None, use_raw=None, log=False, num_categories=7,\n            dendrogram=False, gene_symbols=None, var_group_positions=None, var_group_labels=None,\n            var_group_rotation=None, layer=None, standard_scale=None, swap_axes=False,\n            show_gene_labels=None, show=None, save=None, figsize=None, **kwds):\n    \"\"\"\\\n    Heatmap of the expression values of genes.\n\n    If `groupby` is given, the heatmap is ordered by the respective group. For\n    example, a list of marker genes can be plotted, ordered by clustering. If\n    the `groupby` observation annotation is not categorical the observation\n    annotation is turned into a categorical by binning the data into the number\n    specified in `num_categories`.\n\n    Parameters\n    ----------\n    {common_plot_args}\n    standard_scale : {{'var', 'obs'}}, optional (default: None)\n        Whether or not to standardize that dimension between 0 and 1, meaning for each variable or observation,\n        subtract the minimum and divide each by its maximum.\n    swap_axes: `bool`, optional (default: `False`)\n         By default, the x axis contains `var_names` (e.g. genes) and the y axis the `groupby`\n         categories (if any). By setting `swap_axes` then x are the `groupby` categories and y the `var_names`.\n    show_gene_labels: `bool`, optional (default: `None`).\n         By default gene labels are shown when there are 50 or less genes. Otherwise the labels are removed.\n    {show_save_ax}\n    **kwds : keyword arguments\n        Are passed to `matplotlib.imshow`.\n\n    Returns\n    -------\n    List of :class:`~matplotlib.axes.Axes`\n\n    Examples\n    -------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.pl.heatmap(adata, ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ'],\n    ...               groupby='bulk_labels', dendrogram=True, swap_axes=True)\n    \"\"\"\n    if use_raw is None and adata.raw is not None: use_raw = True\n    if isinstance(var_names, str):\n        var_names = [var_names]\n\n    categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories,\n                                              gene_symbols=gene_symbols, layer=layer)\n\n    if standard_scale == 'obs':\n        obs_tidy = obs_tidy.sub(obs_tidy.min(1), axis=0)\n        obs_tidy = obs_tidy.div(obs_tidy.max(1), axis=0).fillna(0)\n    elif standard_scale == 'var':\n        obs_tidy -= obs_tidy.min(0)\n        obs_tidy = (obs_tidy / obs_tidy.max(0)).fillna(0)\n    elif standard_scale is None:\n        pass\n    else:\n        logg.warn('Unknown type for standard_scale, ignored')\n\n    if groupby is None or len(categories) <= 1:\n        categorical = False\n        # dendrogram can only be computed  between groupby categories\n        dendrogram = False\n    else:\n        categorical = True\n        # get categories colors:\n        if groupby + \"_colors\" in adata.uns:\n            groupby_colors = adata.uns[groupby + \"_colors\"]\n        else:\n            groupby_colors = None\n\n    if dendrogram:\n        dendro_data = _reorder_categories_after_dendrogram(adata, groupby, dendrogram,\n                                                           var_names=var_names,\n                                                           var_group_labels=var_group_labels,\n                                                           var_group_positions=var_group_positions)\n\n        var_group_labels = dendro_data['var_group_labels']\n        var_group_positions = dendro_data['var_group_positions']\n\n        # reorder obs_tidy\n        if dendro_data['var_names_idx_ordered'] is not None:\n            obs_tidy = obs_tidy.iloc[:, dendro_data['var_names_idx_ordered']]\n            var_names = [var_names[x] for x in dendro_data['var_names_idx_ordered']]\n\n        obs_tidy.index = obs_tidy.index.reorder_categories(\n            [categories[x] for x in dendro_data['categories_idx_ordered']], ordered=True)\n\n        # reorder groupby colors\n        if groupby_colors is not None:\n            groupby_colors = [groupby_colors[x] for x in dendro_data['categories_idx_ordered']]\n\n    if show_gene_labels is None:\n        if len(var_names) <= 50:\n            show_gene_labels = True\n        else:\n            show_gene_labels = False\n            logg.warn('Gene labels are not shown when more than 50 genes are visualized. To show '\n                      'gene labels set `show_gene_labels=True`')\n    if categorical:\n        obs_tidy = obs_tidy.sort_index()\n\n    goal_points = 1000\n    obs_tidy = _reduce_and_smooth(obs_tidy, goal_points)\n    colorbar_width = 0.2\n\n    if not swap_axes:\n        # define a layout of 2 rows x 4 columns\n        # first row is for 'brackets' (if no brackets needed, the height of this row is zero)\n        # second row is for main content. This second row is divided into three axes:\n        #   first ax is for the categories defined by `groupby`\n        #   second ax is for the heatmap\n        #   third ax is for the dendrogram\n        #   fourth ax is for colorbar\n\n        dendro_width = 1 if dendrogram else 0\n        groupby_width = 0.2 if categorical else 0\n        if figsize is None:\n            height = 6\n            if show_gene_labels:\n                heatmap_width = len(var_names) * 0.3\n            else:\n                heatmap_width = 8\n            width = heatmap_width + dendro_width + groupby_width\n        else:\n            width, height = figsize\n            heatmap_width = width - (dendro_width + groupby_width)\n\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            # add some space in case 'brackets' want to be plotted on top of the image\n            height_ratios = [0.15, height]\n        else:\n            height_ratios = [0, height]\n\n        width_ratios = [groupby_width, heatmap_width, dendro_width, colorbar_width]\n        fig = pl.figure(figsize=(width, height))\n\n        axs = gridspec.GridSpec(nrows=2, ncols=4, width_ratios=width_ratios, wspace=0.15 / width,\n                                hspace=0.13 / height, height_ratios=height_ratios)\n\n        heatmap_ax = fig.add_subplot(axs[1, 1])\n        im = heatmap_ax.imshow(obs_tidy.values, aspect='auto', **kwds)\n        heatmap_ax.set_ylim(obs_tidy.shape[0] - 0.5, -0.5)\n        heatmap_ax.set_xlim(-0.5, obs_tidy.shape[1] - 0.5)\n        heatmap_ax.tick_params(axis='y', left=False, labelleft=False)\n        heatmap_ax.set_ylabel('')\n        heatmap_ax.grid(False)\n\n        # sns.heatmap(obs_tidy, yticklabels=\"auto\", ax=heatmap_ax, cbar_ax=heatmap_cbar_ax, **kwds)\n        if show_gene_labels:\n            heatmap_ax.tick_params(axis='x', labelsize='small')\n            heatmap_ax.set_xticks(np.arange(len(var_names)))\n            heatmap_ax.set_xticklabels(var_names, rotation=90)\n        else:\n            heatmap_ax.tick_params(axis='x', labelbottom=False, bottom=False)\n\n        # plot colorbar\n        _plot_colorbar(im, fig, axs[1, 3])\n\n        if categorical:\n            groupby_ax = fig.add_subplot(axs[1, 0])\n            ticks, labels, groupby_cmap, norm = _plot_categories_as_colorblocks(groupby_ax, obs_tidy,\n                                                                          colors=groupby_colors, orientation='left')\n\n            # add lines to main heatmap\n            line_positions = np.cumsum(obs_tidy.index.value_counts(sort=False))[:-1]\n            heatmap_ax.hlines(line_positions, -1, len(var_names) + 1, lw=0.5)\n\n        if dendrogram:\n            dendro_ax = fig.add_subplot(axs[1, 2], sharey=heatmap_ax)\n            _plot_dendrogram(dendro_ax, adata, groupby, ticks=ticks, dendrogram_key=dendrogram)\n\n        # plot group legends on top of heatmap_ax (if given)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            gene_groups_ax = fig.add_subplot(axs[0, 1], sharex=heatmap_ax)\n            _plot_gene_groups_brackets(gene_groups_ax, group_positions=var_group_positions,\n                                       group_labels=var_group_labels, rotation=var_group_rotation,\n                                       left_adjustment=-0.3, right_adjustment=0.3)\n\n    # swap axes case\n    else:\n        # define a layout of 3 rows x 3 columns\n        # The first row is for the dendrogram (if not dendrogram height is zero)\n        # second row is for main content. This col is divided into three axes:\n        #   first ax is for the heatmap\n        #   second ax is for 'brackets' if any (othwerise width is zero)\n        #   third ax is for colorbar\n\n        dendro_height = 0.8 if dendrogram else 0\n        groupby_height = 0.13 if categorical else 0\n        if figsize is None:\n            if show_gene_labels:\n                heatmap_height = len(var_names) * 0.18\n            else:\n                heatmap_height = 4\n            width = 10\n            height = heatmap_height + dendro_height + groupby_height  # +2 to account for labels\n        else:\n            width, height = figsize\n            heatmap_height = height - (dendro_height + groupby_height)\n\n        height_ratios = [dendro_height, heatmap_height, groupby_height]\n\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            # add some space in case 'brackets' want to be plotted on top of the image\n            width_ratios = [width, 0.14, colorbar_width]\n        else:\n            width_ratios = [width, 0, colorbar_width]\n\n        fig = pl.figure(figsize=(width, height))\n        axs = gridspec.GridSpec(nrows=3, ncols=3, wspace=0.25 / width,\n                                hspace=0.3 / height,\n                                width_ratios=width_ratios,\n                                height_ratios=height_ratios)\n\n        # plot heatmap\n        heatmap_ax = fig.add_subplot(axs[1, 0])\n\n        im = heatmap_ax.imshow(obs_tidy.T.values, aspect='auto', **kwds)\n        heatmap_ax.set_xlim(0, obs_tidy.shape[0])\n        heatmap_ax.set_ylim(obs_tidy.shape[1] - 0.5, -0.5)\n        heatmap_ax.tick_params(axis='x', bottom=False, labelbottom=False)\n        heatmap_ax.set_xlabel('')\n        heatmap_ax.grid(False)\n        if show_gene_labels:\n            heatmap_ax.tick_params(axis='y', labelsize='small', length=1)\n            heatmap_ax.set_yticks(np.arange(len(var_names)))\n            heatmap_ax.set_yticklabels(var_names, rotation=0)\n        else:\n            heatmap_ax.tick_params(axis='y', labelleft=False, left=False)\n\n        if categorical:\n            groupby_ax = fig.add_subplot(axs[2, 0])\n            ticks, labels, groupby_cmap, norm = _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors=groupby_colors,\n                                                                          orientation='bottom')\n            # add lines to main heatmap\n            line_positions = np.cumsum(obs_tidy.index.value_counts(sort=False))[:-1]\n            heatmap_ax.vlines(line_positions, -1, len(var_names) + 1, lw=0.5)\n\n        if dendrogram:\n            dendro_ax = fig.add_subplot(axs[0, 0], sharex=heatmap_ax)\n            _plot_dendrogram(dendro_ax, adata, groupby, dendrogram_key=dendrogram, ticks=ticks, orientation='top')\n\n        # plot group legends next to the heatmap_ax (if given)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            gene_groups_ax = fig.add_subplot(axs[1, 1])\n            arr = []\n            for idx, pos in enumerate(var_group_positions):\n                arr += [idx] * (pos[1]+1 - pos[0])\n\n            gene_groups_ax.imshow(np.matrix(arr).T, aspect='auto', cmap=groupby_cmap, norm=norm)\n            gene_groups_ax.axis('off')\n\n        # plot colorbar\n        _plot_colorbar(im, fig, axs[1, 2])\n\n    utils.savefig_or_show('heatmap', show=show, save=save)\n\n    return axs", "response": "Plots a single heatmap of the expression values of genes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef matrixplot(adata, var_names, groupby=None, use_raw=None, log=False, num_categories=7,\n               figsize=None, dendrogram=False, gene_symbols=None, var_group_positions=None, var_group_labels=None,\n               var_group_rotation=None, layer=None, standard_scale=None, swap_axes=False, show=None,\n               save=None, **kwds):\n\n    \"\"\"\\\n    Creates a heatmap of the mean expression values per cluster of each var_names\n    If groupby is not given, the matrixplot assumes that all data belongs to a single\n    category.\n\n    Parameters\n    ----------\n    {common_plot_args}\n    standard_scale : {{'var', 'group'}}, optional (default: None)\n        Whether or not to standardize that dimension between 0 and 1, meaning for each variable or group,\n        subtract the minimum and divide each by its maximum.\n    {show_save_ax}\n    **kwds : keyword arguments\n        Are passed to `matplotlib.pyplot.pcolor`.\n\n    Returns\n    -------\n    List of :class:`~matplotlib.axes.Axes`\n\n    Examples\n    --------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.pl.matrixplot(adata, ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ'],\n    ... groupby='bulk_labels', dendrogram=True)\n\n    \"\"\"\n\n    if use_raw is None and adata.raw is not None: use_raw = True\n    if isinstance(var_names, str):\n        var_names = [var_names]\n\n    categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories,\n                                              gene_symbols=gene_symbols, layer=layer)\n    if groupby is None or len(categories) <= 1:\n        # dendrogram can only be computed  between groupby categories\n        dendrogram = False\n\n    mean_obs = obs_tidy.groupby(level=0).mean()\n\n    if standard_scale == 'group':\n        mean_obs = mean_obs.sub(mean_obs.min(1), axis=0)\n        mean_obs = mean_obs.div(mean_obs.max(1), axis=0).fillna(0)\n    elif standard_scale == 'var':\n        mean_obs -= mean_obs.min(0)\n        mean_obs = (mean_obs / mean_obs.max(0)).fillna(0)\n    elif standard_scale is None:\n        pass\n    else:\n        logg.warn('Unknown type for standard_scale, ignored')\n\n    if dendrogram:\n        dendro_data = _reorder_categories_after_dendrogram(adata, groupby, dendrogram,\n                                                           var_names=var_names,\n                                                           var_group_labels=var_group_labels,\n                                                           var_group_positions=var_group_positions)\n\n        var_group_labels = dendro_data['var_group_labels']\n        var_group_positions = dendro_data['var_group_positions']\n\n        # reorder matrix\n        if dendro_data['var_names_idx_ordered'] is not None:\n            # reorder columns (usually genes) if needed. This only happens when\n            # var_group_positions and var_group_labels is set\n            mean_obs = mean_obs.iloc[:,dendro_data['var_names_idx_ordered']]\n\n        # reorder rows (categories) to match the dendrogram order\n        mean_obs = mean_obs.iloc[dendro_data['categories_idx_ordered'], :]\n\n    colorbar_width = 0.2\n\n    if not swap_axes:\n        dendro_width = 0.8 if dendrogram else 0\n        if figsize is None:\n            height = len(categories) * 0.2 + 1  # +1 for labels\n            heatmap_width = len(var_names) * 0.32\n            width = heatmap_width + dendro_width + colorbar_width  # +1.6 to account for the colorbar and  + 1 to account for labels\n        else:\n            width, height = figsize\n            heatmap_width = width - (dendro_width + colorbar_width)\n\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            # add some space in case 'brackets' want to be plotted on top of the image\n            height_ratios = [0.5, 10]\n            height += 0.5\n        else:\n            height_ratios = [0, 10.5]\n\n        # define a layout of 2 rows x 3 columns\n        # first row is for 'brackets' (if no brackets needed, the height of this row is zero)\n        # second row is for main content. This second row\n        # is divided into three axes:\n        #   first ax is for the main matrix figure\n        #   second ax is for the dendrogram\n        #   third ax is for the color bar legend\n        fig = pl.figure(figsize=(width, height))\n        axs = gridspec.GridSpec(nrows=2, ncols=3, wspace=0.02, hspace=0.04,\n                                width_ratios=[heatmap_width, dendro_width, colorbar_width],\n                                height_ratios=height_ratios)\n\n        matrix_ax = fig.add_subplot(axs[1, 0])\n        y_ticks = np.arange(mean_obs.shape[0]) + 0.5\n        matrix_ax.set_yticks(y_ticks)\n        matrix_ax.set_yticklabels([mean_obs.index[idx] for idx in range(mean_obs.shape[0])])\n\n        if dendrogram:\n            dendro_ax = fig.add_subplot(axs[1, 1], sharey=matrix_ax)\n            _plot_dendrogram(dendro_ax, adata, groupby, dendrogram_key=dendrogram, ticks=y_ticks)\n\n        pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds)\n\n        # invert y axis to show categories ordered from top to bottom\n        matrix_ax.set_ylim(mean_obs.shape[0], 0)\n\n        x_ticks = np.arange(mean_obs.shape[1]) + 0.5\n        matrix_ax.set_xticks(x_ticks)\n        matrix_ax.set_xticklabels([mean_obs.columns[idx] for idx in range(mean_obs.shape[1])], rotation=90)\n        matrix_ax.tick_params(axis='both', labelsize='small')\n        matrix_ax.grid(False)\n        matrix_ax.set_xlim(-0.5, len(var_names) + 0.5)\n        matrix_ax.set_ylabel(groupby)\n        matrix_ax.set_xlim(0, mean_obs.shape[1])\n\n        # plot group legends on top of matrix_ax (if given)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            gene_groups_ax = fig.add_subplot(axs[0, 0], sharex=matrix_ax)\n            _plot_gene_groups_brackets(gene_groups_ax, group_positions=var_group_positions,\n                                       group_labels=var_group_labels, rotation=var_group_rotation,\n                                       left_adjustment=0.2, right_adjustment=0.8)\n\n        # plot colorbar\n        _plot_colorbar(pc, fig, axs[1, 2])\n    else:\n        dendro_height = 0.5 if dendrogram else 0\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            # add some space in case 'color blocks' want to be plotted on the right of the image\n            vargroups_width = 0.4\n        else:\n            vargroups_width = 0\n\n        if figsize is None:\n            heatmap_height = len(var_names) * 0.2\n            height = dendro_height + heatmap_height + 1  # +1 for labels\n            heatmap_width = len(categories) * 0.3\n            width = heatmap_width + vargroups_width + colorbar_width\n        else:\n            width, height = figsize\n            heatmap_width = width - (vargroups_width + colorbar_width)\n            heatmap_height = height - dendro_height\n\n        # define a layout of 2 rows x 3 columns\n        # first row is for 'dendrogram' (if no dendrogram is plotted, the height of this row is zero)\n        # second row is for main content. This row\n        # is divided into three axes:\n        #   first ax is for the main matrix figure\n        #   second ax is for the groupby categories (eg. brackets)\n        #   third ax is for the color bar legend\n        fig = pl.figure(figsize=(width, height))\n        axs = gridspec.GridSpec(nrows=2, ncols=3, wspace=0.05, hspace=0.005,\n                                width_ratios=[heatmap_width, vargroups_width, colorbar_width],\n                                height_ratios=[dendro_height, heatmap_height])\n\n        mean_obs = mean_obs.T\n        matrix_ax = fig.add_subplot(axs[1, 0])\n        pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds)\n        y_ticks = np.arange(mean_obs.shape[0]) + 0.5\n        matrix_ax.set_yticks(y_ticks)\n        matrix_ax.set_yticklabels([mean_obs.index[idx] for idx in range(mean_obs.shape[0])])\n\n        x_ticks = np.arange(mean_obs.shape[1]) + 0.5\n        matrix_ax.set_xticks(x_ticks)\n        matrix_ax.set_xticklabels([mean_obs.columns[idx] for idx in range(mean_obs.shape[1])], rotation=90)\n        matrix_ax.tick_params(axis='both', labelsize='small')\n        matrix_ax.grid(False)\n        matrix_ax.set_xlim(0, len(categories))\n        matrix_ax.set_xlabel(groupby)\n        # invert y axis to show var_names ordered from top to bottom\n        matrix_ax.set_ylim(mean_obs.shape[0], 0)\n\n        if dendrogram:\n            dendro_ax = fig.add_subplot(axs[0, 0], sharex=matrix_ax)\n            _plot_dendrogram(dendro_ax, adata, groupby, dendrogram_key=dendrogram, ticks=x_ticks, orientation='top')\n\n        # plot group legends on top of matrix_ax (if given)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            gene_groups_ax = fig.add_subplot(axs[1, 1], sharey=matrix_ax)\n            _plot_gene_groups_brackets(gene_groups_ax, group_positions=var_group_positions,\n                                       group_labels=var_group_labels, rotation=var_group_rotation,\n                                       left_adjustment=0.2, right_adjustment=0.8, orientation='right')\n\n        # plot colorbar\n        _plot_colorbar(pc, fig, axs[1, 2])\n\n    utils.savefig_or_show('matrixplot', show=show, save=save)\n    return axs", "response": "Matrixplot of the mean expression values per cluster of each variable or group."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tracksplot(adata, var_names, groupby, use_raw=None, log=False,\n               dendrogram=False, gene_symbols=None, var_group_positions=None, var_group_labels=None,\n               layer=None, show=None, save=None, figsize=None, **kwds):\n    \"\"\"\\\n    In this type of plot each var_name is plotted as a filled line plot where the\n    y values correspond to the var_name values and x is each of the cells. Best results\n    are obtained when using raw counts that are not log.\n\n    `groupby` is required to sort and order the values using the respective group\n    and should be a categorical value.\n\n    Parameters\n    ----------\n    {common_plot_args}\n    {show_save_ax}\n    **kwds : keyword arguments\n        Are passed to `seaborn.heatmap`.\n\n    Returns\n    -------\n    A list of :class:`~matplotlib.axes.Axes`.\n\n    Examples\n    --------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.pl.tracksplot(adata, ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ'],\n    ...                  'bulk_labels', dendrogram=True)\n    \"\"\"\n\n    if groupby not in adata.obs_keys() or adata.obs[groupby].dtype.name != 'category':\n        raise ValueError('groupby has to be a valid categorical observation. Given value: {}, '\n                         'valid categorical observations: {}'.\n                         format(groupby, [x for x in adata.obs_keys() if adata.obs[x].dtype.name == 'category']))\n\n    categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, None,\n                                              gene_symbols=gene_symbols, layer=layer)\n\n    # get categories colors:\n    if groupby + \"_colors\" not in adata.uns:\n        from ._tools.scatterplots import _set_default_colors_for_categorical_obs\n        _set_default_colors_for_categorical_obs(adata, groupby)\n\n    groupby_colors = adata.uns[groupby + \"_colors\"]\n\n    if dendrogram:\n        # compute dendrogram if needed and reorder\n        # rows and columns to match leaves order.\n        dendro_data = _reorder_categories_after_dendrogram(adata, groupby, dendrogram,\n                                                           var_names=var_names,\n                                                           var_group_labels=var_group_labels,\n                                                           var_group_positions=var_group_positions)\n        # reorder obs_tidy\n        if dendro_data['var_names_idx_ordered'] is not None:\n            obs_tidy = obs_tidy.iloc[:, dendro_data['var_names_idx_ordered']]\n            var_names = [var_names[x] for x in dendro_data['var_names_idx_ordered']]\n\n        obs_tidy.index = obs_tidy.index.reorder_categories(\n            [categories[x] for x in dendro_data['categories_idx_ordered']], ordered=True)\n        categories = [categories[x] for x in dendro_data['categories_idx_ordered']]\n\n        groupby_colors = [groupby_colors[x] for x in dendro_data['categories_idx_ordered']]\n\n    obs_tidy = obs_tidy.sort_index()\n\n    goal_points = 1000\n    obs_tidy = _reduce_and_smooth(obs_tidy, goal_points)\n    # obtain the start and end of each category and make\n    # a list of ranges that will be used to plot a different\n    # color\n    cumsum = [0] + list(np.cumsum(obs_tidy.index.value_counts(sort=False)))\n    x_values = [(x, y) for x, y in zip(cumsum[:-1], cumsum[1:])]\n\n    dendro_height = 1 if dendrogram else 0\n\n    groupby_height = 0.24\n    num_rows = len(var_names) + 2  # +1 because of dendrogram on top and categories at bottom\n    if figsize is None:\n        width = 12\n        track_height = 0.25\n    else:\n        width, height = figsize\n        track_height = (height - (dendro_height + groupby_height)) / len(var_names)\n\n    height_ratios = [dendro_height] + [track_height] * len(var_names) + [groupby_height]\n    height = sum(height_ratios)\n\n    obs_tidy = obs_tidy.T\n\n    fig = pl.figure(figsize=(width, height))\n    axs = gridspec.GridSpec(ncols=2, nrows=num_rows, wspace=1.0 / width,\n                            hspace=0, height_ratios=height_ratios,\n                            width_ratios=[width, 0.14])\n    axs_list = []\n    first_ax = None\n    for idx, var in enumerate(var_names):\n        ax_idx = idx + 1  # this is because of the dendrogram\n        if first_ax is None:\n            ax = fig.add_subplot(axs[ax_idx, 0])\n            first_ax = ax\n        else:\n            ax = fig.add_subplot(axs[ax_idx,0], sharex=first_ax)\n        axs_list.append(ax)\n        for cat_idx, category in enumerate(categories):\n            x_start, x_end = x_values[cat_idx]\n            ax.fill_between(range(x_start, x_end), 0, obs_tidy.iloc[idx, x_start:x_end], lw=0.1,\n                            color=groupby_colors[cat_idx])\n\n        # remove the xticks labels except for the last processed plot.\n        # Because the plots share the x axis it is redundant and less compact to plot the\n        # axis for each plot\n        if idx < len(var_names) - 1:\n            ax.tick_params(labelbottom=False, labeltop=False, bottom=False, top=False)\n            ax.set_xlabel('')\n        if log:\n            ax.set_yscale('log')\n        ax.spines['left'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        ax.spines['bottom'].set_visible(False)\n        ax.grid(False)\n        ymin, ymax = ax.get_ylim()\n        ymax = int(ymax)\n        ax.set_yticks([ymax])\n        tt = ax.set_yticklabels([str(ymax)], ha='left', va='top')\n        ax.spines['right'].set_position(('axes', 1.01))\n        ax.tick_params(axis='y', labelsize='x-small', right=True, left=False, length=2,\n                       which='both', labelright=True, labelleft=False, direction='in')\n        ax.set_ylabel(var, rotation=0, fontsize='small', ha='right', va='bottom')\n        ax.yaxis.set_label_coords(-0.005, 0.1)\n    ax.set_xlim(0, x_end)\n    ax.tick_params(axis='x', bottom=False, labelbottom=False)\n\n    # the ax to plot the groupby categories is split to add a small space\n    # between the rest of the plot and the categories\n    axs2 = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec=axs[num_rows - 1, 0],\n                                            height_ratios=[1, 1])\n\n    groupby_ax = fig.add_subplot(axs2[1])\n\n    ticks, labels, groupby_cmap, norm = _plot_categories_as_colorblocks(groupby_ax, obs_tidy.T, colors=groupby_colors,\n                                                                  orientation='bottom')\n    # add lines to plot\n    overlay_ax = fig.add_subplot(axs[1:-1, 0], sharex=first_ax)\n    line_positions = np.cumsum(obs_tidy.T.index.value_counts(sort=False))[:-1]\n    overlay_ax.vlines(line_positions, 0, 1, lw=0.5, linestyle=\"--\")\n    overlay_ax.axis('off')\n    overlay_ax.set_ylim(0, 1)\n\n    if dendrogram:\n        dendro_ax = fig.add_subplot(axs[0], sharex=first_ax)\n        _plot_dendrogram(dendro_ax, adata, groupby, dendrogram_key=dendrogram, orientation='top', ticks=ticks)\n        axs_list.append(dendro_ax)\n\n    if var_group_positions is not None and len(var_group_positions) > 0:\n        gene_groups_ax = fig.add_subplot(axs[1:-1, 1])\n        arr = []\n        for idx, pos in enumerate(var_group_positions):\n            arr += [idx] * (pos[1]+1 - pos[0])\n\n        gene_groups_ax.imshow(np.matrix(arr).T, aspect='auto', cmap=groupby_cmap, norm=norm)\n        gene_groups_ax.axis('off')\n        axs_list.append(gene_groups_ax)\n\n    utils.savefig_or_show('tracksplot', show=show, save=save)\n    return axs_list", "response": "A function that plots a set of tracks for each variable in the given groupby."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot a dendrogram of the categories defined in groupby.", "response": "def dendrogram(adata, groupby, dendrogram_key=None, orientation='top', remove_labels=False,\n               show=None, save=None):\n    \"\"\"Plots a dendrogram of the categories defined in `groupby`.\n\n    See :func:`~scanpy.tl.dendrogram`.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n    groupby : `str`\n        Categorical data column used to create the dendrogram\n    dendrogram_key : `str`, optional(default: `None`)\n        Key under with the dendrogram information was stored.\n        By default the dendrogram information is stored under .uns['dendrogram_' + groupby].\n    orientation : `str`, optional(default: `top`)\n        Options are `top`, `bottom`, `left`, and `right`. Only when `show_correlation` is False\n    remove_labels : `bool`, optional(default: `False`)\n    {show_save_ax}\n\n    Returns\n    -------\n\n    Examples\n    --------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.dendrogram(adata, 'bulk_labels')\n    >>> sc.pl.dendrogram(adata, 'bulk_labels')\n    \"\"\"\n    fig, ax = pl.subplots()\n    _plot_dendrogram(ax, adata, groupby, dendrogram_key=dendrogram_key,\n                     remove_labels=remove_labels, orientation=orientation)\n\n    utils.savefig_or_show('dendrogram', show=show, save=save)\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots the correlation matrix computed as part of the dendrogram.", "response": "def correlation_matrix(adata, groupby, show_correlation_numbers=False, dendrogram=True, figsize=None,\n                       show=None, save=None, **kwds):\n    \"\"\"Plots the correlation matrix computed as part of `sc.tl.dendrogram`.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n    groupby : `str`\n        Categorical data column used to create the dendrogram\n    show_correlation_numbers : `bool`, optional(default: `False`)\n        If `show_correlation` is True, plot the correlation number on top of each cell.\n    dendrogram: `bool` or `str`, optional (default: `False`)\n        If True or a valid dendrogram key, a dendrogram based on the hierarchical clustering\n        between the `groupby` categories is added. The dendrogram information is computed\n        using :ref:`scanpy.tl.dendrogram`. If `tl.dendrogram` has not been called previously\n        the function is called with default parameters.\n    figsize : (`float`, `float`), optional (default: `None`)\n        By default a figure size that aims to produce a squared correlation matrix plot is used.\n        Format is (width, height)\n    {show_save_ax}\n    **kwds : keyword arguments\n        Only if `show_correlation` is True: Are passed to `matplotlib.pyplot.pcolormesh` when plotting\n        the correlation heatmap. Useful values to pas are `vmax`, `vmin` and `cmap`.\n\n    Returns\n    -------\n\n    Examples\n    --------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.dendrogram(adata, 'bulk_labels')\n    >>> sc.pl.correlation(adata, 'bulk_labels')\n    \"\"\"\n\n    dendrogram_key = _get_dendrogram_key(adata, dendrogram, groupby)\n\n    index = adata.uns[dendrogram_key]['categories_idx_ordered']\n    corr_matrix = adata.uns[dendrogram_key]['correlation_matrix']\n    # reorder matrix columns according to the dendrogram\n    if dendrogram:\n        assert(len(index)) == corr_matrix.shape[0]\n        corr_matrix = corr_matrix[index, :]\n        corr_matrix = corr_matrix[:, index]\n        labels = list(adata.obs[groupby].cat.categories)\n        labels = np.array(labels).astype('str')[index]\n    else:\n        labels = adata.obs[groupby].cat.categories\n    num_rows = corr_matrix.shape[0]\n    colorbar_height = 0.2\n    if dendrogram:\n        dendrogram_width = 1.8\n    else:\n        dendrogram_width = 0\n    if figsize is None:\n        corr_matrix_height = num_rows * 0.6\n        height = corr_matrix_height + colorbar_height\n        width = corr_matrix_height + dendrogram_width\n    else:\n        width, height = figsize\n        corr_matrix_height = height - colorbar_height\n\n    fig = pl.figure(figsize=(width, height))\n    # layout with 2 rows and 2  columns:\n    # row 1: dendrogram + correlation matrix\n    # row 2: nothing + colormap bar (horizontal)\n    gs = gridspec.GridSpec(nrows=2, ncols=2, width_ratios=[dendrogram_width, corr_matrix_height],\n                           height_ratios=[corr_matrix_height, colorbar_height], wspace=0.01, hspace=0.05)\n\n    axs = []\n    corr_matrix_ax = fig.add_subplot(gs[1])\n    if dendrogram:\n        dendro_ax = fig.add_subplot(gs[0], sharey=corr_matrix_ax)\n        _plot_dendrogram(dendro_ax, adata, groupby, dendrogram_key=dendrogram_key,\n                         remove_labels=True, orientation='left',\n                         ticks=np.arange(corr_matrix .shape[0]) + 0.5)\n        axs.append(dendro_ax)\n    # define some default pcolormesh parameters\n    if 'edge_color' not in kwds:\n        if corr_matrix.shape[0] > 30:\n            # when there are too many rows it is better to remove\n            # the black lines surrounding the boxes in the heatmap\n            kwds['edgecolors'] = 'none'\n        else:\n            kwds['edgecolors'] = 'black'\n            kwds['linewidth'] = 0.01\n    if 'vmax' not in kwds and 'vmin' not in kwds:\n        kwds['vmax'] = 1\n        kwds['vmin'] = -1\n    if 'cmap' not in kwds:\n        # by default use a divergent color map\n        kwds['cmap'] = 'bwr'\n\n    img_mat = corr_matrix_ax.pcolormesh(corr_matrix, **kwds)\n    corr_matrix_ax.set_xlim(0, num_rows)\n    corr_matrix_ax.set_ylim(0, num_rows)\n\n    corr_matrix_ax.yaxis.tick_right()\n    corr_matrix_ax.set_yticks(np.arange(corr_matrix .shape[0]) + 0.5)\n    corr_matrix_ax.set_yticklabels(labels)\n\n    corr_matrix_ax.xaxis.set_tick_params(labeltop=True)\n    corr_matrix_ax.xaxis.set_tick_params(labelbottom=False)\n    corr_matrix_ax.set_xticks(np.arange(corr_matrix .shape[0]) + 0.5)\n    corr_matrix_ax.set_xticklabels(labels, rotation=45, ha='left')\n\n    corr_matrix_ax.tick_params(\n        axis='x',\n        which='both',\n        bottom=False,\n        top=False)\n\n    corr_matrix_ax.tick_params(\n        axis='y',\n        which='both',\n        left=False,\n        right=False)\n    print(corr_matrix_ax.get_ylim())\n\n    if show_correlation_numbers:\n        for row in range(num_rows):\n            for col in range(num_rows):\n                corr_matrix_ax.text(row + 0.5, col + 0.5,\n                              \"{:.2f}\".format(corr_matrix[row, col]),\n                              ha='center', va='center')\n\n    axs.append(corr_matrix_ax)\n\n    # Plot colorbar\n    colormap_ax = fig.add_subplot(gs[3])\n    cobar = pl.colorbar(img_mat, cax=colormap_ax, orientation='horizontal')\n    cobar.solids.set_edgecolor(\"face\")\n    axs.append(colormap_ax)\n\n    return axs"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprepares a data frame in which the row index is the categories of the given var_names.", "response": "def _prepare_dataframe(adata, var_names, groupby=None, use_raw=None, log=False,\n                       num_categories=7, layer=None, gene_symbols=None):\n    \"\"\"\n    Given the anndata object, prepares a data frame in which the row index are the categories\n    defined by group by and the columns correspond to var_names.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    var_names : `str` or list of `str`\n        `var_names` should be a valid subset of  `adata.var_names`.\n    groupby : `str` or `None`, optional (default: `None`)\n        The key of the observation grouping to consider. It is expected that\n        groupby is a categorical. If groupby is not a categorical observation,\n        it would be subdivided into `num_categories`.\n    log : `bool`, optional (default: `False`)\n        Use the log of the values\n    use_raw : `bool`, optional (default: `None`)\n        Use `raw` attribute of `adata` if present.\n    num_categories : `int`, optional (default: `7`)\n        Only used if groupby observation is not categorical. This value\n        determines the number of groups into which the groupby observation\n        should be subdivided.\n    gene_symbols : string, optional (default: `None`)\n        Key for field in .var that stores gene symbols.\n\n    Returns\n    -------\n    Tuple of `pandas.DataFrame` and list of categories.\n    \"\"\"\n    from scipy.sparse import issparse\n    sanitize_anndata(adata)\n    if use_raw is None and adata.raw is not None: use_raw = True\n    if isinstance(var_names, str):\n        var_names = [var_names]\n\n    if groupby is not None:\n        if groupby not in adata.obs_keys():\n            raise ValueError('groupby has to be a valid observation. Given value: {}, '\n                             'valid observations: {}'.format(groupby, adata.obs_keys()))\n\n    if gene_symbols is not None and gene_symbols in adata.var.columns:\n        # translate gene_symbols to var_names\n        # slow method but gives a meaningful error en case no gene symbol is found:\n        translated_var_names = []\n        for symbol in var_names:\n            if symbol not in adata.var[gene_symbols].values:\n                logg.error(\"Gene symbol {!r} not found in given gene_symbols column: {!r}\".format(symbol, gene_symbols))\n                return\n            translated_var_names.append(adata.var[adata.var[gene_symbols] == symbol].index[0])\n        symbols = var_names\n        var_names = translated_var_names\n    if layer is not None:\n        if layer not in adata.layers.keys():\n            raise KeyError('Selected layer: {} is not in the layers list. The list of '\n                           'valid layers is: {}'.format(layer, adata.layers.keys()))\n        matrix = adata[:, var_names].layers[layer]\n    elif use_raw:\n        matrix = adata.raw[:, var_names].X\n    else:\n        matrix = adata[:, var_names].X\n\n    if issparse(matrix):\n        matrix = matrix.toarray()\n    if log:\n        matrix = np.log1p(matrix)\n\n    obs_tidy = pd.DataFrame(matrix, columns=var_names)\n    if groupby is None:\n        groupby = ''\n        categorical = pd.Series(np.repeat('', len(obs_tidy))).astype('category')\n    else:\n        if not is_categorical_dtype(adata.obs[groupby]):\n            # if the groupby column is not categorical, turn it into one\n            # by subdividing into  `num_categories` categories\n            categorical = pd.cut(adata.obs[groupby], num_categories)\n        else:\n            categorical = adata.obs[groupby]\n\n    obs_tidy.set_index(categorical, groupby, inplace=True)\n    if gene_symbols is not None:\n        # translate the column names to the symbol names\n        obs_tidy.rename(columns=dict([(var_names[x], symbols[x]) for x in range(len(var_names))]), inplace=True)\n    categories = obs_tidy.index.categories\n\n    return categories, obs_tidy"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot the brackets that represent groups of genes on the given axis.", "response": "def _plot_gene_groups_brackets(gene_groups_ax, group_positions, group_labels,\n                               left_adjustment=-0.3, right_adjustment=0.3, rotation=None, orientation='top'):\n    \"\"\"\n    Draws brackets that represent groups of genes on the give axis.\n    For best results, this axis is located on top of an image whose\n    x axis contains gene names.\n\n    The gene_groups_ax should share the x axis with the main ax.\n\n    Eg: gene_groups_ax = fig.add_subplot(axs[0, 0], sharex=dot_ax)\n\n    This function is used by dotplot, heatmap etc.\n\n    Parameters\n    ----------\n    gene_groups_ax : matplotlib axis\n        In this axis the gene marks are drawn\n    group_positions : list of `tuples`\n        Each item in the list, should contain the start and end position that the\n        bracket should cover.\n        Eg. [(0, 4), (5, 8)] means that there are two brackets, one for the var_names (eg genes)\n        in positions 0-4 and other for positions 5-8\n    group_labels :  list\n        List of group labels\n    left_adjustment : `float`\n        adjustment to plot the bracket start slightly before or after the first gene position.\n        If the value is negative the start is moved before.\n    right_adjustment : `float`\n        adjustment to plot the bracket end slightly before or after the last gene position\n        If the value is negative the start is moved before.\n    rotation : `float` (default None)\n        rotation degrees for the labels. If not given, small labels (<4 characters) are not\n        rotated, otherwise, they are rotated 90 degrees\n    orientation : `str` (default `top`)\n        location of the brackets. Either `top` or `right`\n    Returns\n    -------\n    None\n    \"\"\"\n    import matplotlib.patches as patches\n    from matplotlib.path import Path\n\n    # get the 'brackets' coordinates as lists of start and end positions\n\n    left = [x[0] + left_adjustment for x in group_positions]\n    right = [x[1] + right_adjustment for x in group_positions]\n\n    # verts and codes are used by PathPatch to make the brackets\n    verts = []\n    codes = []\n    if orientation == 'top':\n        # rotate labels if any of them is longer than 4 characters\n        if rotation is None and group_labels is not None and len(group_labels) > 0:\n            if max([len(x) for x in group_labels]) > 4:\n                rotation = 90\n            else:\n                rotation = 0\n        for idx in range(len(left)):\n            verts.append((left[idx], 0))  # lower-left\n            verts.append((left[idx], 0.6))  # upper-left\n            verts.append((right[idx], 0.6))  # upper-right\n            verts.append((right[idx], 0))  # lower-right\n\n            codes.append(Path.MOVETO)\n            codes.append(Path.LINETO)\n            codes.append(Path.LINETO)\n            codes.append(Path.LINETO)\n\n            try:\n                group_x_center = left[idx] + float(right[idx] - left[idx]) / 2\n                gene_groups_ax.text(group_x_center, 1.1, group_labels[idx], ha='center',\n                                    va='bottom', rotation=rotation)\n            except:\n                pass\n    else:\n        top = left\n        bottom = right\n        for idx in range(len(top)):\n            verts.append((0, top[idx]))    # upper-left\n            verts.append((0.15, top[idx]))  # upper-right\n            verts.append((0.15, bottom[idx]))  # lower-right\n            verts.append((0, bottom[idx]))  # lower-left\n\n            codes.append(Path.MOVETO)\n            codes.append(Path.LINETO)\n            codes.append(Path.LINETO)\n            codes.append(Path.LINETO)\n\n            try:\n                diff = bottom[idx] - top[idx]\n                group_y_center = top[idx] + float(diff) / 2\n                if diff * 2 < len(group_labels[idx]):\n                    # cut label to fit available space\n                    group_labels[idx] = group_labels[idx][:int(diff * 2)] + \".\"\n                gene_groups_ax.text(0.6, group_y_center,  group_labels[idx], ha='right',\n                                    va='center', rotation=270, fontsize='small')\n            except Exception as e:\n                print('problems {}'.format(e))\n                pass\n\n    path = Path(verts, codes)\n\n    patch = patches.PathPatch(path, facecolor='none', lw=1.5)\n\n    gene_groups_ax.add_patch(patch)\n    gene_groups_ax.spines['right'].set_visible(False)\n    gene_groups_ax.spines['top'].set_visible(False)\n    gene_groups_ax.spines['left'].set_visible(False)\n    gene_groups_ax.spines['bottom'].set_visible(False)\n    gene_groups_ax.grid(False)\n\n    # remove y ticks\n    gene_groups_ax.tick_params(axis='y', left=False, labelleft=False)\n    # remove x ticks and labels\n    gene_groups_ax.tick_params(axis='x', bottom=False, labelbottom=False, labeltop=False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfunction used by plotting functions that need to reorder the groupby observations based on the dendrogram results.", "response": "def _reorder_categories_after_dendrogram(adata, groupby, dendrogram,\n                                         var_names=None,\n                                         var_group_labels=None,\n                                         var_group_positions=None):\n    \"\"\"\n    Function used by plotting functions that need to reorder the the groupby observations\n    based on the dendrogram results.\n\n    The function checks if a dendrogram has already been precomputed. If not, sc.tl.dendrogram\n    is run with default parameters.\n\n    The results found in .uns[dendrogram_key] are used to reorder var_group_labels\n    and var_group_positions.\n\n\n    Returns\n    -------\n    dictionary with keys: 'categories_idx_ordered','var_group_names_idx_ordered',\n                      'var_group_labels', and 'var_group_positions'\n    \"\"\"\n\n    key = _get_dendrogram_key(adata, dendrogram, groupby)\n\n    dendro_info = adata.uns[key]\n    if groupby != dendro_info['groupby']:\n        raise ValueError(\"Incompatible observations. The precomputed dendrogram contains information \"\n                         \"for the observation: '{}' while the plot is made for the \"\n                         \"observation: '{}. Please run sc.tl.dendrogram \"\n                         \"using the right observation.'\".format(groupby, dendro_info['groupby']))\n\n    has_var_groups = True if var_group_positions is not None and len(var_group_positions) > 0 else False\n\n    categories = adata.obs[dendro_info['groupby']].cat.categories\n\n    # order of groupby categories\n    categories_idx_ordered = dendro_info['categories_idx_ordered']\n\n    if len(categories) != len(categories_idx_ordered):\n        raise ValueError(\"Incompatible observations. Dendrogram data has {} categories but \"\n                         \"current groupby observation {!r} contains {} categories. Most likely \"\n                         \"the underlying groupby observation changed after the initial \"\n                         \"computation of `sc.tl.dendrogram`. Please run sc.tl.dendrogram \"\n                         \"again.'\".format(len(categories_idx_ordered),\n                                          groupby, len(categories)))\n\n    # reorder var_groups (if any)\n    if var_names is not None:\n        var_names_idx_ordered = list(range(len(var_names)))\n\n    if has_var_groups:\n        if list(var_group_labels) == list(categories):\n            positions_ordered = []\n            labels_ordered = []\n            position_start = 0\n            var_names_idx_ordered = []\n            for idx in categories_idx_ordered:\n                position = var_group_positions[idx]\n                _var_names = var_names[position[0]:position[1] + 1]\n                var_names_idx_ordered.extend(range(position[0], position[1] + 1))\n                positions_ordered.append((position_start, position_start + len(_var_names) - 1))\n                position_start += len(_var_names)\n                labels_ordered.append(var_group_labels[idx])\n            var_group_labels = labels_ordered\n            var_group_positions = positions_ordered\n        else:\n            logg.warn(\"Groups are not reordered because the `groupby` categories \"\n                      \"and the `var_group_labels` are different.\\n\"\n                      \"categories: {}\\nvar_group_labels: {}\".format(\n                        _format_first_three_categories(categories),\n                        _format_first_three_categories(var_group_labels)))\n    else:\n        var_names_idx_ordered = None\n\n    var_group_data = {'categories_idx_ordered': categories_idx_ordered,\n                      'var_names_idx_ordered': var_names_idx_ordered,\n                      'var_group_labels': var_group_labels,\n                      'var_group_positions': var_group_positions}\n\n    return var_group_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots a dendrogram on the given ax.", "response": "def _plot_dendrogram(dendro_ax, adata, groupby, dendrogram_key=None, orientation='right', remove_labels=True, ticks=None):\n    \"\"\"\n    Plots a dendrogram on the given ax using the precomputed dendrogram information\n    stored in .uns[dendrogram_key]\n    \"\"\"\n\n    dendrogram_key = _get_dendrogram_key(adata, dendrogram_key, groupby)\n\n    def translate_pos(pos_list, new_ticks, old_ticks):\n        \"\"\"\n        transforms the dendrogram coordinates to a given new position.\n        The xlabel_pos and orig_ticks should be of the same\n        length.\n\n        This is mostly done for the heatmap case, where the position of the\n        dendrogram leaves needs to be adjusted dependening on the size of the category.\n\n        Parameters\n        ----------\n        pos_list :  list of dendrogram positions that should be translated\n        new_ticks : sorted list of goal tick positions (e.g. [0,1,2,3] )\n        old_ticks: sorted list of original tick positions (e.g. [5, 15, 25, 35]), This list is\n                   usually the default position used by scipy.cluster.hierarchy.dendrogram`\n\n        Returns\n        -------\n        translated list of positions\n\n        Examples\n        --------\n        >>> translate([5, 15, 20, 21 ], [0, 1, 2, 3], [5, 15, 25, 35])\n        ... [0, 1, 1.5, 1.6]\n\n        \"\"\"\n        # of given coordinates.\n\n        if not isinstance(old_ticks, list):\n            # assume that the list is a numpy array\n            old_ticks = old_ticks.tolist()\n        new_xs = []\n        for x_val in pos_list:\n            if x_val in old_ticks:\n                new_x_val = new_ticks[old_ticks.index(x_val)]\n            else:\n                # find smaller and bigger indices\n                idx_next = np.searchsorted(old_ticks, x_val, side=\"left\")\n                idx_prev = idx_next - 1\n                old_min = old_ticks[idx_prev]\n                old_max = old_ticks[idx_next]\n                new_min = new_ticks[idx_prev]\n                new_max = new_ticks[idx_next]\n                new_x_val = ((x_val - old_min) / (old_max - old_min)) * (new_max - new_min) + new_min\n            new_xs.append(new_x_val)\n        return new_xs\n\n    dendro_info = adata.uns[dendrogram_key]['dendrogram_info']\n    leaves = dendro_info[\"ivl\"]\n    icoord = np.array(dendro_info['icoord'])\n    dcoord = np.array(dendro_info['dcoord'])\n\n    orig_ticks = np.arange(5, len(leaves) * 10 + 5, 10).astype(float)\n    # check that ticks has the same length as orig_ticks\n    if ticks is not None and len(orig_ticks) != len(ticks):\n        logg.warn(\"ticks argument does not have the same size as orig_ticks. The argument will be ignored\")\n        ticks = None\n\n    for xs, ys in zip(icoord, dcoord):\n        if ticks is not None:\n            xs = translate_pos(xs, ticks, orig_ticks)\n        if orientation in ['right', 'left']:\n            dendro_ax.plot(ys, xs, color='#555555')\n        else:\n            dendro_ax.plot(xs, ys, color='#555555')\n\n    dendro_ax.tick_params(bottom=False, top=False, left=False, right=False)\n    ticks = ticks if ticks is not None else orig_ticks\n    if orientation in ['right', 'left']:\n        dendro_ax.set_yticks(ticks)\n        dendro_ax.set_yticklabels(leaves, fontsize='small', rotation=0)\n        dendro_ax.tick_params(labelbottom=False, labeltop=False)\n        if orientation == 'left':\n            xmin, xmax=dendro_ax.get_xlim()\n            dendro_ax.set_xlim(xmax, xmin)\n            dendro_ax.tick_params(labelleft=False, labelright=True)\n    else:\n        dendro_ax.set_xticks(ticks)\n        dendro_ax.set_xticklabels(leaves, fontsize='small', rotation=90)\n        dendro_ax.tick_params(labelleft=False, labelright=False)\n        if orientation == 'bottom':\n            ymin, ymax=dendro_ax.get_ylim()\n            dendro_ax.set_ylim(ymax, ymin)\n            dendro_ax.tick_params(labeltop=True, labelbottom=False)\n\n    if remove_labels:\n        dendro_ax.tick_params(labelbottom=False, labeltop=False, labelleft=False, labelright=False)\n\n    dendro_ax.grid(False)\n\n    dendro_ax.spines['right'].set_visible(False)\n    dendro_ax.spines['top'].set_visible(False)\n    dendro_ax.spines['left'].set_visible(False)\n    dendro_ax.spines['bottom'].set_visible(False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuses interpolation to reduce the number of observations (cells). This is useful for plotting functions that otherwise will ignore most of the cells' values. The reduction and smoothing is only done per column Parameters ---------- obs_tidy : Pandas DataFrame. rows = obs (eg. cells), cols = vars (eg. genes) goal_size : number of cells to keep Returns -------", "response": "def _reduce_and_smooth(obs_tidy, goal_size):\n    \"\"\"\n    Uses interpolation to reduce the number of observations (cells).\n    This is useful for plotting functions that otherwise will ignore\n    most of the cells' values.\n\n    The reduction and smoothing is only done per column\n\n    Parameters\n    ----------\n    obs_tidy : Pandas DataFrame. rows = obs (eg. cells), cols = vars (eg. genes)\n    goal_size : number of cells to keep\n\n    Returns\n    -------\n\n    \"\"\"\n    if obs_tidy.shape[0] < goal_size:\n        return obs_tidy\n    else:\n        # usually, a large number of cells can not be plotted, thus\n        # it is useful to reduce the number of cells plotted while\n        # smoothing the values. This should be similar to an interpolation\n        # but done per row and not for the entire image.\n        from scipy.interpolate import UnivariateSpline\n        x = range(obs_tidy.shape[0])\n        # maximum number of cells to keep\n        new_x = np.linspace(0, len(x), num=goal_size, endpoint=False)\n        new_df = obs_tidy.iloc[new_x, :].copy()\n        for index, col in obs_tidy.iteritems():\n            spl = UnivariateSpline(x, col.values, s=20)\n            new_df[index] = spl(new_x)\n        return new_df.copy()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors=None, orientation='left', cmap_name='tab20'):\n\n    groupby = obs_tidy.index.name\n    from matplotlib.colors import ListedColormap, BoundaryNorm\n    if colors is None:\n        groupby_cmap = pl.get_cmap(cmap_name)\n    else:\n        groupby_cmap = ListedColormap(colors, groupby + '_cmap')\n    norm = BoundaryNorm(np.arange(groupby_cmap.N+1)-.5, groupby_cmap.N)\n\n    # determine groupby label positions such that they appear\n    # centered next/below to the color code rectangle assigned to the category\n    value_sum = 0\n    ticks = []  # list of centered position of the labels\n    labels = []\n    label2code = {}  # dictionary of numerical values asigned to each label\n    for code, (label, value) in enumerate(obs_tidy.index.value_counts(sort=False).iteritems()):\n        ticks.append(value_sum + (value / 2))\n        labels.append(label)\n        value_sum += value\n        label2code[label] = code\n\n    groupby_ax.grid(False)\n\n    if orientation == 'left':\n        groupby_ax.imshow(np.matrix([label2code[lab] for lab in obs_tidy.index]).T, aspect='auto', cmap=groupby_cmap, norm=norm)\n        if len(labels) > 1:\n            groupby_ax.set_yticks(ticks)\n            groupby_ax.set_yticklabels(labels)\n\n        # remove y ticks\n        groupby_ax.tick_params(axis='y', left=False, labelsize='small')\n        # remove x ticks and labels\n        groupby_ax.tick_params(axis='x', bottom=False, labelbottom=False)\n\n        # remove surrounding lines\n        groupby_ax.spines['right'].set_visible(False)\n        groupby_ax.spines['top'].set_visible(False)\n        groupby_ax.spines['left'].set_visible(False)\n        groupby_ax.spines['bottom'].set_visible(False)\n\n        groupby_ax.set_ylabel(groupby)\n    else:\n        groupby_ax.imshow(np.matrix([label2code[lab] for lab in obs_tidy.index]), aspect='auto', cmap=groupby_cmap, norm=norm)\n        if len(labels) > 1:\n            groupby_ax.set_xticks(ticks)\n            if max([len(x) for x in labels]) < 3:\n                # if the labels are small do not rotate them\n                rotation = 0\n            else:\n                rotation = 90\n            groupby_ax.set_xticklabels(labels, rotation=rotation)\n\n        # remove x ticks\n        groupby_ax.tick_params(axis='x', bottom=False, labelsize='small')\n        # remove y ticks and labels\n        groupby_ax.tick_params(axis='y', left=False, labelleft=False)\n\n        # remove surrounding lines\n        groupby_ax.spines['right'].set_visible(False)\n        groupby_ax.spines['top'].set_visible(False)\n        groupby_ax.spines['left'].set_visible(False)\n        groupby_ax.spines['bottom'].set_visible(False)\n\n        groupby_ax.set_xlabel(groupby)\n\n    return ticks, labels, groupby_cmap, norm", "response": "Plots the categories as colored blocks."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _plot_colorbar(mappable, fig, subplot_spec, max_cbar_height=4):\n    width, height = fig.get_size_inches()\n    if height > max_cbar_height:\n        # to make the colorbar shorter, the\n        # ax is split and the lower portion is used.\n        axs2 = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec=subplot_spec,\n                                                height_ratios=[height - max_cbar_height, max_cbar_height])\n        heatmap_cbar_ax = fig.add_subplot(axs2[1])\n    else:\n        heatmap_cbar_ax = fig.add_subplot(subplot_spec)\n    pl.colorbar(mappable, cax=heatmap_cbar_ax)\n    return heatmap_cbar_ax", "response": "Plots a vertical color bar based on a mappable image."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dpt(adata, n_dcs=10, n_branchings=0, min_group_size=0.01,\n        allow_kendall_tau_shift=True, copy=False):\n    \"\"\"Infer progression of cells through geodesic distance along the graph [Haghverdi16]_ [Wolf19]_.\n\n    Reconstruct the progression of a biological process from snapshot\n    data. `Diffusion Pseudotime` has been introduced by [Haghverdi16]_ and\n    implemented within Scanpy [Wolf18]_. Here, we use a further developed\n    version, which is able to deal with disconnected graphs [Wolf19]_ and can\n    be run in a `hierarchical` mode by setting the parameter\n    `n_branchings>1`. We recommend, however, to only use\n    :func:`~scanpy.api.tl.dpt` for computing pseudotime (`n_branchings=0`) and\n    to detect branchings via :func:`~scanpy.api.paga`. For pseudotime, you need\n    to annotate your data with a root cell. For instance::\n\n        adata.uns['iroot'] = np.flatnonzero(adata.obs['cell_types'] == 'Stem')[0]\n\n    This requires to run :func:`~scanpy.api.pp.neighbors`, first. In order to\n    reproduce the original implementation of DPT, use `method=='gauss'` in\n    this. Using the default `method=='umap'` only leads to minor quantitative\n    differences, though.\n\n    .. versionadded:: 1.1\n\n    :func:`~scanpy.api.tl.dpt` also requires to run\n    :func:`~scanpy.api.tl.diffmap` first. As previously,\n    :func:`~scanpy.api.tl.dpt` came with a default parameter of ``n_dcs=10`` but\n    :func:`~scanpy.api.tl.diffmap` has a default parameter of ``n_comps=15``,\n    you need to pass ``n_comps=10`` in :func:`~scanpy.api.tl.diffmap` in order\n    to exactly reproduce previous :func:`~scanpy.api.tl.dpt` results.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    n_dcs : `int`, optional (default: 10)\n        The number of diffusion components to use.\n    n_branchings : `int`, optional (default: 0)\n        Number of branchings to detect.\n    min_group_size : [0, 1] or `float`, optional (default: 0.01)\n        During recursive splitting of branches ('dpt groups') for `n_branchings`\n        > 1, do not consider groups that contain less than `min_group_size` data\n        points. If a float, `min_group_size` refers to a fraction of the total\n        number of data points.\n    allow_kendall_tau_shift : `bool`, optional (default: `True`)\n        If a very small branch is detected upon splitting, shift away from\n        maximum correlation in Kendall tau criterion of [Haghverdi16]_ to\n        stabilize the splitting.\n    copy : `bool`, optional (default: `False`)\n        Copy instance before computation and return a copy. Otherwise, perform\n        computation inplace and return None.\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n\n    If `n_branchings==0`, no field `dpt_groups` will be written.\n\n    **dpt_pseudotime** : :class:`pandas.Series` (`adata.obs`, dtype `float`)\n        Array of dim (number of samples) that stores the pseudotime of each\n        cell, that is, the DPT distance with respect to the root cell.\n    **dpt_groups** : :class:`pandas.Series` (`adata.obs`, dtype `category`)\n        Array of dim (number of samples) that stores the subgroup id ('0',\n        '1', ...) for each cell. The groups  typically correspond to\n        'progenitor cells', 'undecided cells' or 'branches' of a process.\n\n    Notes\n    -----\n    The tool is similar to the R package `destiny` of [Angerer16]_.\n    \"\"\"\n    # standard errors, warnings etc.\n    adata = adata.copy() if copy else adata\n    if 'neighbors' not in adata.uns:\n        raise ValueError(\n            'You need to run `pp.neighbors` and `tl.diffmap` first.')\n    if 'iroot' not in adata.uns and 'xroot' not in adata.var:\n        logg.warn(\n            'No root cell found. To compute pseudotime, pass the index or '\n            'expression vector of a root cell, one of:\\n'\n            '    adata.uns[\\'iroot\\'] = root_cell_index\\n'\n            '    adata.var[\\'xroot\\'] = adata[root_cell_name, :].X')\n    if 'X_diffmap' not in adata.obsm.keys():\n        logg.warn('Trying to run `tl.dpt` without prior call of `tl.diffmap`. '\n                  'Falling back to `tl.diffmap` with default parameters.')\n        _diffmap(adata)\n    # start with the actual computation\n    dpt = DPT(adata, n_dcs=n_dcs, min_group_size=min_group_size,\n              n_branchings=n_branchings,\n              allow_kendall_tau_shift=allow_kendall_tau_shift)\n    logg.info('computing Diffusion Pseudotime using n_dcs={}'.format(n_dcs), r=True)\n    if n_branchings > 1: logg.info('    this uses a hierarchical implementation')\n    if dpt.iroot is not None:\n        dpt._set_pseudotime()  # pseudotimes are distances from root point\n        adata.uns['iroot'] = dpt.iroot  # update iroot, might have changed when subsampling, for example\n        adata.obs['dpt_pseudotime'] = dpt.pseudotime\n    # detect branchings and partition the data into segments\n    if n_branchings > 0:\n        dpt.branchings_segments()\n        adata.obs['dpt_groups'] = pd.Categorical(\n            values=dpt.segs_names.astype('U'),\n            categories=natsorted(np.array(dpt.segs_names_unique).astype('U')))\n        # the \"change points\" separate segments in the ordering above\n        adata.uns['dpt_changepoints'] = dpt.changepoints\n        # the tip points of segments\n        adata.uns['dpt_grouptips'] = dpt.segs_tips\n        # the ordering according to segments and pseudotime\n        ordering_id = np.zeros(adata.n_obs, dtype=int)\n        for count, idx in enumerate(dpt.indices): ordering_id[idx] = count\n        adata.obs['dpt_order'] = ordering_id\n        adata.obs['dpt_order_indices'] = dpt.indices\n    logg.info('    finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\\n')\n    logg.hint('added\\n'\n           + ('    \\'dpt_pseudotime\\', the pseudotime (adata.obs)'\n              if dpt.iroot is not None else '')\n           + ('\\n    \\'dpt_groups\\', the branching subgroups of dpt (adata.obs)\\n'\n              + '    \\'dpt_order\\', cell order (adata.obs)'\n              if n_branchings > 0 else ''))\n    return adata if copy else None", "response": "Compute the DPT of a biological process from the given data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef branchings_segments(self):\n        self.detect_branchings()\n        self.postprocess_segments()\n        self.set_segs_names()\n        self.order_pseudotime()", "response": "Detect branchings and partition the data into corresponding segments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetect all branchings up to n_branchings. Writes Attributes - > This method is used to detect all branchings up to n_branchings.", "response": "def detect_branchings(self):\n        \"\"\"Detect all branchings up to `n_branchings`.\n\n        Writes Attributes\n        -----------------\n        segs : np.ndarray\n            List of integer index arrays.\n        segs_tips : np.ndarray\n            List of indices of the tips of segments.\n        \"\"\"\n        logg.m('    detect', self.n_branchings,\n               'branching' + ('' if self.n_branchings == 1 else 's'))\n        # a segment is a subset of points of the data set (defined by the\n        # indices of the points in the segment)\n        # initialize the search for branchings with a single segment,\n        # that is, get the indices of the whole data set\n        indices_all = np.arange(self._adata.shape[0], dtype=int)\n        # let's keep a list of segments, the first segment to add is the\n        # whole data set\n        segs = [indices_all]\n        # a segment can as well be defined by the two points that have maximal\n        # distance in the segment, the \"tips\" of the segment\n        #\n        # the rest of the points in the segment is then defined by demanding\n        # them to \"be close to the line segment that connects the tips\", that\n        # is, for such a point, the normalized added distance to both tips is\n        # smaller than one:\n        #     (D[tips[0],i] + D[tips[1],i])/D[tips[0],tips[1] < 1\n        # of course, this condition is fulfilled by the full cylindrical\n        # subspace surrounding that line segment, where the radius of the\n        # cylinder can be infinite\n        #\n        # if D denotes a euclidian distance matrix, a line segment is a linear\n        # object, and the name \"line\" is justified. if we take the\n        # diffusion-based distance matrix Dchosen, which approximates geodesic\n        # distance, with \"line\", we mean the shortest path between two points,\n        # which can be highly non-linear in the original space\n        #\n        # let us define the tips of the whole data set\n        if False:  # this is safe, but not compatible with on-the-fly computation\n            tips_all = np.array(np.unravel_index(np.argmax(self.distances_dpt), self.distances_dpt.shape))\n        else:\n            if self.iroot is not None:\n                tip_0 = np.argmax(self.distances_dpt[self.iroot])\n            else:\n                tip_0 = np.argmax(self.distances_dpt[0])\n            tips_all = np.array([tip_0, np.argmax(self.distances_dpt[tip_0])])\n        # we keep a list of the tips of each segment\n        segs_tips = [tips_all]\n        segs_connects = [[]]\n        segs_undecided = [True]\n        segs_adjacency = [[]]\n        logg.m('    do not consider groups with less than {} points for splitting'\n               .format(self.min_group_size))\n        for ibranch in range(self.n_branchings):\n            iseg, tips3 = self.select_segment(segs, segs_tips, segs_undecided)\n            if iseg == -1:\n                logg.m('    partitioning converged')\n                break\n            logg.m('    branching {}:'.format(ibranch + 1),\n                   'split group', iseg)  # [third start end]\n            # detect branching and update segs and segs_tips\n            self.detect_branching(segs, segs_tips,\n                                  segs_connects,\n                                  segs_undecided,\n                                  segs_adjacency, iseg, tips3)\n        # store as class members\n        self.segs = segs\n        self.segs_tips = segs_tips\n        self.segs_undecided = segs_undecided\n        # the following is a bit too much, but this allows easy storage\n        self.segs_adjacency = sp.sparse.lil_matrix((len(segs), len(segs)), dtype=float)\n        self.segs_connects = sp.sparse.lil_matrix((len(segs), len(segs)), dtype=int)\n        for i, seg_adjacency in enumerate(segs_adjacency):\n            self.segs_connects[i, seg_adjacency] = segs_connects[i]\n        for i in range(len(segs)):\n            for j in range(len(segs)):\n                self.segs_adjacency[i, j] = self.distances_dpt[self.segs_connects[i, j],\n                                                         self.segs_connects[j, i]]\n        self.segs_adjacency = self.segs_adjacency.tocsr()\n        self.segs_connects = self.segs_connects.tocsr()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef select_segment(self, segs, segs_tips, segs_undecided) -> Tuple[int, int]:\n        scores_tips = np.zeros((len(segs), 4))\n        allindices = np.arange(self._adata.shape[0], dtype=int)\n        for iseg, seg in enumerate(segs):\n            # do not consider too small segments\n            if segs_tips[iseg][0] == -1: continue\n            # restrict distance matrix to points in segment\n            if not isinstance(self.distances_dpt, OnFlySymMatrix):\n                Dseg = self.distances_dpt[np.ix_(seg, seg)]\n            else:\n                Dseg = self.distances_dpt.restrict(seg)\n            third_maximizer = None\n            if segs_undecided[iseg]:\n                # check that none of our tips \"connects\" with a tip of the\n                # other segments\n                for jseg in range(len(segs)):\n                    if jseg != iseg:\n                        # take the inner tip, the \"second tip\" of the segment\n                        for itip in range(2):\n                            if (self.distances_dpt[segs_tips[jseg][1], segs_tips[iseg][itip]]\n                                < 0.5 * self.distances_dpt[segs_tips[iseg][~itip], segs_tips[iseg][itip]]):\n                                # logg.m('    group', iseg, 'with tip', segs_tips[iseg][itip],\n                                #        'connects with', jseg, 'with tip', segs_tips[jseg][1], v=4)\n                                # logg.m('    do not use the tip for \"triangulation\"', v=4)\n                                third_maximizer = itip\n            # map the global position to the position within the segment\n            tips = [np.where(allindices[seg] == tip)[0][0]\n                    for tip in segs_tips[iseg]]\n            # find the third point on the segment that has maximal\n            # added distance from the two tip points\n            dseg = Dseg[tips[0]] + Dseg[tips[1]]\n            if not np.isfinite(dseg).any():\n                continue\n            # add this point to tips, it's a third tip, we store it at the first\n            # position in an array called tips3\n            third_tip = np.argmax(dseg)\n            if third_maximizer is not None:\n                # find a fourth point that has maximal distance to all three\n                dseg += Dseg[third_tip]\n                fourth_tip = np.argmax(dseg)\n                if fourth_tip != tips[0] and fourth_tip != third_tip:\n                    tips[1] = fourth_tip\n                    dseg -= Dseg[tips[1]]\n                else:\n                    dseg -= Dseg[third_tip]\n            tips3 = np.append(tips, third_tip)\n            # compute the score as ratio of the added distance to the third tip,\n            # to what it would be if it were on the straight line between the\n            # two first tips, given by Dseg[tips[:2]]\n            # if we did not normalize, there would be a danger of simply\n            # assigning the highest score to the longest segment\n            score = dseg[tips3[2]] / Dseg[tips3[0], tips3[1]]\n            score = len(seg) if self.choose_largest_segment else score  # simply the number of points\n            logg.m('    group', iseg, 'score', score, 'n_points', len(seg),\n                   '(too small)' if len(seg) < self.min_group_size else '', v=4)\n            if len(seg) <= self.min_group_size: score = 0\n            # write result\n            scores_tips[iseg, 0] = score\n            scores_tips[iseg, 1:] = tips3\n        iseg = np.argmax(scores_tips[:, 0])\n        if scores_tips[iseg, 0] == 0: return -1, None\n        tips3 = scores_tips[iseg, 1:].astype(int)\n        return iseg, tips3", "response": "Select the most recent data point that is part of the given list of line segments."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert the format of the segment class members.", "response": "def postprocess_segments(self):\n        \"\"\"Convert the format of the segment class members.\"\"\"\n        # make segs a list of mask arrays, it's easier to store\n        # as there is a hdf5 equivalent\n        for iseg, seg in enumerate(self.segs):\n            mask = np.zeros(self._adata.shape[0], dtype=bool)\n            mask[seg] = True\n            self.segs[iseg] = mask\n        # convert to arrays\n        self.segs = np.array(self.segs)\n        self.segs_tips = np.array(self.segs_tips)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_segs_names(self):\n        segs_names = np.zeros(self._adata.shape[0], dtype=np.int8)\n        self.segs_names_unique = []\n        for iseg, seg in enumerate(self.segs):\n            segs_names[seg] = iseg\n            self.segs_names_unique.append(iseg)\n        self.segs_names = segs_names", "response": "Return a single array that stores integer segment labels."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndefining indices that reflect segment and pseudotime order.", "response": "def order_pseudotime(self):\n        \"\"\"Define indices that reflect segment and pseudotime order.\n\n        Writes\n        ------\n        indices : np.ndarray\n            Index array of shape n, which stores an ordering of the data points\n            with respect to increasing segment index and increasing pseudotime.\n        changepoints : np.ndarray\n            Index array of shape len(ssegs)-1, which stores the indices of\n            points where the segment index changes, with respect to the ordering\n            of indices.\n        \"\"\"\n        # within segs_tips, order tips according to pseudotime\n        if self.iroot is not None:\n            for itips, tips in enumerate(self.segs_tips):\n                if tips[0] != -1:\n                    indices = np.argsort(self.pseudotime[tips])\n                    self.segs_tips[itips] = self.segs_tips[itips][indices]\n                else:\n                    logg.m('    group', itips, 'is very small', v=4)\n        # sort indices according to segments\n        indices = np.argsort(self.segs_names)\n        segs_names = self.segs_names[indices]\n        # find changepoints of segments\n        changepoints = np.arange(indices.size-1)[np.diff(segs_names) == 1] + 1\n        if self.iroot is not None:\n            pseudotime = self.pseudotime[indices]\n            for iseg, seg in enumerate(self.segs):\n                # only consider one segment, it's already ordered by segment\n                seg_sorted = seg[indices]\n                # consider the pseudotime on this segment and sort them\n                seg_indices = np.argsort(pseudotime[seg_sorted])\n                # within the segment, order indices according to increasing pseudotime\n                indices[seg_sorted] = indices[seg_sorted][seg_indices]\n        # define class members\n        self.indices = indices\n        self.changepoints = changepoints"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef detect_branching(self, segs, segs_tips, segs_connects, segs_undecided, segs_adjacency,\n                         iseg, tips3):\n        \"\"\"Detect branching on given segment.\n\n        Updates all list parameters inplace.\n\n        Call function _detect_branching and perform bookkeeping on segs and\n        segs_tips.\n\n        Parameters\n        ----------\n        segs : list of np.ndarray\n            Dchosen distance matrix restricted to segment.\n        segs_tips : list of np.ndarray\n            Stores all tip points for the segments in segs.\n        iseg : int\n            Position of segment under study in segs.\n        tips3 : np.ndarray\n            The three tip points. They form a 'triangle' that contains the data.\n        \"\"\"\n        seg = segs[iseg]\n        # restrict distance matrix to points in segment\n        if not isinstance(self.distances_dpt, OnFlySymMatrix):\n            Dseg = self.distances_dpt[np.ix_(seg, seg)]\n        else:\n            Dseg = self.distances_dpt.restrict(seg)\n        # given the three tip points and the distance matrix detect the\n        # branching on the segment, return the list ssegs of segments that\n        # are defined by splitting this segment\n        result = self._detect_branching(Dseg, tips3, seg)\n        ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk = result\n        # map back to global indices\n        for iseg_new, seg_new in enumerate(ssegs):\n            ssegs[iseg_new] = seg[seg_new]\n            ssegs_tips[iseg_new] = seg[ssegs_tips[iseg_new]]\n            ssegs_connects[iseg_new] = list(seg[ssegs_connects[iseg_new]])\n        # remove previous segment\n        segs.pop(iseg)\n        segs_tips.pop(iseg)\n        # insert trunk/undecided_cells at same position\n        segs.insert(iseg, ssegs[trunk])\n        segs_tips.insert(iseg, ssegs_tips[trunk])\n        # append other segments\n        segs += [seg for iseg, seg in enumerate(ssegs) if iseg != trunk]\n        segs_tips += [seg_tips for iseg, seg_tips in enumerate(ssegs_tips) if iseg != trunk]\n        if len(ssegs) == 4:\n            # insert undecided cells at same position\n            segs_undecided.pop(iseg)\n            segs_undecided.insert(iseg, True)\n        # correct edges in adjacency matrix\n        n_add = len(ssegs) - 1\n        prev_connecting_segments = segs_adjacency[iseg].copy()\n        if self.flavor == 'haghverdi16':\n            segs_adjacency += [[iseg] for i in range(n_add)]\n            segs_connects += [seg_connects for iseg, seg_connects in enumerate(ssegs_connects) if iseg != trunk]\n            prev_connecting_points = segs_connects[iseg]\n            for jseg_cnt, jseg in enumerate(prev_connecting_segments):\n                iseg_cnt = 0\n                for iseg_new, seg_new in enumerate(ssegs):\n                    if iseg_new != trunk:\n                        pos = segs_adjacency[jseg].index(iseg)\n                        connection_to_iseg = segs_connects[jseg][pos]\n                        if connection_to_iseg in seg_new:\n                            kseg = len(segs) - n_add + iseg_cnt\n                            segs_adjacency[jseg][pos] = kseg\n                            pos_2 = segs_adjacency[iseg].index(jseg)\n                            segs_adjacency[iseg].pop(pos_2)\n                            idx = segs_connects[iseg].pop(pos_2)\n                            segs_adjacency[kseg].append(jseg)\n                            segs_connects[kseg].append(idx)\n                            break\n                        iseg_cnt += 1\n            segs_adjacency[iseg] += list(range(len(segs_adjacency) - n_add, len(segs_adjacency)))\n            segs_connects[iseg] += ssegs_connects[trunk]\n        else:\n            segs_adjacency += [[] for i in range(n_add)]\n            segs_connects += [[] for i in range(n_add)]\n            kseg_list = [iseg] + list(range(len(segs) - n_add, len(segs)))\n            for jseg in prev_connecting_segments:\n                pos = segs_adjacency[jseg].index(iseg)\n                distances = []\n                closest_points_in_jseg = []\n                closest_points_in_kseg = []\n                for kseg in kseg_list:\n                    reference_point_in_k = segs_tips[kseg][0]\n                    closest_points_in_jseg.append(segs[jseg][np.argmin(self.distances_dpt[reference_point_in_k, segs[jseg]])])\n                    # do not use the tip in the large segment j, instead, use the closest point\n                    reference_point_in_j = closest_points_in_jseg[-1]  # segs_tips[jseg][0]\n                    closest_points_in_kseg.append(segs[kseg][np.argmin(self.distances_dpt[reference_point_in_j, segs[kseg]])])\n                    distances.append(self.distances_dpt[closest_points_in_jseg[-1], closest_points_in_kseg[-1]])\n                    # print(jseg, '(', segs_tips[jseg][0], closest_points_in_jseg[-1], ')',\n                    #       kseg, '(', segs_tips[kseg][0], closest_points_in_kseg[-1], ') :', distances[-1])\n                idx = np.argmin(distances)\n                kseg_min = kseg_list[idx]\n                segs_adjacency[jseg][pos] = kseg_min\n                segs_connects[jseg][pos] = closest_points_in_kseg[idx]\n                pos_2 = segs_adjacency[iseg].index(jseg)\n                segs_adjacency[iseg].pop(pos_2)\n                segs_connects[iseg].pop(pos_2)\n                segs_adjacency[kseg_min].append(jseg)\n                segs_connects[kseg_min].append(closest_points_in_jseg[idx])\n            # if we split two clusters, we need to check whether the new segments connect to any of the other\n            # old segments\n            # if not, we add a link between the new segments, if yes, we add two links to connect them at the\n            # correct old segments\n            do_not_attach_kseg = False\n            for kseg in kseg_list:\n                distances = []\n                closest_points_in_jseg = []\n                closest_points_in_kseg = []\n                jseg_list = [jseg for jseg in range(len(segs))\n                             if jseg != kseg and jseg not in prev_connecting_segments]\n                for jseg in jseg_list:\n                    reference_point_in_k = segs_tips[kseg][0]\n                    closest_points_in_jseg.append(segs[jseg][np.argmin(self.distances_dpt[reference_point_in_k, segs[jseg]])])\n                    # do not use the tip in the large segment j, instead, use the closest point\n                    reference_point_in_j = closest_points_in_jseg[-1]  # segs_tips[jseg][0]\n                    closest_points_in_kseg.append(segs[kseg][np.argmin(self.distances_dpt[reference_point_in_j, segs[kseg]])])\n                    distances.append(self.distances_dpt[closest_points_in_jseg[-1], closest_points_in_kseg[-1]])\n                idx = np.argmin(distances)\n                jseg_min = jseg_list[idx]\n                if jseg_min not in kseg_list:\n                    segs_adjacency_sparse = sp.sparse.lil_matrix((len(segs), len(segs)), dtype=float)\n                    for i, seg_adjacency in enumerate(segs_adjacency):\n                        segs_adjacency_sparse[i, seg_adjacency] = 1\n                    G = nx.Graph(segs_adjacency_sparse)\n                    paths_all = nx.single_source_dijkstra_path(G, source=kseg)\n                    if jseg_min not in paths_all:\n                        segs_adjacency[jseg_min].append(kseg)\n                        segs_connects[jseg_min].append(closest_points_in_kseg[idx])\n                        segs_adjacency[kseg].append(jseg_min)\n                        segs_connects[kseg].append(closest_points_in_jseg[idx])\n                        logg.m('    attaching new segment', kseg, 'at', jseg_min)\n                        # if we split the cluster, we should not attach kseg\n                        do_not_attach_kseg = True\n                    else:\n                        logg.m('    cannot attach new segment', kseg, 'at', jseg_min,\n                               '(would produce cycle)')\n                        if kseg != kseg_list[-1]:\n                            logg.m('        continue')\n                            continue\n                        else:\n                            logg.m('        do not add another link')\n                            break\n                if jseg_min in kseg_list and not do_not_attach_kseg:\n                    segs_adjacency[jseg_min].append(kseg)\n                    segs_connects[jseg_min].append(closest_points_in_kseg[idx])\n                    segs_adjacency[kseg].append(jseg_min)\n                    segs_connects[kseg].append(closest_points_in_jseg[idx])\n                    break\n        segs_undecided += [False for i in range(n_add)]", "response": "This function detects branching on a set of segments."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetects branching on given segment.", "response": "def _detect_branching(self, Dseg: np.ndarray, tips: np.ndarray, seg_reference=None):\n        \"\"\"Detect branching on given segment.\n\n        Call function __detect_branching three times for all three orderings of\n        tips. Points that do not belong to the same segment in all three\n        orderings are assigned to a fourth segment. The latter is, by Haghverdi\n        et al. (2016) referred to as 'undecided cells'.\n\n        Parameters\n        ----------\n        Dseg\n            Dchosen distance matrix restricted to segment.\n        tips\n            The three tip points. They form a 'triangle' that contains the data.\n\n        Returns\n        -------\n        ssegs : list of np.ndarray\n            List of segments obtained from splitting the single segment defined\n            via the first two tip cells.\n        ssegs_tips : list of np.ndarray\n            List of tips of segments in ssegs.\n        \"\"\"\n        if self.flavor == 'haghverdi16':\n            ssegs = self._detect_branching_single_haghverdi16(Dseg, tips)\n        elif self.flavor == 'wolf17_tri':\n            ssegs = self._detect_branching_single_wolf17_tri(Dseg, tips)\n        elif self.flavor == 'wolf17_bi' or self.flavor == 'wolf17_bi_un':\n            ssegs = self._detect_branching_single_wolf17_bi(Dseg, tips)\n        else:\n            raise ValueError('`flavor` needs to be in {\"haghverdi16\", \"wolf17_tri\", \"wolf17_bi\"}.')\n        # make sure that each data point has a unique association with a segment\n        masks = np.zeros((len(ssegs), Dseg.shape[0]), dtype=bool)\n        for iseg, seg in enumerate(ssegs):\n            masks[iseg][seg] = True\n        nonunique = np.sum(masks, axis=0) > 1\n        ssegs = []\n        for iseg, mask in enumerate(masks):\n            mask[nonunique] = False\n            ssegs.append(np.arange(Dseg.shape[0], dtype=int)[mask])\n        # compute new tips within new segments\n        ssegs_tips = []\n        for inewseg, newseg in enumerate(ssegs):\n            if len(np.flatnonzero(newseg)) <= 1:\n                logg.warn('detected group with only {} cells'.format(np.flatnonzero(newseg)))\n            secondtip = newseg[np.argmax(Dseg[tips[inewseg]][newseg])]\n            ssegs_tips.append([tips[inewseg], secondtip])\n        undecided_cells = np.arange(Dseg.shape[0], dtype=int)[nonunique]\n        if len(undecided_cells) > 0:\n            ssegs.append(undecided_cells)\n            # establish the connecting points with the other segments\n            ssegs_connects = [[], [], [], []]\n            for inewseg, newseg_tips in enumerate(ssegs_tips):\n                reference_point = newseg_tips[0]\n                # closest cell to the new segment within undecided cells\n                closest_cell = undecided_cells[np.argmin(Dseg[reference_point][undecided_cells])]\n                ssegs_connects[inewseg].append(closest_cell)\n                # closest cell to the undecided cells within new segment\n                closest_cell = ssegs[inewseg][np.argmin(Dseg[closest_cell][ssegs[inewseg]])]\n                ssegs_connects[-1].append(closest_cell)\n            # also compute tips for the undecided cells\n            tip_0 = undecided_cells[np.argmax(Dseg[undecided_cells[0]][undecided_cells])]\n            tip_1 = undecided_cells[np.argmax(Dseg[tip_0][undecided_cells])]\n            ssegs_tips.append([tip_0, tip_1])\n            ssegs_adjacency = [[3], [3], [3], [0, 1, 2]]\n            trunk = 3\n        elif len(ssegs) == 3:\n            reference_point = np.zeros(3, dtype=int)\n            reference_point[0] = ssegs_tips[0][0]\n            reference_point[1] = ssegs_tips[1][0]\n            reference_point[2] = ssegs_tips[2][0]\n            closest_points = np.zeros((3, 3), dtype=int)\n            # this is another strategy than for the undecided_cells\n            # here it's possible to use the more symmetric procedure\n            # shouldn't make much of a difference\n            closest_points[0, 1] = ssegs[1][np.argmin(Dseg[reference_point[0]][ssegs[1]])]\n            closest_points[1, 0] = ssegs[0][np.argmin(Dseg[reference_point[1]][ssegs[0]])]\n            closest_points[0, 2] = ssegs[2][np.argmin(Dseg[reference_point[0]][ssegs[2]])]\n            closest_points[2, 0] = ssegs[0][np.argmin(Dseg[reference_point[2]][ssegs[0]])]\n            closest_points[1, 2] = ssegs[2][np.argmin(Dseg[reference_point[1]][ssegs[2]])]\n            closest_points[2, 1] = ssegs[1][np.argmin(Dseg[reference_point[2]][ssegs[1]])]\n            added_dist = np.zeros(3)\n            added_dist[0] = Dseg[closest_points[1, 0], closest_points[0, 1]] + Dseg[closest_points[2, 0], closest_points[0, 2]]\n            added_dist[1] = Dseg[closest_points[0, 1], closest_points[1, 0]] + Dseg[closest_points[2, 1], closest_points[1, 2]]\n            added_dist[2] = Dseg[closest_points[1, 2], closest_points[2, 1]] + Dseg[closest_points[0, 2], closest_points[2, 0]]\n            trunk = np.argmin(added_dist)\n            ssegs_adjacency = [[trunk] if i != trunk else\n                               [j for j in range(3) if j != trunk]\n                               for i in range(3)]\n            ssegs_connects = [[closest_points[i, trunk]] if i != trunk else\n                              [closest_points[trunk, j] for j in range(3) if j != trunk]\n                              for i in range(3)]\n        else:\n            trunk = 0\n            ssegs_adjacency = [[1], [0]]\n            reference_point_in_0 = ssegs_tips[0][0]\n            closest_point_in_1 = ssegs[1][np.argmin(Dseg[reference_point_in_0][ssegs[1]])]\n            reference_point_in_1 = closest_point_in_1  # ssegs_tips[1][0]\n            closest_point_in_0 = ssegs[0][np.argmin(Dseg[reference_point_in_1][ssegs[0]])]\n            ssegs_connects = [[closest_point_in_1], [closest_point_in_0]]\n        return ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _detect_branching_single_haghverdi16(self, Dseg, tips):\n        # compute branchings using different starting points the first index of\n        # tips is the starting point for the other two, the order does not\n        # matter\n        ssegs = []\n        # permutations of tip cells\n        ps = [[0, 1, 2],  # start by computing distances from the first tip\n              [1, 2, 0],  #             -\"-                       second tip\n              [2, 0, 1]]  #             -\"-                       third tip\n        for i, p in enumerate(ps):\n            ssegs.append(self.__detect_branching_haghverdi16(Dseg, tips[p]))\n        return ssegs", "response": "Detect branching on a single Haghverdi 16 - bit segment."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __detect_branching_haghverdi16(self, Dseg, tips):\n        # sort distance from first tip point\n        # then the sequence of distances Dseg[tips[0]][idcs] increases\n        idcs = np.argsort(Dseg[tips[0]])\n        # consider now the sequence of distances from the other\n        # two tip points, which only increase when being close to `tips[0]`\n        # where they become correlated\n        # at the point where this happens, we define a branching point\n        if True:\n            imax = self.kendall_tau_split(Dseg[tips[1]][idcs],\n                                          Dseg[tips[2]][idcs])\n        if False:\n            # if we were in euclidian space, the following should work\n            # as well, but here, it doesn't because the scales in Dseg are\n            # highly different, one would need to write the following equation\n            # in terms of an ordering, such as exploited by the kendall\n            # correlation method above\n            imax = np.argmin(Dseg[tips[0]][idcs]\n                             + Dseg[tips[1]][idcs]\n                             + Dseg[tips[2]][idcs])\n        # init list to store new segments\n        ssegs = []\n        # first new segment: all points until, but excluding the branching point\n        # increasing the following slightly from imax is a more conservative choice\n        # as the criterion based on normalized distances, which follows below,\n        # is less stable\n        if imax > 0.95 * len(idcs) and self.allow_kendall_tau_shift:\n            # if \"everything\" is correlated (very large value of imax), a more\n            # conservative choice amounts to reducing this\n            logg.warn('shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`)')\n            ibranch = int(0.95 * imax)\n        else:\n            # otherwise, a more conservative choice is the following\n            ibranch = imax + 1\n        return idcs[:ibranch]", "response": "Detect branching on given segment."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning splitting index that maximizes correlation in the sequences. Compute difference in Kendall tau for all splitted sequences. For each splitting index i, compute the difference of the two correlation measures kendalltau(a[:i], b[:i]) and kendalltau(a[i:], b[i:]). Returns the splitting index that maximizes kendalltau(a[:i], b[:i]) - kendalltau(a[i:], b[i:]) Parameters ---------- a, b : np.ndarray One dimensional sequences. Returns ------- Splitting index according to above description.", "response": "def kendall_tau_split(self, a, b) -> int:\n        \"\"\"Return splitting index that maximizes correlation in the sequences.\n\n        Compute difference in Kendall tau for all splitted sequences.\n\n        For each splitting index i, compute the difference of the two\n        correlation measures kendalltau(a[:i], b[:i]) and\n        kendalltau(a[i:], b[i:]).\n\n        Returns the splitting index that maximizes\n            kendalltau(a[:i], b[:i]) - kendalltau(a[i:], b[i:])\n\n        Parameters\n        ----------\n        a, b : np.ndarray\n            One dimensional sequences.\n\n        Returns\n        -------\n        Splitting index according to above description.\n        \"\"\"\n        if a.size != b.size:\n            raise ValueError('a and b need to have the same size')\n        if a.ndim != b.ndim != 1:\n            raise ValueError('a and b need to be one-dimensional arrays')\n        import scipy as sp\n        min_length = 5\n        n = a.size\n        idx_range = np.arange(min_length, a.size-min_length-1, dtype=int)\n        corr_coeff = np.zeros(idx_range.size)\n        pos_old = sp.stats.kendalltau(a[:min_length], b[:min_length])[0]\n        neg_old = sp.stats.kendalltau(a[min_length:], b[min_length:])[0]\n        for ii, i in enumerate(idx_range):\n            if True:\n                # compute differences in concordance when adding a[i] and b[i]\n                # to the first subsequence, and removing these elements from\n                # the second subsequence\n                diff_pos, diff_neg = self._kendall_tau_diff(a, b, i)\n                pos = pos_old + self._kendall_tau_add(i, diff_pos, pos_old)\n                neg = neg_old + self._kendall_tau_subtract(n-i, diff_neg, neg_old)\n                pos_old = pos\n                neg_old = neg\n            if False:\n                # computation using sp.stats.kendalltau, takes much longer!\n                # just for debugging purposes\n                pos = sp.stats.kendalltau(a[:i+1], b[:i+1])[0]\n                neg = sp.stats.kendalltau(a[i+1:], b[i+1:])[0]\n            if False:\n                # the following is much slower than using sp.stats.kendalltau,\n                # it is only good for debugging because it allows to compute the\n                # tau-a version, which does not account for ties, whereas\n                # sp.stats.kendalltau computes tau-b version, which accounts for\n                # ties\n                pos = sp.stats.mstats.kendalltau(a[:i], b[:i], use_ties=False)[0]\n                neg = sp.stats.mstats.kendalltau(a[i:], b[i:], use_ties=False)[0]\n            corr_coeff[ii] = pos - neg\n        iimax = np.argmax(corr_coeff)\n        imax = min_length + iimax\n        corr_coeff_max = corr_coeff[iimax]\n        if corr_coeff_max < 0.3:\n            logg.m('    is root itself, never obtain significant correlation', v=4)\n        return imax"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the Kendall tau delta.", "response": "def _kendall_tau_add(self, len_old, diff_pos, tau_old):\n        \"\"\"Compute Kendall tau delta.\n\n        The new sequence has length len_old + 1.\n\n        Parameters\n        ----------\n        len_old : int\n            The length of the old sequence, used to compute tau_old.\n        diff_pos : int\n            Difference between concordant and non-concordant pairs.\n        tau_old : float\n            Kendall rank correlation of the old sequence.\n        \"\"\"\n        return 2./(len_old+1)*(float(diff_pos)/len_old-tau_old)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the Kendall tau delta.", "response": "def _kendall_tau_subtract(self, len_old, diff_neg, tau_old):\n        \"\"\"Compute Kendall tau delta.\n\n        The new sequence has length len_old - 1.\n\n        Parameters\n        ----------\n        len_old : int\n            The length of the old sequence, used to compute tau_old.\n        diff_neg : int\n            Difference between concordant and non-concordant pairs.\n        tau_old : float\n            Kendall rank correlation of the old sequence.\n        \"\"\"\n        return 2./(len_old-2)*(-float(diff_neg)/(len_old-1)+tau_old)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _kendall_tau_diff(self, a: np.ndarray, b: np.ndarray, i) -> Tuple[int, int]:\n        # compute ordering relation of the single points a[i] and b[i]\n        # with all previous points of the sequences a and b, respectively\n        a_pos = np.zeros(a[:i].size, dtype=int)\n        a_pos[a[:i] > a[i]] = 1\n        a_pos[a[:i] < a[i]] = -1\n        b_pos = np.zeros(b[:i].size, dtype=int)\n        b_pos[b[:i] > b[i]] = 1\n        b_pos[b[:i] < b[i]] = -1\n        diff_pos = np.dot(a_pos, b_pos).astype(float)\n\n        # compute ordering relation of the single points a[i] and b[i]\n        # with all later points of the sequences\n        a_neg = np.zeros(a[i:].size, dtype=int)\n        a_neg[a[i:] > a[i]] = 1\n        a_neg[a[i:] < a[i]] = -1\n        b_neg = np.zeros(b[i:].size, dtype=int)\n        b_neg[b[i:] > b[i]] = 1\n        b_neg[b[i:] < b[i]] = -1\n        diff_neg = np.dot(a_neg, b_neg)\n\n        return diff_pos, diff_neg", "response": "Compute the difference in concordance of pairs in split sequences."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef doc_params(**kwds):\n    def dec(obj):\n        obj.__doc__ = dedent(obj.__doc__).format(**kwds)\n        return obj\n    return dec", "response": "Decorator to format the docstrings of the object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merge_groups(adata, key, map_groups, key_added=None, map_colors=None):\n    if key_added is None:\n        key_added = key + '_merged'\n    adata.obs[key_added] = adata.obs[key].map(\n        map_groups).astype(CategoricalDtype())\n    old_categories = adata.obs[key].cat.categories\n    new_categories = adata.obs[key_added].cat.categories\n    # map_colors is passed\n    if map_colors is not None:\n        old_colors = None\n        if key + '_colors' in adata.uns:\n            old_colors = adata.uns[key + '_colors']\n        new_colors = []\n        for group in adata.obs[key_added].cat.categories:\n            if group in map_colors:\n                new_colors.append(map_colors[group])\n            elif group in old_categories and old_colors is not None:\n                new_colors.append(old_colors[old_categories.get_loc(group)])\n            else:\n                raise ValueError('You didn\\'t specify a color for {}.'\n                                 .format(group))\n        adata.uns[key_added + '_colors'] = new_colors\n    # map_colors is not passed\n    elif key + '_colors' in adata.uns:\n        old_colors = adata.uns[key + '_colors']\n        inverse_map_groups = {g: [] for g in new_categories}\n        for old_group in old_categories:\n            inverse_map_groups[map_groups[old_group]].append(old_group)\n        new_colors = []\n        for group in new_categories:\n            # take the largest of the old groups\n            old_group = adata.obs[key][adata.obs[key].isin(\n                inverse_map_groups[group])].value_counts().index[0]\n            new_colors.append(old_colors[old_categories.get_loc(old_group)])\n        adata.uns[key_added + '_colors'] = new_colors", "response": "Merge groups with new ones."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cross_entropy_neighbors_in_rep(adata, use_rep, n_points=3):\n    # see below why we need this\n    if 'X_diffmap' not in adata.obsm.keys():\n        raise ValueError('Run `tl.diffmap` on `adata`, first.')\n\n    adata_ref = adata  # simple renaming, don't need copy here\n    adata_cmp = adata.copy()\n    n_neighbors = adata_ref.uns['neighbors']['params']['n_neighbors']\n    from .neighbors import neighbors\n    neighbors(adata_cmp, n_neighbors=n_neighbors, use_rep=use_rep)\n    from .tools.diffmap import diffmap\n    diffmap(adata_cmp)\n\n    graph_ref = adata_ref.uns['neighbors']['connectivities']\n    graph_cmp = adata_cmp.uns['neighbors']['connectivities']\n\n    graph_ref = graph_ref.tocoo()  # makes a copy\n    graph_cmp = graph_cmp.tocoo()\n\n    edgeset_ref = {e for e in zip(graph_ref.row, graph_ref.col)}\n    edgeset_cmp = {e for e in zip(graph_cmp.row, graph_cmp.col)}\n    edgeset_union = list(edgeset_ref.union(edgeset_cmp))\n\n    edgeset_union_indices = tuple(zip(*edgeset_union))\n    edgeset_union_indices = (np.array(edgeset_union_indices[0]), np.array(edgeset_union_indices[1]))\n\n    n_edges_ref = len(graph_ref.nonzero()[0])\n    n_edges_cmp = len(graph_cmp.nonzero()[0])\n    n_edges_union = len(edgeset_union)\n    logg.msg(\n        '... n_edges_ref', n_edges_ref,\n        'n_edges_cmp', n_edges_cmp,\n        'n_edges_union', n_edges_union)\n\n    graph_ref = graph_ref.tocsr()  # need a copy of the csr graph anyways\n    graph_cmp = graph_cmp.tocsr()\n\n    p_ref = graph_ref[edgeset_union_indices].A1\n    p_cmp = graph_cmp[edgeset_union_indices].A1\n\n    # the following is how one compares it to log_loss form sklearn\n    # p_ref[p_ref.nonzero()] = 1\n    # from sklearn.metrics import log_loss\n    # print(log_loss(p_ref, p_cmp))\n    p_cmp = np.clip(p_cmp, EPS, 1-EPS)\n    ratio = np.clip(p_ref / p_cmp, EPS, None)\n    ratio_1m = np.clip((1 - p_ref) / (1 - p_cmp), EPS, None)\n\n    entropy = np.sum(p_ref * np.log(ratio) + (1-p_ref) * np.log(ratio_1m))\n\n    n_edges_fully_connected = (graph_ref.shape[0]**2 - graph_ref.shape[0])\n    entropy /= n_edges_fully_connected\n\n    fraction_edges = n_edges_ref / n_edges_fully_connected\n    naive_entropy = (fraction_edges * np.log(1./fraction_edges)\n                     + (1-fraction_edges) * np.log(1./(1-fraction_edges)))\n    logg.msg('cross entropy of naive sparse prediction {:.3e}'.format(naive_entropy))\n    logg.msg('cross entropy of random prediction {:.3e}'.format(-np.log(0.5)))\n    logg.info('cross entropy {:.3e}'.format(entropy))\n\n    # for manifold analysis, restrict to largest connected component in\n    # reference\n    # now that we clip at a quite high value below, this might not even be\n    # necessary\n    n_components, labels = scipy.sparse.csgraph.connected_components(graph_ref)\n    largest_component = np.arange(graph_ref.shape[0], dtype=int)\n    if n_components > 1:\n        component_sizes = np.bincount(labels)\n        logg.msg('largest component has size', component_sizes.max())\n        largest_component = np.where(\n            component_sizes == component_sizes.max())[0][0]\n        graph_ref_red = graph_ref.tocsr()[labels == largest_component, :]\n        graph_ref_red = graph_ref_red.tocsc()[:, labels == largest_component]\n        graph_ref_red = graph_ref_red.tocoo()\n        graph_cmp_red = graph_cmp.tocsr()[labels == largest_component, :]\n        graph_cmp_red = graph_cmp_red.tocsc()[:, labels == largest_component]\n        graph_cmp_red = graph_cmp_red.tocoo()\n        edgeset_ref_red = {e for e in zip(graph_ref_red.row, graph_ref_red.col)}\n        edgeset_cmp_red = {e for e in zip(graph_cmp_red.row, graph_cmp_red.col)}\n        edgeset_union_red = edgeset_ref_red.union(edgeset_cmp_red)\n        map_indices = np.where(labels == largest_component)[0]\n        edgeset_union_red = {\n            (map_indices[i], map_indices[j]) for (i, j) in edgeset_union_red}\n\n    from .neighbors import Neighbors\n    neigh_ref = Neighbors(adata_ref)\n    dist_ref = neigh_ref.distances_dpt  # we expect 'X_diffmap' to be already present\n\n    neigh_cmp = Neighbors(adata_cmp)\n    dist_cmp = neigh_cmp.distances_dpt\n\n    d_cmp = np.zeros_like(p_ref)\n    d_ref = np.zeros_like(p_ref)\n    for i, e in enumerate(edgeset_union):\n        # skip contributions that are not in the largest component\n        if n_components > 1 and e not in edgeset_union_red:\n            continue\n        d_cmp[i] = dist_cmp[e]\n        d_ref[i] = dist_ref[e]\n\n    MAX_DIST = 1000\n    d_cmp = np.clip(d_cmp, 0.1, MAX_DIST)  # we don't want to measure collapsing clusters\n    d_ref = np.clip(d_ref, 0.1, MAX_DIST)\n\n    weights = np.array(d_cmp / d_ref)            # disconnected regions\n    weights_overlap = np.array(d_ref / d_cmp)    # overlapping regions\n\n    # the following is just for annotation of figures\n    if 'highlights' not in adata_ref.uns:\n        adata_ref.uns['highlights'] = {}\n    else:\n        # remove old disconnected and overlapping points\n        new_highlights = {}\n        for k, v in adata_ref.uns['highlights'].items():\n            if v != 'O' and v not in {'D0', 'D1', 'D2', 'D3', 'D4'}:\n                new_highlights[k] = v\n        adata_ref.uns['highlights'] = new_highlights\n\n    # points that are maximally disconnected\n    max_weights = np.argpartition(weights, kth=-n_points)[-n_points:]\n    points = list(edgeset_union_indices[0][max_weights])\n    points2 = list(edgeset_union_indices[1][max_weights])\n    found_disconnected_points = False\n    for ip, p in enumerate(points):\n        if d_cmp[max_weights][ip] == MAX_DIST:\n            adata_ref.uns['highlights'][p] = 'D' + str(ip)\n            adata_ref.uns['highlights'][points2[ip]] = 'D' + str(ip)\n            found_disconnected_points = True\n    if found_disconnected_points:\n        logg.msg('most disconnected points', points)\n        logg.msg('    with weights', weights[max_weights].round(1))\n\n    max_weights = np.argpartition(\n        weights_overlap, kth=-n_points)[-n_points:]\n    points = list(edgeset_union_indices[0][max_weights])\n    for p in points:\n        adata_ref.uns['highlights'][p] = 'O'\n    logg.msg('most overlapping points', points)\n    logg.msg('    with weights', weights_overlap[max_weights].round(1))\n    logg.msg('    with d_rep', d_cmp[max_weights].round(1))\n    logg.msg('    with d_ref', d_ref[max_weights].round(1))\n\n    geo_entropy_d = np.sum(weights * p_ref * np.log(ratio))\n    geo_entropy_o = np.sum(weights_overlap * (1-p_ref) * np.log(ratio_1m))\n\n    geo_entropy_d /= n_edges_fully_connected\n    geo_entropy_o /= n_edges_fully_connected\n\n    logg.info('geodesic cross entropy {:.3e}'.format(geo_entropy_d + geo_entropy_o))\n    return entropy, geo_entropy_d, geo_entropy_o", "response": "Compare neighborhood graph of representation based on cross entropy."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_graph_tool_from_adjacency(adjacency, directed=None):\n    import graph_tool as gt\n    adjacency_edge_list = adjacency\n    if not directed:\n        from scipy.sparse import tril\n        adjacency_edge_list = tril(adjacency)\n    g = gt.Graph(directed=directed)\n    g.add_vertex(adjacency.shape[0])  # this adds adjacency.shap[0] vertices\n    g.add_edge_list(np.transpose(adjacency_edge_list.nonzero()))\n    weights = g.new_edge_property('double')\n    for e in g.edges():\n        # graph_tool uses the following convention,\n        # which is opposite to the rest of scanpy\n        weights[e] = adjacency[int(e.source()), int(e.target())]\n    g.edge_properties['weight'] = weights\n    return g", "response": "Get graph_tool graph from adjacency matrix."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_igraph_from_adjacency(adjacency, directed=None):\n    import igraph as ig\n    sources, targets = adjacency.nonzero()\n    weights = adjacency[sources, targets]\n    if isinstance(weights, np.matrix):\n        weights = weights.A1\n    g = ig.Graph(directed=directed)\n    g.add_vertices(adjacency.shape[0])  # this adds adjacency.shap[0] vertices\n    g.add_edges(list(zip(sources, targets)))\n    try:\n        g.es['weight'] = weights\n    except:\n        pass\n    if g.vcount() != adjacency.shape[0]:\n        logg.warn('The constructed graph has only {} nodes. '\n                  'Your adjacency matrix contained redundant nodes.'\n                  .format(g.vcount()))\n    return g", "response": "Get igraph graph from adjacency matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the association matrix between two groups.", "response": "def compute_association_matrix_of_groups(adata, prediction, reference,\n                                         normalization='prediction',\n                                         threshold=0.01, max_n_names=2):\n    \"\"\"Compute overlaps between groups.\n\n    See ``identify_groups`` for identifying the groups.\n\n    Parameters\n    ----------\n    adata : AnnData\n    prediction : str\n        Field name of adata.obs.\n    reference : str\n        Field name of adata.obs.\n    normalization : {'prediction', 'reference'}\n        Whether to normalize with respect to the predicted groups or the\n        reference groups.\n    threshold : float, optional (default: 0.01)\n        Do not consider associations whose overlap is below this fraction.\n    max_n_names : int or None, optional (default: 2)\n        Control how many reference names you want to be associated with per\n        predicted name. Set to `None`, if you want all.\n\n    Returns\n    -------\n    Tuple of\n    asso_names : list of associated reference names (`max_n_names` for each\n        predicted name)\n    asso_matrix : matrix where rows correspond to the predicted labels and\n        columns to the reference labels, entries are proportional to degree of\n        association\n    \"\"\"\n    if normalization not in {'prediction', 'reference'}:\n        raise ValueError('`normalization` needs to be either \"prediction\" or \"reference\".')\n    sanitize_anndata(adata)\n    cats = adata.obs[reference].cat.categories\n    for cat in cats:\n        if cat in settings.categories_to_ignore:\n            logg.info('Ignoring category \\'{}\\' '\n                      'as it\\'s in `settings.categories_to_ignore`.'\n                      .format(cat))\n    asso_names = []\n    asso_matrix = []\n    for ipred_group, pred_group in enumerate(\n            adata.obs[prediction].cat.categories):\n        if '?' in pred_group: pred_group = str(ipred_group)\n        # starting from numpy version 1.13, subtractions of boolean arrays are deprecated\n        mask_pred = adata.obs[prediction].values == pred_group\n        mask_pred_int = mask_pred.astype(np.int8)\n        asso_matrix += [[]]\n        for ref_group in adata.obs[reference].cat.categories:\n            mask_ref = (adata.obs[reference].values == ref_group).astype(np.int8)\n            mask_ref_or_pred = mask_ref.copy()\n            mask_ref_or_pred[mask_pred] = 1\n            # e.g. if the pred group is contained in mask_ref, mask_ref and\n            # mask_ref_or_pred are the same\n            if normalization == 'prediction':\n                # compute which fraction of the predicted group is contained in\n                # the ref group\n                ratio_contained = (np.sum(mask_pred_int) -\n                    np.sum(mask_ref_or_pred - mask_ref)) / np.sum(mask_pred_int)\n            else:\n                # compute which fraction of the reference group is contained in\n                # the predicted group\n                ratio_contained = (np.sum(mask_ref) -\n                    np.sum(mask_ref_or_pred - mask_pred_int)) / np.sum(mask_ref)\n            asso_matrix[-1] += [ratio_contained]\n        name_list_pred = [cats[i] if cats[i] not in settings.categories_to_ignore else ''\n                          for i in np.argsort(asso_matrix[-1])[::-1]\n                          if asso_matrix[-1][i] > threshold]\n        asso_names += ['\\n'.join(name_list_pred[:max_n_names])]\n    Result = namedtuple('compute_association_matrix_of_groups',\n                        ['asso_names', 'asso_matrix'])\n    return Result(asso_names=asso_names, asso_matrix=np.array(asso_matrix))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the group overlap score for a predicted cluster.", "response": "def compute_group_overlap_score(ref_labels, pred_labels,\n                                threshold_overlap_pred=0.5,\n                                threshold_overlap_ref=0.5):\n    \"\"\"How well do the pred_labels explain the ref_labels?\n\n    A predicted cluster explains a reference cluster if it is contained within the reference\n    cluster with at least 50% (threshold_overlap_pred) of its points and these correspond\n    to at least 50% (threshold_overlap_ref) of the reference cluster.\n    \"\"\"\n    ref_unique, ref_counts = np.unique(ref_labels, return_counts=True)\n    ref_dict = dict(zip(ref_unique, ref_counts))\n    pred_unique, pred_counts = np.unique(pred_labels, return_counts=True)\n    pred_dict = dict(zip(pred_unique, pred_counts))\n    summary = []\n    for true in ref_unique:\n        sub_pred_unique, sub_pred_counts = np.unique(pred_labels[true == ref_labels], return_counts=True)\n        relative_overlaps_pred = [sub_pred_counts[i] / pred_dict[n] for i, n in enumerate(sub_pred_unique)]\n        relative_overlaps_ref = [sub_pred_counts[i] / ref_dict[true] for i, n in enumerate(sub_pred_unique)]\n        pred_best_index = np.argmax(relative_overlaps_pred)\n        summary.append(1 if (relative_overlaps_pred[pred_best_index] >= threshold_overlap_pred and\n                             relative_overlaps_ref[pred_best_index] >= threshold_overlap_ref)\n                       else 0)\n        # print(true, sub_pred_unique[pred_best_index], relative_overlaps_pred[pred_best_index],\n        #       relative_overlaps_ref[pred_best_index], summary[-1])\n    return sum(summary)/len(summary)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef identify_groups(ref_labels, pred_labels, return_overlaps=False):\n    ref_unique, ref_counts = np.unique(ref_labels, return_counts=True)\n    ref_dict = dict(zip(ref_unique, ref_counts))\n    pred_unique, pred_counts = np.unique(pred_labels, return_counts=True)\n    pred_dict = dict(zip(pred_unique, pred_counts))\n    associated_predictions = {}\n    associated_overlaps = {}\n    for ref_label in ref_unique:\n        sub_pred_unique, sub_pred_counts = np.unique(pred_labels[ref_label == ref_labels], return_counts=True)\n        relative_overlaps_pred = [sub_pred_counts[i] / pred_dict[n] for i, n in enumerate(sub_pred_unique)]\n        relative_overlaps_ref = [sub_pred_counts[i] / ref_dict[ref_label] for i, n in enumerate(sub_pred_unique)]\n        relative_overlaps = np.c_[relative_overlaps_pred, relative_overlaps_ref]\n        relative_overlaps_min = np.min(relative_overlaps, axis=1)\n        pred_best_index = np.argsort(relative_overlaps_min)[::-1]\n        associated_predictions[ref_label] = sub_pred_unique[pred_best_index]\n        associated_overlaps[ref_label] = relative_overlaps[pred_best_index]\n    if return_overlaps: return associated_predictions, associated_overlaps\n    else: return associated_predictions", "response": "Identify the groups of the given reference and predicted labels."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unique_categories(categories):\n    categories = np.unique(categories)\n    categories = np.setdiff1d(categories, np.array(settings.categories_to_ignore))\n    categories = np.array(natsorted(categories, key=lambda v: v.upper()))\n    return categories", "response": "Pass array - like categories return sorted unique categories."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fill_in_datakeys(example_parameters, dexdata):\n    # default initialization of 'datakey' key with entries from data dictionary\n    for exkey in example_parameters:\n        if 'datakey' not in example_parameters[exkey]:\n            if exkey in dexdata:\n                example_parameters[exkey]['datakey'] = exkey\n            else:\n                example_parameters[exkey]['datakey'] = 'unspecified in dexdata'\n    return example_parameters", "response": "Fill in the datakeys in the examples dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef moving_average(a, n):\n    ret = np.cumsum(a, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n", "response": "Moving average over one - dimensional array."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates old_params with new_params.", "response": "def update_params(old_params, new_params, check=False):\n    \"\"\"Update old_params with new_params.\n\n    If check==False, this merely adds and overwrites the content of old_params.\n\n    If check==True, this only allows updating of parameters that are already\n    present in old_params.\n\n    Parameters\n    ----------\n    old_params : dict\n    new_params : dict\n    check : bool, optional (default: False)\n\n    Returns\n    -------\n    updated_params : dict\n    \"\"\"\n    updated_params = dict(old_params)\n    if new_params:  # allow for new_params to be None\n        for key, val in new_params.items():\n            if key not in old_params and check:\n                raise ValueError('\\'' + key\n                                 + '\\' is not a valid parameter key, '\n                                 + 'consider one of \\n'\n                                 + str(list(old_params.keys())))\n            if val is not None:\n                updated_params[key] = val\n    return updated_params"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads args for single tool.", "response": "def read_args_tool(toolkey, example_parameters, tool_add_args=None):\n    \"\"\"Read args for single tool.\n    \"\"\"\n    import scanpy as sc\n    p = default_tool_argparser(help(toolkey), example_parameters)\n    if tool_add_args is None:\n        p = add_args(p)\n    else:\n        p = tool_add_args(p)\n    args = vars(p.parse_args())\n    args = settings.process_args(args)\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates default parser for single tools.", "response": "def default_tool_argparser(description, example_parameters):\n    \"\"\"Create default parser for single tools.\n    \"\"\"\n    import argparse\n    epilog = '\\n'\n    for k, v in sorted(example_parameters.items()):\n        epilog += '  ' + k + '\\n'\n    p = argparse.ArgumentParser(\n        description=description,\n        add_help=False,\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=('available values for examples (exkey):'+epilog))\n    return p"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef select_groups(adata, groups_order_subset='all', key='groups'):\n    groups_order = adata.obs[key].cat.categories\n    if key + '_masks' in adata.uns:\n        groups_masks = adata.uns[key + '_masks']\n    else:\n        groups_masks = np.zeros((len(adata.obs[key].cat.categories),\n                                 adata.obs[key].values.size), dtype=bool)\n        for iname, name in enumerate(adata.obs[key].cat.categories):\n            # if the name is not found, fallback to index retrieval\n            if adata.obs[key].cat.categories[iname] in adata.obs[key].values:\n                mask = adata.obs[key].cat.categories[iname] == adata.obs[key].values\n            else:\n                mask = str(iname) == adata.obs[key].values\n            groups_masks[iname] = mask\n    groups_ids = list(range(len(groups_order)))\n    if groups_order_subset != 'all':\n        groups_ids = []\n        for name in groups_order_subset:\n            groups_ids.append(\n                np.where(adata.obs[key].cat.categories.values == name)[0][0])\n        if len(groups_ids) == 0:\n            # fallback to index retrieval\n            groups_ids = np.where(\n                np.in1d(np.arange(len(adata.obs[key].cat.categories)).astype(str),\n                                          np.array(groups_order_subset)))[0]\n        if len(groups_ids) == 0:\n            logg.m(np.array(groups_order_subset),\n                   'invalid! specify valid groups_order (or indices) one of',\n                   adata.obs[key].cat.categories)\n            from sys import exit\n            exit(0)\n        groups_masks = groups_masks[groups_ids]\n        groups_order_subset = adata.obs[key].cat.categories[groups_ids].values\n    else:\n        groups_order_subset = groups_order.values\n    return groups_order_subset, groups_masks", "response": "Select subset of groups in adata. obs [ key ]."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pretty_dict_string(d, indent=0):\n    s = ''\n    for key, value in sorted(d.items()):\n        s += '    ' * indent + str(key)\n        if isinstance(value, dict):\n             s += '\\n' + pretty_dict_string(value, indent+1)\n        else:\n             s += '=' + str(value) + '\\n'\n    return s", "response": "Pretty output of nested dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef markdown_dict_string(d):\n    # sort experimental data from simulated data\n    from collections import OrderedDict\n    types = OrderedDict()\n    for key, value in sorted(d.items()):\n        if 'type' in value:\n            if value['type'] not in types:\n                types[value['type']] = []\n            types[value['type']].append(key)\n        else:\n            print(key, 'does not define data type!')\n    # format output\n    s = ''\n    for type in ['scRNAseq', 'scqPCR', 'bulk', 'simulated']:\n        s += '\\nExamples using ' + type + ' data.\\n'\n        for key in types[type]:\n            value = d[key]\n            s += '* [' + key + '](#' + key + ')'\n            if 'ref' in value:\n                if 'doi' in value:\n                    link = 'http://dx.doi.org/' + value['doi']\n                elif 'url' in value:\n                    link = value['url']\n                s += (' - [' +  value['ref'].replace('et al.','*et al.*')\n                             + '](' + link +  ')')\n            if 'title' in value:\n                s += '   \\n*' + value['title'] + '*'\n            s += '\\n'\n    return s", "response": "Return a markdown string that can be pasted in the examples / README. md.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef merge_dicts(*ds):\n    result = ds[0]\n    for d in ds[1:]:\n        result.update(d)\n    return result", "response": "Given any number of dicts shallow copy and merge into a new dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef masks(list_of_index_lists, n):\n    # make a list of mask arrays, it's easier to store\n    # as there is a hdf5 equivalent\n    for il,l in enumerate(list_of_index_lists):\n        mask = np.zeros(n,dtype=bool)\n        mask[l] = True\n        list_of_index_lists[il] = mask\n    # convert to arrays\n    masks = np.array(list_of_index_lists)\n    return masks", "response": "Make an array in which rows store 1d mask arrays from list of index lists."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef warn_with_traceback(message, category, filename, lineno, file=None, line=None):\n    import traceback\n    traceback.print_stack()\n    log = file if hasattr(file, 'write') else sys.stderr\n    settings.write(warnings.formatwarning(message, category, filename, lineno, line))", "response": "Print a warning message with a full traceback"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef subsample(X, subsample=1, seed=0):\n    if subsample == 1 and seed == 0:\n        return X, np.arange(X.shape[0], dtype=int)\n    if seed == 0:\n        # this sequence is defined simply by skipping rows\n        # is faster than sampling\n        rows = np.arange(0, X.shape[0], subsample, dtype=int)\n        n = rows.size\n        Xsampled = np.array(X[rows])\n    if seed > 0:\n        n = int(X.shape[0]/subsample)\n        np.random.seed(seed)\n        Xsampled, rows = subsample_n(X, n=n)\n    logg.m('... subsampled to', n, 'of', X.shape[0], 'data points')\n    return Xsampled, rows", "response": "Subsample a fraction of 1 / subsample samples from the rows of X."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef subsample_n(X, n=0, seed=0):\n    if n < 0:\n        raise ValueError('n must be greater 0')\n    np.random.seed(seed)\n    n = X.shape[0] if (n == 0 or n > X.shape[0]) else n\n    rows = np.random.choice(X.shape[0], size=n, replace=False)\n    Xsampled = X[rows]\n    return Xsampled, rows", "response": "Subsample n samples from rows of array X."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_presence_download(filename, backup_url):\n    import os\n    filename = str(filename) #  Throws error for Path on 3.5\n    if not os.path.exists(filename):\n        from .readwrite import download_progress\n        dr = os.path.dirname(filename)\n        try:\n            os.makedirs(dr)\n        except FileExistsError:\n            pass  # ignore if dir already exists\n        from urllib.request import urlretrieve\n        urlretrieve(backup_url, filename, reporthook=download_progress)", "response": "Check if file is present otherwise download."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hierarch_cluster(M):\n    import scipy as sp\n    import scipy.cluster\n    link = sp.cluster.hierarchy.linkage(M)\n    indices = sp.cluster.hierarchy.leaves_list(link)\n    Mclus = np.array(M[:, indices])\n    Mclus = Mclus[indices, :]\n    if False:\n        pl.matshow(Mclus)\n        pl.colorbar()\n    return Mclus, indices", "response": "Cluster matrix M using hierarchical clustering."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bbknn(adata, batch_key='batch', save_knn=False, copy=False, **kwargs):\n    params = locals()  # Has to be first\n    kwargs = params.pop('kwargs')\n    try:\n        from bbknn import bbknn\n    except ImportError:\n        raise ImportError('Please install bbknn: `pip install bbknn`.')\n    return bbknn(**params, **kwargs)", "response": "Bayesian KNN algorithm for calculating top neighbours in each cell."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef diffmap(adata, n_comps=15, copy=False):\n    if 'neighbors' not in adata.uns:\n        raise ValueError(\n            'You need to run `pp.neighbors` first to compute a neighborhood graph.')\n    if n_comps <= 2:\n        raise ValueError(\n            'Provide any value greater than 2 for `n_comps`. ')\n    adata = adata.copy() if copy else adata\n    _diffmap(adata, n_comps=n_comps)\n    return adata if copy else None", "response": "Diffusion maps for a single - cell network."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding names to the dependency map.", "response": "def _AddToDependencyMap(names):\n   \"\"\" Note: Must be holding the _lazyLock \"\"\"\n   curName = names[0]\n   _topLevelNames.add(curName)\n   for name in names[1:]:\n      _dependencyMap.setdefault(curName, set()).add(name)\n      curName = \".\".join([curName, name])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _CheckForDependency(parent, name):\n   if _allowCapitalizedNames:\n      # If the flag is set, check for both capitalized and\n      # uncapitalized form. This is a temporary fix for handling\n      # vim.EsxCLI namespace.\n      # Ex: If we add vim.EsxCLI.vdisk, then\n      # _dependencyMap['vim.EsxCLI'] will have value ['vdisk'].\n      # When we try to check dependency for vdisk, since the flag\n      # is set, we will uncapitalize EsxCLI and this will result\n      # in attribute error\n      dependents = _dependencyMap.get(parent)\n      if not dependents:\n         uncapitalizedParent = UncapitalizeVmodlName(parent)\n         dependents = _dependencyMap.get(uncapitalizedParent)\n\n      if dependents:\n         if name in dependents or Uncapitalize(name) in dependents:\n            return True\n   else:\n      dependents = _dependencyMap.get(parent)\n      if dependents:\n         if name in dependents:\n            return True\n   return False", "response": "Checks if a dependency exists for a given name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _AddType(type):\n   type.Array = CreateArrayType(type)\n\n   typeNS = GetWsdlNamespace(type._version)\n   newType = _SetWsdlType(typeNS, type._wsdlName, type)\n   if newType != type:\n      raise RuntimeError(\"Duplicate wsdl type %s (already in typemap)\" % (type._wsdlName))\n\n   return type", "response": "Add a type to the typemap."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetVersionNamespace(version):\n   ns = nsMap[version]\n   if not ns:\n      ns = serviceNsMap[version]\n   versionId = versionIdMap[version]\n   if not versionId:\n      namespace = ns\n   else:\n      namespace = '%s/%s' % (ns, versionId)\n   return namespace", "response": "Get version namespace from version"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetServiceVersions(namespace):\n   def compare(a, b):\n      if a == b:\n         return 0\n      if b in parentMap[a]:\n         return -1\n      if a in parentMap[b]:\n         return 1\n      return (a > b) - (a < b)\n\n   if PY3:\n      return sorted([v for (v, n) in iteritems(serviceNsMap) if n == namespace],\n                    key=cmp_to_key(compare))\n   else:\n      return sorted([v for (v, n) in iteritems(serviceNsMap) if n == namespace],\n                    compare)", "response": "Get all the versions for the specified namespace in the list ordered by compatibility"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _SetWsdlMethod(ns, wsdlName, inputMM):\n   _wsdlMethodNSs.add(ns)\n   curMM = _wsdlMethodMap.get( (ns, wsdlName) )\n   # if inputMM is a list\n   if isinstance(inputMM, list):\n      if curMM is None:\n         _wsdlMethodMap[(ns, wsdlName)] = inputMM\n         return inputMM\n      elif isinstance(curMM, list):\n         raise RuntimeError(\n                   \"Duplicate wsdl method %s %s (new class %s vs existing %s)\" % \\\n                   (ns, wsdlName, inputMM[0], curMM[0]))\n      else:\n         return curMM\n   # if inputMM is a ManagedMethod\n   else:\n      if curMM is None or isinstance(curMM, list):\n         _wsdlMethodMap[(ns, wsdlName)] = inputMM\n         return inputMM\n      else:\n         return curMM", "response": "Sets a WSDL method with wsdl namespace and wsdl name and returns the existing method or None if the method is already in the map"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets wsdl method from ns wsdlName", "response": "def GetWsdlMethod(ns, wsdlName):\n   \"\"\" Get wsdl method from ns, wsdlName \"\"\"\n   with _lazyLock:\n      method = _wsdlMethodMap[(ns, wsdlName)]\n      if isinstance(method, ManagedMethod):\n         # The type corresponding to the method is loaded,\n         # just return the method object\n         return method\n      elif method:\n         # The type is not loaded, the map contains the info\n         # to load the type. Load the actual type and\n         # return the method object\n         LoadManagedType(*method)\n         return _wsdlMethodMap[(ns, wsdlName)]\n      else:\n         raise KeyError(\"{0} {1}\".format(ns, name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets type from vmodl name", "response": "def GetVmodlType(name):\n   \"\"\" Get type from vmodl name \"\"\"\n\n   # If the input is already a type, just return\n   if isinstance(name, type):\n      return name\n\n   # Try to get type from vmodl type names table\n   typ = vmodlTypes.get(name)\n   if typ:\n      return typ\n\n   # Else get the type from the _wsdlTypeMap\n   isArray = name.endswith(\"[]\")\n   if isArray:\n      name = name[:-2]\n   ns, wsdlName = _GetWsdlInfo(name)\n   try:\n      typ = GetWsdlType(ns, wsdlName)\n   except KeyError:\n      raise KeyError(name)\n   if typ:\n      return isArray and typ.Array or typ\n   else:\n      raise KeyError(name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _GetActualName(name):\n   if _allowCapitalizedNames:\n      name = UncapitalizeVmodlName(name)\n   for defMap in _dataDefMap, _managedDefMap, _enumDefMap:\n      dic = defMap.get(name)\n      if dic:\n         return dic[0]\n   return None", "response": "Get the actual name for a vmodl name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef explode(self, obj):\n        if obj in self._done:\n            return False\n        result = False\n        for item in self._explode:\n            if hasattr(item, '_moId'):\n                # If it has a _moId it is an instance\n                if obj._moId == item._moId:\n                    result = True\n            else:\n                # If it does not have a _moId it is a template\n                if obj.__class__.__name__ == item.__name__:\n                    result = True\n        if result:\n            self._done.add(obj)\n        return result", "response": "Determine if the object should be exploded."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n\n   args = GetArgs()\n   if args.password:\n      password = args.password\n   else:\n      password = getpass.getpass(prompt='Enter password for host %s and user %s: ' % (args.host,args.user))\n\n   try:\n      vmnames = args.vmname\n      if not len(vmnames):\n         print(\"No virtual machine specified for poweron\")\n         sys.exit()\n\n      context = None\n      if hasattr(ssl, '_create_unverified_context'):\n         context = ssl._create_unverified_context()\n      si = SmartConnect(host=args.host,\n                        user=args.user,\n                        pwd=password,\n                        port=int(args.port),\n                        sslContext=context)\n      if not si:\n         print(\"Cannot connect to specified host using specified username and password\")\n         sys.exit()\n\n      atexit.register(Disconnect, si)\n\n      # Retreive the list of Virtual Machines from the inventory objects\n      # under the rootFolder\n      content = si.content\n      objView = content.viewManager.CreateContainerView(content.rootFolder,\n                                                        [vim.VirtualMachine],\n                                                        True)\n      vmList = objView.view\n      objView.Destroy()\n\n      # Find the vm and power it on\n      tasks = [vm.PowerOn() for vm in vmList if vm.name in vmnames]\n\n      # Wait for power on to complete\n      WaitForTasks(tasks, si)\n\n      print(\"Virtual Machine(s) have been powered on successfully\")\n   except vmodl.MethodFault as e:\n      print(\"Caught vmodl fault : \" + e.msg)\n   except Exception as e:\n      print(\"Caught Exception : \" + str(e))", "response": "Simple command - line program for powering on virtual machines on a system."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the command - line arguments for retrieving all the Virtual Machines", "response": "def GetArgs():\n   \"\"\"\n   Supports the command-line arguments listed below.\n   \"\"\"\n   parser = argparse.ArgumentParser(\n       description='Process args for retrieving all the Virtual Machines')\n   parser.add_argument('-s', '--host', required=True, action='store',\n                       help='Remote host to connect to')\n   parser.add_argument('-o', '--port', type=int, default=443, action='store',\n                       help='Port to connect on')\n   parser.add_argument('-u', '--user', required=True, action='store',\n                       help='User name to use when connecting to host')\n   parser.add_argument('-p', '--password', required=False, action='store',\n                       help='Password to use when connecting to host')\n   args = parser.parse_args()\n   return args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprint information for a particular virtual machine or a folder with depth protection.", "response": "def PrintVmInfo(vm, depth=1):\n   \"\"\"\n   Print information for a particular virtual machine or recurse into a folder\n   or vApp with depth protection\n   \"\"\"\n   maxdepth = 10\n\n   # if this is a group it will have children. if it does, recurse into them\n   # and then return\n   if hasattr(vm, 'childEntity'):\n      if depth > maxdepth:\n         return\n      vmList = vm.childEntity\n      for c in vmList:\n         PrintVmInfo(c, depth+1)\n      return\n\n   # if this is a vApp, it likely contains child VMs\n   # (vApps can nest vApps, but it is hardly a common usecase, so ignore that)\n   if isinstance(vm, vim.VirtualApp):\n      vmList = vm.vm\n      for c in vmList:\n         PrintVmInfo(c, depth + 1)\n      return\n\n   summary = vm.summary\n   print(\"Name       : \", summary.config.name)\n   print(\"Path       : \", summary.config.vmPathName)\n   print(\"Guest      : \", summary.config.guestFullName)\n   annotation = summary.config.annotation\n   if annotation != None and annotation != \"\":\n      print(\"Annotation : \", annotation)\n   print(\"State      : \", summary.runtime.powerState)\n   if summary.guest != None:\n      ip = summary.guest.ipAddress\n      if ip != None and ip != \"\":\n         print(\"IP         : \", ip)\n   if summary.runtime.question != None:\n      print(\"Question  : \", summary.runtime.question.text)\n   print(\"\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef localSslFixup(host, sslContext):\n    if not sslContext and host in ['localhost', '127.0.0.1', '::1']:\n        import ssl\n        if hasattr(ssl, '_create_unverified_context'):\n            sslContext = ssl._create_unverified_context()\n    return sslContext", "response": "This function is used to fix up SSL connection to localhost."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconnects to a server and return a service object.", "response": "def Connect(host='localhost', port=443, user='root', pwd='',\n            service=\"hostd\", adapter=\"SOAP\", namespace=None, path=\"/sdk\",\n            connectionPoolTimeout=CONNECTION_POOL_IDLE_TIMEOUT_SEC,\n            version=None, keyFile=None, certFile=None, thumbprint=None,\n            sslContext=None, b64token=None, mechanism='userpass'):\n   \"\"\"\n   Connect to the specified server, login and return the service\n   instance object.\n\n   Throws any exception back to caller. The service instance object is\n   also saved in the library for easy access.\n\n   Clients should modify the service parameter only when connecting to\n   a VMOMI server other than hostd/vpxd. For both of the latter, the\n   default value is fine.\n\n   @param host: Which host to connect to.\n   @type  host: string\n   @param port: Port\n   @type  port: int\n   @param user: User\n   @type  user: string\n   @param pwd: Password\n   @type  pwd: string\n   @param service: Service\n   @type  service: string\n   @param adapter: Adapter\n   @type  adapter: string\n   @param namespace: Namespace *** Deprecated: Use version instead ***\n   @type  namespace: string\n   @param path: Path\n   @type  path: string\n   @param connectionPoolTimeout: Timeout in secs for idle connections to close, specify negative numbers for never\n                                 closing the connections\n   @type  connectionPoolTimeout: int\n   @param version: Version\n   @type  version: string\n   @param keyFile: ssl key file path\n   @type  keyFile: string\n   @param certFile: ssl cert file path\n   @type  certFile: string\n   @param thumbprint: host cert thumbprint\n   @type  thumbprint: string\n   @param sslContext: SSL Context describing the various SSL options. It is only\n                      supported in Python 2.7.9 or higher.\n   @type  sslContext: SSL.Context\n   @param b64token: base64 encoded token\n   @type  b64token: string\n   @param mechanism: authentication mechanism: userpass or sspi\n   @type  mechanism: string\n   \"\"\"\n   try:\n      info = re.match(_rx, host)\n      if info is not None:\n         host = info.group(1)\n         if host[0] == '[':\n            host = info.group(1)[1:-1]\n         if info.group(2) is not None:\n            port = int(info.group(2)[1:])\n   except ValueError as ve:\n      pass\n\n   sslContext = localSslFixup(host, sslContext)\n\n   if namespace:\n      assert(version is None)\n      version = versionMap[namespace]\n   elif not version:\n      version = \"vim.version.version6\"\n\n   si, stub = None, None\n   if mechanism == 'userpass':\n      si, stub = __Login(host, port, user, pwd, service, adapter, version, path,\n                         keyFile, certFile, thumbprint, sslContext, connectionPoolTimeout)\n   elif mechanism == 'sspi':\n      si, stub = __LoginBySSPI(host, port, service, adapter, version, path,\n                               keyFile, certFile, thumbprint, sslContext, b64token, connectionPoolTimeout)\n   else:\n      raise Exception('''The provided connection mechanism is not available, the\n              supported mechanisms are userpass or sspi''')\n\n   SetSi(si)\n\n   return si"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ConnectNoSSL(host='localhost', port=443, user='root', pwd='',\n                 service=\"hostd\", adapter=\"SOAP\", namespace=None, path=\"/sdk\",\n                 version=None, keyFile=None, certFile=None, thumbprint=None,\n                 b64token=None, mechanism='userpass'):\n   \"\"\"\n   Provides a standard method for connecting to a specified server without SSL\n   verification. Useful when connecting to servers with self-signed certificates\n   or when you wish to ignore SSL altogether. Will attempt to create an unverified\n   SSL context and then connect via the Connect method.\n   \"\"\"\n\n   if hasattr(ssl, '_create_unverified_context'):\n      sslContext = ssl._create_unverified_context()\n   else:\n      sslContext = None\n\n   return Connect(host=host,\n                  port=port,\n                  user=user,\n                  pwd=pwd,\n                  service=service,\n                  adapter=adapter,\n                  namespace=namespace,\n                  path=path,\n                  version=version,\n                  keyFile=keyFile,\n                  certFile=certFile,\n                  thumbprint=thumbprint,\n                  sslContext=sslContext,\n                  b64token=b64token,\n                  mechanism=mechanism)", "response": "Creates a new connection object with no SSL."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __Login(host, port, user, pwd, service, adapter, version, path,\n            keyFile, certFile, thumbprint, sslContext,\n            connectionPoolTimeout=CONNECTION_POOL_IDLE_TIMEOUT_SEC):\n   \"\"\"\n   Private method that performs the actual Connect and returns a\n   connected service instance object.\n\n   @param host: Which host to connect to.\n   @type  host: string\n   @param port: Port\n   @type  port: int\n   @param user: User\n   @type  user: string\n   @param pwd: Password\n   @type  pwd: string\n   @param service: Service\n   @type  service: string\n   @param adapter: Adapter\n   @type  adapter: string\n   @param version: Version\n   @type  version: string\n   @param path: Path\n   @type  path: string\n   @param keyFile: ssl key file path\n   @type  keyFile: string\n   @param certFile: ssl cert file path\n   @type  certFile: string\n   @param thumbprint: host cert thumbprint\n   @type  thumbprint: string\n   @param sslContext: SSL Context describing the various SSL options. It is only\n                      supported in Python 2.7.9 or higher.\n   @type  sslContext: SSL.Context\n   @param connectionPoolTimeout: Timeout in secs for idle connections to close, specify negative numbers for never\n                                 closing the connections\n   @type  connectionPoolTimeout: int\n   \"\"\"\n\n   content, si, stub = __RetrieveContent(host, port, adapter, version, path,\n                                         keyFile, certFile, thumbprint, sslContext, connectionPoolTimeout)\n\n   # Get a ticket if we're connecting to localhost and password is not specified\n   if host == 'localhost' and not pwd:\n      try:\n         (user, pwd) = GetLocalTicket(si, user)\n      except:\n         pass # This is not supported against vCenter, and connecting\n              # with an empty password is fine in debug builds\n\n   # Login\n   try:\n      x = content.sessionManager.Login(user, pwd, None)\n   except vim.fault.InvalidLogin:\n      raise\n   except Exception as e:\n      raise\n   return si, stub", "response": "Private method that performs the actual Connect and returns a service instance object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __LoginBySSPI(host, port, service, adapter, version, path,\n                  keyFile, certFile, thumbprint, sslContext, b64token,\n                  connectionPoolTimeout=CONNECTION_POOL_IDLE_TIMEOUT_SEC):\n   \"\"\"\n   Private method that performs the actual Connect and returns a\n   connected service instance object.\n\n   @param host: Which host to connect to.\n   @type  host: string\n   @param port: Port\n   @type  port: int\n   @param service: Service\n   @type  service: string\n   @param adapter: Adapter\n   @type  adapter: string\n   @param version: Version\n   @type  version: string\n   @param path: Path\n   @type  path: string\n   @param keyFile: ssl key file path\n   @type  keyFile: string\n   @param certFile: ssl cert file path\n   @type  certFile: string\n   @param thumbprint: host cert thumbprint\n   @type  thumbprint: string\n   @param sslContext: SSL Context describing the various SSL options. It is only\n                      supported in Python 2.7.9 or higher.\n   @type  sslContext: SSL.Context\n   @param b64token: base64 encoded token\n   @type  b64token: string\n   @param connectionPoolTimeout: Timeout in secs for idle connections to close, specify negative numbers for never\n                                 closing the connections\n   @type  connectionPoolTimeout: int\n   \"\"\"\n\n   content, si, stub = __RetrieveContent(host, port, adapter, version, path,\n                                         keyFile, certFile, thumbprint, sslContext, connectionPoolTimeout)\n\n   if b64token is None:\n      raise Exception('Token is not defined for sspi login')\n\n   # Login\n   try:\n      x = content.sessionManager.LoginBySSPI(b64token)\n   except vim.fault.InvalidLogin:\n      raise\n   except Exception as e:\n      raise\n   return si, stub", "response": "Private method that performs the actual Connect and returns a connection object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndisconnecting (logout) service instance @param si: Service instance (returned from Connect)", "response": "def __Logout(si):\n   \"\"\"\n   Disconnect (logout) service instance\n   @param si: Service instance (returned from Connect)\n   \"\"\"\n   try:\n      if si:\n         content = si.RetrieveContent()\n         content.sessionManager.Logout()\n   except Exception as e:\n      pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __RetrieveContent(host, port, adapter, version, path, keyFile, certFile,\n                      thumbprint, sslContext, connectionPoolTimeout=CONNECTION_POOL_IDLE_TIMEOUT_SEC):\n   \"\"\"\n   Retrieve service instance for connection.\n   @param host: Which host to connect to.\n   @type  host: string\n   @param port: Port\n   @type  port: int\n   @param adapter: Adapter\n   @type  adapter: string\n   @param version: Version\n   @type  version: string\n   @param path: Path\n   @type  path: string\n   @param keyFile: ssl key file path\n   @type  keyFile: string\n   @param certFile: ssl cert file path\n   @type  certFile: string\n   @param connectionPoolTimeout: Timeout in secs for idle connections to close, specify negative numbers for never\n                                 closing the connections\n   @type  connectionPoolTimeout: int\n   \"\"\"\n\n   # XXX remove the adapter and service arguments once dependent code is fixed\n   if adapter != \"SOAP\":\n      raise ValueError(adapter)\n\n   # Create the SOAP stub adapter\n   stub = SoapStubAdapter(host, port, version=version, path=path,\n                          certKeyFile=keyFile, certFile=certFile,\n                          thumbprint=thumbprint, sslContext=sslContext,\n                          connectionPoolTimeout=connectionPoolTimeout)\n\n   # Get Service instance\n   si = vim.ServiceInstance(\"ServiceInstance\", stub)\n   content = None\n   try:\n      content = si.RetrieveContent()\n   except vmodl.MethodFault:\n      raise\n   except Exception as e:\n      # NOTE (hartsock): preserve the traceback for diagnostics\n      # pulling and preserving the traceback makes diagnosing connection\n      # failures easier since the fault will also include where inside the\n      # library the fault occurred. Without the traceback we have no idea\n      # why the connection failed beyond the message string.\n      (type, value, traceback) = sys.exc_info()\n      if traceback:\n         fault = vim.fault.HostConnectFault(msg=str(e))\n         reraise(vim.fault.HostConnectFault, fault, traceback)\n      else:\n          raise vim.fault.HostConnectFault(msg=str(e))\n\n   return content, si, stub", "response": "Retrieve the content of a connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef SmartStubAdapter(host='localhost', port=443, path='/sdk',\n                     url=None, sock=None, poolSize=5,\n                     certFile=None, certKeyFile=None,\n                     httpProxyHost=None, httpProxyPort=80, sslProxyPath=None,\n                     thumbprint=None, cacertsFile=None, preferredApiVersions=None,\n                     acceptCompressedResponses=True,\n                     connectionPoolTimeout=CONNECTION_POOL_IDLE_TIMEOUT_SEC,\n                     samlToken=None, sslContext=None):\n   \"\"\"\n   Determine the most preferred API version supported by the specified server,\n   then create a soap stub adapter using that version\n\n   The parameters are the same as for pyVmomi.SoapStubAdapter except for\n   version which is renamed to prefferedApiVersions\n\n   @param preferredApiVersions: Acceptable API version(s) (e.g. vim.version.version3)\n                                If a list of versions is specified the versions should\n                                be ordered from most to least preferred.  If None is\n                                specified, the list of versions support by pyVmomi will\n                                be used.\n   @type  preferredApiVersions: string or string list\n   \"\"\"\n   if preferredApiVersions is None:\n      preferredApiVersions = GetServiceVersions('vim25')\n\n   sslContext = localSslFixup(host, sslContext)\n\n   supportedVersion = __FindSupportedVersion('https' if port > 0 else 'http',\n                                             host,\n                                             port,\n                                             path,\n                                             preferredApiVersions,\n                                             sslContext)\n   if supportedVersion is None:\n      raise Exception(\"%s:%s is not a VIM server\" % (host, port))\n\n   return SoapStubAdapter(host=host, port=port, path=path,\n                          url=url, sock=sock, poolSize=poolSize,\n                          certFile=certFile, certKeyFile=certKeyFile,\n                          httpProxyHost=httpProxyHost, httpProxyPort=httpProxyPort,\n                          sslProxyPath=sslProxyPath, thumbprint=thumbprint,\n                          cacertsFile=cacertsFile, version=supportedVersion,\n                          acceptCompressedResponses=acceptCompressedResponses,\n                          connectionPoolTimeout=connectionPoolTimeout,\n                          samlToken=samlToken, sslContext=sslContext)", "response": "Create a new instance of a SmartStubAdapter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconnect to a server using the specified server.", "response": "def SmartConnect(protocol='https', host='localhost', port=443, user='root', pwd='',\n                 service=\"hostd\", path=\"/sdk\", connectionPoolTimeout=CONNECTION_POOL_IDLE_TIMEOUT_SEC,\n                 preferredApiVersions=None, keyFile=None, certFile=None,\n                 thumbprint=None, sslContext=None, b64token=None, mechanism='userpass'):\n   \"\"\"\n   Determine the most preferred API version supported by the specified server,\n   then connect to the specified server using that API version, login and return\n   the service instance object.\n\n   Throws any exception back to caller. The service instance object is\n   also saved in the library for easy access.\n\n   Clients should modify the service parameter only when connecting to\n   a VMOMI server other than hostd/vpxd. For both of the latter, the\n   default value is fine.\n\n   @param protocol: What protocol to use for the connection (e.g. https or http).\n   @type  protocol: string\n   @param host: Which host to connect to.\n   @type  host: string\n   @param port: Port\n   @type  port: int\n   @param user: User\n   @type  user: string\n   @param pwd: Password\n   @type  pwd: string\n   @param service: Service\n   @type  service: string\n   @param path: Path\n   @type  path: string\n   @param connectionPoolTimeout: Timeout in secs for idle connections to close, specify negative numbers for never\n                                 closing the connections\n   @type  connectionPoolTimeout: int\n   @param preferredApiVersions: Acceptable API version(s) (e.g. vim.version.version3)\n                                If a list of versions is specified the versions should\n                                be ordered from most to least preferred.  If None is\n                                specified, the list of versions support by pyVmomi will\n                                be used.\n   @type  preferredApiVersions: string or string list\n   @param keyFile: ssl key file path\n   @type  keyFile: string\n   @param certFile: ssl cert file path\n   @type  certFile: string\n   @param thumbprint: host cert thumbprint\n   @type  thumbprint: string\n   @param sslContext: SSL Context describing the various SSL options. It is only\n                      supported in Python 2.7.9 or higher.\n   @type  sslContext: SSL.Context\n   \"\"\"\n\n   if preferredApiVersions is None:\n      preferredApiVersions = GetServiceVersions('vim25')\n\n   sslContext = localSslFixup(host, sslContext)\n\n   supportedVersion = __FindSupportedVersion(protocol,\n                                             host,\n                                             port,\n                                             path,\n                                             preferredApiVersions,\n                                             sslContext)\n   if supportedVersion is None:\n      raise Exception(\"%s:%s is not a VIM server\" % (host, port))\n\n   portNumber = protocol == \"http\" and -int(port) or int(port)\n\n   return Connect(host=host,\n                  port=portNumber,\n                  user=user,\n                  pwd=pwd,\n                  service=service,\n                  adapter='SOAP',\n                  version=supportedVersion,\n                  path=path,\n                  connectionPoolTimeout=connectionPoolTimeout,\n                  keyFile=keyFile,\n                  certFile=certFile,\n                  thumbprint=thumbprint,\n                  sslContext=sslContext,\n                  b64token=b64token,\n                  mechanism=mechanism)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new connection to a specified server without SSL.", "response": "def SmartConnectNoSSL(protocol='https', host='localhost', port=443, user='root', pwd='',\n                      service=\"hostd\", path=\"/sdk\", connectionPoolTimeout=CONNECTION_POOL_IDLE_TIMEOUT_SEC,\n                      preferredApiVersions=None, keyFile=None, certFile=None,\n                      thumbprint=None, b64token=None, mechanism='userpass'):\n   \"\"\"\n   Provides a standard method for connecting to a specified server without SSL\n   verification. Useful when connecting to servers with self-signed certificates\n   or when you wish to ignore SSL altogether. Will attempt to create an unverified\n   SSL context and then connect via the SmartConnect method.\n   \"\"\"\n\n   if hasattr(ssl, '_create_unverified_context'):\n      sslContext = ssl._create_unverified_context()\n   else:\n      sslContext = None\n\n   return SmartConnect(protocol=protocol,\n                       host=host,\n                       port=port,\n                       user=user,\n                       pwd=pwd,\n                       service=service,\n                       path=path,\n                       connectionPoolTimeout=connectionPoolTimeout,\n                       preferredApiVersions=preferredApiVersions,\n                       keyFile=keyFile,\n                       certFile=certFile,\n                       thumbprint=thumbprint,\n                       sslContext=sslContext,\n                       b64token=b64token,\n                       mechanism=mechanism)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef OpenUrlWithBasicAuth(url, user='root', pwd=''):\n   return requests.get(url, auth=HTTPBasicAuth(user, pwd), verify=False)", "response": "Open a URL with HTTP basic authentication."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef OpenPathWithStub(path, stub):\n   from six.moves import http_client\n   if not hasattr(stub, 'scheme'):\n      raise vmodl.fault.NotSupported()\n   elif stub.scheme == http_client.HTTPConnection:\n      protocol = 'http'\n   elif stub.scheme == http_client.HTTPSConnection:\n      protocol = 'https'\n   else:\n      raise vmodl.fault.NotSupported()\n   hostPort = stub.host\n   url = '%s://%s%s' % (protocol, hostPort, path)\n   headers = {}\n   if stub.cookie:\n      headers[\"Cookie\"] = stub.cookie\n   return requests.get(url, headers=headers, verify=False)", "response": "Open the specified path using HTTP using the specified stub."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef makeUserLoginMethod(username, password, locale=None):\n      '''Return a function that will call the vim.SessionManager.Login() method\n      with the given parameters.  The result of this function can be passed as\n      the \"loginMethod\" to a SessionOrientedStub constructor.'''\n      def _doLogin(soapStub):\n         si = vim.ServiceInstance(\"ServiceInstance\", soapStub)\n         sm = si.content.sessionManager\n         if not sm.currentSession:\n            si.content.sessionManager.Login(username, password, locale)\n\n      return _doLogin", "response": "Returns a function that will call the vim. SessionManager. Login method with the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a function that will call the vim. SessionManager. Login method with the given parameters.", "response": "def makeExtensionLoginMethod(extensionKey):\n      '''Return a function that will call the vim.SessionManager.Login() method\n      with the given parameters.  The result of this function can be passed as\n      the \"loginMethod\" to a SessionOrientedStub constructor.'''\n      def _doLogin(soapStub):\n         si = vim.ServiceInstance(\"ServiceInstance\", soapStub)\n         sm = si.content.sessionManager\n         if not sm.currentSession:\n            si.content.sessionManager.LoginExtensionByCertificate(extensionKey)\n\n      return _doLogin"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef makeCertHokTokenLoginMethod(stsUrl, stsCert=None):\n      '''Return a function that will call the vim.SessionManager.LoginByToken()\n      after obtaining a HoK SAML token from the STS. The result of this function\n      can be passed as the \"loginMethod\" to a SessionOrientedStub constructor.\n\n      @param stsUrl: URL of the SAML Token issuing service. (i.e. SSO server).\n      @param stsCert: public key of the STS service.\n      '''\n      assert(stsUrl)\n\n      def _doLogin(soapStub):\n         from . import sso\n         cert =  soapStub.schemeArgs['cert_file']\n         key = soapStub.schemeArgs['key_file']\n         authenticator = sso.SsoAuthenticator(sts_url=stsUrl,\n                                              sts_cert=stsCert)\n\n         samlAssertion = authenticator.get_hok_saml_assertion(cert,key)\n\n\n         def _requestModifier(request):\n            return sso.add_saml_context(request, samlAssertion, key)\n\n         si = vim.ServiceInstance(\"ServiceInstance\", soapStub)\n         sm = si.content.sessionManager\n         if not sm.currentSession:\n            with soapStub.requestModifier(_requestModifier):\n               try:\n                  soapStub.samlToken = samlAssertion\n                  si.content.sessionManager.LoginByToken()\n               finally:\n                  soapStub.samlToken = None\n\n      return _doLogin", "response": "Returns a function that will call the SessionManager. LoginByToken method after obtaining a HoK SAML token from the STS."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a function that will call the vim.SessionManager.LoginByToken() after obtaining a Bearer token from the STS. The result of this function can be passed as the \"loginMethod\" to a SessionOrientedStub constructor. @param username: username of the user/service registered with STS. @param password: password of the user/service registered with STS. @param stsUrl: URL of the SAML Token issueing service. (i.e. SSO server). @param stsCert: public key of the STS service.", "response": "def makeCredBearerTokenLoginMethod(username,\n                                      password,\n                                      stsUrl,\n                                      stsCert=None):\n      '''Return a function that will call the vim.SessionManager.LoginByToken()\n      after obtaining a Bearer token from the STS. The result of this function\n      can be passed as the \"loginMethod\" to a SessionOrientedStub constructor.\n\n      @param username: username of the user/service registered with STS.\n      @param password: password of the user/service registered with STS.\n      @param stsUrl: URL of the SAML Token issueing service. (i.e. SSO server).\n      @param stsCert: public key of the STS service.\n      '''\n      assert(username)\n      assert(password)\n      assert(stsUrl)\n\n      def _doLogin(soapStub):\n         from . import sso\n         cert = soapStub.schemeArgs['cert_file']\n         key = soapStub.schemeArgs['key_file']\n         authenticator = sso.SsoAuthenticator(sts_url=stsUrl,\n                                              sts_cert=stsCert)\n         samlAssertion = authenticator.get_bearer_saml_assertion(username,\n                                                                 password,\n                                                                 cert,\n                                                                 key)\n         si = vim.ServiceInstance(\"ServiceInstance\", soapStub)\n         sm = si.content.sessionManager\n         if not sm.currentSession:\n            try:\n               soapStub.samlToken = samlAssertion\n               si.content.sessionManager.LoginByToken()\n            finally:\n               soapStub.samlToken = None\n\n      return _doLogin"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n\n   args = GetArgs()\n   if args.password:\n      password = args.password\n   else:\n      password = getpass.getpass(prompt='Enter password for host %s and '\n                                        'user %s: ' % (args.host,args.user))\n\n   context = None\n   if hasattr(ssl, '_create_unverified_context'):\n      context = ssl._create_unverified_context()\n   si = SmartConnect(host=args.host,\n                     user=args.user,\n                     pwd=password,\n                     port=int(args.port),\n                     sslContext=context)\n   if not si:\n       print(\"Could not connect to the specified host using specified \"\n             \"username and password\")\n       return -1\n\n   atexit.register(Disconnect, si)\n\n   obj = VmomiSupport.templateOf(args.type)(args.id, si._stub)\n   print(json.dumps(obj, cls=VmomiSupport.VmomiJSONEncoder,\n                    sort_keys=True, indent=4))", "response": "Simple command - line program for dumping contents of any managed object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nserialize a fault detail object.", "response": "def SerializeFaultDetail(self, val, info):\n      \"\"\" Serialize an object \"\"\"\n      self._SerializeDataObject(val, info, ' xsi:typ=\"{1}\"'.format(val._wsdlName), self.defaultNS)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget xml ns prefix.", "response": "def _NSPrefix(self, ns):\n      \"\"\" Get xml ns prefix. self.nsMap must be set \"\"\"\n      if ns == self.defaultNS:\n         return ''\n      prefix = self.nsMap[ns]\n      return prefix and prefix + ':' or ''"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting fully qualified wsdl name", "response": "def _QName(self, typ, defNS):\n      \"\"\" Get fully qualified wsdl name (prefix:name) \"\"\"\n      attr = ''\n      ns, name = GetQualifiedWsdlName(typ)\n      if ns == defNS:\n         prefix = ''\n      else:\n         try:\n            prefix = self.nsMap[ns]\n         except KeyError:\n            # We have not seen this ns before\n            prefix = ns.split(':', 1)[-1]\n            attr = ' xmlns:{0}=\"{1}\"'.format(prefix, ns)\n      return attr, prefix and prefix + ':' + name or name"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef GetNSAndWsdlname(self, tag):\n      idx = tag.find(\":\")\n      if idx >= 0:\n         prefix, name = tag[:idx], tag[idx + 1:]\n      else:\n         prefix, name = None, tag\n      # Map prefix to ns\n      ns = self._GetNamespaceFromPrefix(prefix)\n      return ns, name", "response": "Map prefix name tag into ns and wsdlname"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsplits a tag into ns name", "response": "def SplitTag(self, tag):\n      \"\"\" Split tag into ns, name \"\"\"\n      idx = tag.find(NS_SEP)\n      if idx >= 0:\n         return tag[:idx], tag[idx + 1:]\n      else:\n         return \"\", tag"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef LookupWsdlType(self, ns, name, allowManagedObjectReference=False):\n      try:\n         return GetWsdlType(ns, name)\n      except KeyError:\n         if allowManagedObjectReference:\n            if name.endswith('ManagedObjectReference') and ns == XMLNS_VMODL_BASE:\n               return GetWsdlType(ns, name[:-len('Reference')])\n         # WARNING!!! This is a temporary hack to get around server not\n         # honoring @service tag (see bug 521744). Once it is fix, I am\n         # going to back out this change\n         if name.endswith('ManagedObjectReference') and allowManagedObjectReference:\n            return GetWsdlType(XMLNS_VMODL_BASE, name[:-len('Reference')])\n         return GuessWsdlType(name)", "response": "Lookup wsdl type. Handle special case for some vmodl version"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef IsPrimitiveType(obj):\n   return (isinstance(obj, types.bool) or isinstance(obj, types.byte) or\n      isinstance(obj, types.short) or isinstance(obj, six.integer_types) or\n      isinstance(obj, types.double) or isinstance(obj, types.float) or\n      isinstance(obj, six.string_types) or\n      isinstance(obj, types.PropertyPath) or\n      isinstance(obj, types.ManagedMethod) or\n      isinstance(obj, types.datetime) or\n      isinstance(obj, types.URI) or isinstance(obj, type))", "response": "See if the passed in object is a Primitive Type"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef DiffAnys(obj1, obj2, looseMatch=False, ignoreArrayOrder=True):\n   differ = Differ(looseMatch = looseMatch, ignoreArrayOrder = ignoreArrayOrder)\n   return differ.DiffAnyObjects(obj1, obj2)", "response": "Diff any two objects."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef DiffAnyObjects(self, oldObj, newObj, isObjLink=False):\n      if oldObj == newObj:\n         return True\n      if not oldObj or not newObj:\n         __Log__.debug('DiffAnyObjects: One of the objects is unset.')\n         return self._looseMatch\n      oldObjInstance = oldObj\n      newObjInstance = newObj\n      if isinstance(oldObj, list):\n         oldObjInstance = oldObj[0]\n      if isinstance(newObj, list):\n         newObjInstance = newObj[0]\n      # Need to see if it is a primitive type first since type information\n      #   will not be available for them.\n      if (IsPrimitiveType(oldObj) and IsPrimitiveType(newObj)\n         and oldObj.__class__.__name__ == newObj.__class__.__name__):\n         if oldObj == newObj:\n            return True\n         elif oldObj == None or newObj == None:\n            __Log__.debug('DiffAnyObjects: One of the objects in None')\n         return False\n      oldType = Type(oldObjInstance)\n      newType = Type(newObjInstance)\n      if oldType != newType:\n         __Log__.debug('DiffAnyObjects: Types do not match %s != %s' %\n            (repr(GetWsdlName(oldObjInstance.__class__)),\n             repr(GetWsdlName(newObjInstance.__class__))))\n         return False\n      elif isinstance(oldObj, list):\n         return self.DiffArrayObjects(oldObj, newObj, isObjLink)\n      elif isinstance(oldObjInstance, types.ManagedObject):\n         return (not oldObj and not newObj) or (oldObj and newObj\n            and oldObj._moId == newObj._moId)\n      elif isinstance(oldObjInstance, types.DataObject):\n         if isObjLink:\n            bMatch = oldObj.GetKey() == newObj.GetKey()\n            LogIf(not bMatch, 'DiffAnyObjects: Keys do not match %s != %s'\n               % (oldObj.GetKey(), newObj.GetKey()))\n            return bMatch\n         return self.DiffDataObjects(oldObj, newObj)\n\n      else:\n         raise TypeError(\"Unknown type: \"+repr(GetWsdlName(oldObj.__class__)))", "response": "Diff any two objects."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef DiffAnyArrays(self, oldObj, newObj, isElementLinks):\n      if len(oldObj) != len(newObj):\n         __Log__.debug('DiffAnyArrays: Array lengths do not match. %d != %d'\n            % (len(oldObj), len(newObj)))\n         return False\n      for i, j in zip(oldObj, newObj):\n         if not self.DiffAnyObjects(i, j, isElementLinks):\n            __Log__.debug('DiffAnyArrays: One of the elements do not match.')\n            return False\n      return True", "response": "Diff two arrays which contain Any objects"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef DiffDataObjects(self, oldObj, newObj):\n      if oldObj == newObj:\n         return True\n      if not oldObj or not newObj:\n         __Log__.debug('DiffDataObjects: One of the objects in None')\n         return False\n      oldType = Type(oldObj)\n      newType = Type(newObj)\n      if oldType != newType:\n         __Log__.debug(\n            'DiffDataObjects: Types do not match for dataobjects. %s != %s'\n            % (oldObj._wsdlName, newObj._wsdlName))\n         return False\n      for prop in oldObj._GetPropertyList():\n         oldProp = getattr(oldObj, prop.name)\n         newProp = getattr(newObj, prop.name)\n         propType = oldObj._GetPropertyInfo(prop.name).type\n         if not oldProp and not newProp:\n            continue\n         elif ((prop.flags & VmomiSupport.F_OPTIONAL) and\n               self._looseMatch and (not newProp or not oldProp)):\n            continue\n         elif not oldProp or not newProp:\n            __Log__.debug(\n               'DiffDataObjects: One of the objects has property %s unset'\n               % prop.name)\n            return False\n\n         bMatch = True\n         if IsPrimitiveType(oldProp):\n            bMatch = oldProp == newProp\n         elif isinstance(oldProp, types.ManagedObject):\n            bMatch = self.DiffAnyObjects(oldProp, newProp, prop.flags\n               & VmomiSupport.F_LINK)\n         elif isinstance(oldProp, types.DataObject):\n            if prop.flags & VmomiSupport.F_LINK:\n               bMatch = oldObj.GetKey() == newObj.GetKey()\n               LogIf(not bMatch, 'DiffDataObjects: Key match failed %s != %s'\n                  % (oldObj.GetKey(), newObj.GetKey()))\n            else:\n               bMatch = self.DiffAnyObjects(oldProp, newProp, prop.flags\n                  & VmomiSupport.F_LINK)\n         elif isinstance(oldProp, list):\n            bMatch = self.DiffArrayObjects(oldProp, newProp, prop.flags\n               & VmomiSupport.F_LINK)\n         else:\n            raise TypeError(\"Unknown type: \"+repr(propType))\n\n         if not bMatch:\n            __Log__.debug('DiffDataObjects: Objects differ in property %s'\n               % prop.name)\n            return False\n      return True", "response": "Diff two data objects."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve dynamic type manager", "response": "def GetTypeManager(self):\n      \"\"\" Get dynamic type manager \"\"\"\n      dynTypeMgr = None\n      if self.hostSystem:\n         try:\n            dynTypeMgr = self.hostSystem.RetrieveDynamicTypeManager()\n         except vmodl.fault.MethodNotFound as err:\n            pass\n\n      if not dynTypeMgr:\n         # Older host not support RetrieveDynamicTypeManager\n         cmdlineTypesMoId = \"ha-dynamic-type-manager\"\n         dynTypeMgr = vmodl.reflect.DynamicTypeManager(cmdlineTypesMoId,\n                                                       self.stub)\n      return dynTypeMgr"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ImportTypes(self, prefix=''):\n      # Use QueryTypeInfo to get all types\n      dynTypeMgr = self.GetTypeManager()\n      filterSpec = None\n      if prefix != '':\n         filterSpec = vmodl.reflect.DynamicTypeManager.TypeFilterSpec(\n                                                              typeSubstr=prefix)\n      allTypes = dynTypeMgr.QueryTypeInfo(filterSpec)\n\n      ## Convert dynamic types to pyVmomi types\n      #\n      DynamicTypeConstructor().CreateTypes(allTypes)\n      return allTypes", "response": "Build dynamic types from the dynamic types manager"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates pyVmomi types from allTypeInfo.", "response": "def CreateTypes(self, allTypes):\n      \"\"\"\n      Create pyVmomi types from vmodl.reflect.DynamicTypeManager.AllTypeInfo\n      \"\"\"\n      enumTypes, dataTypes, managedTypes = self._ConvertAllTypes(allTypes)\n      self._CreateAllTypes(enumTypes, dataTypes, managedTypes)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ConvertAllTypes(self, allTypes):\n      # Generate lists good for VmomiSupport.CreateXYZType\n      enumTypes = self._Filter(self._ConvertEnumType, allTypes.enumTypeInfo)\n      dataTypes = self._Filter(self._ConvertDataType, allTypes.dataTypeInfo)\n      managedTypes = self._Filter(self._ConvertManagedType,\n                                  allTypes.managedTypeInfo)\n      retAllTypes = (enumTypes, dataTypes, managedTypes)\n      return retAllTypes", "response": "Convert all dynamic types to pyVmomi type definitions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _CreateAllTypes(self, enumTypes, dataTypes, managedTypes):\n\n      # Create versions\n      for typeInfo in managedTypes:\n         name = typeInfo[0]\n         version = typeInfo[3]\n         VmomiSupport.AddVersion(version, '', '1.0', 0, name)\n         VmomiSupport.AddVersionParent(version, 'vmodl.version.version0')\n         VmomiSupport.AddVersionParent(version, 'vmodl.version.version1')\n         VmomiSupport.AddVersionParent(version, version)\n\n      # Create partial types\n      for fn, infos in (VmomiSupport.CreateEnumType, enumTypes), \\\n                       (VmomiSupport.CreateDataType, dataTypes), \\\n                       (VmomiSupport.CreateManagedType, managedTypes):\n         for typeInfo in infos:\n            try:\n               fn(*typeInfo)\n            except Exception as err:\n               #Ignore errors due to duplicate importing\n               pass", "response": "Create all pyVmomi types from pyVmomi type definitions"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ConvertAnnotations(self, annotations):\n      flags = 0\n      if annotations:\n         for annotation in annotations:\n            flags |= self._mapFlags.get(annotation.name, 0)\n      return flags", "response": "Convert annotations to pyVmomi flags"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting vmodl. reflect. DynamicTypeManager. ParamTypeInfo to pyVmomi param.", "response": "def _ConvertParamType(self, paramType):\n      \"\"\"\n      Convert vmodl.reflect.DynamicTypeManager.ParamTypeInfo to pyVmomi param\n      definition\n      \"\"\"\n      if paramType:\n         name = paramType.name\n         version = paramType.version\n         aType = paramType.type\n         flags = self._ConvertAnnotations(paramType.annotation)\n         privId = paramType.privId\n         param = (name, aType, version, flags, privId)\n      else:\n         param = None\n      return param"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ConvertMethodType(self, methodType):\n      if methodType:\n         name = methodType.name\n         wsdlName = methodType.wsdlName\n         version = methodType.version\n         params = self._Filter(self._ConvertParamType, methodType.paramTypeInfo)\n         privId = methodType.privId\n         faults = methodType.fault\n\n         # Figure out reture info\n         if methodType.returnTypeInfo:\n            returnTypeInfo = methodType.returnTypeInfo\n            retFlags = self._ConvertAnnotations(returnTypeInfo.annotation)\n            methodRetType = returnTypeInfo.type\n         else:\n            retFlags = 0\n            methodRetType = \"void\"\n         if wsdlName.endswith(\"_Task\"):\n            # TODO: Need a seperate task return type for task, instead of\n            #       hardcode vim.Task as return type\n            retType = \"vim.Task\"\n         else:\n            retType = methodRetType\n         retInfo = (retFlags, retType, methodRetType)\n\n         method = (name, wsdlName, version, params, retInfo, privId, faults)\n      else:\n         method = None\n      return method", "response": "Convert vmodl. reflect. DynamicTypeManager. MethodTypeInfo to pyVmomi method definition."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts vmodl. reflect. DynamicTypeManager. PropertyDescriptor to pyVmomi", "response": "def _ConvertManagedPropertyType(self, propType):\n      \"\"\"\n      Convert vmodl.reflect.DynamicTypeManager.PropertyTypeInfo to pyVmomi\n      managed property definition\n      \"\"\"\n      if propType:\n         name = propType.name\n         version = propType.version\n         aType = propType.type\n         flags = self._ConvertAnnotations(propType.annotation)\n         privId = propType.privId\n         prop = (name, aType, version, flags, privId)\n      else:\n         prop = None\n      return prop"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert vmodl. reflect. DynamicTypeManager. ManagedTypeInfo to pyVmomi", "response": "def _ConvertManagedType(self, managedType):\n      \"\"\"\n      Convert vmodl.reflect.DynamicTypeManager.ManagedTypeInfo to pyVmomi\n      managed type definition\n      \"\"\"\n      if managedType:\n         vmodlName = managedType.name\n         wsdlName = managedType.wsdlName\n         version = managedType.version\n         parent = managedType.base[0]\n         props = self._Filter(self._ConvertManagedPropertyType, managedType.property)\n         methods = self._Filter(self._ConvertMethodType, managedType.method)\n         moType = (vmodlName, wsdlName, parent, version, props, methods)\n      else:\n         moType = None\n      return moType"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ConvertDataPropertyType(self, propType):\n      if propType:\n         name = propType.name\n         version = propType.version\n         aType = propType.type\n         flags = self._ConvertAnnotations(propType.annotation)\n         prop = (name, aType, version, flags)\n      else:\n         prop = None\n      return prop", "response": "Convert vmodl. reflect. DynamicTypeManager. PropertyTypeInfo to pyVmomi\n     "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ConvertDataType(self, dataType):\n      if dataType:\n         vmodlName = dataType.name\n         wsdlName = dataType.wsdlName\n         version = dataType.version\n         parent = dataType.base[0]\n         props = self._Filter(self._ConvertDataPropertyType, dataType.property)\n         doType = (vmodlName, wsdlName, parent, version, props)\n      else:\n         doType = None\n      return doType", "response": "Convert vmodl. reflect. DynamicTypeManager. DataTypeInfo to pyVmomi data type definition."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ConvertEnumType(self, enumType):\n      if enumType:\n         vmodlName = enumType.name\n         wsdlName = enumType.wsdlName\n         version = enumType.version\n         values = enumType.value\n         enumType = (vmodlName, wsdlName, version, values)\n      else:\n         enumType = None\n      return enumType", "response": "Convert vmodl. reflect. DynamicTypeManager. EnumTypeInfo to pyVmomi enum\n     "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse ISO 8601 date time from string.", "response": "def ParseISO8601(datetimeStr):\n   \"\"\"\n   Parse ISO 8601 date time from string.\n   Returns datetime if ok, None otherwise\n   Note: Allows YYYY / YYYY-MM, but truncate YYYY -> YYYY-01-01,\n                                             YYYY-MM -> YYYY-MM-01\n         Truncate microsecond to most significant 6 digits\n   \"\"\"\n   datetimeVal = None\n   match = _dtExpr.match(datetimeStr)\n   if match:\n      try:\n         dt = {}\n         for key, defaultVal in iteritems(_dtExprKeyDefValMap):\n            val = match.group(key)\n            if val:\n               if key == 'microsecond':\n                  val = val[:6] + '0' * (6 - len(val))\n               dt[key] = int(val)\n            elif defaultVal:\n               dt[key] = defaultVal\n\n         # Orig. XMLSchema don't allow all zeros year. But newer draft is ok\n         #if dt['year'] == 0:\n         #   # Year cannot be all zeros\n         #   raise Exception('Year cannot be all zeros')\n\n         # 24 is a special case. It is actually represented as next day 00:00\n         delta = None\n         if dt.get('hour', 0) == 24:\n            # Must be 24:00:00.0\n            if dt.get('minute', 0) == 0 and dt.get('second', 0) == 0 and \\\n               dt.get('microsecond', 0) == 0:\n               dt['hour'] = 23\n               delta = timedelta(hours=1)\n            else:\n               return None\n\n         # Set tzinfo\n         # TODO: dst\n         tzInfo = None\n         val = match.group('tzutc')\n         if val:\n            tzInfo = TZManager.GetTZInfo()\n         else:\n            val = match.group('tzhr')\n            if val:\n               # tz hours offset\n               tzhr = int(val)\n               utcsign = val[0]\n\n               # tz minutes offset\n               tzmin = 0\n               val = match.group('tzmin')\n               if val:\n                  tzmin = tzhr >= 0 and int(val) or -int(val)\n\n               # Better tzname (map UTC +-00:00 to UTC)\n               tzname = 'UTC'\n               if tzhr != 0 or tzmin != 0:\n                  tzname += ' %s%02d:%02d' % (utcsign, abs(tzhr), abs(tzmin))\n\n               tzInfo = TZManager.GetTZInfo(tzname=tzname,\n                                            utcOffset=timedelta(hours=tzhr,\n                                                                minutes=tzmin))\n         if tzInfo:\n            dt['tzinfo'] = tzInfo\n\n         datetimeVal = datetime(**dt)\n         if delta:\n            datetimeVal += delta\n      except Exception as e:\n         pass\n   return datetimeVal"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ISO8601Format(dt):\n   isoStr = dt.strftime('%Y-%m-%dT%H:%M:%S')\n   if dt.microsecond:\n      isoStr += ('.%06d' % dt.microsecond).rstrip('0')\n   if dt.tzinfo:\n      tz = dt.strftime('%z')\n   else:\n      if time.daylight and time.localtime().tm_isdst:\n         utcOffset_minutes = -time.altzone / 60\n      else:\n         utcOffset_minutes = -time.timezone / 60\n      tz = \"%+.2d%.2d\" % (utcOffset_minutes / 60, (abs(utcOffset_minutes) % 60))\n   if tz == '+0000':\n      return isoStr + 'Z'\n   elif tz:\n      return isoStr + tz[:3] + ':' + tz[3:]\n   else:\n      # Local offset is unknown\n      return isoStr + '-00:00'", "response": "Returns ISO8601 formatted string for a Python datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget or add a timezone info.", "response": "def GetTZInfo(tzname='UTC', utcOffset=None, dst=None):\n      \"\"\" Get / Add timezone info \"\"\"\n      key = (tzname, utcOffset, dst)\n      tzInfo = TZManager._tzInfos.get(key)\n      if not tzInfo:\n         tzInfo = TZInfo(tzname, utcOffset, dst)\n         TZManager._tzInfos[key] = tzInfo\n      return tzInfo"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef WaitForTask(task,\n                raiseOnError=True,\n                si=None,\n                pc=None,\n                onProgressUpdate=None):\n    \"\"\"\n    Wait for task to complete.\n\n    @type  raiseOnError      : bool\n    @param raiseOnError      : Any exception thrown is thrown up to the caller\n                               if raiseOnError is set to true.\n    @type  si                : ManagedObjectReference to a ServiceInstance.\n    @param si                : ServiceInstance to use. If None, use the\n                               information from the task.\n    @type  pc                : ManagedObjectReference to a PropertyCollector.\n    @param pc                : Property collector to use. If None, get it from\n                               the ServiceInstance.\n    @type  onProgressUpdate  : callable\n    @param onProgressUpdate  : Callable to call with task progress updates.\n\n        For example::\n\n            def OnTaskProgressUpdate(task, percentDone):\n                print 'Task %s is %d%% complete.' % (task, percentDone)\n    \"\"\"\n\n    if si is None:\n        si = vim.ServiceInstance(\"ServiceInstance\", task._stub)\n    if pc is None:\n        pc = si.content.propertyCollector\n\n    progressUpdater = ProgressUpdater(task, onProgressUpdate)\n    progressUpdater.Update('created')\n\n    filter = CreateFilter(pc, task)\n\n    version, state = None, None\n    # Loop looking for updates till the state moves to a completed state.\n    while state not in (vim.TaskInfo.State.success, vim.TaskInfo.State.error):\n        try:\n            version, state = GetTaskStatus(task, version, pc)\n            progressUpdater.UpdateIfNeeded()\n        except vmodl.fault.ManagedObjectNotFound as e:\n            print(\"Task object has been deleted: %s\" % e.obj)\n            break\n\n    filter.Destroy()\n\n    if state == \"error\":\n        progressUpdater.Update('error: %s' % str(task.info.error))\n        if raiseOnError:\n            raise task.info.error\n        else:\n            print(\"Task reported error: \" + str(task.info.error))\n    else:\n        progressUpdater.Update('completed')\n\n    return state", "response": "Wait for a task to complete."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwaits for multiple tasks to complete.", "response": "def WaitForTasks(tasks,\n                 raiseOnError=True,\n                 si=None,\n                 pc=None,\n                 onProgressUpdate=None,\n                 results=None):\n    \"\"\"\n    Wait for mulitiple tasks to complete. Much faster than calling WaitForTask\n    N times\n    \"\"\"\n\n    if not tasks:\n        return\n\n    if si is None:\n        si = vim.ServiceInstance(\"ServiceInstance\", tasks[0]._stub)\n    if pc is None:\n        pc = si.content.propertyCollector\n    if results is None:\n        results = []\n\n    progressUpdaters = {}\n    for task in tasks:\n        progressUpdater = ProgressUpdater(task, onProgressUpdate)\n        progressUpdater.Update('created')\n        progressUpdaters[str(task)] = progressUpdater\n\n    filter = CreateTasksFilter(pc, tasks)\n\n    try:\n        version, state = None, None\n\n        # Loop looking for updates till the state moves to a completed state.\n        while len(progressUpdaters):\n            update = pc.WaitForUpdates(version)\n            for filterSet in update.filterSet:\n                for objSet in filterSet.objectSet:\n                    task = objSet.obj\n                    taskId = str(task)\n                    for change in objSet.changeSet:\n                        if change.name == 'info':\n                            state = change.val.state\n                        elif change.name == 'info.state':\n                            state = change.val\n                        else:\n                            continue\n\n                        progressUpdater = progressUpdaters.get(taskId)\n                        if not progressUpdater:\n                            continue\n\n                        if state == vim.TaskInfo.State.success:\n                            progressUpdater.Update('completed')\n                            progressUpdaters.pop(taskId)\n                            # cache the results, as task objects could expire if one\n                            # of the tasks take a longer time to complete\n                            results.append(task.info.result)\n                        elif state == vim.TaskInfo.State.error:\n                            err = task.info.error\n                            progressUpdater.Update('error: %s' % str(err))\n                            if raiseOnError:\n                                raise err\n                            else:\n                                print(\"Task %s reported error: %s\" % (taskId, str(err)))\n                                progressUpdaters.pop(taskId)\n                        else:\n                            if onProgressUpdate:\n                                progressUpdater.UpdateIfNeeded()\n            # Move to next version\n            version = update.version\n    finally:\n        if filter:\n            filter.Destroy()\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CreateTasksFilter(pc, tasks):\n    if not tasks:\n        return None\n\n    # First create the object specification as the task object.\n    objspecs = [vmodl.query.PropertyCollector.ObjectSpec(obj=task)\n                for task in tasks]\n\n    # Next, create the property specification as the state.\n    propspec = vmodl.query.PropertyCollector.PropertySpec(\n        type=vim.Task, pathSet=[], all=True)\n\n    # Create a filter spec with the specified object and property spec.\n    filterspec = vmodl.query.PropertyCollector.FilterSpec()\n    filterspec.objectSet = objspecs\n    filterspec.propSet = [propspec]\n\n    # Create the filter\n    return pc.CreateFilter(filterspec, True)", "response": "Create a filter for the specified tasks."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck to see if VM needs to ask a question raises exception WorkItem", "response": "def CheckForQuestionPending(task):\n    \"\"\"\n    Check to see if VM needs to ask a question, throw exception\n    \"\"\"\n\n    vm = task.info.entity\n    if vm is not None and isinstance(vm, vim.VirtualMachine):\n        qst = vm.runtime.question\n        if qst is not None:\n            raise TaskBlocked(\"Task blocked, User Intervention required\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cmd(self, *args, **kwargs):\n        '''adb command, add -s serial by default. return the subprocess.Popen object.'''\n        serial = self.device_serial()\n        if serial:\n            if \" \" in serial:  # TODO how to include special chars on command line\n                serial = \"'%s'\" % serial\n            return self.raw_cmd(*[\"-s\", serial] + list(args))\n        else:\n            return self.raw_cmd(*args)", "response": "adb command add - s serial by default. return the subprocess. Popen object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef devices(self):\n        '''get a dict of attached devices. key is the device serial, value is device name.'''\n        out = self.raw_cmd(\"devices\").communicate()[0].decode(\"utf-8\")\n        match = \"List of devices attached\"\n        index = out.find(match)\n        if index < 0:\n            raise EnvironmentError(\"adb is not working.\")\n        return dict([s.split(\"\\t\") for s in out[index + len(match):].strip().splitlines() if s.strip()])", "response": "get a dict of attached devices. key is the device serial value is the device name. value is the device name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sdk_version(self):\n        '''sdk version of connected device.'''\n        if self.__sdk == 0:\n            try:\n                self.__sdk = int(self.adb.cmd(\"shell\", \"getprop\", \"ro.build.version.sdk\").communicate()[0].decode(\"utf-8\").strip())\n            except:\n                pass\n        return self.__sdk", "response": "sdk version of connected device."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstops the rpc server.", "response": "def stop(self):\n        '''Stop the rpc server.'''\n        if self.uiautomator_process and self.uiautomator_process.poll() is None:\n            res = None\n            try:\n                res = urllib2.urlopen(self.stop_uri)\n                self.uiautomator_process.wait()\n            except:\n                self.uiautomator_process.kill()\n            finally:\n                if res is not None:\n                    res.close()\n                self.uiautomator_process = None\n        try:\n            out = self.adb.cmd(\"shell\", \"ps\", \"-C\", \"uiautomator\").communicate()[0].decode(\"utf-8\").strip().splitlines()\n            if out:\n                index = out[0].split().index(\"PID\")\n                for line in out[1:]:\n                    if len(line.split()) > index:\n                        self.adb.cmd(\"shell\", \"kill\", \"-9\", line.split()[index]).wait()\n        except:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclicking at arbitrary coordinates.", "response": "def click(self, x, y):\n        '''click at arbitrary coordinates.'''\n        return self.server.jsonrpc.click(x, y)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlongs click at arbitrary coordinates.", "response": "def long_click(self, x, y):\n        '''long click at arbitrary coordinates.'''\n        return self.swipe(x, y, x + 1, y + 1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dump(self, filename=None, compressed=True, pretty=True):\n        '''dump device window and pull to local file.'''\n        content = self.server.jsonrpc.dumpWindowHierarchy(compressed, None)\n        if filename:\n            with open(filename, \"wb\") as f:\n                f.write(content.encode(\"utf-8\"))\n        if pretty and \"\\n \" not in content:\n            xml_text = xml.dom.minidom.parseString(content.encode(\"utf-8\"))\n            content = U(xml_text.toprettyxml(indent='  '))\n        return content", "response": "dump device window and pull to local file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef open(self):\n        '''\n        Open notification or quick settings.\n        Usage:\n        d.open.notification()\n        d.open.quick_settings()\n        '''\n        @param_to_property(action=[\"notification\", \"quick_settings\"])\n        def _open(action):\n            if action == \"notification\":\n                return self.server.jsonrpc.openNotification()\n            else:\n                return self.server.jsonrpc.openQuickSettings()\n        return _open", "response": "Decorator for resource opening."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a key from the server.", "response": "def press(self):\n        '''\n        press key via name or key code. Supported key name includes:\n        home, back, left, right, up, down, center, menu, search, enter,\n        delete(or del), recent(recent apps), volume_up, volume_down,\n        volume_mute, camera, power.\n        Usage:\n        d.press.back()  # press back key\n        d.press.menu()  # press home key\n        d.press(89)     # press keycode\n        '''\n        @param_to_property(\n            key=[\"home\", \"back\", \"left\", \"right\", \"up\", \"down\", \"center\",\n                 \"menu\", \"search\", \"enter\", \"delete\", \"del\", \"recent\",\n                 \"volume_up\", \"volume_down\", \"volume_mute\", \"camera\", \"power\"]\n        )\n        def _press(key, meta=None):\n            if isinstance(key, int):\n                return self.server.jsonrpc.pressKeyCode(key, meta) if meta else self.server.jsonrpc.pressKeyCode(key)\n            else:\n                return self.server.jsonrpc.pressKey(str(key))\n        return _press"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nturn on or off screen.", "response": "def screen(self):\n        '''\n        Turn on/off screen.\n        Usage:\n        d.screen.on()\n        d.screen.off()\n\n        d.screen == 'on'  # Check if the screen is on, same as 'd.screenOn'\n        d.screen == 'off'  # Check if the screen is off, same as 'not d.screenOn'\n        '''\n        devive_self = self\n\n        class _Screen(object):\n            def on(self):\n                return devive_self.wakeup()\n\n            def off(self):\n                return devive_self.sleep()\n\n            def __call__(self, action):\n                if action == \"on\":\n                    return self.on()\n                elif action == \"off\":\n                    return self.off()\n                else:\n                    raise AttributeError(\"Invalid parameter: %s\" % action)\n\n            def __eq__(self, value):\n                info = devive_self.info\n                if \"screenOn\" not in info:\n                    raise EnvironmentError(\"Not supported on Android 4.3 and belows.\")\n                if value in [\"on\", \"On\", \"ON\"]:\n                    return info[\"screenOn\"]\n                elif value in [\"off\", \"Off\", \"OFF\"]:\n                    return not info[\"screenOn\"]\n                raise ValueError(\"Invalid parameter. It can only be compared with on/off.\")\n\n            def __ne__(self, value):\n                return not self.__eq__(value)\n\n        return _Screen()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wait(self):\n        '''\n        Waits for the current application to idle or window update event occurs.\n        Usage:\n        d.wait.idle(timeout=1000)\n        d.wait.update(timeout=1000, package_name=\"com.android.settings\")\n        '''\n        @param_to_property(action=[\"idle\", \"update\"])\n        def _wait(action, timeout=1000, package_name=None):\n            if timeout / 1000 + 5 > int(os.environ.get(\"JSONRPC_TIMEOUT\", 90)):\n                http_timeout = timeout / 1000 + 5\n            else:\n                http_timeout = int(os.environ.get(\"JSONRPC_TIMEOUT\", 90))\n            if action == \"idle\":\n                return self.server.jsonrpc_wrap(timeout=http_timeout).waitForIdle(timeout)\n            elif action == \"update\":\n                return self.server.jsonrpc_wrap(timeout=http_timeout).waitForWindowUpdate(package_name, timeout)\n        return _wait", "response": "A method that waits for the current application to idle or window update event occurs."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the text field.", "response": "def set_text(self, text):\n        '''set the text field.'''\n        if text in [None, \"\"]:\n            return self.jsonrpc.clearTextField(self.selector)  # TODO no return\n        else:\n            return self.jsonrpc.setText(self.selector, text)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nclicks on the ui object.", "response": "def click(self):\n        '''\n        click on the ui object.\n        Usage:\n        d(text=\"Clock\").click()  # click on the center of the ui object\n        d(text=\"OK\").click.wait(timeout=3000) # click and wait for the new window update\n        d(text=\"John\").click.topleft() # click on the topleft of the ui object\n        d(text=\"John\").click.bottomright() # click on the bottomright of the ui object\n        '''\n        @param_to_property(action=[\"tl\", \"topleft\", \"br\", \"bottomright\", \"wait\"])\n        def _click(action=None, timeout=3000):\n            if action is None:\n                return self.jsonrpc.click(self.selector)\n            elif action in [\"tl\", \"topleft\", \"br\", \"bottomright\"]:\n                return self.jsonrpc.click(self.selector, action)\n            else:\n                return self.jsonrpc.clickAndWaitForNewWindow(self.selector, timeout)\n        return _click"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef long_click(self):\n        '''\n        Perform a long click action on the object.\n        Usage:\n        d(text=\"Image\").long_click()  # long click on the center of the ui object\n        d(text=\"Image\").long_click.topleft()  # long click on the topleft of the ui object\n        d(text=\"Image\").long_click.bottomright()  # long click on the topleft of the ui object\n        '''\n        @param_to_property(corner=[\"tl\", \"topleft\", \"br\", \"bottomright\"])\n        def _long_click(corner=None):\n            info = self.info\n            if info[\"longClickable\"]:\n                if corner:\n                    return self.jsonrpc.longClick(self.selector, corner)\n                else:\n                    return self.jsonrpc.longClick(self.selector)\n            else:\n                bounds = info.get(\"visibleBounds\") or info.get(\"bounds\")\n                if corner in [\"tl\", \"topleft\"]:\n                    x = (5 * bounds[\"left\"] + bounds[\"right\"]) / 6\n                    y = (5 * bounds[\"top\"] + bounds[\"bottom\"]) / 6\n                elif corner in [\"br\", \"bottomright\"]:\n                    x = (bounds[\"left\"] + 5 * bounds[\"right\"]) / 6\n                    y = (bounds[\"top\"] + 5 * bounds[\"bottom\"]) / 6\n                else:\n                    x = (bounds[\"left\"] + bounds[\"right\"]) / 2\n                    y = (bounds[\"top\"] + bounds[\"bottom\"]) / 2\n                return self.device.long_click(x, y)\n        return _long_click", "response": "Perform a long click action on the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndrags the ui object to other point or ui object.", "response": "def drag(self):\n        '''\n        Drag the ui object to other point or ui object.\n        Usage:\n        d(text=\"Clock\").drag.to(x=100, y=100)  # drag to point (x,y)\n        d(text=\"Clock\").drag.to(text=\"Remove\") # drag to another object\n        '''\n        def to(obj, *args, **kwargs):\n            if len(args) >= 2 or \"x\" in kwargs or \"y\" in kwargs:\n                drag_to = lambda x, y, steps=100: self.jsonrpc.dragTo(self.selector, x, y, steps)\n            else:\n                drag_to = lambda steps=100, **kwargs: self.jsonrpc.dragTo(self.selector, Selector(**kwargs), steps)\n            return drag_to(*args, **kwargs)\n        return type(\"Drag\", (object,), {\"to\": to})()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gesture(self, start1, start2, *args, **kwargs):\n        '''\n        perform two point gesture.\n        Usage:\n        d().gesture(startPoint1, startPoint2).to(endPoint1, endPoint2, steps)\n        d().gesture(startPoint1, startPoint2, endPoint1, endPoint2, steps)\n        '''\n        def to(obj_self, end1, end2, steps=100):\n            ctp = lambda pt: point(*pt) if type(pt) == tuple else pt  # convert tuple to point\n            s1, s2, e1, e2 = ctp(start1), ctp(start2), ctp(end1), ctp(end2)\n            return self.jsonrpc.gesture(self.selector, s1, s2, e1, e2, steps)\n        obj = type(\"Gesture\", (object,), {\"to\": to})()\n        return obj if len(args) == 0 else to(None, *args, **kwargs)", "response": "Perform a two point gesture."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pinch(self):\n        '''\n        Perform two point gesture from edge to center(in) or center to edge(out).\n        Usages:\n        d().pinch.In(percent=100, steps=10)\n        d().pinch.Out(percent=100, steps=100)\n        '''\n        @param_to_property(in_or_out=[\"In\", \"Out\"])\n        def _pinch(in_or_out=\"Out\", percent=100, steps=50):\n            if in_or_out in [\"Out\", \"out\"]:\n                return self.jsonrpc.pinchOut(self.selector, percent, steps)\n            elif in_or_out in [\"In\", \"in\"]:\n                return self.jsonrpc.pinchIn(self.selector, percent, steps)\n        return _pinch", "response": "Perform two point gesture from edge to center to edge ( in or out."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming swipe action. if device platform greater than API 18, percent can be used and value between 0 and 1 Usages: d().swipe.right() d().swipe.left(steps=10) d().swipe.up(steps=10) d().swipe.down() d().swipe(\"right\", steps=20) d().swipe(\"right\", steps=20, percent=0.5)", "response": "def swipe(self):\n        '''\n        Perform swipe action. if device platform greater than API 18, percent can be used and value between 0 and 1\n        Usages:\n        d().swipe.right()\n        d().swipe.left(steps=10)\n        d().swipe.up(steps=10)\n        d().swipe.down()\n        d().swipe(\"right\", steps=20)\n        d().swipe(\"right\", steps=20, percent=0.5)\n        '''\n        @param_to_property(direction=[\"up\", \"down\", \"right\", \"left\"])\n        def _swipe(direction=\"left\", steps=10, percent=1):\n            if percent == 1:\n                return self.jsonrpc.swipe(self.selector, direction, steps)\n            else:\n                return self.jsonrpc.swipe(self.selector, direction, percent, steps)\n        return _swipe"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwait until the ui object exists or is gone or exists.", "response": "def wait(self):\n        '''\n        Wait until the ui object gone or exist.\n        Usage:\n        d(text=\"Clock\").wait.gone()  # wait until it's gone.\n        d(text=\"Settings\").wait.exists() # wait until it appears.\n        '''\n        @param_to_property(action=[\"exists\", \"gone\"])\n        def _wait(action, timeout=3000):\n            if timeout / 1000 + 5 > int(os.environ.get(\"JSONRPC_TIMEOUT\", 90)):\n                http_timeout = timeout / 1000 + 5\n            else:\n                http_timeout = int(os.environ.get(\"JSONRPC_TIMEOUT\", 90))\n            method = self.device.server.jsonrpc_wrap(\n                timeout=http_timeout\n            ).waitUntilGone if action == \"gone\" else self.device.server.jsonrpc_wrap(timeout=http_timeout).waitForExists\n            return method(self.selector, timeout)\n        return _wait"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fling(self):\n        '''\n        Perform fling action.\n        Usage:\n        d().fling()  # default vertically, forward\n        d().fling.horiz.forward()\n        d().fling.vert.backward()\n        d().fling.toBeginning(max_swipes=100) # vertically\n        d().fling.horiz.toEnd()\n        '''\n        @param_to_property(\n            dimention=[\"vert\", \"vertically\", \"vertical\", \"horiz\", \"horizental\", \"horizentally\"],\n            action=[\"forward\", \"backward\", \"toBeginning\", \"toEnd\"]\n        )\n        def _fling(dimention=\"vert\", action=\"forward\", max_swipes=1000):\n            vertical = dimention in [\"vert\", \"vertically\", \"vertical\"]\n            if action == \"forward\":\n                return self.jsonrpc.flingForward(self.selector, vertical)\n            elif action == \"backward\":\n                return self.jsonrpc.flingBackward(self.selector, vertical)\n            elif action == \"toBeginning\":\n                return self.jsonrpc.flingToBeginning(self.selector, vertical, max_swipes)\n            elif action == \"toEnd\":\n                return self.jsonrpc.flingToEnd(self.selector, vertical, max_swipes)\n\n        return _fling", "response": "Perform fling action.\n        Usage:\n        d().fling()  # default vertically, forward\n        d().fling.horiz.forward()\n        d().fling.vert.backward()\n        d().fling.toBeginning(max_swipes=100) # vertically\n        d().fling.horiz.toEnd()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef scroll(self):\n        '''\n        Perfrom scroll action.\n        Usage:\n        d().scroll(steps=50) # default vertically and forward\n        d().scroll.horiz.forward(steps=100)\n        d().scroll.vert.backward(steps=100)\n        d().scroll.horiz.toBeginning(steps=100, max_swipes=100)\n        d().scroll.vert.toEnd(steps=100)\n        d().scroll.horiz.to(text=\"Clock\")\n        '''\n        def __scroll(vertical, forward, steps=100):\n            method = self.jsonrpc.scrollForward if forward else self.jsonrpc.scrollBackward\n            return method(self.selector, vertical, steps)\n\n        def __scroll_to_beginning(vertical, steps=100, max_swipes=1000):\n            return self.jsonrpc.scrollToBeginning(self.selector, vertical, max_swipes, steps)\n\n        def __scroll_to_end(vertical, steps=100, max_swipes=1000):\n            return self.jsonrpc.scrollToEnd(self.selector, vertical, max_swipes, steps)\n\n        def __scroll_to(vertical, **kwargs):\n            return self.jsonrpc.scrollTo(self.selector, Selector(**kwargs), vertical)\n\n        @param_to_property(\n            dimention=[\"vert\", \"vertically\", \"vertical\", \"horiz\", \"horizental\", \"horizentally\"],\n            action=[\"forward\", \"backward\", \"toBeginning\", \"toEnd\", \"to\"])\n        def _scroll(dimention=\"vert\", action=\"forward\", **kwargs):\n            vertical = dimention in [\"vert\", \"vertically\", \"vertical\"]\n            if action in [\"forward\", \"backward\"]:\n                return __scroll(vertical, action == \"forward\", **kwargs)\n            elif action == \"toBeginning\":\n                return __scroll_to_beginning(vertical, **kwargs)\n            elif action == \"toEnd\":\n                return __scroll_to_end(vertical, **kwargs)\n            elif action == \"to\":\n                return __scroll_to(vertical, **kwargs)\n        return _scroll", "response": "This method is used to scroll the page in the page."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef minimize(model,\n             data,\n             algo,\n             max_evals,\n             trials,\n             functions=None,\n             rseed=1337,\n             notebook_name=None,\n             verbose=True,\n             eval_space=False,\n             return_space=False,\n             keep_temp=False):\n    \"\"\"\n    Minimize a keras model for given data and implicit hyperparameters.\n\n    Parameters\n    ----------\n    model: A function defining a keras model with hyperas templates, which returns a\n        valid hyperopt results dictionary, e.g.\n        return {'loss': -acc, 'status': STATUS_OK}\n    data: A parameter-less function that defines and return all data needed in the above\n        model definition.\n    algo: A hyperopt algorithm, like tpe.suggest or rand.suggest\n    max_evals: Maximum number of optimization runs\n    trials: A hyperopt trials object, used to store intermediate results for all\n        optimization runs\n    rseed: Integer random seed for experiments\n    notebook_name: If running from an ipython notebook, provide filename (not path)\n    verbose: Print verbose output\n    eval_space: Evaluate the best run in the search space such that 'choice's contain actually meaningful values instead of mere indices\n    return_space: Return the hyperopt search space object (e.g. for further processing) as last return value\n    keep_temp: Keep temp_model.py file on the filesystem\n\n    Returns\n    -------\n    If `return_space` is False: A pair consisting of the results dictionary of the best run and the corresponding\n    keras model.\n    If `return_space` is True: The pair of best result and corresponding keras model, and the hyperopt search space\n    \"\"\"\n    best_run, space = base_minimizer(model=model,\n                                     data=data,\n                                     functions=functions,\n                                     algo=algo,\n                                     max_evals=max_evals,\n                                     trials=trials,\n                                     rseed=rseed,\n                                     full_model_string=None,\n                                     notebook_name=notebook_name,\n                                     verbose=verbose,\n                                     keep_temp=keep_temp)\n\n    best_model = None\n    for trial in trials:\n        vals = trial.get('misc').get('vals')\n        # unpack the values from lists without overwriting the mutable dict within 'trial'\n        unpacked_vals = unpack_hyperopt_vals(vals)\n        # identify the best_run (comes with unpacked values from the hyperopt function `base.Trials.argmin`)\n        if unpacked_vals == best_run and 'model' in trial.get('result').keys():\n            best_model = trial.get('result').get('model')\n\n    if eval_space is True:\n        # evaluate the search space\n        best_run = eval_hyperopt_space(space, best_run)\n\n    if return_space is True:\n        # return the space as well\n        return best_run, best_model, space\n    else:\n        # the default case for backwards compatibility with expanded return arguments\n        return best_run, best_model", "response": "Minimize a keras model for given data and implicit hyperparameters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef with_line_numbers(code):\n    max_number_length = str(len(str(len(code))))\n    format_str = \"{:>\" + max_number_length + \"d}: {:}\"\n    return \"\\n\".join([format_str.format(line_number + 1, line) for line_number, line in enumerate(code.split(\"\\n\"))])", "response": "Returns a string with line numbers added to each line of a source code fragment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef determine_indent(str):\n    indent = None\n    reg = r\"\"\"\n      ^(?P<previous_indent>\\s*)\\S.+?:\\n      # line starting a block, i. e. '   for i in x:\\n'\n      ((\\s*)\\n)*                             # empty lines\n      (?P=previous_indent)(?P<indent>\\s+)\\S  # first indented line of the new block, i. e. '      d'(..oStuff())\n      \"\"\"\n\n    matches = re.compile(reg, re.MULTILINE | re.VERBOSE).finditer(str)\n    for block_start in matches:\n        new_indent = block_start.groupdict()['indent']\n        if indent and new_indent != indent:\n            warnings.warn('Inconsistent indentation detected.'\n                          'Found \"%s\" (length: %i) as well as \"%s\" (length: %i)' % (\n                              indent, len(indent), new_indent, len(new_indent)))\n        indent = new_indent\n    return indent", "response": "Determines the character used for indenting in a given source code fragement."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unpack_hyperopt_vals(vals):\n    assert isinstance(vals, dict), \"Parameter must be given as dict.\"\n    ret = {}\n    for k, v in list(vals.items()):\n        try:\n            ret[k] = v[0]\n        except (TypeError, IndexError):\n            ret[k] = v\n    return ret", "response": "Unpack values from a hyperopt return dictionary where values are wrapped in a list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the index of the colon in the function signature.", "response": "def find_signature_end(model_string):\n    \"\"\"\n    Find the index of the colon in the function signature.\n    :param model_string: string\n        source code of the model\n    :return: int\n        the index of the colon\n    \"\"\"\n    index, brace_depth = 0, 0\n    while index < len(model_string):\n        ch = model_string[index]\n        if brace_depth == 0 and ch == ':':\n            break\n        if ch == '#':  # Ignore comments\n            index += 1\n            while index < len(model_string) and model_string[index] != '\\n':\n                index += 1\n            index += 1\n        elif ch in ['\"', \"'\"]:  # Skip strings\n            string_depth = 0\n            while index < len(model_string) and model_string[index] == ch:\n                string_depth += 1\n                index += 1\n            if string_depth == 2:\n                string_depth = 1\n            index += string_depth\n            while index < len(model_string):\n                if model_string[index] == '\\\\':\n                    index += 2\n                elif model_string[index] == ch:\n                    string_depth -= 1\n                    if string_depth == 0:\n                        break\n                    index += 1\n                else:\n                    index += 1\n            index += 1\n        elif ch == '(':\n            brace_depth += 1\n            index += 1\n        elif ch == ')':\n            brace_depth -= 1\n            index += 1\n        else:\n            index += 1\n    return index"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new model for the current language.", "response": "def create_model(x_train, y_train, x_test, y_test):\n    \"\"\"\n    Create your model...\n    \"\"\"\n    layer_1_size = {{quniform(12, 256, 4)}}\n    l1_dropout = {{uniform(0.001, 0.7)}}\n    params = {\n        'l1_size': layer_1_size,\n        'l1_dropout': l1_dropout\n    }\n    num_classes = 10\n    model = Sequential()\n    model.add(Dense(int(layer_1_size), activation='relu'))\n    model.add(Dropout(l1_dropout))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=RMSprop(),\n                  metrics=['accuracy'])\n    model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))\n    score, acc = model.evaluate(x_test, y_test, verbose=0)\n    out = {\n        'loss': -acc,\n        'score': score,\n        'status': STATUS_OK,\n        'model_params': params,\n    }\n    # optionally store a dump of your model here so you can get it from the database later\n    temp_name = tempfile.gettempdir()+'/'+next(tempfile._get_candidate_names()) + '.h5'\n    model.save(temp_name)\n    with open(temp_name, 'rb') as infile:\n        model_bytes = infile.read()\n    out['model_serial'] = model_bytes\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates Keras model providing function that creates Keras model and returns dictionary with key loss status and model.", "response": "def model(x_train, y_train, x_test, y_test):\n    \"\"\"\n    Model providing function:\n\n    Create Keras model with double curly brackets dropped-in as needed.\n    Return value has to be a valid python dictionary with two customary keys:\n        - loss: Specify a numeric evaluation metric to be minimized\n        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n    The last one is optional, though recommended, namely:\n        - model: specify the model just created so that we can later use it again.\n    \"\"\"\n    model = Sequential()\n    model.add(Dense(512, input_shape=(784,)))\n    model.add(Activation('relu'))\n    model.add(Dropout({{uniform(0, 1)}}))\n    model.add(Dense({{choice([256, 512, 1024])}}))\n    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n    model.add(Dropout({{uniform(0, 1)}}))\n\n    # If we choose 'four', add an additional fourth layer\n    if {{choice(['three', 'four'])}} == 'four':\n        model.add(Dense(100))\n\n        # We can also choose between complete sets of layers\n\n        model.add({{choice([Dropout(0.5), Activation('linear')])}})\n        model.add(Activation('relu'))\n\n    model.add(Dense(10))\n    model.add(Activation('softmax'))\n\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n                  optimizer={{choice(['rmsprop', 'adam', 'sgd'])}})\n\n    model.fit(x_train, y_train,\n              batch_size={{choice([64, 128])}},\n              epochs=1,\n              verbose=2,\n              validation_data=(x_test, y_test))\n    score, acc = model.evaluate(x_test, y_test, verbose=0)\n    print('Test accuracy:', acc)\n    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompress data with zlib level 0.", "response": "def _zlib_no_compress(data):\n    \"\"\"Compress data with zlib level 0.\"\"\"\n    cobj = zlib.compressobj(0)\n    return b\"\".join([cobj.compress(data), cobj.flush()])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse BSON codec options.", "response": "def _parse_codec_options(options):\n    \"\"\"Parse BSON codec options.\"\"\"\n    return CodecOptions(\n        document_class=options.get(\n            'document_class', DEFAULT_CODEC_OPTIONS.document_class),\n        tz_aware=options.get(\n            'tz_aware', DEFAULT_CODEC_OPTIONS.tz_aware),\n        uuid_representation=options.get(\n            'uuidrepresentation', DEFAULT_CODEC_OPTIONS.uuid_representation),\n        unicode_decode_error_handler=options.get(\n            'unicode_decode_error_handler',\n            DEFAULT_CODEC_OPTIONS.unicode_decode_error_handler),\n        tzinfo=options.get('tzinfo', DEFAULT_CODEC_OPTIONS.tzinfo),\n        type_registry=options.get(\n            'type_registry', DEFAULT_CODEC_OPTIONS.type_registry))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef with_options(self, **kwargs):\n        return CodecOptions(\n            kwargs.get('document_class', self.document_class),\n            kwargs.get('tz_aware', self.tz_aware),\n            kwargs.get('uuid_representation', self.uuid_representation),\n            kwargs.get('unicode_decode_error_handler',\n                       self.unicode_decode_error_handler),\n            kwargs.get('tzinfo', self.tzinfo),\n            kwargs.get('type_registry', self.type_registry)\n        )", "response": "Return a copy of this CodecOptions overriding some options."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _merge_command(run, full_result, offset, result):\n    affected = result.get(\"n\", 0)\n\n    if run.op_type == _INSERT:\n        full_result[\"nInserted\"] += affected\n\n    elif run.op_type == _DELETE:\n        full_result[\"nRemoved\"] += affected\n\n    elif run.op_type == _UPDATE:\n        upserted = result.get(\"upserted\")\n        if upserted:\n            n_upserted = len(upserted)\n            for doc in upserted:\n                doc[\"index\"] = run.index(doc[\"index\"] + offset)\n            full_result[\"upserted\"].extend(upserted)\n            full_result[\"nUpserted\"] += n_upserted\n            full_result[\"nMatched\"] += (affected - n_upserted)\n        else:\n            full_result[\"nMatched\"] += affected\n        full_result[\"nModified\"] += result[\"nModified\"]\n\n    write_errors = result.get(\"writeErrors\")\n    if write_errors:\n        for doc in write_errors:\n            # Leave the server response intact for APM.\n            replacement = doc.copy()\n            idx = doc[\"index\"] + offset\n            replacement[\"index\"] = run.index(idx)\n            # Add the failed operation to the error document.\n            replacement[_UOP] = run.ops[idx]\n            full_result[\"writeErrors\"].append(replacement)\n\n    wc_error = result.get(\"writeConcernError\")\n    if wc_error:\n        full_result[\"writeConcernErrors\"].append(wc_error)", "response": "Merge a command result into the full bulk result."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nraise a BulkWriteError from the full bulk api result.", "response": "def _raise_bulk_write_error(full_result):\n    \"\"\"Raise a BulkWriteError from the full bulk api result.\n    \"\"\"\n    if full_result[\"writeErrors\"]:\n        full_result[\"writeErrors\"].sort(\n            key=lambda error: error[\"index\"])\n    raise BulkWriteError(full_result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_update(self, selector, update, multi=False, upsert=False,\n                   collation=None, array_filters=None):\n        \"\"\"Create an update document and add it to the list of ops.\n        \"\"\"\n        validate_ok_for_update(update)\n        cmd = SON([('q', selector), ('u', update),\n                   ('multi', multi), ('upsert', upsert)])\n        collation = validate_collation_or_none(collation)\n        if collation is not None:\n            self.uses_collation = True\n            cmd['collation'] = collation\n        if array_filters is not None:\n            self.uses_array_filters = True\n            cmd['arrayFilters'] = array_filters\n        if multi:\n            # A bulk_write containing an update_many is not retryable.\n            self.is_retryable = False\n        self.ops.append((_UPDATE, cmd))", "response": "Create an update document and add it to the list of ops."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef execute_command(self, generator, write_concern, session):\n        # nModified is only reported for write commands, not legacy ops.\n        full_result = {\n            \"writeErrors\": [],\n            \"writeConcernErrors\": [],\n            \"nInserted\": 0,\n            \"nUpserted\": 0,\n            \"nMatched\": 0,\n            \"nModified\": 0,\n            \"nRemoved\": 0,\n            \"upserted\": [],\n        }\n        op_id = _randint()\n\n        def retryable_bulk(session, sock_info, retryable):\n            self._execute_command(\n                generator, write_concern, session, sock_info, op_id,\n                retryable, full_result)\n\n        client = self.collection.database.client\n        with client._tmp_session(session) as s:\n            client._retry_with_session(\n                self.is_retryable, retryable_bulk, s, self)\n\n        if full_result[\"writeErrors\"] or full_result[\"writeConcernErrors\"]:\n            _raise_bulk_write_error(full_result)\n        return full_result", "response": "Execute a command using the specified generator and session."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execute_insert_no_results(self, sock_info, run, op_id, acknowledged):\n        command = SON([('insert', self.collection.name),\n                       ('ordered', self.ordered)])\n        concern = {'w': int(self.ordered)}\n        command['writeConcern'] = concern\n        if self.bypass_doc_val and sock_info.max_wire_version >= 4:\n            command['bypassDocumentValidation'] = True\n        db = self.collection.database\n        bwc = _BulkWriteContext(\n            db.name, command, sock_info, op_id, db.client._event_listeners,\n            session=None)\n        # Legacy batched OP_INSERT.\n        _do_batched_insert(\n            self.collection.full_name, run.ops, True, acknowledged, concern,\n            not self.ordered, self.collection.codec_options, bwc)", "response": "Execute insert returning no results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef execute_op_msg_no_results(self, sock_info, generator):\n        db_name = self.collection.database.name\n        client = self.collection.database.client\n        listeners = client._event_listeners\n        op_id = _randint()\n\n        if not self.current_run:\n            self.current_run = next(generator)\n        run = self.current_run\n\n        while run:\n            cmd = SON([(_COMMANDS[run.op_type], self.collection.name),\n                       ('ordered', False),\n                       ('writeConcern', {'w': 0})])\n            bwc = _BulkWriteContext(db_name, cmd, sock_info, op_id,\n                                    listeners, None)\n\n            while run.idx_offset < len(run.ops):\n                check_keys = run.op_type == _INSERT\n                ops = islice(run.ops, run.idx_offset, None)\n                # Run as many ops as possible.\n                request_id, msg, to_send = _do_bulk_write_command(\n                    self.namespace, run.op_type, cmd, ops, check_keys,\n                    self.collection.codec_options, bwc)\n                if not to_send:\n                    raise InvalidOperation(\"cannot do an empty bulk write\")\n                run.idx_offset += len(to_send)\n                # Though this isn't strictly a \"legacy\" write, the helper\n                # handles publishing commands and sending our message\n                # without receiving a result. Send 0 for max_doc_size\n                # to disable size checking. Size checking is handled while\n                # the documents are encoded to BSON.\n                bwc.legacy_write(request_id, msg, 0, False, to_send)\n            self.current_run = run = next(generator, None)", "response": "Execute bulk write commands with OP_MSG and w = 0 writeConcern unordered."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexecuting bulk write commands with OP_MSG and w = 0 WriteConcern ordered.", "response": "def execute_command_no_results(self, sock_info, generator):\n        \"\"\"Execute write commands with OP_MSG and w=0 WriteConcern, ordered.\n        \"\"\"\n        full_result = {\n            \"writeErrors\": [],\n            \"writeConcernErrors\": [],\n            \"nInserted\": 0,\n            \"nUpserted\": 0,\n            \"nMatched\": 0,\n            \"nModified\": 0,\n            \"nRemoved\": 0,\n            \"upserted\": [],\n        }\n        # Ordered bulk writes have to be acknowledged so that we stop\n        # processing at the first error, even when the application\n        # specified unacknowledged writeConcern.\n        write_concern = WriteConcern()\n        op_id = _randint()\n        try:\n            self._execute_command(\n                generator, write_concern, None,\n                sock_info, op_id, False, full_result)\n        except OperationFailure:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes all operations returning no results.", "response": "def execute_no_results(self, sock_info, generator):\n        \"\"\"Execute all operations, returning no results (w=0).\n        \"\"\"\n        if self.uses_collation:\n            raise ConfigurationError(\n                'Collation is unsupported for unacknowledged writes.')\n        if self.uses_array_filters:\n            raise ConfigurationError(\n                'arrayFilters is unsupported for unacknowledged writes.')\n        # Cannot have both unacknowledged writes and bypass document validation.\n        if self.bypass_doc_val and sock_info.max_wire_version >= 4:\n            raise OperationFailure(\"Cannot set bypass_document_validation with\"\n                                   \" unacknowledged write concern\")\n\n        # OP_MSG\n        if sock_info.max_wire_version > 5:\n            if self.ordered:\n                return self.execute_command_no_results(sock_info, generator)\n            return self.execute_op_msg_no_results(sock_info, generator)\n\n        coll = self.collection\n        # If ordered is True we have to send GLE or use write\n        # commands so we can abort on the first error.\n        write_concern = WriteConcern(w=int(self.ordered))\n        op_id = _randint()\n\n        next_run = next(generator)\n        while next_run:\n            # An ordered bulk write needs to send acknowledged writes to short\n            # circuit the next run. However, the final message on the final\n            # run can be unacknowledged.\n            run = next_run\n            next_run = next(generator, None)\n            needs_ack = self.ordered and next_run is not None\n            try:\n                if run.op_type == _INSERT:\n                    self.execute_insert_no_results(\n                        sock_info, run, op_id, needs_ack)\n                elif run.op_type == _UPDATE:\n                    for operation in run.ops:\n                        doc = operation['u']\n                        check_keys = True\n                        if doc and next(iter(doc)).startswith('$'):\n                            check_keys = False\n                        coll._update(\n                            sock_info,\n                            operation['q'],\n                            doc,\n                            operation['upsert'],\n                            check_keys,\n                            operation['multi'],\n                            write_concern=write_concern,\n                            op_id=op_id,\n                            ordered=self.ordered,\n                            bypass_doc_val=self.bypass_doc_val)\n                else:\n                    for operation in run.ops:\n                        coll._delete(sock_info,\n                                     operation['q'],\n                                     not operation['limit'],\n                                     write_concern,\n                                     op_id,\n                                     self.ordered)\n            except OperationFailure:\n                if self.ordered:\n                    break"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute the bulk command.", "response": "def execute(self, write_concern, session):\n        \"\"\"Execute operations.\n        \"\"\"\n        if not self.ops:\n            raise InvalidOperation('No operations to execute')\n        if self.executed:\n            raise InvalidOperation('Bulk operations can '\n                                   'only be executed once.')\n        self.executed = True\n        write_concern = write_concern or self.collection.write_concern\n        session = _validate_session_write_concern(session, write_concern)\n\n        if self.ordered:\n            generator = self.gen_ordered()\n        else:\n            generator = self.gen_unordered()\n\n        client = self.collection.database.client\n        if not write_concern.acknowledged:\n            with client._socket_for_writes(session) as sock_info:\n                self.execute_no_results(sock_info, generator)\n        else:\n            return self.execute_command(generator, write_concern, session)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting all provided operations.", "response": "def execute(self, write_concern=None):\n        \"\"\"Execute all provided operations.\n\n        :Parameters:\n          - write_concern (optional): the write concern for this bulk\n            execution.\n        \"\"\"\n        if write_concern is not None:\n            write_concern = WriteConcern(**write_concern)\n        return self.__bulk.execute(write_concern, session=None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns a _Query or _GetMore operation and returns a Response object.", "response": "def run_operation_with_response(\n            self,\n            sock_info,\n            operation,\n            set_slave_okay,\n            listeners,\n            exhaust,\n            unpack_res):\n        \"\"\"Run a _Query or _GetMore operation and return a Response object.\n\n        This method is used only to run _Query/_GetMore operations from\n        cursors.\n        Can raise ConnectionFailure, OperationFailure, etc.\n\n        :Parameters:\n          - `operation`: A _Query or _GetMore object.\n          - `set_slave_okay`: Pass to operation.get_message.\n          - `all_credentials`: dict, maps auth source to MongoCredential.\n          - `listeners`: Instance of _EventListeners or None.\n          - `exhaust`: If True, then this is an exhaust cursor operation.\n          - `unpack_res`: A callable that decodes the wire protocol response.\n        \"\"\"\n        duration = None\n        publish = listeners.enabled_for_commands\n        if publish:\n            start = datetime.now()\n\n        send_message = not operation.exhaust_mgr\n\n        if send_message:\n            use_cmd = operation.use_command(sock_info, exhaust)\n            message = operation.get_message(\n                set_slave_okay, sock_info, use_cmd)\n            request_id, data, max_doc_size = self._split_message(message)\n        else:\n            use_cmd = False\n            request_id = 0\n\n        if publish:\n            cmd, dbn = operation.as_command(sock_info)\n            listeners.publish_command_start(\n                cmd, dbn, request_id, sock_info.address)\n            start = datetime.now()\n\n        try:\n            if send_message:\n                sock_info.send_message(data, max_doc_size)\n                reply = sock_info.receive_message(request_id)\n            else:\n                reply = sock_info.receive_message(None)\n\n            # Unpack and check for command errors.\n            if use_cmd:\n                user_fields = _CURSOR_DOC_FIELDS\n                legacy_response = False\n            else:\n                user_fields = None\n                legacy_response = True\n            docs = unpack_res(reply, operation.cursor_id,\n                              operation.codec_options,\n                              legacy_response=legacy_response,\n                              user_fields=user_fields)\n            if use_cmd:\n                first = docs[0]\n                operation.client._process_response(\n                    first, operation.session)\n                _check_command_response(first)\n        except Exception as exc:\n            if publish:\n                duration = datetime.now() - start\n                if isinstance(exc, (NotMasterError, OperationFailure)):\n                    failure = exc.details\n                else:\n                    failure = _convert_exception(exc)\n                listeners.publish_command_failure(\n                    duration, failure, operation.name,\n                    request_id, sock_info.address)\n            raise\n\n        if publish:\n            duration = datetime.now() - start\n            # Must publish in find / getMore / explain command response\n            # format.\n            if use_cmd:\n                res = docs[0]\n            elif operation.name == \"explain\":\n                res = docs[0] if docs else {}\n            else:\n                res = {\"cursor\": {\"id\": reply.cursor_id,\n                                  \"ns\": operation.namespace()},\n                       \"ok\": 1}\n                if operation.name == \"find\":\n                    res[\"cursor\"][\"firstBatch\"] = docs\n                else:\n                    res[\"cursor\"][\"nextBatch\"] = docs\n            listeners.publish_command_success(\n                duration, res, operation.name, request_id,\n                sock_info.address)\n\n        if exhaust:\n            response = ExhaustResponse(\n                data=reply,\n                address=self._description.address,\n                socket_info=sock_info,\n                pool=self._pool,\n                duration=duration,\n                request_id=request_id,\n                from_command=use_cmd,\n                docs=docs)\n        else:\n            response = Response(\n                data=reply,\n                address=self._description.address,\n                duration=duration,\n                request_id=request_id,\n                from_command=use_cmd,\n                docs=docs)\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if this server supports retryable writes.", "response": "def retryable_writes_supported(self):\n        \"\"\"Checks if this server supports retryable writes.\"\"\"\n        return (\n            self._ls_timeout_minutes is not None and\n            self._server_type in (SERVER_TYPE.Mongos, SERVER_TYPE.RSPrimary))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntaking message data compresses it and adds an OP_COMPRESSED header.", "response": "def _compress(operation, data, ctx):\n    \"\"\"Takes message data, compresses it, and adds an OP_COMPRESSED header.\"\"\"\n    compressed = ctx.compress(data)\n    request_id = _randint()\n\n    header = _pack_compression_header(\n        _COMPRESSION_HEADER_SIZE + len(compressed), # Total message length\n        request_id, # Request id\n        0, # responseTo\n        2012, # operation id\n        operation, # original operation id\n        len(data), # uncompressed message length\n        ctx.compressor_id) # compressor id\n    return request_id, header + compressed"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking message data and adds a message header based on the operation.", "response": "def __pack_message(operation, data):\n    \"\"\"Takes message data and adds a message header based on the operation.\n\n    Returns the resultant message string.\n    \"\"\"\n    rid = _randint()\n    message = _pack_header(16 + len(data), rid, 0, operation)\n    return rid, message + data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an OP_INSERT message.", "response": "def _insert(collection_name, docs, check_keys, flags, opts):\n    \"\"\"Get an OP_INSERT message\"\"\"\n    encode = _dict_to_bson  # Make local. Uses extensions.\n    if len(docs) == 1:\n        encoded = encode(docs[0], check_keys, opts)\n        return b\"\".join([\n            b\"\\x00\\x00\\x00\\x00\",  # Flags don't matter for one doc.\n            _make_c_string(collection_name),\n            encoded]), len(encoded)\n\n    encoded = [encode(doc, check_keys, opts) for doc in docs]\n    if not encoded:\n        raise InvalidOperation(\"cannot do an empty bulk insert\")\n    return b\"\".join([\n        _pack_int(flags),\n        _make_c_string(collection_name),\n        b\"\".join(encoded)]), max(map(len, encoded))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _insert_compressed(\n        collection_name, docs, check_keys, continue_on_error, opts, ctx):\n    \"\"\"Internal compressed unacknowledged insert message helper.\"\"\"\n    op_insert, max_bson_size = _insert(\n        collection_name, docs, check_keys, continue_on_error, opts)\n    rid, msg = _compress(2002, op_insert, ctx)\n    return rid, msg, max_bson_size", "response": "Internal compressed unacknowledged insert message helper."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget an insert message.", "response": "def insert(collection_name, docs, check_keys,\n           safe, last_error_args, continue_on_error, opts, ctx=None):\n    \"\"\"Get an **insert** message.\"\"\"\n    if ctx:\n        return _insert_compressed(\n            collection_name, docs, check_keys, continue_on_error, opts, ctx)\n    return _insert_uncompressed(collection_name, docs, check_keys, safe,\n                                last_error_args, continue_on_error, opts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets an OP_UPDATE message.", "response": "def _update(collection_name, upsert, multi, spec, doc, check_keys, opts):\n    \"\"\"Get an OP_UPDATE message.\"\"\"\n    flags = 0\n    if upsert:\n        flags += 1\n    if multi:\n        flags += 2\n    encode = _dict_to_bson  # Make local. Uses extensions.\n    encoded_update = encode(doc, check_keys, opts)\n    return b\"\".join([\n        _ZERO_32,\n        _make_c_string(collection_name),\n        _pack_int(flags),\n        encode(spec, False, opts),\n        encoded_update]), len(encoded_update)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(collection_name, upsert, multi, spec,\n           doc, safe, last_error_args, check_keys, opts, ctx=None):\n    \"\"\"Get an **update** message.\"\"\"\n    if ctx:\n        return _update_compressed(\n            collection_name, upsert, multi, spec, doc, check_keys, opts, ctx)\n    return _update_uncompressed(collection_name, upsert, multi, spec,\n                                doc, safe, last_error_args, check_keys, opts)", "response": "Get an update message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an OP_MSG message with no header.", "response": "def _op_msg_no_header(flags, command, identifier, docs, check_keys, opts):\n    \"\"\"Get a OP_MSG message.\n\n    Note: this method handles multiple documents in a type one payload but\n    it does not perform batch splitting and the total message size is\n    only checked *after* generating the entire message.\n    \"\"\"\n    # Encode the command document in payload 0 without checking keys.\n    encoded = _dict_to_bson(command, False, opts)\n    flags_type = _pack_op_msg_flags_type(flags, 0)\n    total_size = len(encoded)\n    max_doc_size = 0\n    if identifier:\n        type_one = _pack_byte(1)\n        cstring = _make_c_string(identifier)\n        encoded_docs = [_dict_to_bson(doc, check_keys, opts) for doc in docs]\n        size = len(cstring) + sum(len(doc) for doc in encoded_docs) + 4\n        encoded_size = _pack_int(size)\n        total_size += size\n        max_doc_size = max(len(doc) for doc in encoded_docs)\n        data = ([flags_type, encoded, type_one, encoded_size, cstring] +\n                encoded_docs)\n    else:\n        data = [flags_type, encoded]\n    return b''.join(data), total_size, max_doc_size"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _op_msg_compressed(flags, command, identifier, docs, check_keys, opts,\n                       ctx):\n    \"\"\"Internal OP_MSG message helper.\"\"\"\n    msg, total_size, max_bson_size = _op_msg_no_header(\n        flags, command, identifier, docs, check_keys, opts)\n    rid, msg = _compress(2013, msg, ctx)\n    return rid, msg, total_size, max_bson_size", "response": "Internal OP_MSG message helper."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _op_msg_uncompressed(flags, command, identifier, docs, check_keys, opts):\n    data, total_size, max_bson_size = _op_msg_no_header(\n        flags, command, identifier, docs, check_keys, opts)\n    request_id, op_message = __pack_message(2013, data)\n    return request_id, op_message, total_size, max_bson_size", "response": "Internal compressed OP_MSG message helper."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting an OP_MSG message.", "response": "def _op_msg(flags, command, dbname, read_preference, slave_ok, check_keys,\n            opts, ctx=None):\n    \"\"\"Get a OP_MSG message.\"\"\"\n    command['$db'] = dbname\n    if \"$readPreference\" not in command:\n        if slave_ok and not read_preference.mode:\n            command[\"$readPreference\"] = (\n                ReadPreference.PRIMARY_PREFERRED.document)\n        else:\n            command[\"$readPreference\"] = read_preference.document\n    name = next(iter(command))\n    try:\n        identifier = _FIELD_MAP.get(name)\n        docs = command.pop(identifier)\n    except KeyError:\n        identifier = \"\"\n        docs = None\n    try:\n        if ctx:\n            return _op_msg_compressed(\n                flags, command, identifier, docs, check_keys, opts, ctx)\n        return _op_msg_uncompressed(\n            flags, command, identifier, docs, check_keys, opts)\n    finally:\n        # Add the field back to the command.\n        if identifier:\n            command[identifier] = docs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting an OP_QUERY message.", "response": "def _query(options, collection_name, num_to_skip,\n           num_to_return, query, field_selector, opts, check_keys):\n    \"\"\"Get an OP_QUERY message.\"\"\"\n    encoded = _dict_to_bson(query, check_keys, opts)\n    if field_selector:\n        efs = _dict_to_bson(field_selector, False, opts)\n    else:\n        efs = b\"\"\n    max_bson_size = max(len(encoded), len(efs))\n    return b\"\".join([\n        _pack_int(options),\n        _make_c_string(collection_name),\n        _pack_int(num_to_skip),\n        _pack_int(num_to_return),\n        encoded,\n        efs]), max_bson_size"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef query(options, collection_name, num_to_skip, num_to_return,\n          query, field_selector, opts, check_keys=False, ctx=None):\n    \"\"\"Get a **query** message.\"\"\"\n    if ctx:\n        return _query_compressed(options, collection_name, num_to_skip,\n                                 num_to_return, query, field_selector,\n                                 opts, check_keys, ctx)\n    return _query_uncompressed(options, collection_name, num_to_skip,\n                               num_to_return, query, field_selector, opts,\n                               check_keys)", "response": "Get a **query** message."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_more(collection_name, num_to_return, cursor_id):\n    return b\"\".join([\n        _ZERO_32,\n        _make_c_string(collection_name),\n        _pack_int(num_to_return),\n        _pack_long_long(cursor_id)])", "response": "Get an OP_GET_MORE message."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a ** getMore ** message.", "response": "def get_more(collection_name, num_to_return, cursor_id, ctx=None):\n    \"\"\"Get a **getMore** message.\"\"\"\n    if ctx:\n        return _get_more_compressed(\n            collection_name, num_to_return, cursor_id, ctx)\n    return _get_more_uncompressed(collection_name, num_to_return, cursor_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _delete(collection_name, spec, opts, flags):\n    encoded = _dict_to_bson(spec, False, opts)  # Uses extensions.\n    return b\"\".join([\n        _ZERO_32,\n        _make_c_string(collection_name),\n        _pack_int(flags),\n        encoded]), len(encoded)", "response": "Get an OP_DELETE message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a **delete** message.", "response": "def delete(\n        collection_name, spec, safe, last_error_args, opts, flags=0, ctx=None):\n    \"\"\"Get a **delete** message.\n\n    `opts` is a CodecOptions. `flags` is a bit vector that may contain\n    the SingleRemove flag or not:\n\n    http://docs.mongodb.org/meta-driver/latest/legacy/mongodb-wire-protocol/#op-delete\n    \"\"\"\n    if ctx:\n        return _delete_compressed(collection_name, spec, opts, flags, ctx)\n    return _delete_uncompressed(\n        collection_name, spec, safe, last_error_args, opts, flags)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef kill_cursors(cursor_ids):\n    num_cursors = len(cursor_ids)\n    pack = struct.Struct(\"<ii\" + (\"q\" * num_cursors)).pack\n    op_kill_cursors = pack(0, num_cursors, *cursor_ids)\n    return __pack_message(2007, op_kill_cursors)", "response": "Get a ** killCursors** message."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _raise_document_too_large(operation, doc_size, max_size):\n    if operation == \"insert\":\n        raise DocumentTooLarge(\"BSON document too large (%d bytes)\"\n                               \" - the connected server supports\"\n                               \" BSON document sizes up to %d\"\n                               \" bytes.\" % (doc_size, max_size))\n    else:\n        # There's nothing intelligent we can say\n        # about size for update and delete\n        raise DocumentTooLarge(\"%r command document too large\" % (operation,))", "response": "Internal helper for raising DocumentTooLarge."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _do_batched_insert(collection_name, docs, check_keys,\n                       safe, last_error_args, continue_on_error, opts,\n                       ctx):\n    \"\"\"Insert `docs` using multiple batches.\n    \"\"\"\n    def _insert_message(insert_message, send_safe):\n        \"\"\"Build the insert message with header and GLE.\n        \"\"\"\n        request_id, final_message = __pack_message(2002, insert_message)\n        if send_safe:\n            request_id, error_message, _ = __last_error(collection_name,\n                                                        last_error_args)\n            final_message += error_message\n        return request_id, final_message\n\n    send_safe = safe or not continue_on_error\n    last_error = None\n    data = StringIO()\n    data.write(struct.pack(\"<i\", int(continue_on_error)))\n    data.write(_make_c_string(collection_name))\n    message_length = begin_loc = data.tell()\n    has_docs = False\n    to_send = []\n    encode = _dict_to_bson  # Make local\n    compress = ctx.compress and not (safe or send_safe)\n    for doc in docs:\n        encoded = encode(doc, check_keys, opts)\n        encoded_length = len(encoded)\n        too_large = (encoded_length > ctx.max_bson_size)\n\n        message_length += encoded_length\n        if message_length < ctx.max_message_size and not too_large:\n            data.write(encoded)\n            to_send.append(doc)\n            has_docs = True\n            continue\n\n        if has_docs:\n            # We have enough data, send this message.\n            try:\n                if compress:\n                    rid, msg = None, data.getvalue()\n                else:\n                    rid, msg = _insert_message(data.getvalue(), send_safe)\n                ctx.legacy_bulk_insert(\n                    rid, msg, 0, send_safe, to_send, compress)\n            # Exception type could be OperationFailure or a subtype\n            # (e.g. DuplicateKeyError)\n            except OperationFailure as exc:\n                # Like it says, continue on error...\n                if continue_on_error:\n                    # Store exception details to re-raise after the final batch.\n                    last_error = exc\n                # With unacknowledged writes just return at the first error.\n                elif not safe:\n                    return\n                # With acknowledged writes raise immediately.\n                else:\n                    raise\n\n        if too_large:\n            _raise_document_too_large(\n                \"insert\", encoded_length, ctx.max_bson_size)\n\n        message_length = begin_loc + encoded_length\n        data.seek(begin_loc)\n        data.truncate()\n        data.write(encoded)\n        to_send = [doc]\n\n    if not has_docs:\n        raise InvalidOperation(\"cannot do an empty bulk insert\")\n\n    if compress:\n        request_id, msg = None, data.getvalue()\n    else:\n        request_id, msg = _insert_message(data.getvalue(), safe)\n    ctx.legacy_bulk_insert(request_id, msg, 0, safe, to_send, compress)\n\n    # Re-raise any exception stored due to continue_on_error\n    if last_error is not None:\n        raise last_error", "response": "Insert a list of documents using multiple batches."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _batched_op_msg_impl(\n        operation, command, docs, check_keys, ack, opts, ctx, buf):\n    \"\"\"Create a batched OP_MSG write.\"\"\"\n    max_bson_size = ctx.max_bson_size\n    max_write_batch_size = ctx.max_write_batch_size\n    max_message_size = ctx.max_message_size\n\n    flags = b\"\\x00\\x00\\x00\\x00\" if ack else b\"\\x02\\x00\\x00\\x00\"\n    # Flags\n    buf.write(flags)\n\n    # Type 0 Section\n    buf.write(b\"\\x00\")\n    buf.write(_dict_to_bson(command, False, opts))\n\n    # Type 1 Section\n    buf.write(b\"\\x01\")\n    size_location = buf.tell()\n    # Save space for size\n    buf.write(b\"\\x00\\x00\\x00\\x00\")\n    try:\n        buf.write(_OP_MSG_MAP[operation])\n    except KeyError:\n        raise InvalidOperation('Unknown command')\n\n    if operation in (_UPDATE, _DELETE):\n        check_keys = False\n\n    to_send = []\n    idx = 0\n    for doc in docs:\n        # Encode the current operation\n        value = _dict_to_bson(doc, check_keys, opts)\n        doc_length = len(value)\n        new_message_size = buf.tell() + doc_length\n        # Does first document exceed max_message_size?\n        doc_too_large = (idx == 0 and (new_message_size > max_message_size))\n        # When OP_MSG is used unacknowleged we have to check\n        # document size client side or applications won't be notified.\n        # Otherwise we let the server deal with documents that are too large\n        # since ordered=False causes those documents to be skipped instead of\n        # halting the bulk write operation.\n        unacked_doc_too_large = (not ack and (doc_length > max_bson_size))\n        if doc_too_large or unacked_doc_too_large:\n            write_op = list(_FIELD_MAP.keys())[operation]\n            _raise_document_too_large(\n                write_op, len(value), max_bson_size)\n        # We have enough data, return this batch.\n        if new_message_size > max_message_size:\n            break\n        buf.write(value)\n        to_send.append(doc)\n        idx += 1\n        # We have enough documents, return this batch.\n        if idx == max_write_batch_size:\n            break\n\n    # Write type 1 section size\n    length = buf.tell()\n    buf.seek(size_location)\n    buf.write(_pack_int(length - size_location))\n\n    return to_send, length", "response": "Create a batched OP_MSG write."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _encode_batched_op_msg(\n        operation, command, docs, check_keys, ack, opts, ctx):\n    \"\"\"Encode the next batched insert, update, or delete operation\n    as OP_MSG.\n    \"\"\"\n    buf = StringIO()\n\n    to_send, _ = _batched_op_msg_impl(\n        operation, command, docs, check_keys, ack, opts, ctx, buf)\n    return buf.getvalue(), to_send", "response": "Encode the next batched insert update or delete OP_MSG as OP_MSG."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _batched_op_msg_compressed(\n        operation, command, docs, check_keys, ack, opts, ctx):\n    \"\"\"Create the next batched insert, update, or delete operation\n    with OP_MSG, compressed.\n    \"\"\"\n    data, to_send = _encode_batched_op_msg(\n        operation, command, docs, check_keys, ack, opts, ctx)\n\n    request_id, msg = _compress(\n        2013,\n        data,\n        ctx.sock_info.compression_context)\n    return request_id, msg, to_send", "response": "Create the next batched insert update or delete operation\n    with OP_MSG compressed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _do_batched_op_msg(\n        namespace, operation, command, docs, check_keys, opts, ctx):\n    \"\"\"Create the next batched insert, update, or delete operation\n    using OP_MSG.\n    \"\"\"\n    command['$db'] = namespace.split('.', 1)[0]\n    if 'writeConcern' in command:\n        ack = bool(command['writeConcern'].get('w', 1))\n    else:\n        ack = True\n    if ctx.sock_info.compression_context:\n        return _batched_op_msg_compressed(\n            operation, command, docs, check_keys, ack, opts, ctx)\n    return _batched_op_msg(\n        operation, command, docs, check_keys, ack, opts, ctx)", "response": "Create the next batched insert update or delete operation using OP_MSG."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the next batched insert update or delete command compressed.", "response": "def _batched_write_command_compressed(\n        namespace, operation, command, docs, check_keys, opts, ctx):\n    \"\"\"Create the next batched insert, update, or delete command, compressed.\n    \"\"\"\n    data, to_send = _encode_batched_write_command(\n        namespace, operation, command, docs, check_keys, opts, ctx)\n\n    request_id, msg = _compress(\n        2004,\n        data,\n        ctx.sock_info.compression_context)\n    return request_id, msg, to_send"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _encode_batched_write_command(\n        namespace, operation, command, docs, check_keys, opts, ctx):\n    \"\"\"Encode the next batched insert, update, or delete command.\n    \"\"\"\n    buf = StringIO()\n\n    to_send, _ = _batched_write_command_impl(\n        namespace, operation, command, docs, check_keys, opts, ctx, buf)\n    return buf.getvalue(), to_send", "response": "Encode the next batched insert update or delete command."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _batched_write_command(\n        namespace, operation, command, docs, check_keys, opts, ctx):\n    \"\"\"Create the next batched insert, update, or delete command.\n    \"\"\"\n    buf = StringIO()\n\n    # Save space for message length and request id\n    buf.write(_ZERO_64)\n    # responseTo, opCode\n    buf.write(b\"\\x00\\x00\\x00\\x00\\xd4\\x07\\x00\\x00\")\n\n    # Write OP_QUERY write command\n    to_send, length = _batched_write_command_impl(\n        namespace, operation, command, docs, check_keys, opts, ctx, buf)\n\n    # Header - request id and message length\n    buf.seek(4)\n    request_id = _randint()\n    buf.write(_pack_int(request_id))\n    buf.seek(0)\n    buf.write(_pack_int(length))\n\n    return request_id, buf.getvalue(), to_send", "response": "Create the next batched insert update or delete command."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _do_batched_write_command(\n        namespace, operation, command, docs, check_keys, opts, ctx):\n    \"\"\"Batched write commands entry point.\"\"\"\n    if ctx.sock_info.compression_context:\n        return _batched_write_command_compressed(\n            namespace, operation, command, docs, check_keys, opts, ctx)\n    return _batched_write_command(\n        namespace, operation, command, docs, check_keys, opts, ctx)", "response": "Batched write command entry point."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _do_bulk_write_command(\n        namespace, operation, command, docs, check_keys, opts, ctx):\n    \"\"\"Bulk write commands entry point.\"\"\"\n    if ctx.sock_info.max_wire_version > 5:\n        return _do_batched_op_msg(\n            namespace, operation, command, docs, check_keys, opts, ctx)\n    return _do_batched_write_command(\n        namespace, operation, command, docs, check_keys, opts, ctx)", "response": "Bulk write command entry point."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a batched OP_QUERY write command.", "response": "def _batched_write_command_impl(\n        namespace, operation, command, docs, check_keys, opts, ctx, buf):\n    \"\"\"Create a batched OP_QUERY write command.\"\"\"\n    max_bson_size = ctx.max_bson_size\n    max_write_batch_size = ctx.max_write_batch_size\n    # Max BSON object size + 16k - 2 bytes for ending NUL bytes.\n    # Server guarantees there is enough room: SERVER-10643.\n    max_cmd_size = max_bson_size + _COMMAND_OVERHEAD\n\n    # No options\n    buf.write(_ZERO_32)\n    # Namespace as C string\n    buf.write(b(namespace))\n    buf.write(_ZERO_8)\n    # Skip: 0, Limit: -1\n    buf.write(_SKIPLIM)\n\n    # Where to write command document length\n    command_start = buf.tell()\n    buf.write(bson.BSON.encode(command))\n\n    # Start of payload\n    buf.seek(-1, 2)\n    # Work around some Jython weirdness.\n    buf.truncate()\n    try:\n        buf.write(_OP_MAP[operation])\n    except KeyError:\n        raise InvalidOperation('Unknown command')\n\n    if operation in (_UPDATE, _DELETE):\n        check_keys = False\n\n    # Where to write list document length\n    list_start = buf.tell() - 4\n    to_send = []\n    idx = 0\n    for doc in docs:\n        # Encode the current operation\n        key = b(str(idx))\n        value = bson.BSON.encode(doc, check_keys, opts)\n        # Is there enough room to add this document? max_cmd_size accounts for\n        # the two trailing null bytes.\n        enough_data = (buf.tell() + len(key) + len(value)) >= max_cmd_size\n        enough_documents = (idx >= max_write_batch_size)\n        if enough_data or enough_documents:\n            if not idx:\n                write_op = list(_FIELD_MAP.keys())[operation]\n                _raise_document_too_large(\n                    write_op, len(value), max_bson_size)\n            break\n        buf.write(_BSONOBJ)\n        buf.write(key)\n        buf.write(_ZERO_8)\n        buf.write(value)\n        to_send.append(doc)\n        idx += 1\n\n    # Finalize the current OP_QUERY message.\n    # Close list and command documents\n    buf.write(_ZERO_16)\n\n    # Write document lengths and request id\n    length = buf.tell()\n    buf.seek(list_start)\n    buf.write(_pack_int(length - list_start - 1))\n    buf.seek(command_start)\n    buf.write(_pack_int(length - command_start))\n\n    return to_send, length"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _first_batch(sock_info, db, coll, query, ntoreturn,\n                 slave_ok, codec_options, read_preference, cmd, listeners):\n    \"\"\"Simple query helper for retrieving a first (and possibly only) batch.\"\"\"\n    query = _Query(\n        0, db, coll, 0, query, None, codec_options,\n        read_preference, ntoreturn, 0, DEFAULT_READ_CONCERN, None, None,\n        None)\n\n    name = next(iter(cmd))\n    publish = listeners.enabled_for_commands\n    if publish:\n        start = datetime.datetime.now()\n\n    request_id, msg, max_doc_size = query.get_message(slave_ok, sock_info)\n\n    if publish:\n        encoding_duration = datetime.datetime.now() - start\n        listeners.publish_command_start(\n            cmd, db, request_id, sock_info.address)\n        start = datetime.datetime.now()\n\n    sock_info.send_message(msg, max_doc_size)\n    reply = sock_info.receive_message(request_id)\n    try:\n        docs = reply.unpack_response(None, codec_options)\n    except Exception as exc:\n        if publish:\n            duration = (datetime.datetime.now() - start) + encoding_duration\n            if isinstance(exc, (NotMasterError, OperationFailure)):\n                failure = exc.details\n            else:\n                failure = _convert_exception(exc)\n            listeners.publish_command_failure(\n                duration, failure, name, request_id, sock_info.address)\n        raise\n    # listIndexes\n    if 'cursor' in cmd:\n        result = {\n            u'cursor': {\n                u'firstBatch': docs,\n                u'id': reply.cursor_id,\n                u'ns': u'%s.%s' % (db, coll)\n            },\n            u'ok': 1.0\n        }\n    # fsyncUnlock, currentOp\n    else:\n        result = docs[0] if docs else {}\n        result[u'ok'] = 1.0\n    if publish:\n        duration = (datetime.datetime.now() - start) + encoding_duration\n        listeners.publish_command_success(\n            duration, result, name, request_id, sock_info.address)\n\n    return result", "response": "Simple query helper for retrieving a first batch of objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a find command document for this query.", "response": "def as_command(self, sock_info):\n        \"\"\"Return a find command document for this query.\"\"\"\n        # We use the command twice: on the wire and for command monitoring.\n        # Generate it once, for speed and to avoid repeating side-effects.\n        if self._as_command is not None:\n            return self._as_command\n\n        explain = '$explain' in self.spec\n        cmd = _gen_find_command(\n            self.coll, self.spec, self.fields, self.ntoskip,\n            self.limit, self.batch_size, self.flags, self.read_concern,\n            self.collation, self.session)\n        if explain:\n            self.name = 'explain'\n            cmd = SON([('explain', cmd)])\n        session = self.session\n        if session:\n            session._apply_to(cmd, False, self.read_preference)\n            # Explain does not support readConcern.\n            if (not explain and session.options.causal_consistency\n                    and session.operation_time is not None\n                    and not session._in_transaction):\n                cmd.setdefault(\n                    'readConcern', {})[\n                    'afterClusterTime'] = session.operation_time\n        sock_info.send_cluster_time(cmd, session, self.client)\n        self._as_command = cmd, self.db\n        return self._as_command"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a message from the database.", "response": "def get_message(self, set_slave_ok, sock_info, use_cmd=False):\n        \"\"\"Get a query message, possibly setting the slaveOk bit.\"\"\"\n        if set_slave_ok:\n            # Set the slaveOk bit.\n            flags = self.flags | 4\n        else:\n            flags = self.flags\n\n        ns = self.namespace()\n        spec = self.spec\n\n        if use_cmd:\n            spec = self.as_command(sock_info)[0]\n            if sock_info.op_msg_enabled:\n                request_id, msg, size, _ = _op_msg(\n                    0, spec, self.db, self.read_preference,\n                    set_slave_ok, False, self.codec_options,\n                    ctx=sock_info.compression_context)\n                return request_id, msg, size\n            ns = _UJOIN % (self.db, \"$cmd\")\n            ntoreturn = -1  # All DB commands return 1 document\n        else:\n            # OP_QUERY treats ntoreturn of -1 and 1 the same, return\n            # one document and close the cursor. We have to use 2 for\n            # batch size if 1 is specified.\n            ntoreturn = self.batch_size == 1 and 2 or self.batch_size\n            if self.limit:\n                if ntoreturn:\n                    ntoreturn = min(self.limit, ntoreturn)\n                else:\n                    ntoreturn = self.limit\n\n        if sock_info.is_mongos:\n            spec = _maybe_add_read_preference(spec,\n                                              self.read_preference)\n\n        return query(flags, ns, self.ntoskip, ntoreturn,\n                     spec, None if use_cmd else self.fields,\n                     self.codec_options, ctx=sock_info.compression_context)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef as_command(self, sock_info):\n        # See _Query.as_command for an explanation of this caching.\n        if self._as_command is not None:\n            return self._as_command\n\n        cmd = _gen_get_more_command(self.cursor_id, self.coll,\n                                    self.ntoreturn,\n                                    self.max_await_time_ms)\n\n        if self.session:\n            self.session._apply_to(cmd, False, self.read_preference)\n        sock_info.send_cluster_time(cmd, self.session, self.client)\n        self._as_command = cmd, self.db\n        return self._as_command", "response": "Return a getMore command document for this query."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_message(self, dummy0, sock_info, use_cmd=False):\n\n        ns = self.namespace()\n        ctx = sock_info.compression_context\n\n        if use_cmd:\n            spec = self.as_command(sock_info)[0]\n            if sock_info.op_msg_enabled:\n                request_id, msg, size, _ = _op_msg(\n                    0, spec, self.db, ReadPreference.PRIMARY,\n                    False, False, self.codec_options,\n                    ctx=sock_info.compression_context)\n                return request_id, msg, size\n            ns = _UJOIN % (self.db, \"$cmd\")\n            return query(0, ns, 0, -1, spec, None, self.codec_options, ctx=ctx)\n\n        return get_more(ns, self.ntoreturn, self.cursor_id, ctx)", "response": "Get a getmore message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck the response header from the database without decoding BSON.", "response": "def raw_response(self, cursor_id=None):\n        \"\"\"Check the response header from the database, without decoding BSON.\n\n        Check the response for errors and unpack.\n\n        Can raise CursorNotFound, NotMasterError, ExecutionTimeout, or\n        OperationFailure.\n\n        :Parameters:\n          - `cursor_id` (optional): cursor_id we sent to get this response -\n            used for raising an informative exception when we get cursor id not\n            valid at server response.\n        \"\"\"\n        if self.flags & 1:\n            # Shouldn't get this response if we aren't doing a getMore\n            if cursor_id is None:\n                raise ProtocolError(\"No cursor id for getMore operation\")\n\n            # Fake a getMore command response. OP_GET_MORE provides no\n            # document.\n            msg = \"Cursor not found, cursor id: %d\" % (cursor_id,)\n            errobj = {\"ok\": 0, \"errmsg\": msg, \"code\": 43}\n            raise CursorNotFound(msg, 43, errobj)\n        elif self.flags & 2:\n            error_object = bson.BSON(self.documents).decode()\n            # Fake the ok field if it doesn't exist.\n            error_object.setdefault(\"ok\", 0)\n            if error_object[\"$err\"].startswith(\"not master\"):\n                raise NotMasterError(error_object[\"$err\"], error_object)\n            elif error_object.get(\"code\") == 50:\n                raise ExecutionTimeout(error_object.get(\"$err\"),\n                                       error_object.get(\"code\"),\n                                       error_object)\n            raise OperationFailure(\"database error: %s\" %\n                                   error_object.get(\"$err\"),\n                                   error_object.get(\"code\"),\n                                   error_object)\n        return [self.documents]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nunpack a response from the server and decode the BSON document.", "response": "def unpack_response(self, cursor_id=None,\n                        codec_options=_UNICODE_REPLACE_CODEC_OPTIONS,\n                        user_fields=None, legacy_response=False):\n        \"\"\"Unpack a response from the database and decode the BSON document(s).\n\n        Check the response for errors and unpack, returning a dictionary\n        containing the response data.\n\n        Can raise CursorNotFound, NotMasterError, ExecutionTimeout, or\n        OperationFailure.\n\n        :Parameters:\n          - `cursor_id` (optional): cursor_id we sent to get this response -\n            used for raising an informative exception when we get cursor id not\n            valid at server response\n          - `codec_options` (optional): an instance of\n            :class:`~bson.codec_options.CodecOptions`\n        \"\"\"\n        self.raw_response(cursor_id)\n        if legacy_response:\n            return bson.decode_all(self.documents, codec_options)\n        return bson._decode_all_selective(\n            self.documents, codec_options, user_fields)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unpack(cls, msg):\n        # PYTHON-945: ignore starting_from field.\n        flags, cursor_id, _, number_returned = cls.UNPACK_FROM(msg)\n\n        # Convert Python 3 memoryview to bytes. Note we should call\n        # memoryview.tobytes() if we start using memoryview in Python 2.7.\n        documents = bytes(msg[20:])\n        return cls(flags, cursor_id, number_returned, documents)", "response": "Construct an _OpReply from raw bytes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nunpacking an OP_MSG command response.", "response": "def unpack_response(self, cursor_id=None,\n                        codec_options=_UNICODE_REPLACE_CODEC_OPTIONS,\n                        user_fields=None, legacy_response=False):\n        \"\"\"Unpack a OP_MSG command response.\n\n        :Parameters:\n          - `cursor_id` (optional): Ignored, for compatibility with _OpReply.\n          - `codec_options` (optional): an instance of\n            :class:`~bson.codec_options.CodecOptions`\n        \"\"\"\n        # If _OpMsg is in-use, this cannot be a legacy response.\n        assert not legacy_response\n        return bson._decode_all_selective(\n            self.payload_document, codec_options, user_fields)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unpack(cls, msg):\n        flags, first_payload_type, first_payload_size = cls.UNPACK_FROM(msg)\n        if flags != 0:\n            raise ProtocolError(\"Unsupported OP_MSG flags (%r)\" % (flags,))\n        if first_payload_type != 0:\n            raise ProtocolError(\n                \"Unsupported OP_MSG payload type (%r)\" % (first_payload_type,))\n\n        if len(msg) != first_payload_size + 5:\n            raise ProtocolError(\"Unsupported OP_MSG reply: >1 section\")\n\n        # Convert Python 3 memoryview to bytes. Note we should call\n        # memoryview.tobytes() if we start using memoryview in Python 2.7.\n        payload_document = bytes(msg[5:])\n        return cls(flags, payload_document)", "response": "Construct an _OpMsg from raw bytes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef has_known_servers(self):\n        return any(s for s in self._server_descriptions.values()\n                   if s.is_server_type_known)", "response": "Whether there are any Servers of types besides Unknown."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding an index to the index cache for ensure_index operations.", "response": "def _cache_index(self, dbname, collection, index, cache_for):\n        \"\"\"Add an index to the index cache for ensure_index operations.\"\"\"\n        now = datetime.datetime.utcnow()\n        expire = datetime.timedelta(seconds=cache_for) + now\n\n        with self.__index_cache_lock:\n            if dbname not in self.__index_cache:\n                self.__index_cache[dbname] = {}\n                self.__index_cache[dbname][collection] = {}\n                self.__index_cache[dbname][collection][index] = expire\n\n            elif collection not in self.__index_cache[dbname]:\n                self.__index_cache[dbname][collection] = {}\n                self.__index_cache[dbname][collection][index] = expire\n\n            else:\n                self.__index_cache[dbname][collection][index] = expire"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef watch(self, pipeline=None, full_document='default', resume_after=None,\n              max_await_time_ms=None, batch_size=None, collation=None,\n              start_at_operation_time=None, session=None):\n        \"\"\"Watch changes on this cluster.\n\n        Performs an aggregation with an implicit initial ``$changeStream``\n        stage and returns a\n        :class:`~pymongo.change_stream.ClusterChangeStream` cursor which\n        iterates over changes on all databases on this cluster.\n\n        Introduced in MongoDB 4.0.\n\n        .. code-block:: python\n\n           with client.watch() as stream:\n               for change in stream:\n                   print(change)\n\n        The :class:`~pymongo.change_stream.ClusterChangeStream` iterable\n        blocks until the next change document is returned or an error is\n        raised. If the\n        :meth:`~pymongo.change_stream.ClusterChangeStream.next` method\n        encounters a network error when retrieving a batch from the server,\n        it will automatically attempt to recreate the cursor such that no\n        change events are missed. Any error encountered during the resume\n        attempt indicates there may be an outage and will be raised.\n\n        .. code-block:: python\n\n            try:\n                with client.watch(\n                        [{'$match': {'operationType': 'insert'}}]) as stream:\n                    for insert_change in stream:\n                        print(insert_change)\n            except pymongo.errors.PyMongoError:\n                # The ChangeStream encountered an unrecoverable error or the\n                # resume attempt failed to recreate the cursor.\n                logging.error('...')\n\n        For a precise description of the resume process see the\n        `change streams specification`_.\n\n        :Parameters:\n          - `pipeline` (optional): A list of aggregation pipeline stages to\n            append to an initial ``$changeStream`` stage. Not all\n            pipeline stages are valid after a ``$changeStream`` stage, see the\n            MongoDB documentation on change streams for the supported stages.\n          - `full_document` (optional): The fullDocument to pass as an option\n            to the ``$changeStream`` stage. Allowed values: 'default',\n            'updateLookup'.  Defaults to 'default'.\n            When set to 'updateLookup', the change notification for partial\n            updates will include both a delta describing the changes to the\n            document, as well as a copy of the entire document that was\n            changed from some time after the change occurred.\n          - `resume_after` (optional): The logical starting point for this\n            change stream.\n          - `max_await_time_ms` (optional): The maximum time in milliseconds\n            for the server to wait for changes before responding to a getMore\n            operation.\n          - `batch_size` (optional): The maximum number of documents to return\n            per batch.\n          - `collation` (optional): The :class:`~pymongo.collation.Collation`\n            to use for the aggregation.\n          - `start_at_operation_time` (optional): If provided, the resulting\n            change stream will only return changes that occurred at or after\n            the specified :class:`~bson.timestamp.Timestamp`. Requires\n            MongoDB >= 4.0.\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n\n        :Returns:\n          A :class:`~pymongo.change_stream.ClusterChangeStream` cursor.\n\n        .. versionadded:: 3.7\n\n        .. mongodoc:: changeStreams\n\n        .. _change streams specification:\n            https://github.com/mongodb/specifications/blob/master/source/change-streams/change-streams.rst\n        \"\"\"\n        return ClusterChangeStream(\n            self.admin, pipeline, full_document, resume_after, max_await_time_ms,\n            batch_size, collation, start_at_operation_time, session\n        )", "response": "Watch all the databases on this cluster and return a new cursor which is returned."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending endSessions command to all available databases.", "response": "def _end_sessions(self, session_ids):\n        \"\"\"Send endSessions command(s) with the given session ids.\"\"\"\n        try:\n            # Use SocketInfo.command directly to avoid implicitly creating\n            # another session.\n            with self._socket_for_reads(\n                    ReadPreference.PRIMARY_PREFERRED,\n                    None) as (sock_info, slave_ok):\n                if not sock_info.supports_sessions:\n                    return\n\n                for i in range(0, len(session_ids), common._MAX_END_SESSIONS):\n                    spec = SON([('endSessions',\n                                 session_ids[i:i + common._MAX_END_SESSIONS])])\n                    sock_info.command(\n                        'admin', spec, slave_ok=slave_ok, client=self)\n        except PyMongoError:\n            # Drivers MUST ignore any errors returned by the endSessions\n            # command.\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef close(self):\n        session_ids = self._topology.pop_all_sessions()\n        if session_ids:\n            self._end_sessions(session_ids)\n        # Stop the periodic task thread and then run _process_periodic_tasks\n        # to send pending killCursor requests before closing the topology.\n        self._kill_cursors_executor.close()\n        self._process_periodic_tasks()\n        self._topology.close()", "response": "Close the client and clean up all resources."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_topology(self):\n        self._topology.open()\n        with self.__lock:\n            self._kill_cursors_executor.open()\n        return self._topology", "response": "Get the internal : class ~pymongo. topology. Topology object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nselect a server to run an operation on this client.", "response": "def _select_server(self, server_selector, session, address=None):\n        \"\"\"Select a server to run an operation on this client.\n\n        :Parameters:\n          - `server_selector`: The server selector to use if the session is\n            not pinned and no address is given.\n          - `session`: The ClientSession for the next operation, or None. May\n            be pinned to a mongos server address.\n          - `address` (optional): Address when sending a message\n            to a specific server, used for getMore.\n        \"\"\"\n        try:\n            topology = self._get_topology()\n            address = address or (session and session._pinned_address)\n            if address:\n                # We're running a getMore or this session is pinned to a mongos.\n                server = topology.select_server_by_address(address)\n                if not server:\n                    raise AutoReconnect('server %s:%d no longer available'\n                                        % address)\n            else:\n                server = topology.select_server(server_selector)\n                # Pin this session to the selected server if it's performing a\n                # sharded transaction.\n                if server.description.mongos and (session and\n                                                  session._in_transaction):\n                    session._pin_mongos(server)\n            return server\n        except PyMongoError as exc:\n            if session and exc.has_error_label(\"TransientTransactionError\"):\n                session._unpin_mongos()\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun a _Query/_GetMore operation and return a Response.", "response": "def _run_operation_with_response(self, operation, unpack_res,\n                                     exhaust=False, address=None):\n        \"\"\"Run a _Query/_GetMore operation and return a Response.\n\n        :Parameters:\n          - `operation`: a _Query or _GetMore object.\n          - `unpack_res`: A callable that decodes the wire protocol response.\n          - `exhaust` (optional): If True, the socket used stays checked out.\n            It is returned along with its Pool in the Response.\n          - `address` (optional): Optional address when sending a message\n            to a specific server, used for getMore.\n        \"\"\"\n        if operation.exhaust_mgr:\n            server = self._select_server(\n                operation.read_preference, operation.session, address=address)\n\n            with self._reset_on_error(server.description.address,\n                                      operation.session):\n                return server.run_operation_with_response(\n                    operation.exhaust_mgr.sock,\n                    operation,\n                    True,\n                    self._event_listeners,\n                    exhaust,\n                    unpack_res)\n\n        def _cmd(session, server, sock_info, slave_ok):\n            return server.run_operation_with_response(\n                sock_info,\n                operation,\n                slave_ok,\n                self._event_listeners,\n                exhaust,\n                unpack_res)\n\n        return self._retryable_read(\n            _cmd, operation.read_preference, operation.session,\n            address=address,\n            retryable=isinstance(operation, message._Query),\n            exhaust=exhaust)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _reset_on_error(self, server_address, session):\n        try:\n            try:\n                yield\n            except PyMongoError as exc:\n                if session and exc.has_error_label(\n                        \"TransientTransactionError\"):\n                    session._unpin_mongos()\n                raise\n        except NetworkTimeout:\n            # The socket has been closed. Don't reset the server.\n            # Server Discovery And Monitoring Spec: \"When an application\n            # operation fails because of any network error besides a socket\n            # timeout....\"\n            raise\n        except NotMasterError:\n            # \"When the client sees a \"not master\" error it MUST replace the\n            # server's description with type Unknown. It MUST request an\n            # immediate check of the server.\"\n            self._reset_server_and_request_check(server_address)\n            raise\n        except ConnectionFailure:\n            # \"Client MUST replace the server's description with type Unknown\n            # ... MUST NOT request an immediate check of the server.\"\n            self.__reset_server(server_address)\n            raise\n        except OperationFailure as exc:\n            if exc.code in helpers._RETRYABLE_ERROR_CODES:\n                # Do not request an immediate check since the server is likely\n                # shutting down.\n                self.__reset_server(server_address)\n            raise", "response": "A helper function to reset the server s internal state based on the SDAM spec."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _retry_with_session(self, retryable, func, session, bulk):\n        retryable = (retryable and self.retry_writes\n                     and session and not session._in_transaction)\n        last_error = None\n        retrying = False\n\n        def is_retrying():\n            return bulk.retrying if bulk else retrying\n        # Increment the transaction id up front to ensure any retry attempt\n        # will use the proper txnNumber, even if server or socket selection\n        # fails before the command can be sent.\n        if retryable:\n            session._start_retryable_write()\n            if bulk:\n                bulk.started_retryable_write = True\n\n        while True:\n            try:\n                server = self._select_server(writable_server_selector, session)\n                supports_session = (\n                    session is not None and\n                    server.description.retryable_writes_supported)\n                with self._get_socket(server, session) as sock_info:\n                    if retryable and not supports_session:\n                        if is_retrying():\n                            # A retry is not possible because this server does\n                            # not support sessions raise the last error.\n                            raise last_error\n                        retryable = False\n                    return func(session, sock_info, retryable)\n            except ServerSelectionTimeoutError:\n                if is_retrying():\n                    # The application may think the write was never attempted\n                    # if we raise ServerSelectionTimeoutError on the retry\n                    # attempt. Raise the original exception instead.\n                    raise last_error\n                # A ServerSelectionTimeoutError error indicates that there may\n                # be a persistent outage. Attempting to retry in this case will\n                # most likely be a waste of time.\n                raise\n            except ConnectionFailure as exc:\n                if not retryable or is_retrying():\n                    raise\n                if bulk:\n                    bulk.retrying = True\n                else:\n                    retrying = True\n                last_error = exc\n            except BulkWriteError as exc:\n                if not retryable or is_retrying():\n                    raise\n                # Check the last writeConcernError to determine if this\n                # BulkWriteError is retryable.\n                wces = exc.details['writeConcernErrors']\n                wce = wces[-1] if wces else {}\n                if wce.get('code', 0) not in helpers._RETRYABLE_ERROR_CODES:\n                    raise\n                if bulk:\n                    bulk.retrying = True\n                else:\n                    retrying = True\n                last_error = exc\n            except OperationFailure as exc:\n                if not retryable or is_retrying():\n                    raise\n                if exc.code not in helpers._RETRYABLE_ERROR_CODES:\n                    raise\n                if bulk:\n                    bulk.retrying = True\n                else:\n                    retrying = True\n                last_error = exc", "response": "Execute an operation with at most one consecutive retries\nSetException Returns the result of func."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes an operation with at most one consecutive retries Returns the result of the operation on success.", "response": "def _retryable_read(self, func, read_pref, session, address=None,\n                        retryable=True, exhaust=False):\n        \"\"\"Execute an operation with at most one consecutive retries\n\n        Returns func()'s return value on success. On error retries the same\n        command once.\n\n        Re-raises any exception thrown by func().\n        \"\"\"\n        retryable = (retryable and\n                     self.retry_reads\n                     and not (session and session._in_transaction))\n        last_error = None\n        retrying = False\n\n        while True:\n            try:\n                server = self._select_server(\n                    read_pref, session, address=address)\n                if not server.description.retryable_reads_supported:\n                    retryable = False\n                with self._slaveok_for_server(read_pref, server, session,\n                                              exhaust=exhaust) as (sock_info,\n                                                                   slave_ok):\n                    if retrying and not retryable:\n                        # A retry is not possible because this server does\n                        # not support retryable reads, raise the last error.\n                        raise last_error\n                    return func(session, server, sock_info, slave_ok)\n            except ServerSelectionTimeoutError:\n                if retrying:\n                    # The application may think the write was never attempted\n                    # if we raise ServerSelectionTimeoutError on the retry\n                    # attempt. Raise the original exception instead.\n                    raise last_error\n                # A ServerSelectionTimeoutError error indicates that there may\n                # be a persistent outage. Attempting to retry in this case will\n                # most likely be a waste of time.\n                raise\n            except ConnectionFailure as exc:\n                if not retryable or retrying:\n                    raise\n                retrying = True\n                last_error = exc\n            except OperationFailure as exc:\n                if not retryable or retrying:\n                    raise\n                if exc.code not in helpers._RETRYABLE_ERROR_CODES:\n                    raise\n                retrying = True\n                last_error = exc"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncloses the given cursor.", "response": "def close_cursor(self, cursor_id, address=None):\n        \"\"\"DEPRECATED - Send a kill cursors message soon with the given id.\n\n        Raises :class:`TypeError` if `cursor_id` is not an instance of\n        ``(int, long)``. What closing the cursor actually means\n        depends on this client's cursor manager.\n\n        This method may be called from a :class:`~pymongo.cursor.Cursor`\n        destructor during garbage collection, so it isn't safe to take a\n        lock or do network I/O. Instead, we schedule the cursor to be closed\n        soon on a background thread.\n\n        :Parameters:\n          - `cursor_id`: id of cursor to close\n          - `address` (optional): (host, port) pair of the cursor's server.\n            If it is not provided, the client attempts to close the cursor on\n            the primary or standalone, or a mongos server.\n\n        .. versionchanged:: 3.7\n           Deprecated.\n\n        .. versionchanged:: 3.0\n           Added ``address`` parameter.\n        \"\"\"\n        warnings.warn(\n            \"close_cursor is deprecated.\",\n            DeprecationWarning,\n            stacklevel=2)\n        if not isinstance(cursor_id, integer_types):\n            raise TypeError(\"cursor_id must be an instance of (int, long)\")\n\n        self._close_cursor(cursor_id, address)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _close_cursor(self, cursor_id, address):\n        if self.__cursor_manager is not None:\n            self.__cursor_manager.close(cursor_id, address)\n        else:\n            self.__kill_cursors_queue.append((address, [cursor_id]))", "response": "Send a kill cursors message with the given id."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _close_cursor_now(self, cursor_id, address=None, session=None):\n        if not isinstance(cursor_id, integer_types):\n            raise TypeError(\"cursor_id must be an instance of (int, long)\")\n\n        if self.__cursor_manager is not None:\n            self.__cursor_manager.close(cursor_id, address)\n        else:\n            try:\n                self._kill_cursors(\n                    [cursor_id], address, self._get_topology(), session)\n            except PyMongoError:\n                # Make another attempt to kill the cursor later.\n                self.__kill_cursors_queue.append((address, [cursor_id]))", "response": "Send a kill cursors message with the given id."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a kill cursors message with the given ids.", "response": "def _kill_cursors(self, cursor_ids, address, topology, session):\n        \"\"\"Send a kill cursors message with the given ids.\"\"\"\n        listeners = self._event_listeners\n        publish = listeners.enabled_for_commands\n        if address:\n            # address could be a tuple or _CursorAddress, but\n            # select_server_by_address needs (host, port).\n            server = topology.select_server_by_address(tuple(address))\n        else:\n            # Application called close_cursor() with no address.\n            server = topology.select_server(writable_server_selector)\n\n        try:\n            namespace = address.namespace\n            db, coll = namespace.split('.', 1)\n        except AttributeError:\n            namespace = None\n            db = coll = \"OP_KILL_CURSORS\"\n\n        spec = SON([('killCursors', coll), ('cursors', cursor_ids)])\n        with server.get_socket(self.__all_credentials) as sock_info:\n            if sock_info.max_wire_version >= 4 and namespace is not None:\n                sock_info.command(db, spec, session=session, client=self)\n            else:\n                if publish:\n                    start = datetime.datetime.now()\n                request_id, msg = message.kill_cursors(cursor_ids)\n                if publish:\n                    duration = datetime.datetime.now() - start\n                    # Here and below, address could be a tuple or\n                    # _CursorAddress. We always want to publish a\n                    # tuple to match the rest of the monitoring\n                    # API.\n                    listeners.publish_command_start(\n                        spec, db, request_id, tuple(address))\n                    start = datetime.datetime.now()\n\n                try:\n                    sock_info.send_message(msg, 0)\n                except Exception as exc:\n                    if publish:\n                        dur = ((datetime.datetime.now() - start) + duration)\n                        listeners.publish_command_failure(\n                            dur, message._convert_exception(exc),\n                            'killCursors', request_id,\n                            tuple(address))\n                    raise\n\n                if publish:\n                    duration = ((datetime.datetime.now() - start) + duration)\n                    # OP_KILL_CURSORS returns no reply, fake one.\n                    reply = {'cursorsUnknown': cursor_ids, 'ok': 1}\n                    listeners.publish_command_success(\n                        duration, reply, 'killCursors', request_id,\n                        tuple(address))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess any pending kill cursors requests and maintain connection pool parameters.", "response": "def _process_periodic_tasks(self):\n        \"\"\"Process any pending kill cursors requests and\n        maintain connection pool parameters.\"\"\"\n        address_to_cursor_ids = defaultdict(list)\n\n        # Other threads or the GC may append to the queue concurrently.\n        while True:\n            try:\n                address, cursor_ids = self.__kill_cursors_queue.pop()\n            except IndexError:\n                break\n\n            address_to_cursor_ids[address].extend(cursor_ids)\n\n        # Don't re-open topology if it's closed and there's no pending cursors.\n        if address_to_cursor_ids:\n            topology = self._get_topology()\n            for address, cursor_ids in address_to_cursor_ids.items():\n                try:\n                    self._kill_cursors(\n                        cursor_ids, address, topology, session=None)\n                except Exception:\n                    helpers._handle_exception()\n        try:\n            self._topology.update_pool()\n        except Exception:\n            helpers._handle_exception()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start_session(self,\n                      causal_consistency=True,\n                      default_transaction_options=None):\n        \"\"\"Start a logical session.\n\n        This method takes the same parameters as\n        :class:`~pymongo.client_session.SessionOptions`. See the\n        :mod:`~pymongo.client_session` module for details and examples.\n\n        Requires MongoDB 3.6. It is an error to call :meth:`start_session`\n        if this client has been authenticated to multiple databases using the\n        deprecated method :meth:`~pymongo.database.Database.authenticate`.\n\n        A :class:`~pymongo.client_session.ClientSession` may only be used with\n        the MongoClient that started it.\n\n        :Returns:\n          An instance of :class:`~pymongo.client_session.ClientSession`.\n\n        .. versionadded:: 3.6\n        \"\"\"\n        return self.__start_session(\n            False,\n            causal_consistency=causal_consistency,\n            default_transaction_options=default_transaction_options)", "response": "Start a logical session."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ensure_session(self, session=None):\n        if session:\n            return session\n\n        try:\n            # Don't make implicit sessions causally consistent. Applications\n            # should always opt-in.\n            return self.__start_session(True, causal_consistency=False)\n        except (ConfigurationError, InvalidOperation):\n            # Sessions not supported, or multiple users authenticated.\n            return None", "response": "Ensures that a temporary session is created."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _tmp_session(self, session, close=True):\n        if session:\n            # Don't call end_session.\n            yield session\n            return\n\n        s = self._ensure_session(session)\n        if s and close:\n            with s:\n                # Call end_session when we exit this scope.\n                yield s\n        elif s:\n            try:\n                # Only call end_session on error.\n                yield s\n            except Exception:\n                s.end_session()\n                raise\n        else:\n            yield None", "response": "Yields a temporary session."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef server_info(self, session=None):\n        return self.admin.command(\"buildinfo\",\n                                  read_preference=ReadPreference.PRIMARY,\n                                  session=session)", "response": "Get information about the MongoDB server we re connected to."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_databases(self, session=None, **kwargs):\n        cmd = SON([(\"listDatabases\", 1)])\n        cmd.update(kwargs)\n        admin = self._database_default_options(\"admin\")\n        res = admin._retryable_read_command(cmd, session=session)\n        # listDatabases doesn't return a cursor (yet). Fake one.\n        cursor = {\n            \"id\": 0,\n            \"firstBatch\": res[\"databases\"],\n            \"ns\": \"admin.$cmd\",\n        }\n        return CommandCursor(admin[\"$cmd\"], cursor, None)", "response": "Get a cursor over the databases of the connected server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_database_names(self, session=None):\n        return [doc[\"name\"]\n                for doc in self.list_databases(session, nameOnly=True)]", "response": "Get a list of the names of all databases on the connected server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef database_names(self, session=None):\n        warnings.warn(\"database_names is deprecated. Use list_database_names \"\n                      \"instead.\", DeprecationWarning, stacklevel=2)\n        return self.list_database_names(session)", "response": "Deprecated. Use list_database_names instead."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef drop_database(self, name_or_database, session=None):\n        name = name_or_database\n        if isinstance(name, database.Database):\n            name = name.name\n\n        if not isinstance(name, string_type):\n            raise TypeError(\"name_or_database must be an instance \"\n                            \"of %s or a Database\" % (string_type.__name__,))\n\n        self._purge_index(name)\n        with self._socket_for_writes(session) as sock_info:\n            self[name]._command(\n                sock_info,\n                \"dropDatabase\",\n                read_preference=ReadPreference.PRIMARY,\n                write_concern=self._write_concern_for(session),\n                parse_write_concern_error=True,\n                session=session)", "response": "Drop a database.\n\n        Raises :class:`TypeError` if `name_or_database` is not an instance of\n        :class:`basestring` (:class:`str` in python 3) or\n        :class:`~pymongo.database.Database`.\n\n        :Parameters:\n          - `name_or_database`: the name of a database to drop, or a\n            :class:`~pymongo.database.Database` instance representing the\n            database to drop\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n\n        .. note:: The :attr:`~pymongo.mongo_client.MongoClient.write_concern` of\n           this client is automatically applied to this operation when using\n           MongoDB >= 3.4.\n\n        .. versionchanged:: 3.4\n           Apply this client's write concern automatically to this operation\n           when connected to MongoDB >= 3.4."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_default_database(self, default=None, codec_options=None,\n            read_preference=None, write_concern=None, read_concern=None):\n        \"\"\"Get the database named in the MongoDB connection URI.\n\n        >>> uri = 'mongodb://host/my_database'\n        >>> client = MongoClient(uri)\n        >>> db = client.get_default_database()\n        >>> assert db.name == 'my_database'\n        >>> db = client.get_database()\n        >>> assert db.name == 'my_database'\n\n        Useful in scripts where you want to choose which database to use\n        based only on the URI in a configuration file.\n\n        :Parameters:\n          - `default` (optional): the database name to use if no database name\n            was provided in the URI.\n          - `codec_options` (optional): An instance of\n            :class:`~bson.codec_options.CodecOptions`. If ``None`` (the\n            default) the :attr:`codec_options` of this :class:`MongoClient` is\n            used.\n          - `read_preference` (optional): The read preference to use. If\n            ``None`` (the default) the :attr:`read_preference` of this\n            :class:`MongoClient` is used. See :mod:`~pymongo.read_preferences`\n            for options.\n          - `write_concern` (optional): An instance of\n            :class:`~pymongo.write_concern.WriteConcern`. If ``None`` (the\n            default) the :attr:`write_concern` of this :class:`MongoClient` is\n            used.\n          - `read_concern` (optional): An instance of\n            :class:`~pymongo.read_concern.ReadConcern`. If ``None`` (the\n            default) the :attr:`read_concern` of this :class:`MongoClient` is\n            used.\n\n        .. versionchanged:: 3.8\n           Undeprecated. Added the ``default``, ``codec_options``,\n           ``read_preference``, ``write_concern`` and ``read_concern``\n           parameters.\n\n        .. versionchanged:: 3.5\n           Deprecated, use :meth:`get_database` instead.\n        \"\"\"\n        if self.__default_database_name is None and default is None:\n            raise ConfigurationError(\n                'No default database name defined or provided.')\n\n        return database.Database(\n            self, self.__default_database_name or default, codec_options,\n            read_preference, write_concern, read_concern)", "response": "Get the default database name for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _database_default_options(self, name):\n        return self.get_database(\n            name, codec_options=DEFAULT_CODEC_OPTIONS,\n            read_preference=ReadPreference.PRIMARY,\n            write_concern=DEFAULT_WRITE_CONCERN)", "response": "Get a Database instance with the default settings."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unlock(self, session=None):\n        cmd = SON([(\"fsyncUnlock\", 1)])\n        with self._socket_for_writes(session) as sock_info:\n            if sock_info.max_wire_version >= 4:\n                try:\n                    with self._tmp_session(session) as s:\n                        sock_info.command(\n                            \"admin\", cmd, session=s, client=self)\n                except OperationFailure as exc:\n                    # Ignore \"DB not locked\" to replicate old behavior\n                    if exc.code != 125:\n                        raise\n            else:\n                message._first_batch(sock_info, \"admin\", \"$cmd.sys.unlock\",\n                                     {}, -1, True, self.codec_options,\n                                     ReadPreference.PRIMARY, cmd,\n                                     self._event_listeners)", "response": "Unlock a previously locked server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nissue appropriate warnings when deprecated options are present in the options dictionary. Removes deprecated option key value pairs if the options dictionary is found to also have the renamed option.", "response": "def _handle_option_deprecations(options):\n    \"\"\"Issue appropriate warnings when deprecated options are present in the\n    options dictionary. Removes deprecated option key, value pairs if the\n    options dictionary is found to also have the renamed option.\"\"\"\n    undeprecated_options = _CaseInsensitiveDictionary()\n    for key, value in iteritems(options):\n        optname = str(key).lower()\n        if optname in URI_OPTIONS_DEPRECATION_MAP:\n            renamed_key = URI_OPTIONS_DEPRECATION_MAP[optname]\n            if renamed_key.lower() in options:\n                warnings.warn(\"Deprecated option '%s' ignored in favor of \"\n                              \"'%s'.\" % (str(key), renamed_key))\n                continue\n            warnings.warn(\"Option '%s' is deprecated, use '%s' instead.\" % (\n                          str(key), renamed_key))\n        undeprecated_options[str(key)] = value\n    return undeprecated_options"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrename keys in the options dictionary to their internally - used names.", "response": "def _normalize_options(options):\n    \"\"\"Renames keys in the options dictionary to their internally-used\n    names.\"\"\"\n    normalized_options = {}\n    for key, value in iteritems(options):\n        optname = str(key).lower()\n        intname = INTERNAL_URI_OPTION_NAME_MAP.get(optname, key)\n        normalized_options[intname] = options[key]\n    return normalized_options"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split_options(opts, validate=True, warn=False, normalize=True):\n    and_idx = opts.find(\"&\")\n    semi_idx = opts.find(\";\")\n    try:\n        if and_idx >= 0 and semi_idx >= 0:\n            raise InvalidURI(\"Can not mix '&' and ';' for option separators.\")\n        elif and_idx >= 0:\n            options = _parse_options(opts, \"&\")\n        elif semi_idx >= 0:\n            options = _parse_options(opts, \";\")\n        elif opts.find(\"=\") != -1:\n            options = _parse_options(opts, None)\n        else:\n            raise ValueError\n    except ValueError:\n        raise InvalidURI(\"MongoDB URI options are key=value pairs.\")\n\n    options = _handle_option_deprecations(options)\n\n    if validate:\n        options = validate_options(options, warn)\n\n    if normalize:\n        options = _normalize_options(options)\n\n    return options", "response": "Takes the options portion of a MongoDB URI validates each option\n    and returns the options in a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_uri(uri, default_port=DEFAULT_PORT, validate=True, warn=False):\n    if uri.startswith(SCHEME):\n        is_srv = False\n        scheme_free = uri[SCHEME_LEN:]\n    elif uri.startswith(SRV_SCHEME):\n        if not _HAVE_DNSPYTHON:\n            raise ConfigurationError('The \"dnspython\" module must be '\n                                     'installed to use mongodb+srv:// URIs')\n        is_srv = True\n        scheme_free = uri[SRV_SCHEME_LEN:]\n    else:\n        raise InvalidURI(\"Invalid URI scheme: URI must \"\n                         \"begin with '%s' or '%s'\" % (SCHEME, SRV_SCHEME))\n\n    if not scheme_free:\n        raise InvalidURI(\"Must provide at least one hostname or IP.\")\n\n    user = None\n    passwd = None\n    dbase = None\n    collection = None\n    options = {}\n\n    host_part, _, path_part = scheme_free.partition('/')\n    if not host_part:\n        host_part = path_part\n        path_part = \"\"\n\n    if not path_part and '?' in host_part:\n        raise InvalidURI(\"A '/' is required between \"\n                         \"the host list and any options.\")\n\n    if '@' in host_part:\n        userinfo, _, hosts = host_part.rpartition('@')\n        user, passwd = parse_userinfo(userinfo)\n    else:\n        hosts = host_part\n\n    if '/' in hosts:\n        raise InvalidURI(\"Any '/' in a unix domain socket must be\"\n                         \" percent-encoded: %s\" % host_part)\n\n    hosts = unquote_plus(hosts)\n\n    if is_srv:\n        nodes = split_hosts(hosts, default_port=None)\n        if len(nodes) != 1:\n            raise InvalidURI(\n                \"%s URIs must include one, \"\n                \"and only one, hostname\" % (SRV_SCHEME,))\n        fqdn, port = nodes[0]\n        if port is not None:\n            raise InvalidURI(\n                \"%s URIs must not include a port number\" % (SRV_SCHEME,))\n        nodes = _get_dns_srv_hosts(fqdn)\n\n        try:\n            plist = fqdn.split(\".\")[1:]\n        except Exception:\n            raise ConfigurationError(\"Invalid URI host\")\n        slen = len(plist)\n        if slen < 2:\n            raise ConfigurationError(\"Invalid URI host\")\n        for node in nodes:\n            try:\n                nlist = node[0].split(\".\")[1:][-slen:]\n            except Exception:\n                raise ConfigurationError(\"Invalid SRV host\")\n            if plist != nlist:\n                raise ConfigurationError(\"Invalid SRV host\")\n\n        dns_options = _get_dns_txt_options(fqdn)\n        if dns_options:\n            options = split_options(dns_options, validate, warn)\n            if set(options) - _ALLOWED_TXT_OPTS:\n                raise ConfigurationError(\n                    \"Only authSource and replicaSet are supported from DNS\")\n        options[\"ssl\"] = True if validate else 'true'\n    else:\n        nodes = split_hosts(hosts, default_port=default_port)\n\n    if path_part:\n        if path_part[0] == '?':\n            opts = unquote_plus(path_part[1:])\n        else:\n            dbase, _, opts = map(unquote_plus, path_part.partition('?'))\n            if '.' in dbase:\n                dbase, collection = dbase.split('.', 1)\n\n            if _BAD_DB_CHARS.search(dbase):\n                raise InvalidURI('Bad database name \"%s\"' % dbase)\n\n        if opts:\n            options.update(split_options(opts, validate, warn))\n\n    if dbase is not None:\n        dbase = unquote_plus(dbase)\n    if collection is not None:\n        collection = unquote_plus(collection)\n\n    return {\n        'nodelist': nodes,\n        'username': user,\n        'password': passwd,\n        'database': dbase,\n        'collection': collection,\n        'options': options\n    }", "response": "Parse and validate a MongoDB URI."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef new_file(self, **kwargs):\n        # No need for __ensure_index_files_id() here; GridIn ensures\n        # the (files_id, n) index when needed.\n        return GridIn(\n            self.__collection, disable_md5=self.__disable_md5, **kwargs)", "response": "Create a new file in GridFS."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a file from GridFS by _id.", "response": "def get(self, file_id, session=None):\n        \"\"\"Get a file from GridFS by ``\"_id\"``.\n\n        Returns an instance of :class:`~gridfs.grid_file.GridOut`,\n        which provides a file-like interface for reading.\n\n        :Parameters:\n          - `file_id`: ``\"_id\"`` of the file to get\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n        \"\"\"\n        gout = GridOut(self.__collection, file_id, session=session)\n\n        # Raise NoFile now, instead of on first attribute access.\n        gout._ensure_file()\n        return gout"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_version(self, filename=None, version=-1, session=None, **kwargs):\n        query = kwargs\n        if filename is not None:\n            query[\"filename\"] = filename\n\n        cursor = self.__files.find(query, session=session)\n        if version < 0:\n            skip = abs(version) - 1\n            cursor.limit(-1).skip(skip).sort(\"uploadDate\", DESCENDING)\n        else:\n            cursor.limit(-1).skip(version).sort(\"uploadDate\", ASCENDING)\n        try:\n            doc = next(cursor)\n            return GridOut(\n                self.__collection, file_document=doc, session=session)\n        except StopIteration:\n            raise NoFile(\"no version %d for filename %r\" % (version, filename))", "response": "Get a version of a file from GridFS by filename or metadata fields."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the most recent version of a file in GridFS by filename or metadata fields.", "response": "def get_last_version(self, filename=None, session=None, **kwargs):\n        \"\"\"Get the most recent version of a file in GridFS by ``\"filename\"``\n        or metadata fields.\n\n        Equivalent to calling :meth:`get_version` with the default\n        `version` (``-1``).\n\n        :Parameters:\n          - `filename`: ``\"filename\"`` of the file to get, or `None`\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`\n          - `**kwargs` (optional): find files by custom metadata.\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n        \"\"\"\n        return self.get_version(filename=filename, session=session, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete a file from GridFS by _id.", "response": "def delete(self, file_id, session=None):\n        \"\"\"Delete a file from GridFS by ``\"_id\"``.\n\n        Deletes all data belonging to the file with ``\"_id\"``:\n        `file_id`.\n\n        .. warning:: Any processes/threads reading from the file while\n           this method is executing will likely see an invalid/corrupt\n           file. Care should be taken to avoid concurrent reads to a file\n           while it is being deleted.\n\n        .. note:: Deletes of non-existent files are considered successful\n           since the end result is the same: no file with that _id remains.\n\n        :Parameters:\n          - `file_id`: ``\"_id\"`` of the file to delete\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n\n        .. versionchanged:: 3.1\n           ``delete`` no longer ensures indexes.\n        \"\"\"\n        self.__files.delete_one({\"_id\": file_id}, session=session)\n        self.__chunks.delete_many({\"files_id\": file_id}, session=session)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list(self, session=None):\n        # With an index, distinct includes documents with no filename\n        # as None.\n        return [\n            name for name in self.__files.distinct(\"filename\", session=session)\n            if name is not None]", "response": "Returns a list of all files stored in this instance of a GridFS."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if a file exists in this instance of GridFS.", "response": "def exists(self, document_or_id=None, session=None, **kwargs):\n        \"\"\"Check if a file exists in this instance of :class:`GridFS`.\n\n        The file to check for can be specified by the value of its\n        ``_id`` key, or by passing in a query document. A query\n        document can be passed in as dictionary, or by using keyword\n        arguments. Thus, the following three calls are equivalent:\n\n        >>> fs.exists(file_id)\n        >>> fs.exists({\"_id\": file_id})\n        >>> fs.exists(_id=file_id)\n\n        As are the following two calls:\n\n        >>> fs.exists({\"filename\": \"mike.txt\"})\n        >>> fs.exists(filename=\"mike.txt\")\n\n        And the following two:\n\n        >>> fs.exists({\"foo\": {\"$gt\": 12}})\n        >>> fs.exists(foo={\"$gt\": 12})\n\n        Returns ``True`` if a matching file exists, ``False``\n        otherwise. Calls to :meth:`exists` will not automatically\n        create appropriate indexes; application developers should be\n        sure to create indexes if needed and as appropriate.\n\n        :Parameters:\n          - `document_or_id` (optional): query document, or _id of the\n            document to check for\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`\n          - `**kwargs` (optional): keyword arguments are used as a\n            query document, if they're present.\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n        \"\"\"\n        if kwargs:\n            f = self.__files.find_one(kwargs, [\"_id\"], session=session)\n        else:\n            f = self.__files.find_one(document_or_id, [\"_id\"], session=session)\n\n        return f is not None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef open_upload_stream(self, filename, chunk_size_bytes=None,\n                           metadata=None, session=None):\n        \"\"\"Opens a Stream that the application can write the contents of the\n        file to.\n\n        The user must specify the filename, and can choose to add any\n        additional information in the metadata field of the file document or\n        modify the chunk size.\n        For example::\n\n          my_db = MongoClient().test\n          fs = GridFSBucket(my_db)\n          grid_in = fs.open_upload_stream(\n                \"test_file\", chunk_size_bytes=4,\n                metadata={\"contentType\": \"text/plain\"})\n          grid_in.write(\"data I want to store!\")\n          grid_in.close()  # uploaded on close\n\n        Returns an instance of :class:`~gridfs.grid_file.GridIn`.\n\n        Raises :exc:`~gridfs.errors.NoFile` if no such version of\n        that file exists.\n        Raises :exc:`~ValueError` if `filename` is not a string.\n\n        :Parameters:\n          - `filename`: The name of the file to upload.\n          - `chunk_size_bytes` (options): The number of bytes per chunk of this\n            file. Defaults to the chunk_size_bytes in :class:`GridFSBucket`.\n          - `metadata` (optional): User data for the 'metadata' field of the\n            files collection document. If not provided the metadata field will\n            be omitted from the files collection document.\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n        \"\"\"\n        validate_string(\"filename\", filename)\n\n        opts = {\"filename\": filename,\n                \"chunk_size\": (chunk_size_bytes if chunk_size_bytes\n                               is not None else self._chunk_size_bytes)}\n        if metadata is not None:\n            opts[\"metadata\"] = metadata\n\n        return GridIn(\n            self._collection,\n            session=session,\n            disable_md5=self._disable_md5,\n            **opts)", "response": "Opens a Stream that the application can write the contents of the file to."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef upload_from_stream(self, filename, source, chunk_size_bytes=None,\n                           metadata=None, session=None):\n        \"\"\"Uploads a user file to a GridFS bucket.\n\n        Reads the contents of the user file from `source` and uploads\n        it to the file `filename`. Source can be a string or file-like object.\n        For example::\n\n          my_db = MongoClient().test\n          fs = GridFSBucket(my_db)\n          file_id = fs.upload_from_stream(\n              \"test_file\",\n              \"data I want to store!\",\n              chunk_size_bytes=4,\n              metadata={\"contentType\": \"text/plain\"})\n\n        Returns the _id of the uploaded file.\n\n        Raises :exc:`~gridfs.errors.NoFile` if no such version of\n        that file exists.\n        Raises :exc:`~ValueError` if `filename` is not a string.\n\n        :Parameters:\n          - `filename`: The name of the file to upload.\n          - `source`: The source stream of the content to be uploaded. Must be\n            a file-like object that implements :meth:`read` or a string.\n          - `chunk_size_bytes` (options): The number of bytes per chunk of this\n            file. Defaults to the chunk_size_bytes of :class:`GridFSBucket`.\n          - `metadata` (optional): User data for the 'metadata' field of the\n            files collection document. If not provided the metadata field will\n            be omitted from the files collection document.\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n        \"\"\"\n        with self.open_upload_stream(\n                filename, chunk_size_bytes, metadata, session=session) as gin:\n            gin.write(source)\n\n        return gin._id", "response": "Uploads a user file from a stream to a GridFS bucket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndownloading the contents of filename to destination.", "response": "def download_to_stream_by_name(self, filename, destination, revision=-1,\n                                   session=None):\n        \"\"\"Write the contents of `filename` (with optional `revision`) to\n        `destination`.\n\n        For example::\n\n          my_db = MongoClient().test\n          fs = GridFSBucket(my_db)\n          # Get file to write to\n          file = open('myfile','wb')\n          fs.download_to_stream_by_name(\"test_file\", file)\n\n        Raises :exc:`~gridfs.errors.NoFile` if no such version of\n        that file exists.\n\n        Raises :exc:`~ValueError` if `filename` is not a string.\n\n        :Parameters:\n          - `filename`: The name of the file to read from.\n          - `destination`: A file-like object that implements :meth:`write`.\n          - `revision` (optional): Which revision (documents with the same\n            filename and different uploadDate) of the file to retrieve.\n            Defaults to -1 (the most recent revision).\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`\n\n        :Note: Revision numbers are defined as follows:\n\n          - 0 = the original stored file\n          - 1 = the first revision\n          - 2 = the second revision\n          - etc...\n          - -2 = the second most recent revision\n          - -1 = the most recent revision\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n        \"\"\"\n        with self.open_download_stream_by_name(\n                filename, revision, session=session) as gout:\n            for chunk in gout:\n                destination.write(chunk)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rename(self, file_id, new_filename, session=None):\n        result = self._files.update_one({\"_id\": file_id},\n                                        {\"$set\": {\"filename\": new_filename}},\n                                        session=session)\n        if not result.matched_count:\n            raise NoFile(\"no files could be renamed %r because none \"\n                         \"matched file_id %i\" % (new_filename, file_id))", "response": "Renames the stored file with the specified file_id."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef with_options(self, codec_options=None, read_preference=None,\n                     write_concern=None, read_concern=None):\n        \"\"\"Get a clone of this database changing the specified settings.\n\n          >>> db1.read_preference\n          Primary()\n          >>> from pymongo import ReadPreference\n          >>> db2 = db1.with_options(read_preference=ReadPreference.SECONDARY)\n          >>> db1.read_preference\n          Primary()\n          >>> db2.read_preference\n          Secondary(tag_sets=None)\n\n        :Parameters:\n          - `codec_options` (optional): An instance of\n            :class:`~bson.codec_options.CodecOptions`. If ``None`` (the\n            default) the :attr:`codec_options` of this :class:`Collection`\n            is used.\n          - `read_preference` (optional): The read preference to use. If\n            ``None`` (the default) the :attr:`read_preference` of this\n            :class:`Collection` is used. See :mod:`~pymongo.read_preferences`\n            for options.\n          - `write_concern` (optional): An instance of\n            :class:`~pymongo.write_concern.WriteConcern`. If ``None`` (the\n            default) the :attr:`write_concern` of this :class:`Collection`\n            is used.\n          - `read_concern` (optional): An instance of\n            :class:`~pymongo.read_concern.ReadConcern`. If ``None`` (the\n            default) the :attr:`read_concern` of this :class:`Collection`\n            is used.\n\n        .. versionadded:: 3.8\n        \"\"\"\n        return Database(self.client,\n                        self.__name,\n                        codec_options or self.codec_options,\n                        read_preference or self.read_preference,\n                        write_concern or self.write_concern,\n                        read_concern or self.read_concern)", "response": "Returns a new Database instance with the specified settings."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_collection(self, name, codec_options=None,\n                          read_preference=None, write_concern=None,\n                          read_concern=None, session=None, **kwargs):\n        \"\"\"Create a new :class:`~pymongo.collection.Collection` in this\n        database.\n\n        Normally collection creation is automatic. This method should\n        only be used to specify options on\n        creation. :class:`~pymongo.errors.CollectionInvalid` will be\n        raised if the collection already exists.\n\n        Options should be passed as keyword arguments to this method. Supported\n        options vary with MongoDB release. Some examples include:\n\n          - \"size\": desired initial size for the collection (in\n            bytes). For capped collections this size is the max\n            size of the collection.\n          - \"capped\": if True, this is a capped collection\n          - \"max\": maximum number of objects if capped (optional)\n\n        See the MongoDB documentation for a full list of supported options by\n        server version.\n\n        :Parameters:\n          - `name`: the name of the collection to create\n          - `codec_options` (optional): An instance of\n            :class:`~bson.codec_options.CodecOptions`. If ``None`` (the\n            default) the :attr:`codec_options` of this :class:`Database` is\n            used.\n          - `read_preference` (optional): The read preference to use. If\n            ``None`` (the default) the :attr:`read_preference` of this\n            :class:`Database` is used.\n          - `write_concern` (optional): An instance of\n            :class:`~pymongo.write_concern.WriteConcern`. If ``None`` (the\n            default) the :attr:`write_concern` of this :class:`Database` is\n            used.\n          - `read_concern` (optional): An instance of\n            :class:`~pymongo.read_concern.ReadConcern`. If ``None`` (the\n            default) the :attr:`read_concern` of this :class:`Database` is\n            used.\n          - `collation` (optional): An instance of\n            :class:`~pymongo.collation.Collation`.\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n          - `**kwargs` (optional): additional keyword arguments will\n            be passed as options for the create collection command\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n\n        .. versionchanged:: 3.4\n           Added the collation option.\n\n        .. versionchanged:: 3.0\n           Added the codec_options, read_preference, and write_concern options.\n\n        .. versionchanged:: 2.2\n           Removed deprecated argument: options\n        \"\"\"\n        with self.__client._tmp_session(session) as s:\n            if name in self.list_collection_names(\n                    filter={\"name\": name}, session=s):\n                raise CollectionInvalid(\"collection %s already exists\" % name)\n\n            return Collection(self, name, True, codec_options,\n                              read_preference, write_concern,\n                              read_concern, session=s, **kwargs)", "response": "Creates a new collection in the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwatch all changes on this database and return a new cursor.", "response": "def watch(self, pipeline=None, full_document='default', resume_after=None,\n              max_await_time_ms=None, batch_size=None, collation=None,\n              start_at_operation_time=None, session=None):\n        \"\"\"Watch changes on this database.\n\n        Performs an aggregation with an implicit initial ``$changeStream``\n        stage and returns a\n        :class:`~pymongo.change_stream.DatabaseChangeStream` cursor which\n        iterates over changes on all collections in this database.\n\n        Introduced in MongoDB 4.0.\n\n        .. code-block:: python\n\n           with db.watch() as stream:\n               for change in stream:\n                   print(change)\n\n        The :class:`~pymongo.change_stream.DatabaseChangeStream` iterable\n        blocks until the next change document is returned or an error is\n        raised. If the\n        :meth:`~pymongo.change_stream.DatabaseChangeStream.next` method\n        encounters a network error when retrieving a batch from the server,\n        it will automatically attempt to recreate the cursor such that no\n        change events are missed. Any error encountered during the resume\n        attempt indicates there may be an outage and will be raised.\n\n        .. code-block:: python\n\n            try:\n                with db.watch(\n                        [{'$match': {'operationType': 'insert'}}]) as stream:\n                    for insert_change in stream:\n                        print(insert_change)\n            except pymongo.errors.PyMongoError:\n                # The ChangeStream encountered an unrecoverable error or the\n                # resume attempt failed to recreate the cursor.\n                logging.error('...')\n\n        For a precise description of the resume process see the\n        `change streams specification`_.\n\n        :Parameters:\n          - `pipeline` (optional): A list of aggregation pipeline stages to\n            append to an initial ``$changeStream`` stage. Not all\n            pipeline stages are valid after a ``$changeStream`` stage, see the\n            MongoDB documentation on change streams for the supported stages.\n          - `full_document` (optional): The fullDocument to pass as an option\n            to the ``$changeStream`` stage. Allowed values: 'default',\n            'updateLookup'.  Defaults to 'default'.\n            When set to 'updateLookup', the change notification for partial\n            updates will include both a delta describing the changes to the\n            document, as well as a copy of the entire document that was\n            changed from some time after the change occurred.\n          - `resume_after` (optional): The logical starting point for this\n            change stream.\n          - `max_await_time_ms` (optional): The maximum time in milliseconds\n            for the server to wait for changes before responding to a getMore\n            operation.\n          - `batch_size` (optional): The maximum number of documents to return\n            per batch.\n          - `collation` (optional): The :class:`~pymongo.collation.Collation`\n            to use for the aggregation.\n          - `start_at_operation_time` (optional): If provided, the resulting\n            change stream will only return changes that occurred at or after\n            the specified :class:`~bson.timestamp.Timestamp`. Requires\n            MongoDB >= 4.0.\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n\n        :Returns:\n          A :class:`~pymongo.change_stream.DatabaseChangeStream` cursor.\n\n        .. versionadded:: 3.7\n\n        .. mongodoc:: changeStreams\n\n        .. _change streams specification:\n            https://github.com/mongodb/specifications/blob/master/source/change-streams/change-streams.rst\n        \"\"\"\n        return DatabaseChangeStream(\n            self, pipeline, full_document, resume_after, max_await_time_ms,\n            batch_size, collation, start_at_operation_time, session\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _retryable_read_command(self, command, value=1, check=True,\n                allowable_errors=None, read_preference=None,\n                codec_options=DEFAULT_CODEC_OPTIONS, session=None, **kwargs):\n        \"\"\"Same as command but used for retryable read commands.\"\"\"\n        if read_preference is None:\n            read_preference = ((session and session._txn_read_preference())\n                               or ReadPreference.PRIMARY)\n\n        def _cmd(session, server, sock_info, slave_ok):\n            return self._command(sock_info, command, slave_ok, value,\n                                 check, allowable_errors, read_preference,\n                                 codec_options, session=session, **kwargs)\n\n        return self.__client._retryable_read(\n            _cmd, read_preference, session)", "response": "Same as command but used for retryable read commands."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_collections(self, session=None, filter=None, **kwargs):\n        if filter is not None:\n            kwargs['filter'] = filter\n        read_pref = ((session and session._txn_read_preference())\n                     or ReadPreference.PRIMARY)\n\n        def _cmd(session, server, sock_info, slave_okay):\n            return self._list_collections(\n                sock_info, slave_okay, session, read_preference=read_pref,\n                **kwargs)\n\n        return self.__client._retryable_read(\n            _cmd, read_pref, session)", "response": "Get a cursor over the collectons of this database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a list of all the collection names in this database.", "response": "def list_collection_names(self, session=None, filter=None, **kwargs):\n        \"\"\"Get a list of all the collection names in this database.\n\n        For example, to list all non-system collections::\n\n            filter = {\"name\": {\"$regex\": r\"^(?!system\\.)\"}}\n            db.list_collection_names(filter=filter)\n\n        :Parameters:\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n          - `filter` (optional):  A query document to filter the list of\n            collections returned from the listCollections command.\n          - `**kwargs` (optional): Optional parameters of the\n            `listCollections command\n            <https://docs.mongodb.com/manual/reference/command/listCollections/>`_\n            can be passed as keyword arguments to this method. The supported\n            options differ by server version.\n\n        .. versionchanged:: 3.8\n           Added the ``filter`` and ``**kwargs`` parameters.\n\n        .. versionadded:: 3.6\n        \"\"\"\n        if filter is None:\n            kwargs[\"nameOnly\"] = True\n        else:\n            # The enumerate collections spec states that \"drivers MUST NOT set\n            # nameOnly if a filter specifies any keys other than name.\"\n            common.validate_is_mapping(\"filter\", filter)\n            kwargs[\"filter\"] = filter\n            if not filter or (len(filter) == 1 and \"name\" in filter):\n                kwargs[\"nameOnly\"] = True\n\n        return [result[\"name\"]\n                for result in self.list_collections(session=session, **kwargs)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of all the collection names in this database.", "response": "def collection_names(self, include_system_collections=True,\n                         session=None):\n        \"\"\"**DEPRECATED**: Get a list of all the collection names in this\n        database.\n\n        :Parameters:\n          - `include_system_collections` (optional): if ``False`` list\n            will not include system collections (e.g ``system.indexes``)\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n\n        .. versionchanged:: 3.7\n           Deprecated. Use :meth:`list_collection_names` instead.\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n        \"\"\"\n        warnings.warn(\"collection_names is deprecated. Use \"\n                      \"list_collection_names instead.\",\n                      DeprecationWarning, stacklevel=2)\n        kws = {} if include_system_collections else _SYSTEM_FILTER\n        return [result[\"name\"]\n                for result in self.list_collections(session=session,\n                                                    nameOnly=True, **kws)]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef drop_collection(self, name_or_collection, session=None):\n        name = name_or_collection\n        if isinstance(name, Collection):\n            name = name.name\n\n        if not isinstance(name, string_type):\n            raise TypeError(\"name_or_collection must be an \"\n                            \"instance of %s\" % (string_type.__name__,))\n\n        self.__client._purge_index(self.__name, name)\n\n        with self.__client._socket_for_writes(session) as sock_info:\n            return self._command(\n                sock_info, 'drop', value=_unicode(name),\n                allowable_errors=['ns not found'],\n                write_concern=self._write_concern_for(session),\n                parse_write_concern_error=True,\n                session=session)", "response": "Drop a collection.\n\n        :Parameters:\n          - `name_or_collection`: the name of a collection to drop or the\n            collection object itself\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n\n        .. note:: The :attr:`~pymongo.database.Database.write_concern` of\n           this database is automatically applied to this operation when using\n           MongoDB >= 3.4.\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n\n        .. versionchanged:: 3.4\n           Apply this database's write concern automatically to this operation\n           when connected to MongoDB >= 3.4."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_collection(self, name_or_collection,\n                            scandata=False, full=False, session=None):\n        \"\"\"Validate a collection.\n\n        Returns a dict of validation info. Raises CollectionInvalid if\n        validation fails.\n\n        :Parameters:\n          - `name_or_collection`: A Collection object or the name of a\n            collection to validate.\n          - `scandata`: Do extra checks beyond checking the overall\n            structure of the collection.\n          - `full`: Have the server do a more thorough scan of the\n            collection. Use with `scandata` for a thorough scan\n            of the structure of the collection and the individual\n            documents.\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n        \"\"\"\n        name = name_or_collection\n        if isinstance(name, Collection):\n            name = name.name\n\n        if not isinstance(name, string_type):\n            raise TypeError(\"name_or_collection must be an instance of \"\n                            \"%s or Collection\" % (string_type.__name__,))\n\n        result = self.command(\"validate\", _unicode(name),\n                              scandata=scandata, full=full, session=session)\n\n        valid = True\n        # Pre 1.9 results\n        if \"result\" in result:\n            info = result[\"result\"]\n            if info.find(\"exception\") != -1 or info.find(\"corrupt\") != -1:\n                raise CollectionInvalid(\"%s invalid: %s\" % (name, info))\n        # Sharded results\n        elif \"raw\" in result:\n            for _, res in iteritems(result[\"raw\"]):\n                if \"result\" in res:\n                    info = res[\"result\"]\n                    if (info.find(\"exception\") != -1 or\n                            info.find(\"corrupt\") != -1):\n                        raise CollectionInvalid(\"%s invalid: \"\n                                                \"%s\" % (name, info))\n                elif not res.get(\"valid\", False):\n                    valid = False\n                    break\n        # Post 1.9 non-sharded results.\n        elif not result.get(\"valid\", False):\n            valid = False\n\n        if not valid:\n            raise CollectionInvalid(\"%s invalid: %r\" % (name, result))\n\n        return result", "response": "Validate a collection.\n\n        Returns a dict of validation info. Raises CollectionInvalid if\n        validation fails.\n\n        :Parameters:\n          - `name_or_collection`: A Collection object or the name of a\n            collection to validate.\n          - `scandata`: Do extra checks beyond checking the overall\n            structure of the collection.\n          - `full`: Have the server do a more thorough scan of the\n            collection. Use with `scandata` for a thorough scan\n            of the structure of the collection and the individual\n            documents.\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef current_op(self, include_all=False, session=None):\n        cmd = SON([(\"currentOp\", 1), (\"$all\", include_all)])\n        with self.__client._socket_for_writes(session) as sock_info:\n            if sock_info.max_wire_version >= 4:\n                return self.__client.admin._command(\n                    sock_info, cmd, codec_options=self.codec_options,\n                    session=session)\n            else:\n                spec = {\"$all\": True} if include_all else {}\n                return _first_batch(sock_info, \"admin\", \"$cmd.sys.inprog\",\n                                    spec, -1, True, self.codec_options,\n                                    ReadPreference.PRIMARY, cmd,\n                                    self.client._event_listeners)", "response": "Get information on currently running idle operations."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef profiling_level(self, session=None):\n        result = self.command(\"profile\", -1, session=session)\n\n        assert result[\"was\"] >= 0 and result[\"was\"] <= 2\n        return result[\"was\"]", "response": "Get the database s current profiling level."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_profiling_level(self, level, slow_ms=None, session=None):\n        if not isinstance(level, int) or level < 0 or level > 2:\n            raise ValueError(\"level must be one of (OFF, SLOW_ONLY, ALL)\")\n\n        if slow_ms is not None and not isinstance(slow_ms, int):\n            raise TypeError(\"slow_ms must be an integer\")\n\n        if slow_ms is not None:\n            self.command(\"profile\", level, slowms=slow_ms, session=session)\n        else:\n            self.command(\"profile\", level, session=session)", "response": "Sets the database s profiling level."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_user(self, name, password=None, read_only=None, session=None,\n                 **kwargs):\n        \"\"\"**DEPRECATED**: Create user `name` with password `password`.\n\n        Add a new user with permissions for this :class:`Database`.\n\n        .. note:: Will change the password if user `name` already exists.\n\n        .. note:: add_user is deprecated and will be removed in PyMongo\n          4.0. Starting with MongoDB 2.6 user management is handled with four\n          database commands, createUser_, usersInfo_, updateUser_, and\n          dropUser_.\n\n          To create a user::\n\n            db.command(\"createUser\", \"admin\", pwd=\"password\", roles=[\"root\"])\n\n          To create a read-only user::\n\n            db.command(\"createUser\", \"user\", pwd=\"password\", roles=[\"read\"])\n\n          To change a password::\n\n            db.command(\"updateUser\", \"user\", pwd=\"newpassword\")\n\n          Or change roles::\n\n            db.command(\"updateUser\", \"user\", roles=[\"readWrite\"])\n\n        .. _createUser: https://docs.mongodb.com/manual/reference/command/createUser/\n        .. _usersInfo: https://docs.mongodb.com/manual/reference/command/usersInfo/\n        .. _updateUser: https://docs.mongodb.com/manual/reference/command/updateUser/\n        .. _dropUser: https://docs.mongodb.com/manual/reference/command/createUser/\n\n        .. warning:: Never create or modify users over an insecure network without\n          the use of TLS. See :doc:`/examples/tls` for more information.\n\n        :Parameters:\n          - `name`: the name of the user to create\n          - `password` (optional): the password of the user to create. Can not\n            be used with the ``userSource`` argument.\n          - `read_only` (optional): if ``True`` the user will be read only\n          - `**kwargs` (optional): optional fields for the user document\n            (e.g. ``userSource``, ``otherDBRoles``, or ``roles``). See\n            `<http://docs.mongodb.org/manual/reference/privilege-documents>`_\n            for more information.\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n\n        .. versionchanged:: 3.7\n           Added support for SCRAM-SHA-256 users with MongoDB 4.0 and later.\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter. Deprecated add_user.\n\n        .. versionchanged:: 2.5\n           Added kwargs support for optional fields introduced in MongoDB 2.4\n\n        .. versionchanged:: 2.2\n           Added support for read only users\n        \"\"\"\n        warnings.warn(\"add_user is deprecated and will be removed in PyMongo \"\n                      \"4.0. Use db.command with createUser or updateUser \"\n                      \"instead\", DeprecationWarning, stacklevel=2)\n        if not isinstance(name, string_type):\n            raise TypeError(\"name must be an \"\n                            \"instance of %s\" % (string_type.__name__,))\n        if password is not None:\n            if not isinstance(password, string_type):\n                raise TypeError(\"password must be an \"\n                                \"instance of %s\" % (string_type.__name__,))\n            if len(password) == 0:\n                raise ValueError(\"password can't be empty\")\n        if read_only is not None:\n            read_only = common.validate_boolean('read_only', read_only)\n            if 'roles' in kwargs:\n                raise ConfigurationError(\"Can not use \"\n                                         \"read_only and roles together\")\n\n        try:\n            uinfo = self.command(\"usersInfo\", name, session=session)\n            # Create the user if not found in uinfo, otherwise update one.\n            self._create_or_update_user(\n                (not uinfo[\"users\"]), name, password, read_only,\n                session=session, **kwargs)\n        except OperationFailure as exc:\n            # Unauthorized. Attempt to create the user in case of\n            # localhost exception.\n            if exc.code == 13:\n                self._create_or_update_user(\n                    True, name, password, read_only, session=session, **kwargs)\n            else:\n                raise", "response": "Add a new user to the database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_user(self, name, session=None):\n        warnings.warn(\"remove_user is deprecated and will be removed in \"\n                      \"PyMongo 4.0. Use db.command with dropUser \"\n                      \"instead\", DeprecationWarning, stacklevel=2)\n        cmd = SON([(\"dropUser\", name)])\n        # Don't send {} as writeConcern.\n        if self.write_concern.acknowledged and self.write_concern.document:\n            cmd[\"writeConcern\"] = self.write_concern.document\n        self.command(cmd, session=session)", "response": "Remove user from this database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __send_message(self, operation):\n        def kill():\n            self.__killed = True\n            self.__end_session(True)\n\n        client = self.__collection.database.client\n        try:\n            response = client._run_operation_with_response(\n                operation, self._unpack_response, address=self.__address)\n        except OperationFailure:\n            kill()\n            raise\n        except NotMasterError:\n            # Don't send kill cursors to another server after a \"not master\"\n            # error. It's completely pointless.\n            kill()\n            raise\n        except ConnectionFailure:\n            # Don't try to send kill cursors on another socket\n            # or to another server. It can cause a _pinValue\n            # assertion on some server releases if we get here\n            # due to a socket timeout.\n            kill()\n            raise\n        except Exception:\n            # Close the cursor\n            self.__die()\n            raise\n\n        from_command = response.from_command\n        reply = response.data\n        docs = response.docs\n\n        if from_command:\n            cursor = docs[0]['cursor']\n            documents = cursor['nextBatch']\n            self.__id = cursor['id']\n        else:\n            documents = docs\n            self.__id = reply.cursor_id\n\n        if self.__id == 0:\n            kill()\n        self.__data = deque(documents)", "response": "Send a getmore message and handle the response."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrefresh the cursor with more data from the server.", "response": "def _refresh(self):\n        \"\"\"Refreshes the cursor with more data from the server.\n\n        Returns the length of self.__data after refresh. Will exit early if\n        self.__data is already non-empty. Raises OperationFailure when the\n        cursor cannot be refreshed due to an error on the query.\n        \"\"\"\n        if len(self.__data) or self.__killed:\n            return len(self.__data)\n\n        if self.__id:  # Get More\n            dbname, collname = self.__ns.split('.', 1)\n            read_pref = self.__collection._read_preference_for(self.session)\n            self.__send_message(\n                self._getmore_class(dbname,\n                                    collname,\n                                    self.__batch_size,\n                                    self.__id,\n                                    self.__collection.codec_options,\n                                    read_pref,\n                                    self.__session,\n                                    self.__collection.database.client,\n                                    self.__max_await_time_ms,\n                                    False))\n        else:  # Cursor id is zero nothing else to return\n            self.__killed = True\n            self.__end_session(True)\n\n        return len(self.__data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadvancing the cursor blocking for at most one getMore command.", "response": "def _try_next(self, get_more_allowed):\n        \"\"\"Advance the cursor blocking for at most one getMore command.\"\"\"\n        if not len(self.__data) and not self.__killed and get_more_allowed:\n            self._refresh()\n        if len(self.__data):\n            coll = self.__collection\n            return coll.database._fix_outgoing(self.__data.popleft(), coll)\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _random(cls):\n        pid = os.getpid()\n        if pid != cls._pid:\n            cls._pid = pid\n            cls.__random = _random_bytes()\n        return cls.__random", "response": "Generate a 5 - byte random number once per process."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __generate(self):\n\n        # 4 bytes current time\n        oid = struct.pack(\">I\", int(time.time()))\n\n        # 5 bytes random\n        oid += ObjectId._random()\n\n        # 3 bytes inc\n        with ObjectId._inc_lock:\n            oid += struct.pack(\">I\", ObjectId._inc)[1:4]\n            ObjectId._inc = (ObjectId._inc + 1) % (_MAX_COUNTER_VALUE + 1)\n\n        self.__id = oid", "response": "Generate a new value for this ObjectId."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generation_time(self):\n        timestamp = struct.unpack(\">I\", self.__id[0:4])[0]\n        return datetime.datetime.fromtimestamp(timestamp, utc)", "response": "A datetime. datetime instance representing the time of the generation of this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a socket configured to be used by the given address and PoolOptions.", "response": "def _configured_socket(address, options):\n    \"\"\"Given (host, port) and PoolOptions, return a configured socket.\n\n    Can raise socket.error, ConnectionFailure, or CertificateError.\n\n    Sets socket's SSL and timeout options.\n    \"\"\"\n    sock = _create_connection(address, options)\n    ssl_context = options.ssl_context\n\n    if ssl_context is not None:\n        host = address[0]\n        try:\n            # According to RFC6066, section 3, IPv4 and IPv6 literals are\n            # not permitted for SNI hostname.\n            # Previous to Python 3.7 wrap_socket would blindly pass\n            # IP addresses as SNI hostname.\n            # https://bugs.python.org/issue32185\n            # We have to pass hostname / ip address to wrap_socket\n            # to use SSLContext.check_hostname.\n            if _HAVE_SNI and (not is_ip_address(host) or _PY37PLUS):\n                sock = ssl_context.wrap_socket(sock, server_hostname=host)\n            else:\n                sock = ssl_context.wrap_socket(sock)\n        except _SSLCertificateError:\n            sock.close()\n            # Raise CertificateError directly like we do after match_hostname\n            # below.\n            raise\n        except IOError as exc:\n            sock.close()\n            # We raise AutoReconnect for transient and permanent SSL handshake\n            # failures alike. Permanent handshake failures, like protocol\n            # mismatch, will be turned into ServerSelectionTimeoutErrors later.\n            _raise_connection_failure(address, exc, \"SSL handshake failed: \")\n        if (ssl_context.verify_mode and not\n                getattr(ssl_context, \"check_hostname\", False) and\n                options.ssl_match_hostname):\n            try:\n                match_hostname(sock.getpeercert(), hostname=host)\n            except CertificateError:\n                sock.close()\n                raise\n\n    sock.settimeout(options.socket_timeout)\n    return sock"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef command(self, dbname, spec, slave_ok=False,\n                read_preference=ReadPreference.PRIMARY,\n                codec_options=DEFAULT_CODEC_OPTIONS, check=True,\n                allowable_errors=None, check_keys=False,\n                read_concern=None,\n                write_concern=None,\n                parse_write_concern_error=False,\n                collation=None,\n                session=None,\n                client=None,\n                retryable_write=False,\n                publish_events=True,\n                user_fields=None):\n        \"\"\"Execute a command or raise an error.\n\n        :Parameters:\n          - `dbname`: name of the database on which to run the command\n          - `spec`: a command document as a dict, SON, or mapping object\n          - `slave_ok`: whether to set the SlaveOkay wire protocol bit\n          - `read_preference`: a read preference\n          - `codec_options`: a CodecOptions instance\n          - `check`: raise OperationFailure if there are errors\n          - `allowable_errors`: errors to ignore if `check` is True\n          - `check_keys`: if True, check `spec` for invalid keys\n          - `read_concern`: The read concern for this command.\n          - `write_concern`: The write concern for this command.\n          - `parse_write_concern_error`: Whether to parse the\n            ``writeConcernError`` field in the command response.\n          - `collation`: The collation for this command.\n          - `session`: optional ClientSession instance.\n          - `client`: optional MongoClient for gossipping $clusterTime.\n          - `retryable_write`: True if this command is a retryable write.\n          - `publish_events`: Should we publish events for this command?\n          - `user_fields` (optional): Response fields that should be decoded\n            using the TypeDecoders from codec_options, passed to\n            bson._decode_all_selective.\n        \"\"\"\n        self.validate_session(client, session)\n        session = _validate_session_write_concern(session, write_concern)\n\n        # Ensure command name remains in first place.\n        if not isinstance(spec, ORDERED_TYPES):\n            spec = SON(spec)\n\n        if (read_concern and self.max_wire_version < 4\n                and not read_concern.ok_for_legacy):\n            raise ConfigurationError(\n                'read concern level of %s is not valid '\n                'with a max wire version of %d.'\n                % (read_concern.level, self.max_wire_version))\n        if not (write_concern is None or write_concern.acknowledged or\n                collation is None):\n            raise ConfigurationError(\n                'Collation is unsupported for unacknowledged writes.')\n        if (self.max_wire_version >= 5 and\n                write_concern and\n                not write_concern.is_server_default):\n            spec['writeConcern'] = write_concern.document\n        elif self.max_wire_version < 5 and collation is not None:\n            raise ConfigurationError(\n                'Must be connected to MongoDB 3.4+ to use a collation.')\n\n        if session:\n            session._apply_to(spec, retryable_write, read_preference)\n        self.send_cluster_time(spec, session, client)\n        listeners = self.listeners if publish_events else None\n        unacknowledged = write_concern and not write_concern.acknowledged\n        if self.op_msg_enabled:\n            self._raise_if_not_writable(unacknowledged)\n        try:\n            return command(self.sock, dbname, spec, slave_ok,\n                           self.is_mongos, read_preference, codec_options,\n                           session, client, check, allowable_errors,\n                           self.address, check_keys, listeners,\n                           self.max_bson_size, read_concern,\n                           parse_write_concern_error=parse_write_concern_error,\n                           collation=collation,\n                           compression_ctx=self.compression_context,\n                           use_op_msg=self.op_msg_enabled,\n                           unacknowledged=unacknowledged,\n                           user_fields=user_fields)\n        except OperationFailure:\n            raise\n        # Catch socket.error, KeyboardInterrupt, etc. and close ourselves.\n        except BaseException as error:\n            self._raise_connection_failure(error)", "response": "Execute a command on the specified database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a message to the server and return a dict.", "response": "def legacy_write(self, request_id, msg, max_doc_size, with_last_error):\n        \"\"\"Send OP_INSERT, etc., optionally returning response as a dict.\n\n        Can raise ConnectionFailure or OperationFailure.\n\n        :Parameters:\n          - `request_id`: an int.\n          - `msg`: bytes, an OP_INSERT, OP_UPDATE, or OP_DELETE message,\n            perhaps with a getlasterror command appended.\n          - `max_doc_size`: size in bytes of the largest document in `msg`.\n          - `with_last_error`: True if a getlasterror command is appended.\n        \"\"\"\n        self._raise_if_not_writable(not with_last_error)\n\n        self.send_message(msg, max_doc_size)\n        if with_last_error:\n            reply = self.receive_message(request_id)\n            return helpers._check_gle_response(reply.command_response())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_command(self, request_id, msg):\n        self.send_message(msg, 0)\n        reply = self.receive_message(request_id)\n        result = reply.command_response()\n\n        # Raises NotMasterError or OperationFailure.\n        helpers._check_command_response(result)\n        return result", "response": "Send an insert etc. command to the server and return the response as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_session(self, client, session):\n        if session:\n            if session._client is not client:\n                raise InvalidOperation(\n                    'Can only use session with the MongoClient that'\n                    ' started it')\n            if session._authset != self.authset:\n                raise InvalidOperation(\n                    'Cannot use session after authenticating with different'\n                    ' credentials')", "response": "Validate this session before use with client."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd cluster time for MongoDB > = 3. 6.", "response": "def send_cluster_time(self, command, session, client):\n        \"\"\"Add cluster time for MongoDB >= 3.6.\"\"\"\n        if self.max_wire_version >= 6 and client:\n            client._send_cluster_time(command, session)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_stale_sockets(self):\n        if self.opts.max_idle_time_seconds is not None:\n            with self.lock:\n                while (self.sockets and\n                       self.sockets[-1].idle_time_seconds() > self.opts.max_idle_time_seconds):\n                    sock_info = self.sockets.pop()\n                    sock_info.close()\n        while True:\n            with self.lock:\n                if (len(self.sockets) + self.active_sockets >=\n                        self.opts.min_pool_size):\n                    # There are enough sockets in the pool.\n                    break\n\n            # We must acquire the semaphore to respect max_pool_size.\n            if not self._socket_semaphore.acquire(False):\n                break\n            try:\n                sock_info = self.connect()\n                with self.lock:\n                    self.sockets.appendleft(sock_info)\n            finally:\n                self._socket_semaphore.release()", "response": "Removes stale sockets then adds new ones if pool is too small."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconnects to the Mongo server and return a new SocketInfo object.", "response": "def connect(self):\n        \"\"\"Connect to Mongo and return a new SocketInfo.\n\n        Can raise ConnectionFailure or CertificateError.\n\n        Note that the pool does not keep a reference to the socket -- you\n        must call return_socket() when you're done with it.\n        \"\"\"\n        sock = None\n        try:\n            sock = _configured_socket(self.address, self.opts)\n        except socket.error as error:\n            if sock is not None:\n                sock.close()\n            _raise_connection_failure(self.address, error)\n\n        sock_info = SocketInfo(sock, self, self.address)\n        if self.handshake:\n            sock_info.ismaster(self.opts.metadata, None)\n        return sock_info"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_socket_no_auth(self):\n        # We use the pid here to avoid issues with fork / multiprocessing.\n        # See test.test_client:TestClient.test_fork for an example of\n        # what could go wrong otherwise\n        if self.pid != os.getpid():\n            self.reset()\n\n        # Get a free socket or create one.\n        if not self._socket_semaphore.acquire(\n                True, self.opts.wait_queue_timeout):\n            self._raise_wait_queue_timeout()\n        with self.lock:\n            self.active_sockets += 1\n\n        # We've now acquired the semaphore and must release it on error.\n        try:\n            try:\n                # set.pop() isn't atomic in Jython less than 2.7, see\n                # http://bugs.jython.org/issue1854\n                with self.lock:\n                    # Can raise ConnectionFailure.\n                    sock_info = self.sockets.popleft()\n            except IndexError:\n                # Can raise ConnectionFailure or CertificateError.\n                sock_info = self.connect()\n            else:\n                # Can raise ConnectionFailure.\n                sock_info = self._check(sock_info)\n        except Exception:\n            self._socket_semaphore.release()\n            with self.lock:\n                self.active_sockets -= 1\n            raise\n\n        return sock_info", "response": "Get or create a SocketInfo. Can raise ConnectionFailure or CertificateError."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef return_socket(self, sock_info):\n        if self.pid != os.getpid():\n            self.reset()\n        else:\n            if sock_info.pool_id != self.pool_id:\n                sock_info.close()\n            elif not sock_info.closed:\n                sock_info.update_last_checkin_time()\n                with self.lock:\n                    self.sockets.appendleft(sock_info)\n\n        self._socket_semaphore.release()\n        with self.lock:\n            self.active_sockets -= 1", "response": "Return the socket to the pool or discard it."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting a command over a socket.", "response": "def command(sock, dbname, spec, slave_ok, is_mongos,\n            read_preference, codec_options, session, client, check=True,\n            allowable_errors=None, address=None,\n            check_keys=False, listeners=None, max_bson_size=None,\n            read_concern=None,\n            parse_write_concern_error=False,\n            collation=None,\n            compression_ctx=None,\n            use_op_msg=False,\n            unacknowledged=False,\n            user_fields=None):\n    \"\"\"Execute a command over the socket, or raise socket.error.\n\n    :Parameters:\n      - `sock`: a raw socket instance\n      - `dbname`: name of the database on which to run the command\n      - `spec`: a command document as an ordered dict type, eg SON.\n      - `slave_ok`: whether to set the SlaveOkay wire protocol bit\n      - `is_mongos`: are we connected to a mongos?\n      - `read_preference`: a read preference\n      - `codec_options`: a CodecOptions instance\n      - `session`: optional ClientSession instance.\n      - `client`: optional MongoClient instance for updating $clusterTime.\n      - `check`: raise OperationFailure if there are errors\n      - `allowable_errors`: errors to ignore if `check` is True\n      - `address`: the (host, port) of `sock`\n      - `check_keys`: if True, check `spec` for invalid keys\n      - `listeners`: An instance of :class:`~pymongo.monitoring.EventListeners`\n      - `max_bson_size`: The maximum encoded bson size for this server\n      - `read_concern`: The read concern for this command.\n      - `parse_write_concern_error`: Whether to parse the ``writeConcernError``\n        field in the command response.\n      - `collation`: The collation for this command.\n      - `compression_ctx`: optional compression Context.\n      - `use_op_msg`: True if we should use OP_MSG.\n      - `unacknowledged`: True if this is an unacknowledged command.\n      - `user_fields` (optional): Response fields that should be decoded\n        using the TypeDecoders from codec_options, passed to\n        bson._decode_all_selective.\n    \"\"\"\n    name = next(iter(spec))\n    ns = dbname + '.$cmd'\n    flags = 4 if slave_ok else 0\n\n    # Publish the original command document, perhaps with lsid and $clusterTime.\n    orig = spec\n    if is_mongos and not use_op_msg:\n        spec = message._maybe_add_read_preference(spec, read_preference)\n    if read_concern and not (session and session._in_transaction):\n        if read_concern.level:\n            spec['readConcern'] = read_concern.document\n        if (session and session.options.causal_consistency\n                and session.operation_time is not None):\n            spec.setdefault(\n                'readConcern', {})['afterClusterTime'] = session.operation_time\n    if collation is not None:\n        spec['collation'] = collation\n\n    publish = listeners is not None and listeners.enabled_for_commands\n    if publish:\n        start = datetime.datetime.now()\n\n    if compression_ctx and name.lower() in _NO_COMPRESSION:\n        compression_ctx = None\n\n    if use_op_msg:\n        flags = 2 if unacknowledged else 0\n        request_id, msg, size, max_doc_size = message._op_msg(\n            flags, spec, dbname, read_preference, slave_ok, check_keys,\n            codec_options, ctx=compression_ctx)\n        # If this is an unacknowledged write then make sure the encoded doc(s)\n        # are small enough, otherwise rely on the server to return an error.\n        if (unacknowledged and max_bson_size is not None and\n                max_doc_size > max_bson_size):\n            message._raise_document_too_large(name, size, max_bson_size)\n    else:\n        request_id, msg, size = message.query(\n            flags, ns, 0, -1, spec, None, codec_options, check_keys,\n            compression_ctx)\n\n    if (max_bson_size is not None\n            and size > max_bson_size + message._COMMAND_OVERHEAD):\n        message._raise_document_too_large(\n            name, size, max_bson_size + message._COMMAND_OVERHEAD)\n\n    if publish:\n        encoding_duration = datetime.datetime.now() - start\n        listeners.publish_command_start(orig, dbname, request_id, address)\n        start = datetime.datetime.now()\n\n    try:\n        sock.sendall(msg)\n        if use_op_msg and unacknowledged:\n            # Unacknowledged, fake a successful command response.\n            response_doc = {\"ok\": 1}\n        else:\n            reply = receive_message(sock, request_id)\n            unpacked_docs = reply.unpack_response(\n                codec_options=codec_options, user_fields=user_fields)\n\n            response_doc = unpacked_docs[0]\n            if client:\n                client._process_response(response_doc, session)\n            if check:\n                helpers._check_command_response(\n                    response_doc, None, allowable_errors,\n                    parse_write_concern_error=parse_write_concern_error)\n    except Exception as exc:\n        if publish:\n            duration = (datetime.datetime.now() - start) + encoding_duration\n            if isinstance(exc, (NotMasterError, OperationFailure)):\n                failure = exc.details\n            else:\n                failure = message._convert_exception(exc)\n            listeners.publish_command_failure(\n                duration, failure, name, request_id, address)\n        raise\n    if publish:\n        duration = (datetime.datetime.now() - start) + encoding_duration\n        listeners.publish_command_success(\n            duration, response_doc, name, request_id, address)\n    return response_doc"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreceive a BSON message or raise socket. error.", "response": "def receive_message(sock, request_id, max_message_size=MAX_MESSAGE_SIZE):\n    \"\"\"Receive a raw BSON message or raise socket.error.\"\"\"\n    # Ignore the response's request id.\n    length, _, response_to, op_code = _UNPACK_HEADER(\n        _receive_data_on_socket(sock, 16))\n    # No request_id for exhaust cursor \"getMore\".\n    if request_id is not None:\n        if request_id != response_to:\n            raise ProtocolError(\"Got response id %r but expected \"\n                                \"%r\" % (response_to, request_id))\n    if length <= 16:\n        raise ProtocolError(\"Message length (%r) not longer than standard \"\n                            \"message header size (16)\" % (length,))\n    if length > max_message_size:\n        raise ProtocolError(\"Message length (%r) is larger than server max \"\n                            \"message size (%r)\" % (length, max_message_size))\n    if op_code == 2012:\n        op_code, _, compressor_id = _UNPACK_COMPRESSION_HEADER(\n            _receive_data_on_socket(sock, 9))\n        data = decompress(\n            _receive_data_on_socket(sock, length - 25), compressor_id)\n    else:\n        data = _receive_data_on_socket(sock, length - 16)\n\n    try:\n        unpack_reply = _UNPACK_REPLY[op_code]\n    except KeyError:\n        raise ProtocolError(\"Got opcode %r but expected \"\n                            \"%r\" % (op_code, _UNPACK_REPLY.keys()))\n    return unpack_reply(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if the socket has been closed False otherwise.", "response": "def socket_closed(self, sock):\n        \"\"\"Return True if we know socket has been closed, False otherwise.\n        \"\"\"\n        while True:\n            try:\n                if self._poller:\n                    with self._lock:\n                        self._poller.register(sock, _EVENT_MASK)\n                        try:\n                            rd = self._poller.poll(0)\n                        finally:\n                            self._poller.unregister(sock)\n                else:\n                    rd, _, _ = select.select([sock], [], [], 0)\n            except (RuntimeError, KeyError):\n                # RuntimeError is raised during a concurrent poll. KeyError\n                # is raised by unregister if the socket is not in the poller.\n                # These errors should not be possible since we protect the\n                # poller with a mutex.\n                raise\n            except ValueError:\n                # ValueError is raised by register/unregister/select if the\n                # socket file descriptor is negative or outside the range for\n                # select (> 1023).\n                return True\n            except (_SELECT_ERROR, IOError) as exc:\n                if _errno_from_exception(exc) in (errno.EINTR, errno.EAGAIN):\n                    continue\n                return True\n            except Exception:\n                # Any other exceptions should be attributed to a closed\n                # or invalid socket.\n                return True\n            return len(rd) > 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_object_size(data, position, obj_end):\n    try:\n        obj_size = _UNPACK_INT(data[position:position + 4])[0]\n    except struct.error as exc:\n        raise InvalidBSON(str(exc))\n    end = position + obj_size - 1\n    if data[end:end + 1] != b\"\\x00\":\n        raise InvalidBSON(\"bad eoo\")\n    if end >= obj_end:\n        raise InvalidBSON(\"invalid object length\")\n    # If this is the top-level document, validate the total size too.\n    if position == 0 and obj_size != obj_end:\n        raise InvalidBSON(\"invalid object length\")\n    return obj_size, end", "response": "Validate and return a BSON document s size."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecode a BSON subdocument to opts. document_class or bson. dbref. DBRef.", "response": "def _get_object(data, position, obj_end, opts, dummy):\n    \"\"\"Decode a BSON subdocument to opts.document_class or bson.dbref.DBRef.\"\"\"\n    obj_size, end = _get_object_size(data, position, obj_end)\n    if _raw_document_class(opts.document_class):\n        return (opts.document_class(data[position:end + 1], opts),\n                position + obj_size)\n\n    obj = _elements_to_dict(data, position + 4, end, opts)\n\n    position += obj_size\n    if \"$ref\" in obj:\n        return (DBRef(obj.pop(\"$ref\"), obj.pop(\"$id\", None),\n                      obj.pop(\"$db\", None), obj), position)\n    return obj, position"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecoding a BSON array to python list.", "response": "def _get_array(data, position, obj_end, opts, element_name):\n    \"\"\"Decode a BSON array to python list.\"\"\"\n    size = _UNPACK_INT(data[position:position + 4])[0]\n    end = position + size - 1\n    if data[end:end + 1] != b\"\\x00\":\n        raise InvalidBSON(\"bad eoo\")\n\n    position += 4\n    end -= 1\n    result = []\n\n    # Avoid doing global and attribute lookups in the loop.\n    append = result.append\n    index = data.index\n    getter = _ELEMENT_GETTER\n    decoder_map = opts.type_registry._decoder_map\n\n    while position < end:\n        element_type = data[position:position + 1]\n        # Just skip the keys.\n        position = index(b'\\x00', position) + 1\n        try:\n            value, position = getter[element_type](\n                data, position, obj_end, opts, element_name)\n        except KeyError:\n            _raise_unknown_type(element_type, element_name)\n\n        if decoder_map:\n            custom_decoder = decoder_map.get(type(value))\n            if custom_decoder is not None:\n                value = custom_decoder(value)\n\n        append(value)\n\n    if position != end + 1:\n        raise InvalidBSON('bad array length')\n    return result, position + 1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _element_to_dict(data, position, obj_end, opts):\n    element_type = data[position:position + 1]\n    position += 1\n    element_name, position = _get_c_string(data, position, opts)\n    try:\n        value, position = _ELEMENT_GETTER[element_type](data, position,\n                                                        obj_end, opts,\n                                                        element_name)\n    except KeyError:\n        _raise_unknown_type(element_type, element_name)\n\n    if opts.type_registry._decoder_map:\n        custom_decoder = opts.type_registry._decoder_map.get(type(value))\n        if custom_decoder is not None:\n            value = custom_decoder(value)\n\n    return element_name, value, position", "response": "Decode a single key value pair."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _elements_to_dict(data, position, obj_end, opts, result=None):\n    if result is None:\n        result = opts.document_class()\n    end = obj_end - 1\n    while position < end:\n        key, value, position = _element_to_dict(data, position, obj_end, opts)\n        result[key] = value\n    if position != obj_end:\n        raise InvalidBSON('bad object or element length')\n    return result", "response": "Decode a BSON document into result."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _bson_to_dict(data, opts):\n    try:\n        if _raw_document_class(opts.document_class):\n            return opts.document_class(data, opts)\n        _, end = _get_object_size(data, 0, len(data))\n        return _elements_to_dict(data, 4, end, opts)\n    except InvalidBSON:\n        raise\n    except Exception:\n        # Change exception type to InvalidBSON but preserve traceback.\n        _, exc_value, exc_tb = sys.exc_info()\n        reraise(InvalidBSON, exc_value, exc_tb)", "response": "Decode a BSON string to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _name_value_to_bson(name, value, check_keys, opts,\n                        in_custom_call=False,\n                        in_fallback_call=False):\n    \"\"\"Encode a single name, value pair.\"\"\"\n    # First see if the type is already cached. KeyError will only ever\n    # happen once per subtype.\n    try:\n        return _ENCODERS[type(value)](name, value, check_keys, opts)\n    except KeyError:\n        pass\n\n    # Second, fall back to trying _type_marker. This has to be done\n    # before the loop below since users could subclass one of our\n    # custom types that subclasses a python built-in (e.g. Binary)\n    marker = getattr(value, \"_type_marker\", None)\n    if isinstance(marker, int) and marker in _MARKERS:\n        func = _MARKERS[marker]\n        # Cache this type for faster subsequent lookup.\n        _ENCODERS[type(value)] = func\n        return func(name, value, check_keys, opts)\n\n    # Third, check if a type encoder is registered for this type.\n    # Note that subtypes of registered custom types are not auto-encoded.\n    if not in_custom_call and opts.type_registry._encoder_map:\n        custom_encoder = opts.type_registry._encoder_map.get(type(value))\n        if custom_encoder is not None:\n            return _name_value_to_bson(\n                name, custom_encoder(value), check_keys, opts,\n                in_custom_call=True)\n\n    # Fourth, test each base type. This will only happen once for\n    # a subtype of a supported base type. Unlike in the C-extensions, this\n    # is done after trying the custom type encoder because checking for each\n    # subtype is expensive.\n    for base in _BUILT_IN_TYPES:\n        if isinstance(value, base):\n            func = _ENCODERS[base]\n            # Cache this type for faster subsequent lookup.\n            _ENCODERS[type(value)] = func\n            return func(name, value, check_keys, opts)\n\n    # As a last resort, try using the fallback encoder, if the user has\n    # provided one.\n    fallback_encoder = opts.type_registry._fallback_encoder\n    if not in_fallback_call and fallback_encoder is not None:\n        return _name_value_to_bson(\n            name, fallback_encoder(value), check_keys, opts,\n            in_fallback_call=True)\n\n    raise InvalidDocument(\n        \"cannot encode object: %r, of type: %r\" % (value, type(value)))", "response": "Encode a single name value pair."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _datetime_to_millis(dtm):\n    if dtm.utcoffset() is not None:\n        dtm = dtm - dtm.utcoffset()\n    return int(calendar.timegm(dtm.timetuple()) * 1000 +\n               dtm.microsecond // 1000)", "response": "Convert datetime to milliseconds since epoch UTC."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _decode_all_selective(data, codec_options, fields):\n    if not codec_options.type_registry._decoder_map:\n        return decode_all(data, codec_options)\n\n    if not fields:\n        return decode_all(data, codec_options.with_options(type_registry=None))\n\n    # Decode documents for internal use.\n    from bson.raw_bson import RawBSONDocument\n    internal_codec_options = codec_options.with_options(\n        document_class=RawBSONDocument, type_registry=None)\n    _doc = _bson_to_dict(data, internal_codec_options)\n    return [_decode_selective(_doc, fields, codec_options,)]", "response": "Decode a single document while using user - provided\n    custom decoding logic."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the number of documents in the current object.", "response": "def count(self, with_limit_and_skip=False):\n        \"\"\"**DEPRECATED** - Get the size of the results set for this query.\n\n        The :meth:`count` method is deprecated and **not** supported in a\n        transaction. Please use\n        :meth:`~pymongo.collection.Collection.count_documents` instead.\n\n        Returns the number of documents in the results set for this query. Does\n        not take :meth:`limit` and :meth:`skip` into account by default - set\n        `with_limit_and_skip` to ``True`` if that is the desired behavior.\n        Raises :class:`~pymongo.errors.OperationFailure` on a database error.\n\n        When used with MongoDB >= 2.6, :meth:`~count` uses any :meth:`~hint`\n        applied to the query. In the following example the hint is passed to\n        the count command:\n\n          collection.find({'field': 'value'}).hint('field_1').count()\n\n        The :meth:`count` method obeys the\n        :attr:`~pymongo.collection.Collection.read_preference` of the\n        :class:`~pymongo.collection.Collection` instance on which\n        :meth:`~pymongo.collection.Collection.find` was called.\n\n        :Parameters:\n          - `with_limit_and_skip` (optional): take any :meth:`limit` or\n            :meth:`skip` that has been applied to this cursor into account when\n            getting the count\n\n        .. note:: The `with_limit_and_skip` parameter requires server\n           version **>= 1.1.4-**\n\n        .. versionchanged:: 3.7\n           Deprecated.\n\n        .. versionchanged:: 2.8\n           The :meth:`~count` method now supports :meth:`~hint`.\n        \"\"\"\n        warnings.warn(\"count is deprecated. Use Collection.count_documents \"\n                      \"instead.\", DeprecationWarning, stacklevel=2)\n        validate_boolean(\"with_limit_and_skip\", with_limit_and_skip)\n        cmd = SON([(\"count\", self.__collection.name),\n                   (\"query\", self.__spec)])\n        if self.__max_time_ms is not None:\n            cmd[\"maxTimeMS\"] = self.__max_time_ms\n        if self.__comment:\n            cmd[\"comment\"] = self.__comment\n\n        if self.__hint is not None:\n            cmd[\"hint\"] = self.__hint\n\n        if with_limit_and_skip:\n            if self.__limit:\n                cmd[\"limit\"] = self.__limit\n            if self.__skip:\n                cmd[\"skip\"] = self.__skip\n\n        return self.__collection._count(\n            cmd, self.__collation, session=self.__session)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __send_message(self, operation):\n        client = self.__collection.database.client\n\n        try:\n            response = client._run_operation_with_response(\n                operation, self._unpack_response, exhaust=self.__exhaust,\n                address=self.__address)\n        except OperationFailure:\n            self.__killed = True\n\n            # Make sure exhaust socket is returned immediately, if necessary.\n            self.__die()\n\n            # If this is a tailable cursor the error is likely\n            # due to capped collection roll over. Setting\n            # self.__killed to True ensures Cursor.alive will be\n            # False. No need to re-raise.\n            if self.__query_flags & _QUERY_OPTIONS[\"tailable_cursor\"]:\n                return\n            raise\n        except NotMasterError:\n            # Don't send kill cursors to another server after a \"not master\"\n            # error. It's completely pointless.\n            self.__killed = True\n\n            # Make sure exhaust socket is returned immediately, if necessary.\n            self.__die()\n\n            raise\n        except ConnectionFailure:\n            # Don't try to send kill cursors on another socket\n            # or to another server. It can cause a _pinValue\n            # assertion on some server releases if we get here\n            # due to a socket timeout.\n            self.__killed = True\n            self.__die()\n            raise\n        except Exception:\n            # Close the cursor\n            self.__die()\n            raise\n\n        self.__address = response.address\n        if self.__exhaust and not self.__exhaust_mgr:\n            # 'response' is an ExhaustResponse.\n            self.__exhaust_mgr = _SocketManager(response.socket_info,\n                                                response.pool)\n\n        cmd_name = operation.name\n        docs = response.docs\n        if response.from_command:\n            if cmd_name != \"explain\":\n                cursor = docs[0]['cursor']\n                self.__id = cursor['id']\n                if cmd_name == 'find':\n                    documents = cursor['firstBatch']\n                else:\n                    documents = cursor['nextBatch']\n                self.__data = deque(documents)\n                self.__retrieved += len(documents)\n            else:\n                self.__id = 0\n                self.__data = deque(docs)\n                self.__retrieved += len(docs)\n        else:\n            self.__id = response.data.cursor_id\n            self.__data = deque(docs)\n            self.__retrieved += response.data.number_returned\n\n        if self.__id == 0:\n            self.__killed = True\n            # Don't wait for garbage collection to call __del__, return the\n            # socket and the session to the pool now.\n            self.__die()\n\n        if self.__limit and self.__id and self.__limit <= self.__retrieved:\n            self.__die()", "response": "Send a query or getmore operation and handles the response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _refresh(self):\n        if len(self.__data) or self.__killed:\n            return len(self.__data)\n\n        if not self.__session:\n            self.__session = self.__collection.database.client._ensure_session()\n\n        if self.__id is None:  # Query\n            if (self.__min or self.__max) and not self.__hint:\n                warnings.warn(\"using a min/max query operator without \"\n                              \"specifying a Cursor.hint is deprecated. A \"\n                              \"hint will be required when using min/max in \"\n                              \"PyMongo 4.0\",\n                              DeprecationWarning, stacklevel=3)\n            q = self._query_class(self.__query_flags,\n                                  self.__collection.database.name,\n                                  self.__collection.name,\n                                  self.__skip,\n                                  self.__query_spec(),\n                                  self.__projection,\n                                  self.__codec_options,\n                                  self._read_preference(),\n                                  self.__limit,\n                                  self.__batch_size,\n                                  self.__read_concern,\n                                  self.__collation,\n                                  self.__session,\n                                  self.__collection.database.client)\n            self.__send_message(q)\n        elif self.__id:  # Get More\n            if self.__limit:\n                limit = self.__limit - self.__retrieved\n                if self.__batch_size:\n                    limit = min(limit, self.__batch_size)\n            else:\n                limit = self.__batch_size\n\n            # Exhaust cursors don't send getMore messages.\n            g = self._getmore_class(self.__collection.database.name,\n                                    self.__collection.name,\n                                    limit,\n                                    self.__id,\n                                    self.__codec_options,\n                                    self._read_preference(),\n                                    self.__session,\n                                    self.__collection.database.client,\n                                    self.__max_await_time_ms,\n                                    self.__exhaust_mgr)\n            self.__send_message(g)\n\n        return len(self.__data)", "response": "Refreshes the cursor with more data from Mongo."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an explain plan record for this cursor.", "response": "def explain(self):\n        \"\"\"Returns an explain plan record for this cursor.\n\n        .. mongodoc:: explain\n        \"\"\"\n        clone = self._clone(deepcopy=True, base=Cursor(self.collection))\n        return clone.explain()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_session_write_concern(session, write_concern):\n    if session:\n        if write_concern is not None and not write_concern.acknowledged:\n            # For unacknowledged writes without an explicit session,\n            # drivers SHOULD NOT use an implicit session. If a driver\n            # creates an implicit session for unacknowledged writes\n            # without an explicit session, the driver MUST NOT send the\n            # session ID.\n            if session._implicit:\n                return None\n            else:\n                raise ConfigurationError(\n                    'Explicit sessions are incompatible with '\n                    'unacknowledged write concern: %r' % (\n                        write_concern,))\n    return session", "response": "Validate that an explicit session is not used with an unacknowledged write."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the inherited TransactionOption value.", "response": "def _inherit_option(self, name, val):\n        \"\"\"Return the inherited TransactionOption value.\"\"\"\n        if val:\n            return val\n        txn_opts = self.options.default_transaction_options\n        val = txn_opts and getattr(txn_opts, name)\n        if val:\n            return val\n        return getattr(self.client, name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes a callback in a transaction.", "response": "def with_transaction(self, callback, read_concern=None, write_concern=None,\n                         read_preference=None):\n        \"\"\"Execute a callback in a transaction.\n\n        This method starts a transaction on this session, executes ``callback``\n        once, and then commits the transaction. For example::\n\n          def callback(session):\n              orders = session.client.db.orders\n              inventory = session.client.db.inventory\n              orders.insert_one({\"sku\": \"abc123\", \"qty\": 100}, session=session)\n              inventory.update_one({\"sku\": \"abc123\", \"qty\": {\"$gte\": 100}},\n                                   {\"$inc\": {\"qty\": -100}}, session=session)\n\n          with client.start_session() as session:\n              session.with_transaction(callback)\n\n        To pass arbitrary arguments to the ``callback``, wrap your callable\n        with a ``lambda`` like this::\n\n          def callback(session, custom_arg, custom_kwarg=None):\n              # Transaction operations...\n\n          with client.start_session() as session:\n              session.with_transaction(\n                  lambda s: callback(s, \"custom_arg\", custom_kwarg=1))\n\n        In the event of an exception, ``with_transaction`` may retry the commit\n        or the entire transaction, therefore ``callback`` may be invoked\n        multiple times by a single call to ``with_transaction``. Developers\n        should be mindful of this possiblity when writing a ``callback`` that\n        modifies application state or has any other side-effects.\n        Note that even when the ``callback`` is invoked multiple times,\n        ``with_transaction`` ensures that the transaction will be committed\n        at-most-once on the server.\n\n        The ``callback`` should not attempt to start new transactions, but\n        should simply run operations meant to be contained within a\n        transaction. The ``callback`` should also not commit the transaction;\n        this is handled automatically by ``with_transaction``. If the\n        ``callback`` does commit or abort the transaction without error,\n        however, ``with_transaction`` will return without taking further\n        action.\n\n        When ``callback`` raises an exception, ``with_transaction``\n        automatically aborts the current transaction. When ``callback`` or\n        :meth:`~ClientSession.commit_transaction` raises an exception that\n        includes the ``\"TransientTransactionError\"`` error label,\n        ``with_transaction`` starts a new transaction and re-executes\n        the ``callback``.\n\n        When :meth:`~ClientSession.commit_transaction` raises an exception with\n        the ``\"UnknownTransactionCommitResult\"`` error label,\n        ``with_transaction`` retries the commit until the result of the\n        transaction is known.\n\n        This method will cease retrying after 120 seconds has elapsed. This\n        timeout is not configurable and any exception raised by the\n        ``callback`` or by :meth:`ClientSession.commit_transaction` after the\n        timeout is reached will be re-raised. Applications that desire a\n        different timeout duration should not use this method.\n\n        :Parameters:\n          - `callback`: The callable ``callback`` to run inside a transaction.\n            The callable must accept a single argument, this session. Note,\n            under certain error conditions the callback may be run multiple\n            times.\n          - `read_concern` (optional): The\n            :class:`~pymongo.read_concern.ReadConcern` to use for this\n            transaction.\n          - `write_concern` (optional): The\n            :class:`~pymongo.write_concern.WriteConcern` to use for this\n            transaction.\n          - `read_preference` (optional): The read preference to use for this\n            transaction. If ``None`` (the default) the :attr:`read_preference`\n            of this :class:`Database` is used. See\n            :mod:`~pymongo.read_preferences` for options.\n\n        :Returns:\n          The return value of the ``callback``.\n\n        .. versionadded:: 3.9\n        \"\"\"\n        start_time = monotonic.time()\n        while True:\n            self.start_transaction(\n                read_concern, write_concern, read_preference)\n            try:\n                ret = callback(self)\n            except Exception as exc:\n                if self._in_transaction:\n                    self.abort_transaction()\n                if (isinstance(exc, PyMongoError) and\n                        exc.has_error_label(\"TransientTransactionError\") and\n                        _within_time_limit(start_time)):\n                    # Retry the entire transaction.\n                    continue\n                raise\n\n            if self._transaction.state in (\n                    _TxnState.NONE, _TxnState.COMMITTED, _TxnState.ABORTED):\n                # Assume callback intentionally ended the transaction.\n                return ret\n\n            while True:\n                try:\n                    self.commit_transaction()\n                except PyMongoError as exc:\n                    if (exc.has_error_label(\"UnknownTransactionCommitResult\")\n                            and _within_time_limit(start_time)):\n                        # Retry the commit.\n                        continue\n\n                    if (exc.has_error_label(\"TransientTransactionError\") and\n                            _within_time_limit(start_time)):\n                        # Retry the entire transaction.\n                        break\n                    raise\n\n                # Commit succeeded.\n                return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting a multi - statement transaction.", "response": "def start_transaction(self, read_concern=None, write_concern=None,\n                          read_preference=None):\n        \"\"\"Start a multi-statement transaction.\n\n        Takes the same arguments as :class:`TransactionOptions`.\n\n        .. versionadded:: 3.7\n        \"\"\"\n        self._check_ended()\n\n        if self._in_transaction:\n            raise InvalidOperation(\"Transaction already in progress\")\n\n        read_concern = self._inherit_option(\"read_concern\", read_concern)\n        write_concern = self._inherit_option(\"write_concern\", write_concern)\n        read_preference = self._inherit_option(\n            \"read_preference\", read_preference)\n\n        self._transaction.opts = TransactionOptions(\n            read_concern, write_concern, read_preference)\n        self._transaction.reset()\n        self._transaction.state = _TxnState.STARTING\n        self._start_retryable_write()\n        return _TransactionContext(self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncommit a multi - statement transaction.", "response": "def commit_transaction(self):\n        \"\"\"Commit a multi-statement transaction.\n\n        .. versionadded:: 3.7\n        \"\"\"\n        self._check_ended()\n        retry = False\n        state = self._transaction.state\n        if state is _TxnState.NONE:\n            raise InvalidOperation(\"No transaction started\")\n        elif state in (_TxnState.STARTING, _TxnState.COMMITTED_EMPTY):\n            # Server transaction was never started, no need to send a command.\n            self._transaction.state = _TxnState.COMMITTED_EMPTY\n            return\n        elif state is _TxnState.ABORTED:\n            raise InvalidOperation(\n                \"Cannot call commitTransaction after calling abortTransaction\")\n        elif state is _TxnState.COMMITTED:\n            # We're explicitly retrying the commit, move the state back to\n            # \"in progress\" so that _in_transaction returns true.\n            self._transaction.state = _TxnState.IN_PROGRESS\n            retry = True\n\n        try:\n            self._finish_transaction_with_retry(\"commitTransaction\", retry)\n        except ConnectionFailure as exc:\n            # We do not know if the commit was successfully applied on the\n            # server or if it satisfied the provided write concern, set the\n            # unknown commit error label.\n            exc._remove_error_label(\"TransientTransactionError\")\n            _reraise_with_unknown_commit(exc)\n        except WTimeoutError as exc:\n            # We do not know if the commit has satisfied the provided write\n            # concern, add the unknown commit error label.\n            _reraise_with_unknown_commit(exc)\n        except OperationFailure as exc:\n            if exc.code not in _UNKNOWN_COMMIT_ERROR_CODES:\n                # The server reports errorLabels in the case.\n                raise\n            # We do not know if the commit was successfully applied on the\n            # server or if it satisfied the provided write concern, set the\n            # unknown commit error label.\n            _reraise_with_unknown_commit(exc)\n        finally:\n            self._transaction.state = _TxnState.COMMITTED"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef abort_transaction(self):\n        self._check_ended()\n\n        state = self._transaction.state\n        if state is _TxnState.NONE:\n            raise InvalidOperation(\"No transaction started\")\n        elif state is _TxnState.STARTING:\n            # Server transaction was never started, no need to send a command.\n            self._transaction.state = _TxnState.ABORTED\n            return\n        elif state is _TxnState.ABORTED:\n            raise InvalidOperation(\"Cannot call abortTransaction twice\")\n        elif state in (_TxnState.COMMITTED, _TxnState.COMMITTED_EMPTY):\n            raise InvalidOperation(\n                \"Cannot call abortTransaction after calling commitTransaction\")\n\n        try:\n            self._finish_transaction_with_retry(\"abortTransaction\", False)\n        except (OperationFailure, ConnectionFailure):\n            # The transactions spec says to ignore abortTransaction errors.\n            pass\n        finally:\n            self._transaction.state = _TxnState.ABORTED", "response": "Abort a multi - statement transaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns commit or abort with one retry after any retryable error.", "response": "def _finish_transaction_with_retry(self, command_name, explict_retry):\n        \"\"\"Run commit or abort with one retry after any retryable error.\n\n        :Parameters:\n          - `command_name`: Either \"commitTransaction\" or \"abortTransaction\".\n          - `explict_retry`: True when this is an explict commit retry attempt,\n            ie the application called session.commit_transaction() twice.\n        \"\"\"\n        # This can be refactored with MongoClient._retry_with_session.\n        try:\n            return self._finish_transaction(command_name, explict_retry)\n        except ServerSelectionTimeoutError:\n            raise\n        except ConnectionFailure as exc:\n            try:\n                return self._finish_transaction(command_name, True)\n            except ServerSelectionTimeoutError:\n                # Raise the original error so the application can infer that\n                # an attempt was made.\n                raise exc\n        except OperationFailure as exc:\n            if exc.code not in _RETRYABLE_ERROR_CODES:\n                raise\n            try:\n                return self._finish_transaction(command_name, True)\n            except ServerSelectionTimeoutError:\n                # Raise the original error so the application can infer that\n                # an attempt was made.\n                raise exc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the cluster time for this session.", "response": "def advance_cluster_time(self, cluster_time):\n        \"\"\"Update the cluster time for this session.\n\n        :Parameters:\n          - `cluster_time`: The\n            :data:`~pymongo.client_session.ClientSession.cluster_time` from\n            another `ClientSession` instance.\n        \"\"\"\n        if not isinstance(cluster_time, abc.Mapping):\n            raise TypeError(\n                \"cluster_time must be a subclass of collections.Mapping\")\n        if not isinstance(cluster_time.get(\"clusterTime\"), Timestamp):\n            raise ValueError(\"Invalid cluster_time\")\n        self._advance_cluster_time(cluster_time)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the operation time for this session.", "response": "def advance_operation_time(self, operation_time):\n        \"\"\"Update the operation time for this session.\n\n        :Parameters:\n          - `operation_time`: The\n            :data:`~pymongo.client_session.ClientSession.operation_time` from\n            another `ClientSession` instance.\n        \"\"\"\n        if not isinstance(operation_time, Timestamp):\n            raise TypeError(\"operation_time must be an instance \"\n                            \"of bson.timestamp.Timestamp\")\n        self._advance_operation_time(operation_time)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _process_response(self, reply):\n        self._advance_cluster_time(reply.get('$clusterTime'))\n        self._advance_operation_time(reply.get('operationTime'))\n        if self._in_transaction and self._transaction.sharded:\n            recovery_token = reply.get('recoveryToken')\n            if recovery_token:\n                self._transaction.recovery_token = recovery_token", "response": "Process a response to a command that was run with this session."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npins this session to the given mongos Server.", "response": "def _pin_mongos(self, server):\n        \"\"\"Pin this session to the given mongos Server.\"\"\"\n        self._transaction.sharded = True\n        self._transaction.pinned_address = server.description.address"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses write concern options.", "response": "def _parse_write_concern(options):\n    \"\"\"Parse write concern options.\"\"\"\n    concern = options.get('w')\n    wtimeout = options.get('wtimeoutms')\n    j = options.get('journal')\n    fsync = options.get('fsync')\n    return WriteConcern(concern, wtimeout, j, fsync)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_pool_options(options):\n    max_pool_size = options.get('maxpoolsize', common.MAX_POOL_SIZE)\n    min_pool_size = options.get('minpoolsize', common.MIN_POOL_SIZE)\n    default_idle_seconds = common.validate_timeout_or_none(\n        'maxidletimems', common.MAX_IDLE_TIME_MS)\n    max_idle_time_seconds = options.get('maxidletimems', default_idle_seconds)\n    if max_pool_size is not None and min_pool_size > max_pool_size:\n        raise ValueError(\"minPoolSize must be smaller or equal to maxPoolSize\")\n    connect_timeout = options.get('connecttimeoutms', common.CONNECT_TIMEOUT)\n    socket_keepalive = options.get('socketkeepalive', True)\n    socket_timeout = options.get('sockettimeoutms')\n    wait_queue_timeout = options.get('waitqueuetimeoutms')\n    wait_queue_multiple = options.get('waitqueuemultiple')\n    event_listeners = options.get('event_listeners')\n    appname = options.get('appname')\n    driver = options.get('driver')\n    compression_settings = CompressionSettings(\n        options.get('compressors', []),\n        options.get('zlibcompressionlevel', -1))\n    ssl_context, ssl_match_hostname = _parse_ssl_options(options)\n    return PoolOptions(max_pool_size,\n                       min_pool_size,\n                       max_idle_time_seconds,\n                       connect_timeout, socket_timeout,\n                       wait_queue_timeout, wait_queue_multiple,\n                       ssl_context, ssl_match_hostname, socket_keepalive,\n                       _EventListeners(event_listeners),\n                       appname,\n                       driver,\n                       compression_settings)", "response": "Parse connection pool options."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks the response to a command.", "response": "def _check_command_response(response, msg=None, allowable_errors=None,\n                            parse_write_concern_error=False):\n    \"\"\"Check the response to a command for errors.\n    \"\"\"\n    if \"ok\" not in response:\n        # Server didn't recognize our message as a command.\n        raise OperationFailure(response.get(\"$err\"),\n                               response.get(\"code\"),\n                               response)\n\n    if parse_write_concern_error and 'writeConcernError' in response:\n        _raise_write_concern_error(response['writeConcernError'])\n\n    if not response[\"ok\"]:\n\n        details = response\n        # Mongos returns the error details in a 'raw' object\n        # for some errors.\n        if \"raw\" in response:\n            for shard in itervalues(response[\"raw\"]):\n                # Grab the first non-empty raw error from a shard.\n                if shard.get(\"errmsg\") and not shard.get(\"ok\"):\n                    details = shard\n                    break\n\n        errmsg = details[\"errmsg\"]\n        if allowable_errors is None or errmsg not in allowable_errors:\n\n            code = details.get(\"code\")\n            # Server is \"not master\" or \"recovering\"\n            if code in _NOT_MASTER_CODES:\n                raise NotMasterError(errmsg, response)\n            elif (\"not master\" in errmsg\n                  or \"node is recovering\" in errmsg):\n                raise NotMasterError(errmsg, response)\n\n            # Server assertion failures\n            if errmsg == \"db assertion failure\":\n                errmsg = (\"db assertion failure, assertion: '%s'\" %\n                          details.get(\"assertion\", \"\"))\n                raise OperationFailure(errmsg,\n                                       details.get(\"assertionCode\"),\n                                       response)\n\n            # Other errors\n            # findAndModify with upsert can raise duplicate key error\n            if code in (11000, 11001, 12582):\n                raise DuplicateKeyError(errmsg, code, response)\n            elif code == 50:\n                raise ExecutionTimeout(errmsg, code, response)\n            elif code == 43:\n                raise CursorNotFound(errmsg, code, response)\n\n            msg = msg or \"%s\"\n            raise OperationFailure(msg % errmsg, code, response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck the response from getlasterror and raise an exception if it fails.", "response": "def _check_gle_response(result):\n    \"\"\"Return getlasterror response as a dict, or raise OperationFailure.\"\"\"\n    # Did getlasterror itself fail?\n    _check_command_response(result)\n\n    if result.get(\"wtimeout\", False):\n        # MongoDB versions before 1.8.0 return the error message in an \"errmsg\"\n        # field. If \"errmsg\" exists \"err\" will also exist set to None, so we\n        # have to check for \"errmsg\" first.\n        raise WTimeoutError(result.get(\"errmsg\", result.get(\"err\")),\n                            result.get(\"code\"),\n                            result)\n\n    error_msg = result.get(\"err\", \"\")\n    if error_msg is None:\n        return result\n\n    if error_msg.startswith(\"not master\"):\n        raise NotMasterError(error_msg, result)\n\n    details = result\n\n    # mongos returns the error code in an error object for some errors.\n    if \"errObjects\" in result:\n        for errobj in result[\"errObjects\"]:\n            if errobj.get(\"err\") == error_msg:\n                details = errobj\n                break\n\n    code = details.get(\"code\")\n    if code in (11000, 11001, 12582):\n        raise DuplicateKeyError(details[\"err\"], code, result)\n    raise OperationFailure(details[\"err\"], code, result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_write_command_response(result):\n    # Prefer write errors over write concern errors\n    write_errors = result.get(\"writeErrors\")\n    if write_errors:\n        _raise_last_write_error(write_errors)\n\n    error = result.get(\"writeConcernError\")\n    if error:\n        _raise_write_concern_error(error)", "response": "Check if the response from a write command is valid."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nraising last error in bulk write result.", "response": "def _raise_last_error(bulk_write_result):\n    \"\"\"Backward compatibility helper for insert error handling.\n    \"\"\"\n    # Prefer write errors over write concern errors\n    write_errors = bulk_write_result.get(\"writeErrors\")\n    if write_errors:\n        _raise_last_write_error(write_errors)\n\n    _raise_write_concern_error(bulk_write_result[\"writeConcernErrors\"][-1])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstart monitoring or restart after a fork.", "response": "def open(self):\n        \"\"\"Start monitoring, or restart after a fork.\n\n        No effect if called multiple times.\n\n        .. warning:: Topology is shared among multiple threads and is protected\n          by mutual exclusion. Using Topology from a process other than the one\n          that initialized it will emit a warning and may result in deadlock. To\n          prevent this from happening, MongoClient must be created after any\n          forking.\n\n        \"\"\"\n        if self._pid is None:\n            self._pid = os.getpid()\n        else:\n            if os.getpid() != self._pid:\n                warnings.warn(\n                    \"MongoClient opened before fork. Create MongoClient only \"\n                    \"after forking. See PyMongo's documentation for details: \"\n                    \"http://api.mongodb.org/python/current/faq.html#\"\n                    \"is-pymongo-fork-safe\")\n\n        with self._lock:\n            self._ensure_opened()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef select_servers(self,\n                       selector,\n                       server_selection_timeout=None,\n                       address=None):\n        \"\"\"Return a list of Servers matching selector, or time out.\n\n        :Parameters:\n          - `selector`: function that takes a list of Servers and returns\n            a subset of them.\n          - `server_selection_timeout` (optional): maximum seconds to wait.\n            If not provided, the default value common.SERVER_SELECTION_TIMEOUT\n            is used.\n          - `address`: optional server address to select.\n\n        Calls self.open() if needed.\n\n        Raises exc:`ServerSelectionTimeoutError` after\n        `server_selection_timeout` if no matching servers are found.\n        \"\"\"\n        if server_selection_timeout is None:\n            server_timeout = self._settings.server_selection_timeout\n        else:\n            server_timeout = server_selection_timeout\n\n        with self._lock:\n            server_descriptions = self._select_servers_loop(\n                selector, server_timeout, address)\n\n            return [self.get_server_by_address(sd.address)\n                    for sd in server_descriptions]", "response": "Select a list of Servers matching selector."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _select_servers_loop(self, selector, timeout, address):\n        now = _time()\n        end_time = now + timeout\n        server_descriptions = self._description.apply_selector(\n            selector, address, custom_selector=self._settings.server_selector)\n\n        while not server_descriptions:\n            # No suitable servers.\n            if timeout == 0 or now > end_time:\n                raise ServerSelectionTimeoutError(\n                    self._error_message(selector))\n\n            self._ensure_opened()\n            self._request_check_all()\n\n            # Release the lock and wait for the topology description to\n            # change, or for a timeout. We won't miss any changes that\n            # came after our most recent apply_selector call, since we've\n            # held the lock until now.\n            self._condition.wait(common.MIN_HEARTBEAT_INTERVAL)\n            self._description.check_compatible()\n            now = _time()\n            server_descriptions = self._description.apply_selector(\n                selector, address,\n                custom_selector=self._settings.server_selector)\n\n        self._description.check_compatible()\n        return server_descriptions", "response": "Select servers in the topology."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _process_change(self, server_description):\n        td_old = self._description\n        if self._publish_server:\n            old_server_description = td_old._server_descriptions[\n                server_description.address]\n            self._events.put((\n                self._listeners.publish_server_description_changed,\n                (old_server_description, server_description,\n                 server_description.address, self._topology_id)))\n\n        self._description = updated_topology_description(\n            self._description, server_description)\n\n        self._update_servers()\n        self._receive_cluster_time_no_lock(server_description.cluster_time)\n\n        if self._publish_tp:\n            self._events.put((\n                self._listeners.publish_topology_description_changed,\n                (td_old, self._description, self._topology_id)))\n\n        # Wake waiters in select_servers().\n        self._condition.notify_all()", "response": "Process a new ServerDescription on an opened topology."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing a new ServerDescription after an ismaster call completes.", "response": "def on_change(self, server_description):\n        \"\"\"Process a new ServerDescription after an ismaster call completes.\"\"\"\n        # We do no I/O holding the lock.\n        with self._lock:\n            # Monitors may continue working on ismaster calls for some time\n            # after a call to Topology.close, so this method may be called at\n            # any time. Ensure the topology is open before processing the\n            # change.\n            # Any monitored server was definitely in the topology description\n            # once. Check if it's still in the description or if some state-\n            # change removed it. E.g., we got a host list from the primary\n            # that didn't include this server.\n            if (self._opened and\n                    self._description.has_server(server_description.address)):\n                self._process_change(server_description)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_server_session(self):\n        with self._lock:\n            session_timeout = self._description.logical_session_timeout_minutes\n            if session_timeout is None:\n                # Maybe we need an initial scan? Can raise ServerSelectionError.\n                if self._description.topology_type == TOPOLOGY_TYPE.Single:\n                    if not self._description.has_known_servers:\n                        self._select_servers_loop(\n                            any_server_selector,\n                            self._settings.server_selection_timeout,\n                            None)\n                elif not self._description.readable_servers:\n                    self._select_servers_loop(\n                        readable_server_selector,\n                        self._settings.server_selection_timeout,\n                        None)\n\n            session_timeout = self._description.logical_session_timeout_minutes\n            if session_timeout is None:\n                raise ConfigurationError(\n                    \"Sessions are not supported by this MongoDB deployment\")\n\n            return self._session_pool.get_server_session(session_timeout)", "response": "Start or resume a server session or raise ConfigurationError."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting monitors or restart after a fork.", "response": "def _ensure_opened(self):\n        \"\"\"Start monitors, or restart after a fork.\n\n        Hold the lock when calling this.\n        \"\"\"\n        if not self._opened:\n            self._opened = True\n            self._update_servers()\n\n            # Start or restart the events publishing thread.\n            if self._publish_tp or self._publish_server:\n                self.__events_executor.open()\n\n        # Ensure that the monitors are open.\n        for server in itervalues(self._servers):\n            server.open()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_with_retry(self):\n        # According to the spec, if an ismaster call fails we reset the\n        # server's pool. If a server was once connected, change its type\n        # to Unknown only after retrying once.\n        address = self._server_description.address\n        retry = True\n        if self._server_description.server_type == SERVER_TYPE.Unknown:\n            retry = False\n\n        start = _time()\n        try:\n            return self._check_once()\n        except ReferenceError:\n            raise\n        except Exception as error:\n            error_time = _time() - start\n            if self._publish:\n                self._listeners.publish_server_heartbeat_failed(\n                    address, error_time, error)\n            self._topology.reset_pool(address)\n            default = ServerDescription(address, error=error)\n            if not retry:\n                self._avg_round_trip_time.reset()\n                # Server type defaults to Unknown.\n                return default\n\n            # Try a second and final time. If it fails return original error.\n            # Always send metadata: this is a new connection.\n            start = _time()\n            try:\n                return self._check_once()\n            except ReferenceError:\n                raise\n            except Exception as error:\n                error_time = _time() - start\n                if self._publish:\n                    self._listeners.publish_server_heartbeat_failed(\n                        address, error_time, error)\n                self._avg_round_trip_time.reset()\n                return default", "response": "Try a second call to the server and return a ServerDescription."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a socket is in master state.", "response": "def _check_with_socket(self, sock_info):\n        \"\"\"Return (IsMaster, round_trip_time).\n\n        Can raise ConnectionFailure or OperationFailure.\n        \"\"\"\n        start = _time()\n        try:\n            return (sock_info.ismaster(self._pool.opts.metadata,\n                                       self._topology.max_cluster_time()),\n                    _time() - start)\n        except OperationFailure as exc:\n            # Update max cluster time even when isMaster fails.\n            self._topology.receive_cluster_time(\n                exc.details.get('$clusterTime'))\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nclears the given database or collection object s type registry.", "response": "def _clear_entity_type_registry(entity, **kwargs):\n    \"\"\"Clear the given database/collection object's type registry.\"\"\"\n    codecopts = entity.codec_options.with_options(type_registry=None)\n    return entity.with_options(codec_options=codecopts, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nabort the current object.", "response": "def abort(self):\n        \"\"\"Remove all chunks/files that may have been uploaded and close.\n        \"\"\"\n        self._coll.chunks.delete_many(\n            {\"files_id\": self._file['_id']}, session=self._session)\n        self._coll.files.delete_one(\n            {\"_id\": self._file['_id']}, session=self._session)\n        object.__setattr__(self, \"_closed\", True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef readchunk(self):\n        received = len(self.__buffer)\n        chunk_data = EMPTY\n        chunk_size = int(self.chunk_size)\n\n        if received > 0:\n            chunk_data = self.__buffer\n        elif self.__position < int(self.length):\n            chunk_number = int((received + self.__position) / chunk_size)\n            if self.__chunk_iter is None:\n                self.__chunk_iter = _GridOutChunkIterator(\n                    self, self.__chunks, self._session, chunk_number)\n\n            chunk = self.__chunk_iter.next()\n            chunk_data = chunk[\"data\"][self.__position % chunk_size:]\n\n            if not chunk_data:\n                raise CorruptGridFile(\"truncated chunk\")\n\n        self.__position += len(chunk_data)\n        self.__buffer = EMPTY\n        return chunk_data", "response": "Reads a chunk at a time."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading at most size bytes from the file.", "response": "def read(self, size=-1):\n        \"\"\"Read at most `size` bytes from the file (less if there\n        isn't enough data).\n\n        The bytes are returned as an instance of :class:`str` (:class:`bytes`\n        in python 3). If `size` is negative or omitted all data is read.\n\n        :Parameters:\n          - `size` (optional): the number of bytes to read\n\n        .. versionchanged:: 3.8\n           This method now only checks for extra chunks after reading the\n           entire file. Previously, this method would check for extra chunks\n           on every call.\n        \"\"\"\n        self._ensure_file()\n\n        remainder = int(self.length) - self.__position\n        if size < 0 or size > remainder:\n            size = remainder\n\n        if size == 0:\n            return EMPTY\n\n        received = 0\n        data = StringIO()\n        while received < size:\n            chunk_data = self.readchunk()\n            received += len(chunk_data)\n            data.write(chunk_data)\n\n        # Detect extra chunks after reading the entire file.\n        if size == remainder and self.__chunk_iter:\n            try:\n                self.__chunk_iter.next()\n            except StopIteration:\n                pass\n\n        self.__position -= received - size\n\n        # Return 'size' bytes and store the rest.\n        data.seek(size)\n        self.__buffer = data.read()\n        data.seek(0)\n        return data.read(size)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads one line or up to size bytes from the file.", "response": "def readline(self, size=-1):\n        \"\"\"Read one line or up to `size` bytes from the file.\n\n        :Parameters:\n         - `size` (optional): the maximum number of bytes to read\n        \"\"\"\n        remainder = int(self.length) - self.__position\n        if size < 0 or size > remainder:\n            size = remainder\n\n        if size == 0:\n            return EMPTY\n\n        received = 0\n        data = StringIO()\n        while received < size:\n            chunk_data = self.readchunk()\n            pos = chunk_data.find(NEWLN, 0, size)\n            if pos != -1:\n                size = received + pos + 1\n\n            received += len(chunk_data)\n            data.write(chunk_data)\n            if pos != -1:\n                break\n\n        self.__position -= received - size\n\n        # Return 'size' bytes and store the rest.\n        data.seek(size)\n        self.__buffer = data.read()\n        data.seek(0)\n        return data.read(size)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _next_with_retry(self):\n        if self._cursor is None:\n            self._create_cursor()\n\n        try:\n            return self._cursor.next()\n        except CursorNotFound:\n            self._cursor.close()\n            self._create_cursor()\n            return self._cursor.next()", "response": "Return the next chunk and retry once on CursorNotFound."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef next(self):\n        # Work around \"super is not iterable\" issue in Python 3.x\n        next_file = super(GridOutCursor, self).next()\n        return GridOut(self.__root_collection, file_document=next_file,\n                       session=self.session)", "response": "Get next GridOut object from cursor.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds and return a mechanism specific credentials tuple.", "response": "def _build_credentials_tuple(mech, source, user, passwd, extra, database):\n    \"\"\"Build and return a mechanism specific credentials tuple.\n    \"\"\"\n    if mech != 'MONGODB-X509' and user is None:\n        raise ConfigurationError(\"%s requires a username.\" % (mech,))\n    if mech == 'GSSAPI':\n        if source is not None and source != '$external':\n            raise ValueError(\n                \"authentication source must be $external or None for GSSAPI\")\n        properties = extra.get('authmechanismproperties', {})\n        service_name = properties.get('SERVICE_NAME', 'mongodb')\n        canonicalize = properties.get('CANONICALIZE_HOST_NAME', False)\n        service_realm = properties.get('SERVICE_REALM')\n        props = GSSAPIProperties(service_name=service_name,\n                                 canonicalize_host_name=canonicalize,\n                                 service_realm=service_realm)\n        # Source is always $external.\n        return MongoCredential(mech, '$external', user, passwd, props, None)\n    elif mech == 'MONGODB-X509':\n        if passwd is not None:\n            raise ConfigurationError(\n                \"Passwords are not supported by MONGODB-X509\")\n        if source is not None and source != '$external':\n            raise ValueError(\n                \"authentication source must be \"\n                \"$external or None for MONGODB-X509\")\n        # user can be None.\n        return MongoCredential(mech, '$external', user, None, None, None)\n    elif mech == 'PLAIN':\n        source_database = source or database or '$external'\n        return MongoCredential(mech, source_database, user, passwd, None, None)\n    else:\n        source_database = source or database or 'admin'\n        if passwd is None:\n            raise ConfigurationError(\"A password is required.\")\n        return MongoCredential(\n            mech, source_database, user, passwd, None, _Cache())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _password_digest(username, password):\n    if not isinstance(password, string_type):\n        raise TypeError(\"password must be an \"\n                        \"instance of %s\" % (string_type.__name__,))\n    if len(password) == 0:\n        raise ValueError(\"password can't be empty\")\n    if not isinstance(username, string_type):\n        raise TypeError(\"password must be an \"\n                        \"instance of  %s\" % (string_type.__name__,))\n\n    md5hash = hashlib.md5()\n    data = \"%s:mongo:%s\" % (username, password)\n    md5hash.update(data.encode('utf-8'))\n    return _unicode(md5hash.hexdigest())", "response": "Get a password digest to use for authentication."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets an auth key to use for authentication.", "response": "def _auth_key(nonce, username, password):\n    \"\"\"Get an auth key to use for authentication.\n    \"\"\"\n    digest = _password_digest(username, password)\n    md5hash = hashlib.md5()\n    data = \"%s%s%s\" % (nonce, username, digest)\n    md5hash.update(data.encode('utf-8'))\n    return _unicode(md5hash.hexdigest())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _command(self, sock_info, command, slave_ok=False,\n                 read_preference=None,\n                 codec_options=None, check=True, allowable_errors=None,\n                 read_concern=None,\n                 write_concern=None,\n                 collation=None,\n                 session=None,\n                 retryable_write=False,\n                 user_fields=None):\n        \"\"\"Internal command helper.\n\n        :Parameters:\n          - `sock_info` - A SocketInfo instance.\n          - `command` - The command itself, as a SON instance.\n          - `slave_ok`: whether to set the SlaveOkay wire protocol bit.\n          - `codec_options` (optional) - An instance of\n            :class:`~bson.codec_options.CodecOptions`.\n          - `check`: raise OperationFailure if there are errors\n          - `allowable_errors`: errors to ignore if `check` is True\n          - `read_concern` (optional) - An instance of\n            :class:`~pymongo.read_concern.ReadConcern`.\n          - `write_concern`: An instance of\n            :class:`~pymongo.write_concern.WriteConcern`. This option is only\n            valid for MongoDB 3.4 and above.\n          - `collation` (optional) - An instance of\n            :class:`~pymongo.collation.Collation`.\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n          - `retryable_write` (optional): True if this command is a retryable\n            write.\n          - `user_fields` (optional): Response fields that should be decoded\n            using the TypeDecoders from codec_options, passed to\n            bson._decode_all_selective.\n\n        :Returns:\n          The result document.\n        \"\"\"\n        with self.__database.client._tmp_session(session) as s:\n            return sock_info.command(\n                self.__database.name,\n                command,\n                slave_ok,\n                read_preference or self._read_preference_for(session),\n                codec_options or self.codec_options,\n                check,\n                allowable_errors,\n                read_concern=read_concern,\n                write_concern=write_concern,\n                parse_write_concern_error=True,\n                collation=collation,\n                session=s,\n                client=self.__database.client,\n                retryable_write=retryable_write,\n                user_fields=user_fields)", "response": "Internal helper method for command helper."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a create command with the given options.", "response": "def __create(self, options, collation, session):\n        \"\"\"Sends a create command with the given options.\n        \"\"\"\n        cmd = SON([(\"create\", self.__name)])\n        if options:\n            if \"size\" in options:\n                options[\"size\"] = float(options[\"size\"])\n            cmd.update(options)\n        with self._socket_for_writes(session) as sock_info:\n            self._command(\n                sock_info, cmd, read_preference=ReadPreference.PRIMARY,\n                write_concern=self._write_concern_for(session),\n                collation=collation, session=session)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bulk_write(self, requests, ordered=True,\n                   bypass_document_validation=False, session=None):\n        \"\"\"Send a batch of write operations to the server.\n\n        Requests are passed as a list of write operation instances (\n        :class:`~pymongo.operations.InsertOne`,\n        :class:`~pymongo.operations.UpdateOne`,\n        :class:`~pymongo.operations.UpdateMany`,\n        :class:`~pymongo.operations.ReplaceOne`,\n        :class:`~pymongo.operations.DeleteOne`, or\n        :class:`~pymongo.operations.DeleteMany`).\n\n          >>> for doc in db.test.find({}):\n          ...     print(doc)\n          ...\n          {u'x': 1, u'_id': ObjectId('54f62e60fba5226811f634ef')}\n          {u'x': 1, u'_id': ObjectId('54f62e60fba5226811f634f0')}\n          >>> # DeleteMany, UpdateOne, and UpdateMany are also available.\n          ...\n          >>> from pymongo import InsertOne, DeleteOne, ReplaceOne\n          >>> requests = [InsertOne({'y': 1}), DeleteOne({'x': 1}),\n          ...             ReplaceOne({'w': 1}, {'z': 1}, upsert=True)]\n          >>> result = db.test.bulk_write(requests)\n          >>> result.inserted_count\n          1\n          >>> result.deleted_count\n          1\n          >>> result.modified_count\n          0\n          >>> result.upserted_ids\n          {2: ObjectId('54f62ee28891e756a6e1abd5')}\n          >>> for doc in db.test.find({}):\n          ...     print(doc)\n          ...\n          {u'x': 1, u'_id': ObjectId('54f62e60fba5226811f634f0')}\n          {u'y': 1, u'_id': ObjectId('54f62ee2fba5226811f634f1')}\n          {u'z': 1, u'_id': ObjectId('54f62ee28891e756a6e1abd5')}\n\n        :Parameters:\n          - `requests`: A list of write operations (see examples above).\n          - `ordered` (optional): If ``True`` (the default) requests will be\n            performed on the server serially, in the order provided. If an error\n            occurs all remaining operations are aborted. If ``False`` requests\n            will be performed on the server in arbitrary order, possibly in\n            parallel, and all operations will be attempted.\n          - `bypass_document_validation`: (optional) If ``True``, allows the\n            write to opt-out of document level validation. Default is\n            ``False``.\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n\n        :Returns:\n          An instance of :class:`~pymongo.results.BulkWriteResult`.\n\n        .. seealso:: :ref:`writes-and-ids`\n\n        .. note:: `bypass_document_validation` requires server version\n          **>= 3.2**\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n\n        .. versionchanged:: 3.2\n          Added bypass_document_validation support\n\n        .. versionadded:: 3.0\n        \"\"\"\n        common.validate_list(\"requests\", requests)\n\n        blk = _Bulk(self, ordered, bypass_document_validation)\n        for request in requests:\n            try:\n                request._add_to_bulk(blk)\n            except AttributeError:\n                raise TypeError(\"%r is not a valid request\" % (request,))\n\n        write_concern = self._write_concern_for(session)\n        bulk_api_result = blk.execute(write_concern, session)\n        if bulk_api_result is not None:\n            return BulkWriteResult(bulk_api_result, True)\n        return BulkWriteResult({}, False)", "response": "Send a batch of write operations to the server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _legacy_write(self, sock_info, name, cmd, op_id,\n                      bypass_doc_val, func, *args):\n        \"\"\"Internal legacy unacknowledged write helper.\"\"\"\n        # Cannot have both unacknowledged write and bypass document validation.\n        if bypass_doc_val and sock_info.max_wire_version >= 4:\n            raise OperationFailure(\"Cannot set bypass_document_validation with\"\n                                   \" unacknowledged write concern\")\n        listeners = self.database.client._event_listeners\n        publish = listeners.enabled_for_commands\n\n        if publish:\n            start = datetime.datetime.now()\n        args = args + (sock_info.compression_context,)\n        rqst_id, msg, max_size = func(*args)\n        if publish:\n            duration = datetime.datetime.now() - start\n            listeners.publish_command_start(\n                cmd, self.__database.name, rqst_id, sock_info.address, op_id)\n            start = datetime.datetime.now()\n        try:\n            result = sock_info.legacy_write(rqst_id, msg, max_size, False)\n        except Exception as exc:\n            if publish:\n                dur = (datetime.datetime.now() - start) + duration\n                if isinstance(exc, OperationFailure):\n                    details = exc.details\n                    # Succeed if GLE was successful and this is a write error.\n                    if details.get(\"ok\") and \"n\" in details:\n                        reply = message._convert_write_result(\n                            name, cmd, details)\n                        listeners.publish_command_success(\n                            dur, reply, name, rqst_id, sock_info.address, op_id)\n                        raise\n                else:\n                    details = message._convert_exception(exc)\n                listeners.publish_command_failure(\n                    dur, details, name, rqst_id, sock_info.address, op_id)\n            raise\n        if publish:\n            if result is not None:\n                reply = message._convert_write_result(name, cmd, result)\n            else:\n                # Comply with APM spec.\n                reply = {'ok': 1}\n            duration = (datetime.datetime.now() - start) + duration\n            listeners.publish_command_success(\n                duration, reply, name, rqst_id, sock_info.address, op_id)\n        return result", "response": "Internal legacy unacknowledged write helper."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _insert_one(\n            self, doc, ordered,\n            check_keys, manipulate, write_concern, op_id, bypass_doc_val,\n            session):\n        \"\"\"Internal helper for inserting a single document.\"\"\"\n        if manipulate:\n            doc = self.__database._apply_incoming_manipulators(doc, self)\n            if not isinstance(doc, RawBSONDocument) and '_id' not in doc:\n                doc['_id'] = ObjectId()\n            doc = self.__database._apply_incoming_copying_manipulators(doc,\n                                                                       self)\n        write_concern = write_concern or self.write_concern\n        acknowledged = write_concern.acknowledged\n        command = SON([('insert', self.name),\n                       ('ordered', ordered),\n                       ('documents', [doc])])\n        if not write_concern.is_server_default:\n            command['writeConcern'] = write_concern.document\n\n        def _insert_command(session, sock_info, retryable_write):\n            if not sock_info.op_msg_enabled and not acknowledged:\n                # Legacy OP_INSERT.\n                return self._legacy_write(\n                    sock_info, 'insert', command, op_id,\n                    bypass_doc_val, message.insert, self.__full_name,\n                    [doc], check_keys, False, write_concern.document, False,\n                    self.__write_response_codec_options)\n\n            if bypass_doc_val and sock_info.max_wire_version >= 4:\n                command['bypassDocumentValidation'] = True\n\n            result = sock_info.command(\n                self.__database.name,\n                command,\n                write_concern=write_concern,\n                codec_options=self.__write_response_codec_options,\n                check_keys=check_keys,\n                session=session,\n                client=self.__database.client,\n                retryable_write=retryable_write)\n\n            _check_write_command_response(result)\n\n        self.__database.client._retryable_write(\n            acknowledged, _insert_command, session)\n\n        if not isinstance(doc, RawBSONDocument):\n            return doc.get('_id')", "response": "Internal helper for inserting a single document."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef insert_one(self, document, bypass_document_validation=False,\n                   session=None):\n        \"\"\"Insert a single document.\n\n          >>> db.test.count_documents({'x': 1})\n          0\n          >>> result = db.test.insert_one({'x': 1})\n          >>> result.inserted_id\n          ObjectId('54f112defba522406c9cc208')\n          >>> db.test.find_one({'x': 1})\n          {u'x': 1, u'_id': ObjectId('54f112defba522406c9cc208')}\n\n        :Parameters:\n          - `document`: The document to insert. Must be a mutable mapping\n            type. If the document does not have an _id field one will be\n            added automatically.\n          - `bypass_document_validation`: (optional) If ``True``, allows the\n            write to opt-out of document level validation. Default is\n            ``False``.\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n\n        :Returns:\n          - An instance of :class:`~pymongo.results.InsertOneResult`.\n\n        .. seealso:: :ref:`writes-and-ids`\n\n        .. note:: `bypass_document_validation` requires server version\n          **>= 3.2**\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n\n        .. versionchanged:: 3.2\n          Added bypass_document_validation support\n\n        .. versionadded:: 3.0\n        \"\"\"\n        common.validate_is_document_type(\"document\", document)\n        if not (isinstance(document, RawBSONDocument) or \"_id\" in document):\n            document[\"_id\"] = ObjectId()\n\n        write_concern = self._write_concern_for(session)\n        return InsertOneResult(\n            self._insert(document,\n                         write_concern=write_concern,\n                         bypass_doc_val=bypass_document_validation,\n                         session=session),\n            write_concern.acknowledged)", "response": "Insert a single document into the database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _update(self, sock_info, criteria, document, upsert=False,\n                check_keys=True, multi=False, manipulate=False,\n                write_concern=None, op_id=None, ordered=True,\n                bypass_doc_val=False, collation=None, array_filters=None,\n                session=None, retryable_write=False):\n        \"\"\"Internal update / replace helper.\"\"\"\n        common.validate_boolean(\"upsert\", upsert)\n        if manipulate:\n            document = self.__database._fix_incoming(document, self)\n        collation = validate_collation_or_none(collation)\n        write_concern = write_concern or self.write_concern\n        acknowledged = write_concern.acknowledged\n        update_doc = SON([('q', criteria),\n                          ('u', document),\n                          ('multi', multi),\n                          ('upsert', upsert)])\n        if collation is not None:\n            if sock_info.max_wire_version < 5:\n                raise ConfigurationError(\n                    'Must be connected to MongoDB 3.4+ to use collations.')\n            elif not acknowledged:\n                raise ConfigurationError(\n                    'Collation is unsupported for unacknowledged writes.')\n            else:\n                update_doc['collation'] = collation\n        if array_filters is not None:\n            if sock_info.max_wire_version < 6:\n                raise ConfigurationError(\n                    'Must be connected to MongoDB 3.6+ to use array_filters.')\n            elif not acknowledged:\n                raise ConfigurationError(\n                    'arrayFilters is unsupported for unacknowledged writes.')\n            else:\n                update_doc['arrayFilters'] = array_filters\n        command = SON([('update', self.name),\n                       ('ordered', ordered),\n                       ('updates', [update_doc])])\n        if not write_concern.is_server_default:\n            command['writeConcern'] = write_concern.document\n\n        if not sock_info.op_msg_enabled and not acknowledged:\n            # Legacy OP_UPDATE.\n            return self._legacy_write(\n                sock_info, 'update', command, op_id,\n                bypass_doc_val, message.update, self.__full_name, upsert,\n                multi, criteria, document, False, write_concern.document,\n                check_keys, self.__write_response_codec_options)\n\n        # Update command.\n        if bypass_doc_val and sock_info.max_wire_version >= 4:\n            command['bypassDocumentValidation'] = True\n\n        # The command result has to be published for APM unmodified\n        # so we make a shallow copy here before adding updatedExisting.\n        result = sock_info.command(\n            self.__database.name,\n            command,\n            write_concern=write_concern,\n            codec_options=self.__write_response_codec_options,\n            session=session,\n            client=self.__database.client,\n            retryable_write=retryable_write).copy()\n        _check_write_command_response(result)\n        # Add the updatedExisting field for compatibility.\n        if result.get('n') and 'upserted' not in result:\n            result['updatedExisting'] = True\n        else:\n            result['updatedExisting'] = False\n            # MongoDB >= 2.6.0 returns the upsert _id in an array\n            # element. Break it out for backward compatibility.\n            if 'upserted' in result:\n                result['upserted'] = result['upserted'][0]['_id']\n\n        if not acknowledged:\n            return None\n        return result", "response": "Internal update / replace helper."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef replace_one(self, filter, replacement, upsert=False,\n                    bypass_document_validation=False, collation=None,\n                    session=None):\n        \"\"\"Replace a single document matching the filter.\n\n          >>> for doc in db.test.find({}):\n          ...     print(doc)\n          ...\n          {u'x': 1, u'_id': ObjectId('54f4c5befba5220aa4d6dee7')}\n          >>> result = db.test.replace_one({'x': 1}, {'y': 1})\n          >>> result.matched_count\n          1\n          >>> result.modified_count\n          1\n          >>> for doc in db.test.find({}):\n          ...     print(doc)\n          ...\n          {u'y': 1, u'_id': ObjectId('54f4c5befba5220aa4d6dee7')}\n\n        The *upsert* option can be used to insert a new document if a matching\n        document does not exist.\n\n          >>> result = db.test.replace_one({'x': 1}, {'x': 1}, True)\n          >>> result.matched_count\n          0\n          >>> result.modified_count\n          0\n          >>> result.upserted_id\n          ObjectId('54f11e5c8891e756a6e1abd4')\n          >>> db.test.find_one({'x': 1})\n          {u'x': 1, u'_id': ObjectId('54f11e5c8891e756a6e1abd4')}\n\n        :Parameters:\n          - `filter`: A query that matches the document to replace.\n          - `replacement`: The new document.\n          - `upsert` (optional): If ``True``, perform an insert if no documents\n            match the filter.\n          - `bypass_document_validation`: (optional) If ``True``, allows the\n            write to opt-out of document level validation. Default is\n            ``False``.\n          - `collation` (optional): An instance of\n            :class:`~pymongo.collation.Collation`. This option is only supported\n            on MongoDB 3.4 and above.\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n\n        :Returns:\n          - An instance of :class:`~pymongo.results.UpdateResult`.\n\n        .. note:: `bypass_document_validation` requires server version\n          **>= 3.2**\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n\n        .. versionchanged:: 3.4\n          Added the `collation` option.\n\n        .. versionchanged:: 3.2\n          Added bypass_document_validation support\n\n        .. versionadded:: 3.0\n        \"\"\"\n        common.validate_is_mapping(\"filter\", filter)\n        common.validate_ok_for_replace(replacement)\n\n        write_concern = self._write_concern_for(session)\n        return UpdateResult(\n            self._update_retryable(\n                filter, replacement, upsert,\n                write_concern=write_concern,\n                bypass_doc_val=bypass_document_validation,\n                collation=collation, session=session),\n            write_concern.acknowledged)", "response": "Replace a single document matching the filter."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_one(self, filter, update, upsert=False,\n                   bypass_document_validation=False,\n                   collation=None, array_filters=None, session=None):\n        \"\"\"Update a single document matching the filter.\n\n          >>> for doc in db.test.find():\n          ...     print(doc)\n          ...\n          {u'x': 1, u'_id': 0}\n          {u'x': 1, u'_id': 1}\n          {u'x': 1, u'_id': 2}\n          >>> result = db.test.update_one({'x': 1}, {'$inc': {'x': 3}})\n          >>> result.matched_count\n          1\n          >>> result.modified_count\n          1\n          >>> for doc in db.test.find():\n          ...     print(doc)\n          ...\n          {u'x': 4, u'_id': 0}\n          {u'x': 1, u'_id': 1}\n          {u'x': 1, u'_id': 2}\n\n        :Parameters:\n          - `filter`: A query that matches the document to update.\n          - `update`: The modifications to apply.\n          - `upsert` (optional): If ``True``, perform an insert if no documents\n            match the filter.\n          - `bypass_document_validation`: (optional) If ``True``, allows the\n            write to opt-out of document level validation. Default is\n            ``False``.\n          - `collation` (optional): An instance of\n            :class:`~pymongo.collation.Collation`. This option is only supported\n            on MongoDB 3.4 and above.\n          - `array_filters` (optional): A list of filters specifying which\n            array elements an update should apply. Requires MongoDB 3.6+.\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n\n        :Returns:\n          - An instance of :class:`~pymongo.results.UpdateResult`.\n\n        .. note:: `bypass_document_validation` requires server version\n          **>= 3.2**\n\n        .. versionchanged:: 3.6\n           Added the `array_filters` and ``session`` parameters.\n\n        .. versionchanged:: 3.4\n          Added the `collation` option.\n\n        .. versionchanged:: 3.2\n          Added bypass_document_validation support\n\n        .. versionadded:: 3.0\n        \"\"\"\n        common.validate_is_mapping(\"filter\", filter)\n        common.validate_ok_for_update(update)\n        common.validate_list_or_none('array_filters', array_filters)\n\n        write_concern = self._write_concern_for(session)\n        return UpdateResult(\n            self._update_retryable(\n                filter, update, upsert, check_keys=False,\n                write_concern=write_concern,\n                bypass_doc_val=bypass_document_validation,\n                collation=collation, array_filters=array_filters,\n                session=session),\n            write_concern.acknowledged)", "response": "Update a single document matching the filter."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nalias for :meth:`~pymongo.database.Database.drop_collection`. :Parameters: - `session` (optional): a :class:`~pymongo.client_session.ClientSession`. The following two calls are equivalent: >>> db.foo.drop() >>> db.drop_collection(\"foo\") .. versionchanged:: 3.7 :meth:`drop` now respects this :class:`Collection`'s :attr:`write_concern`. .. versionchanged:: 3.6 Added ``session`` parameter.", "response": "def drop(self, session=None):\n        \"\"\"Alias for :meth:`~pymongo.database.Database.drop_collection`.\n\n        :Parameters:\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n\n        The following two calls are equivalent:\n\n          >>> db.foo.drop()\n          >>> db.drop_collection(\"foo\")\n\n        .. versionchanged:: 3.7\n           :meth:`drop` now respects this :class:`Collection`'s :attr:`write_concern`.\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n        \"\"\"\n        dbo = self.__database.client.get_database(\n            self.__database.name,\n            self.codec_options,\n            self.read_preference,\n            self.write_concern,\n            self.read_concern)\n        dbo.drop_collection(self.__name, session=session)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_one(self, filter, collation=None, session=None):\n        write_concern = self._write_concern_for(session)\n        return DeleteResult(\n            self._delete_retryable(\n                filter, False,\n                write_concern=write_concern,\n                collation=collation, session=session),\n            write_concern.acknowledged)", "response": "Delete a single document matching the filter."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_many(self, filter, collation=None, session=None):\n        write_concern = self._write_concern_for(session)\n        return DeleteResult(\n            self._delete_retryable(\n                filter, True,\n                write_concern=write_concern,\n                collation=collation, session=session),\n            write_concern.acknowledged)", "response": "Delete one or more documents matching the filter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nquery the database and retrieve batches of raw BSON.", "response": "def find_raw_batches(self, *args, **kwargs):\n        \"\"\"Query the database and retrieve batches of raw BSON.\n\n        Similar to the :meth:`find` method but returns a\n        :class:`~pymongo.cursor.RawBatchCursor`.\n\n        This example demonstrates how to work with raw batches, but in practice\n        raw batches should be passed to an external library that can decode\n        BSON into another data type, rather than used with PyMongo's\n        :mod:`bson` module.\n\n          >>> import bson\n          >>> cursor = db.test.find_raw_batches()\n          >>> for batch in cursor:\n          ...     print(bson.decode_all(batch))\n\n        .. note:: find_raw_batches does not support sessions.\n\n        .. versionadded:: 3.6\n        \"\"\"\n        # OP_MSG with document stream returns is required to support\n        # sessions.\n        if \"session\" in kwargs:\n            raise ConfigurationError(\n                \"find_raw_batches does not support sessions\")\n        return RawBatchCursor(self, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _aggregate_one_result(\n            self, sock_info, slave_ok, cmd, collation=None, session=None):\n        \"\"\"Internal helper to run an aggregate that returns a single result.\"\"\"\n        result = self._command(\n            sock_info,\n            cmd,\n            slave_ok,\n            codec_options=self.__write_response_codec_options,\n            read_concern=self.read_concern,\n            collation=collation,\n            session=session)\n        batch = result['cursor']['firstBatch']\n        return batch[0] if batch else None", "response": "Internal helper to run an aggregate that returns a single result."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef estimated_document_count(self, **kwargs):\n        if 'session' in kwargs:\n            raise ConfigurationError(\n                'estimated_document_count does not support sessions')\n        cmd = SON([('count', self.__name)])\n        cmd.update(kwargs)\n        return self._count(cmd)", "response": "Get an estimate of the number of documents in this collection using the specified options."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncounting the number of documents in this collection. The :meth:`count_documents` method is supported in a transaction. All optional parameters should be passed as keyword arguments to this method. Valid options include: - `skip` (int): The number of matching documents to skip before returning results. - `limit` (int): The maximum number of documents to count. Must be a positive integer. If not provided, no limit is imposed. - `maxTimeMS` (int): The maximum amount of time to allow this operation to run, in milliseconds. - `collation` (optional): An instance of :class:`~pymongo.collation.Collation`. This option is only supported on MongoDB 3.4 and above. - `hint` (string or list of tuples): The index to use. Specify either the index name as a string or the index specification as a list of tuples (e.g. [('a', pymongo.ASCENDING), ('b', pymongo.ASCENDING)]). This option is only supported on MongoDB 3.6 and above. The :meth:`count_documents` method obeys the :attr:`read_preference` of this :class:`Collection`. .. note:: When migrating from :meth:`count` to :meth:`count_documents` the following query operators must be replaced: +-------------+-------------------------------------+ | Operator | Replacement | +=============+=====================================+ | $where | `$expr`_ | +-------------+-------------------------------------+ | $near | `$geoWithin`_ with `$center`_ | +-------------+-------------------------------------+ | $nearSphere | `$geoWithin`_ with `$centerSphere`_ | +-------------+-------------------------------------+ $expr requires MongoDB 3.6+ :Parameters: - `filter` (required): A query document that selects which documents to count in the collection. Can be an empty document to count all documents. - `session` (optional): a :class:`~pymongo.client_session.ClientSession`. - `**kwargs` (optional): See list of options above. .. versionadded:: 3.7 .. _$expr: https://docs.mongodb.com/manual/reference/operator/query/expr/ .. _$geoWithin: https://docs.mongodb.com/manual/reference/operator/query/geoWithin/ .. _$center: https://docs.mongodb.com/manual/reference/operator/query/center/#op._S_center .. _$centerSphere: https://docs.mongodb.com/manual/reference/operator/query/centerSphere/#op._S_centerSphere", "response": "def count_documents(self, filter, session=None, **kwargs):\n        \"\"\"Count the number of documents in this collection.\n\n        The :meth:`count_documents` method is supported in a transaction.\n\n        All optional parameters should be passed as keyword arguments\n        to this method. Valid options include:\n\n          - `skip` (int): The number of matching documents to skip before\n            returning results.\n          - `limit` (int): The maximum number of documents to count. Must be\n            a positive integer. If not provided, no limit is imposed.\n          - `maxTimeMS` (int): The maximum amount of time to allow this\n            operation to run, in milliseconds.\n          - `collation` (optional): An instance of\n            :class:`~pymongo.collation.Collation`. This option is only supported\n            on MongoDB 3.4 and above.\n          - `hint` (string or list of tuples): The index to use. Specify either\n            the index name as a string or the index specification as a list of\n            tuples (e.g. [('a', pymongo.ASCENDING), ('b', pymongo.ASCENDING)]).\n            This option is only supported on MongoDB 3.6 and above.\n\n        The :meth:`count_documents` method obeys the :attr:`read_preference` of\n        this :class:`Collection`.\n\n        .. note:: When migrating from :meth:`count` to :meth:`count_documents`\n           the following query operators must be replaced:\n\n           +-------------+-------------------------------------+\n           | Operator    | Replacement                         |\n           +=============+=====================================+\n           | $where      | `$expr`_                            |\n           +-------------+-------------------------------------+\n           | $near       | `$geoWithin`_ with `$center`_       |\n           +-------------+-------------------------------------+\n           | $nearSphere | `$geoWithin`_ with `$centerSphere`_ |\n           +-------------+-------------------------------------+\n\n           $expr requires MongoDB 3.6+\n\n        :Parameters:\n          - `filter` (required): A query document that selects which documents\n            to count in the collection. Can be an empty document to count all\n            documents.\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n          - `**kwargs` (optional): See list of options above.\n\n        .. versionadded:: 3.7\n\n        .. _$expr: https://docs.mongodb.com/manual/reference/operator/query/expr/\n        .. _$geoWithin: https://docs.mongodb.com/manual/reference/operator/query/geoWithin/\n        .. _$center: https://docs.mongodb.com/manual/reference/operator/query/center/#op._S_center\n        .. _$centerSphere: https://docs.mongodb.com/manual/reference/operator/query/centerSphere/#op._S_centerSphere\n        \"\"\"\n        pipeline = [{'$match': filter}]\n        if 'skip' in kwargs:\n            pipeline.append({'$skip': kwargs.pop('skip')})\n        if 'limit' in kwargs:\n            pipeline.append({'$limit': kwargs.pop('limit')})\n        pipeline.append({'$group': {'_id': 1, 'n': {'$sum': 1}}})\n        cmd = SON([('aggregate', self.__name),\n                   ('pipeline', pipeline),\n                   ('cursor', {})])\n        if \"hint\" in kwargs and not isinstance(kwargs[\"hint\"], string_type):\n            kwargs[\"hint\"] = helpers._index_document(kwargs[\"hint\"])\n        collation = validate_collation_or_none(kwargs.pop('collation', None))\n        cmd.update(kwargs)\n\n        def _cmd(session, server, sock_info, slave_ok):\n            result = self._aggregate_one_result(\n                sock_info, slave_ok, cmd, collation, session)\n            if not result:\n                return 0\n            return result['n']\n\n        return self.__database.client._retryable_read(\n            _cmd, self._read_preference_for(session), session)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the number of documents in this collection.", "response": "def count(self, filter=None, session=None, **kwargs):\n        \"\"\"**DEPRECATED** - Get the number of documents in this collection.\n\n        The :meth:`count` method is deprecated and **not** supported in a\n        transaction. Please use :meth:`count_documents` or\n        :meth:`estimated_document_count` instead.\n\n        All optional count parameters should be passed as keyword arguments\n        to this method. Valid options include:\n\n          - `skip` (int): The number of matching documents to skip before\n            returning results.\n          - `limit` (int): The maximum number of documents to count. A limit\n            of 0 (the default) is equivalent to setting no limit.\n          - `maxTimeMS` (int): The maximum amount of time to allow the count\n            command to run, in milliseconds.\n          - `collation` (optional): An instance of\n            :class:`~pymongo.collation.Collation`. This option is only supported\n            on MongoDB 3.4 and above.\n          - `hint` (string or list of tuples): The index to use. Specify either\n            the index name as a string or the index specification as a list of\n            tuples (e.g. [('a', pymongo.ASCENDING), ('b', pymongo.ASCENDING)]).\n\n        The :meth:`count` method obeys the :attr:`read_preference` of\n        this :class:`Collection`.\n\n        .. note:: When migrating from :meth:`count` to :meth:`count_documents`\n           the following query operators must be replaced:\n\n           +-------------+-------------------------------------+\n           | Operator    | Replacement                         |\n           +=============+=====================================+\n           | $where      | `$expr`_                            |\n           +-------------+-------------------------------------+\n           | $near       | `$geoWithin`_ with `$center`_       |\n           +-------------+-------------------------------------+\n           | $nearSphere | `$geoWithin`_ with `$centerSphere`_ |\n           +-------------+-------------------------------------+\n\n           $expr requires MongoDB 3.6+\n\n        :Parameters:\n          - `filter` (optional): A query document that selects which documents\n            to count in the collection.\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n          - `**kwargs` (optional): See list of options above.\n\n        .. versionchanged:: 3.7\n           Deprecated.\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n\n        .. versionchanged:: 3.4\n           Support the `collation` option.\n\n        .. _$expr: https://docs.mongodb.com/manual/reference/operator/query/expr/\n        .. _$geoWithin: https://docs.mongodb.com/manual/reference/operator/query/geoWithin/\n        .. _$center: https://docs.mongodb.com/manual/reference/operator/query/center/#op._S_center\n        .. _$centerSphere: https://docs.mongodb.com/manual/reference/operator/query/centerSphere/#op._S_centerSphere\n        \"\"\"\n        warnings.warn(\"count is deprecated. Use estimated_document_count or \"\n                      \"count_documents instead. Please note that $where must \"\n                      \"be replaced by $expr, $near must be replaced by \"\n                      \"$geoWithin with $center, and $nearSphere must be \"\n                      \"replaced by $geoWithin with $centerSphere\",\n                      DeprecationWarning, stacklevel=2)\n        cmd = SON([(\"count\", self.__name)])\n        if filter is not None:\n            if \"query\" in kwargs:\n                raise ConfigurationError(\"can't pass both filter and query\")\n            kwargs[\"query\"] = filter\n        if \"hint\" in kwargs and not isinstance(kwargs[\"hint\"], string_type):\n            kwargs[\"hint\"] = helpers._index_document(kwargs[\"hint\"])\n        collation = validate_collation_or_none(kwargs.pop('collation', None))\n        cmd.update(kwargs)\n        return self._count(cmd, collation, session)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_indexes(self, indexes, session=None, **kwargs):\n        common.validate_list('indexes', indexes)\n        names = []\n        with self._socket_for_writes(session) as sock_info:\n            supports_collations = sock_info.max_wire_version >= 5\n            def gen_indexes():\n                for index in indexes:\n                    if not isinstance(index, IndexModel):\n                        raise TypeError(\n                            \"%r is not an instance of \"\n                            \"pymongo.operations.IndexModel\" % (index,))\n                    document = index.document\n                    if \"collation\" in document and not supports_collations:\n                        raise ConfigurationError(\n                            \"Must be connected to MongoDB \"\n                            \"3.4+ to use collations.\")\n                    names.append(document[\"name\"])\n                    yield document\n            cmd = SON([('createIndexes', self.name),\n                       ('indexes', list(gen_indexes()))])\n            cmd.update(kwargs)\n            self._command(\n                sock_info, cmd, read_preference=ReadPreference.PRIMARY,\n                codec_options=_UNICODE_REPLACE_CODEC_OPTIONS,\n                write_concern=self._write_concern_for(session),\n                session=session)\n        return names", "response": "Create one or more indexes on this collection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __create_index(self, keys, index_options, session, **kwargs):\n        index_doc = helpers._index_document(keys)\n        index = {\"key\": index_doc}\n        collation = validate_collation_or_none(\n            index_options.pop('collation', None))\n        index.update(index_options)\n\n        with self._socket_for_writes(session) as sock_info:\n            if collation is not None:\n                if sock_info.max_wire_version < 5:\n                    raise ConfigurationError(\n                        'Must be connected to MongoDB 3.4+ to use collations.')\n                else:\n                    index['collation'] = collation\n            cmd = SON([('createIndexes', self.name), ('indexes', [index])])\n            cmd.update(kwargs)\n            self._command(\n                sock_info, cmd, read_preference=ReadPreference.PRIMARY,\n                codec_options=_UNICODE_REPLACE_CODEC_OPTIONS,\n                write_concern=self._write_concern_for(session),\n                session=session)", "response": "Internal create index helper."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate an index on this collection.", "response": "def create_index(self, keys, session=None, **kwargs):\n        \"\"\"Creates an index on this collection.\n\n        Takes either a single key or a list of (key, direction) pairs.\n        The key(s) must be an instance of :class:`basestring`\n        (:class:`str` in python 3), and the direction(s) must be one of\n        (:data:`~pymongo.ASCENDING`, :data:`~pymongo.DESCENDING`,\n        :data:`~pymongo.GEO2D`, :data:`~pymongo.GEOHAYSTACK`,\n        :data:`~pymongo.GEOSPHERE`, :data:`~pymongo.HASHED`,\n        :data:`~pymongo.TEXT`).\n\n        To create a single key ascending index on the key ``'mike'`` we just\n        use a string argument::\n\n          >>> my_collection.create_index(\"mike\")\n\n        For a compound index on ``'mike'`` descending and ``'eliot'``\n        ascending we need to use a list of tuples::\n\n          >>> my_collection.create_index([(\"mike\", pymongo.DESCENDING),\n          ...                             (\"eliot\", pymongo.ASCENDING)])\n\n        All optional index creation parameters should be passed as\n        keyword arguments to this method. For example::\n\n          >>> my_collection.create_index([(\"mike\", pymongo.DESCENDING)],\n          ...                            background=True)\n\n        Valid options include, but are not limited to:\n\n          - `name`: custom name to use for this index - if none is\n            given, a name will be generated.\n          - `unique`: if ``True`` creates a uniqueness constraint on the index.\n          - `background`: if ``True`` this index should be created in the\n            background.\n          - `sparse`: if ``True``, omit from the index any documents that lack\n            the indexed field.\n          - `bucketSize`: for use with geoHaystack indexes.\n            Number of documents to group together within a certain proximity\n            to a given longitude and latitude.\n          - `min`: minimum value for keys in a :data:`~pymongo.GEO2D`\n            index.\n          - `max`: maximum value for keys in a :data:`~pymongo.GEO2D`\n            index.\n          - `expireAfterSeconds`: <int> Used to create an expiring (TTL)\n            collection. MongoDB will automatically delete documents from\n            this collection after <int> seconds. The indexed field must\n            be a UTC datetime or the data will not expire.\n          - `partialFilterExpression`: A document that specifies a filter for\n            a partial index.\n          - `collation` (optional): An instance of\n            :class:`~pymongo.collation.Collation`. This option is only supported\n            on MongoDB 3.4 and above.\n\n        See the MongoDB documentation for a full list of supported options by\n        server version.\n\n        .. warning:: `dropDups` is not supported by MongoDB 3.0 or newer. The\n          option is silently ignored by the server and unique index builds\n          using the option will fail if a duplicate value is detected.\n\n        .. note:: `partialFilterExpression` requires server version **>= 3.2**\n\n        .. note:: The :attr:`~pymongo.collection.Collection.write_concern` of\n           this collection is automatically applied to this operation when using\n           MongoDB >= 3.4.\n\n        :Parameters:\n          - `keys`: a single key or a list of (key, direction)\n            pairs specifying the index to create\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n          - `**kwargs` (optional): any additional index creation\n            options (see the above list) should be passed as keyword\n            arguments\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter. Added support for passing maxTimeMS\n           in kwargs.\n        .. versionchanged:: 3.4\n           Apply this collection's write concern automatically to this operation\n           when connected to MongoDB >= 3.4. Support the `collation` option.\n        .. versionchanged:: 3.2\n            Added partialFilterExpression to support partial indexes.\n        .. versionchanged:: 3.0\n            Renamed `key_or_list` to `keys`. Removed the `cache_for` option.\n            :meth:`create_index` no longer caches index names. Removed support\n            for the drop_dups and bucket_size aliases.\n\n        .. mongodoc:: indexes\n        \"\"\"\n        keys = helpers._index_list(keys)\n        name = kwargs.setdefault(\"name\", helpers._gen_index_name(keys))\n        cmd_options = {}\n        if \"maxTimeMS\" in kwargs:\n            cmd_options[\"maxTimeMS\"] = kwargs.pop(\"maxTimeMS\")\n        self.__create_index(keys, kwargs, session, **cmd_options)\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrebuild all indexes on this collection.", "response": "def reindex(self, session=None, **kwargs):\n        \"\"\"Rebuilds all indexes on this collection.\n\n        :Parameters:\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n          - `**kwargs` (optional): optional arguments to the reIndex\n            command (like maxTimeMS) can be passed as keyword arguments.\n\n        .. warning:: reindex blocks all other operations (indexes\n           are built in the foreground) and will be slow for large\n           collections.\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter. Added support for arbitrary keyword\n           arguments.\n\n        .. versionchanged:: 3.4\n           Apply this collection's write concern automatically to this operation\n           when connected to MongoDB >= 3.4.\n\n        .. versionchanged:: 3.5\n           We no longer apply this collection's write concern to this operation.\n           MongoDB 3.4 silently ignored the write concern. MongoDB 3.6+ returns\n           an error if we include the write concern.\n        \"\"\"\n        cmd = SON([(\"reIndex\", self.__name)])\n        cmd.update(kwargs)\n        with self._socket_for_writes(session) as sock_info:\n            return self._command(\n                sock_info, cmd, read_preference=ReadPreference.PRIMARY,\n                session=session)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_indexes(self, session=None):\n        codec_options = CodecOptions(SON)\n        coll = self.with_options(codec_options=codec_options,\n                                 read_preference=ReadPreference.PRIMARY)\n        read_pref = ((session and session._txn_read_preference())\n                     or ReadPreference.PRIMARY)\n\n        def _cmd(session, server, sock_info, slave_ok):\n            cmd = SON([(\"listIndexes\", self.__name), (\"cursor\", {})])\n            if sock_info.max_wire_version > 2:\n                with self.__database.client._tmp_session(session, False) as s:\n                    try:\n                        cursor = self._command(sock_info, cmd, slave_ok,\n                                               read_pref,\n                                               codec_options,\n                                               session=s)[\"cursor\"]\n                    except OperationFailure as exc:\n                        # Ignore NamespaceNotFound errors to match the behavior\n                        # of reading from *.system.indexes.\n                        if exc.code != 26:\n                            raise\n                        cursor = {'id': 0, 'firstBatch': []}\n                return CommandCursor(coll, cursor, sock_info.address,\n                                     session=s,\n                                     explicit_session=session is not None)\n            else:\n                res = message._first_batch(\n                    sock_info, self.__database.name, \"system.indexes\",\n                    {\"ns\": self.__full_name}, 0, slave_ok, codec_options,\n                    read_pref, cmd,\n                    self.database.client._event_listeners)\n                cursor = res[\"cursor\"]\n                # Note that a collection can only have 64 indexes, so there\n                # will never be a getMore call.\n                return CommandCursor(coll, cursor, sock_info.address)\n\n        return self.__database.client._retryable_read(\n            _cmd, read_pref, session)", "response": "Get a cursor over the index documents for this collection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef index_information(self, session=None):\n        cursor = self.list_indexes(session=session)\n        info = {}\n        for index in cursor:\n            index[\"key\"] = index[\"key\"].items()\n            index = dict(index)\n            info[index.pop(\"name\")] = index\n        return info", "response": "Get information on this collection s indexes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef options(self, session=None):\n        dbo = self.__database.client.get_database(\n            self.__database.name,\n            self.codec_options,\n            self.read_preference,\n            self.write_concern,\n            self.read_concern)\n        cursor = dbo.list_collections(\n            session=session, filter={\"name\": self.__name})\n\n        result = None\n        for doc in cursor:\n            result = doc\n            break\n\n        if not result:\n            return {}\n\n        options = result.get(\"options\", {})\n        if \"create\" in options:\n            del options[\"create\"]\n\n        return options", "response": "Get the options set on this collection."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms an aggregate operation on the specified collection.", "response": "def aggregate(self, pipeline, session=None, **kwargs):\n        \"\"\"Perform an aggregation using the aggregation framework on this\n        collection.\n\n        All optional `aggregate command`_ parameters should be passed as\n        keyword arguments to this method. Valid options include, but are not\n        limited to:\n\n          - `allowDiskUse` (bool): Enables writing to temporary files. When set\n            to True, aggregation stages can write data to the _tmp subdirectory\n            of the --dbpath directory. The default is False.\n          - `maxTimeMS` (int): The maximum amount of time to allow the operation\n            to run in milliseconds.\n          - `batchSize` (int): The maximum number of documents to return per\n            batch. Ignored if the connected mongod or mongos does not support\n            returning aggregate results using a cursor, or `useCursor` is\n            ``False``.\n          - `collation` (optional): An instance of\n            :class:`~pymongo.collation.Collation`. This option is only supported\n            on MongoDB 3.4 and above.\n          - `useCursor` (bool): Deprecated. Will be removed in PyMongo 4.0.\n\n        The :meth:`aggregate` method obeys the :attr:`read_preference` of this\n        :class:`Collection`. Please note that using the ``$out`` pipeline stage\n        requires a read preference of\n        :attr:`~pymongo.read_preferences.ReadPreference.PRIMARY` (the default).\n        The server will raise an error if the ``$out`` pipeline stage is used\n        with any other read preference.\n\n        .. note:: This method does not support the 'explain' option. Please\n           use :meth:`~pymongo.database.Database.command` instead. An\n           example is included in the :ref:`aggregate-examples` documentation.\n\n        .. note:: The :attr:`~pymongo.collection.Collection.write_concern` of\n           this collection is automatically applied to this operation when using\n           MongoDB >= 3.4.\n\n        :Parameters:\n          - `pipeline`: a list of aggregation pipeline stages\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n          - `**kwargs` (optional): See list of options above.\n\n        :Returns:\n          A :class:`~pymongo.command_cursor.CommandCursor` over the result\n          set.\n\n        .. versionchanged:: 3.9\n           Apply this collection's read concern to pipelines containing the\n           `$out` stage when connected to MongoDB >= 4.2.\n        .. versionchanged:: 3.6\n           Added the `session` parameter. Added the `maxAwaitTimeMS` option.\n           Deprecated the `useCursor` option.\n        .. versionchanged:: 3.4\n           Apply this collection's write concern automatically to this operation\n           when connected to MongoDB >= 3.4. Support the `collation` option.\n        .. versionchanged:: 3.0\n           The :meth:`aggregate` method always returns a CommandCursor. The\n           pipeline argument must be a list.\n        .. versionchanged:: 2.7\n           When the cursor option is used, return\n           :class:`~pymongo.command_cursor.CommandCursor` instead of\n           :class:`~pymongo.cursor.Cursor`.\n        .. versionchanged:: 2.6\n           Added cursor support.\n        .. versionadded:: 2.3\n\n        .. seealso:: :doc:`/examples/aggregation`\n\n        .. _aggregate command:\n            https://docs.mongodb.com/manual/reference/command/aggregate\n        \"\"\"\n        with self.__database.client._tmp_session(session, close=False) as s:\n            return self._aggregate(pipeline,\n                                   CommandCursor,\n                                   kwargs.get('batchSize'),\n                                   session=s,\n                                   explicit_session=session is not None,\n                                   **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef aggregate_raw_batches(self, pipeline, **kwargs):\n        # OP_MSG with document stream returns is required to support\n        # sessions.\n        if \"session\" in kwargs:\n            raise ConfigurationError(\n                \"aggregate_raw_batches does not support sessions\")\n        return self._aggregate(pipeline, RawBatchCommandCursor, 0,\n                               None, False, **kwargs)", "response": "Perform an aggregation and retrieve batches of raw BSON."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef group(self, key, condition, initial, reduce, finalize=None, **kwargs):\n        warnings.warn(\"The group method is deprecated and will be removed in \"\n                      \"PyMongo 4.0. Use the aggregate method with the $group \"\n                      \"stage or the map_reduce method instead.\",\n                      DeprecationWarning, stacklevel=2)\n        group = {}\n        if isinstance(key, string_type):\n            group[\"$keyf\"] = Code(key)\n        elif key is not None:\n            group = {\"key\": helpers._fields_list_to_dict(key, \"key\")}\n        group[\"ns\"] = self.__name\n        group[\"$reduce\"] = Code(reduce)\n        group[\"cond\"] = condition\n        group[\"initial\"] = initial\n        if finalize is not None:\n            group[\"finalize\"] = Code(finalize)\n\n        cmd = SON([(\"group\", group)])\n        collation = validate_collation_or_none(kwargs.pop('collation', None))\n        cmd.update(kwargs)\n\n        with self._socket_for_reads(session=None) as (sock_info, slave_ok):\n            return self._command(sock_info, cmd, slave_ok,\n                                 collation=collation,\n                                 user_fields={'retval': 1})[\"retval\"]", "response": "Perform a group by operation on a set of keys."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rename(self, new_name, session=None, **kwargs):\n        if not isinstance(new_name, string_type):\n            raise TypeError(\"new_name must be an \"\n                            \"instance of %s\" % (string_type.__name__,))\n\n        if not new_name or \"..\" in new_name:\n            raise InvalidName(\"collection names cannot be empty\")\n        if new_name[0] == \".\" or new_name[-1] == \".\":\n            raise InvalidName(\"collecion names must not start or end with '.'\")\n        if \"$\" in new_name and not new_name.startswith(\"oplog.$main\"):\n            raise InvalidName(\"collection names must not contain '$'\")\n\n        new_name = \"%s.%s\" % (self.__database.name, new_name)\n        cmd = SON([(\"renameCollection\", self.__full_name), (\"to\", new_name)])\n        cmd.update(kwargs)\n        write_concern = self._write_concern_for_cmd(cmd, session)\n\n        with self._socket_for_writes(session) as sock_info:\n            with self.__database.client._tmp_session(session) as s:\n                return sock_info.command(\n                    'admin', cmd,\n                    write_concern=write_concern,\n                    parse_write_concern_error=True,\n                    session=s, client=self.__database.client)", "response": "Rename this collection.\n\n        If operating in auth mode, client must be authorized as an\n        admin to perform this operation. Raises :class:`TypeError` if\n        `new_name` is not an instance of :class:`basestring`\n        (:class:`str` in python 3). Raises :class:`~pymongo.errors.InvalidName`\n        if `new_name` is not a valid collection name.\n\n        :Parameters:\n          - `new_name`: new name for this collection\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n          - `**kwargs` (optional): additional arguments to the rename command\n            may be passed as keyword arguments to this helper method\n            (i.e. ``dropTarget=True``)\n\n        .. note:: The :attr:`~pymongo.collection.Collection.write_concern` of\n           this collection is automatically applied to this operation when using\n           MongoDB >= 3.4.\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n\n        .. versionchanged:: 3.4\n           Apply this collection's write concern automatically to this operation\n           when connected to MongoDB >= 3.4."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef distinct(self, key, filter=None, session=None, **kwargs):\n        if not isinstance(key, string_type):\n            raise TypeError(\"key must be an \"\n                            \"instance of %s\" % (string_type.__name__,))\n        cmd = SON([(\"distinct\", self.__name),\n                   (\"key\", key)])\n        if filter is not None:\n            if \"query\" in kwargs:\n                raise ConfigurationError(\"can't pass both filter and query\")\n            kwargs[\"query\"] = filter\n        collation = validate_collation_or_none(kwargs.pop('collation', None))\n        cmd.update(kwargs)\n        def _cmd(session, server, sock_info, slave_ok):\n            return self._command(\n                sock_info, cmd, slave_ok, read_concern=self.read_concern,\n                collation=collation, session=session,\n                user_fields={\"values\": 1})[\"values\"]\n\n        return self.__database.client._retryable_read(\n            _cmd, self._read_preference_for(session), session)", "response": "Get a list of distinct values for key among all documents in this collection."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming a mapReduce operation on this collection.", "response": "def map_reduce(self, map, reduce, out, full_response=False, session=None,\n                   **kwargs):\n        \"\"\"Perform a map/reduce operation on this collection.\n\n        If `full_response` is ``False`` (default) returns a\n        :class:`~pymongo.collection.Collection` instance containing\n        the results of the operation. Otherwise, returns the full\n        response from the server to the `map reduce command`_.\n\n        :Parameters:\n          - `map`: map function (as a JavaScript string)\n          - `reduce`: reduce function (as a JavaScript string)\n          - `out`: output collection name or `out object` (dict). See\n            the `map reduce command`_ documentation for available options.\n            Note: `out` options are order sensitive. :class:`~bson.son.SON`\n            can be used to specify multiple options.\n            e.g. SON([('replace', <collection name>), ('db', <database name>)])\n          - `full_response` (optional): if ``True``, return full response to\n            this command - otherwise just return the result collection\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n          - `**kwargs` (optional): additional arguments to the\n            `map reduce command`_ may be passed as keyword arguments to this\n            helper method, e.g.::\n\n            >>> db.test.map_reduce(map, reduce, \"myresults\", limit=2)\n\n        .. note:: The :meth:`map_reduce` method does **not** obey the\n           :attr:`read_preference` of this :class:`Collection`. To run\n           mapReduce on a secondary use the :meth:`inline_map_reduce` method\n           instead.\n\n        .. note:: The :attr:`~pymongo.collection.Collection.write_concern` of\n           this collection is automatically applied to this operation (if the\n           output is not inline) when using MongoDB >= 3.4.\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n\n        .. versionchanged:: 3.4\n           Apply this collection's write concern automatically to this operation\n           when connected to MongoDB >= 3.4.\n\n        .. seealso:: :doc:`/examples/aggregation`\n\n        .. versionchanged:: 3.4\n           Added the `collation` option.\n        .. versionchanged:: 2.2\n           Removed deprecated arguments: merge_output and reduce_output\n\n        .. _map reduce command: http://docs.mongodb.org/manual/reference/command/mapReduce/\n\n        .. mongodoc:: mapreduce\n\n        \"\"\"\n        if not isinstance(out, (string_type, abc.Mapping)):\n            raise TypeError(\"'out' must be an instance of \"\n                            \"%s or a mapping\" % (string_type.__name__,))\n\n        response = self._map_reduce(map, reduce, out, session,\n                                    ReadPreference.PRIMARY, **kwargs)\n\n        if full_response or not response.get('result'):\n            return response\n        elif isinstance(response['result'], dict):\n            dbase = response['result']['db']\n            coll = response['result']['collection']\n            return self.__database.client[dbase][coll]\n        else:\n            return self.__database[response[\"result\"]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef inline_map_reduce(self, map, reduce, full_response=False, session=None,\n                          **kwargs):\n        \"\"\"Perform an inline map/reduce operation on this collection.\n\n        Perform the map/reduce operation on the server in RAM. A result\n        collection is not created. The result set is returned as a list\n        of documents.\n\n        If `full_response` is ``False`` (default) returns the\n        result documents in a list. Otherwise, returns the full\n        response from the server to the `map reduce command`_.\n\n        The :meth:`inline_map_reduce` method obeys the :attr:`read_preference`\n        of this :class:`Collection`.\n\n        :Parameters:\n          - `map`: map function (as a JavaScript string)\n          - `reduce`: reduce function (as a JavaScript string)\n          - `full_response` (optional): if ``True``, return full response to\n            this command - otherwise just return the result collection\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n          - `**kwargs` (optional): additional arguments to the\n            `map reduce command`_ may be passed as keyword arguments to this\n            helper method, e.g.::\n\n            >>> db.test.inline_map_reduce(map, reduce, limit=2)\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n\n        .. versionchanged:: 3.4\n           Added the `collation` option.\n\n        \"\"\"\n        res = self._map_reduce(map, reduce, {\"inline\": 1}, session,\n                               self.read_preference, **kwargs)\n\n        if full_response:\n            return res\n        else:\n            return res.get(\"results\")", "response": "This method performs an inline map reduce operation on this collection."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind a single document and deletes it.", "response": "def find_one_and_delete(self, filter,\n                            projection=None, sort=None, session=None, **kwargs):\n        \"\"\"Finds a single document and deletes it, returning the document.\n\n          >>> db.test.count_documents({'x': 1})\n          2\n          >>> db.test.find_one_and_delete({'x': 1})\n          {u'x': 1, u'_id': ObjectId('54f4e12bfba5220aa4d6dee8')}\n          >>> db.test.count_documents({'x': 1})\n          1\n\n        If multiple documents match *filter*, a *sort* can be applied.\n\n          >>> for doc in db.test.find({'x': 1}):\n          ...     print(doc)\n          ...\n          {u'x': 1, u'_id': 0}\n          {u'x': 1, u'_id': 1}\n          {u'x': 1, u'_id': 2}\n          >>> db.test.find_one_and_delete(\n          ...     {'x': 1}, sort=[('_id', pymongo.DESCENDING)])\n          {u'x': 1, u'_id': 2}\n\n        The *projection* option can be used to limit the fields returned.\n\n          >>> db.test.find_one_and_delete({'x': 1}, projection={'_id': False})\n          {u'x': 1}\n\n        :Parameters:\n          - `filter`: A query that matches the document to delete.\n          - `projection` (optional): a list of field names that should be\n            returned in the result document or a mapping specifying the fields\n            to include or exclude. If `projection` is a list \"_id\" will\n            always be returned. Use a mapping to exclude fields from\n            the result (e.g. projection={'_id': False}).\n          - `sort` (optional): a list of (key, direction) pairs\n            specifying the sort order for the query. If multiple documents\n            match the query, they are sorted and the first is deleted.\n          - `session` (optional): a\n            :class:`~pymongo.client_session.ClientSession`.\n          - `**kwargs` (optional): additional command arguments can be passed\n            as keyword arguments (for example maxTimeMS can be used with\n            recent server versions).\n\n        .. versionchanged:: 3.6\n           Added ``session`` parameter.\n\n        .. versionchanged:: 3.2\n           Respects write concern.\n\n        .. warning:: Starting in PyMongo 3.2, this command uses the\n           :class:`~pymongo.write_concern.WriteConcern` of this\n           :class:`~pymongo.collection.Collection` when connected to MongoDB >=\n           3.2. Note that using an elevated write concern with this command may\n           be slower compared to using the default write concern.\n\n        .. versionchanged:: 3.4\n           Added the `collation` option.\n        .. versionadded:: 3.0\n\n        \"\"\"\n        kwargs['remove'] = True\n        return self.__find_and_modify(filter, projection, sort,\n                                      session=session, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove(self, spec_or_id=None, multi=True, **kwargs):\n        warnings.warn(\"remove is deprecated. Use delete_one or delete_many \"\n                      \"instead.\", DeprecationWarning, stacklevel=2)\n        if spec_or_id is None:\n            spec_or_id = {}\n        if not isinstance(spec_or_id, abc.Mapping):\n            spec_or_id = {\"_id\": spec_or_id}\n        write_concern = None\n        collation = validate_collation_or_none(kwargs.pop('collation', None))\n        if kwargs:\n            write_concern = WriteConcern(**kwargs)\n        return self._delete_retryable(\n            spec_or_id, multi, write_concern, collation=collation)", "response": "Remove a document from this collection."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_and_modify(self, query={}, update=None,\n                        upsert=False, sort=None, full_response=False,\n                        manipulate=False, **kwargs):\n        \"\"\"Update and return an object.\n\n        **DEPRECATED** - Use :meth:`find_one_and_delete`,\n        :meth:`find_one_and_replace`, or :meth:`find_one_and_update` instead.\n        \"\"\"\n        warnings.warn(\"find_and_modify is deprecated, use find_one_and_delete\"\n                      \", find_one_and_replace, or find_one_and_update instead\",\n                      DeprecationWarning, stacklevel=2)\n\n        if not update and not kwargs.get('remove', None):\n            raise ValueError(\"Must either update or remove\")\n\n        if update and kwargs.get('remove', None):\n            raise ValueError(\"Can't do both update and remove\")\n\n        # No need to include empty args\n        if query:\n            kwargs['query'] = query\n        if update:\n            kwargs['update'] = update\n        if upsert:\n            kwargs['upsert'] = upsert\n        if sort:\n            # Accept a list of tuples to match Cursor's sort parameter.\n            if isinstance(sort, list):\n                kwargs['sort'] = helpers._index_document(sort)\n            # Accept OrderedDict, SON, and dict with len == 1 so we\n            # don't break existing code already using find_and_modify.\n            elif (isinstance(sort, ORDERED_TYPES) or\n                  isinstance(sort, dict) and len(sort) == 1):\n                warnings.warn(\"Passing mapping types for `sort` is deprecated,\"\n                              \" use a list of (key, direction) pairs instead\",\n                              DeprecationWarning, stacklevel=2)\n                kwargs['sort'] = sort\n            else:\n                raise TypeError(\"sort must be a list of (key, direction) \"\n                                \"pairs, a dict of len 1, or an instance of \"\n                                \"SON or OrderedDict\")\n\n        fields = kwargs.pop(\"fields\", None)\n        if fields is not None:\n            kwargs[\"fields\"] = helpers._fields_list_to_dict(fields, \"fields\")\n\n        collation = validate_collation_or_none(kwargs.pop('collation', None))\n\n        cmd = SON([(\"findAndModify\", self.__name)])\n        cmd.update(kwargs)\n\n        write_concern = self._write_concern_for_cmd(cmd, None)\n\n        def _find_and_modify(session, sock_info, retryable_write):\n            if (sock_info.max_wire_version >= 4 and\n                    not write_concern.is_server_default):\n                cmd['writeConcern'] = write_concern.document\n            result = self._command(\n                sock_info, cmd, read_preference=ReadPreference.PRIMARY,\n                allowable_errors=[_NO_OBJ_ERROR], collation=collation,\n                session=session, retryable_write=retryable_write,\n                user_fields=_FIND_AND_MODIFY_DOC_FIELDS)\n\n            _check_write_command_response(result)\n            return result\n\n        out = self.__database.client._retryable_write(\n            write_concern.acknowledged, _find_and_modify, None)\n\n        if not out['ok']:\n            if out[\"errmsg\"] == _NO_OBJ_ERROR:\n                return None\n            else:\n                # Should never get here b/c of allowable_errors\n                raise ValueError(\"Unexpected Error: %s\" % (out,))\n\n        if full_response:\n            return out\n        else:\n            document = out.get('value')\n            if manipulate:\n                document = self.__database._fix_outgoing(document, self)\n            return document", "response": "Find and modify a set of items in the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the full aggregation pipeline for this ChangeStream.", "response": "def _full_pipeline(self):\n        \"\"\"Return the full aggregation pipeline for this ChangeStream.\"\"\"\n        options = self._pipeline_options()\n        full_pipeline = [{'$changeStream': options}]\n        full_pipeline.extend(self._pipeline)\n        return full_pipeline"}
